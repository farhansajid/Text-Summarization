{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hybrid WF-T5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPVA9F7zEhLlU0iF4x3PLRT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yuiRkb3_sJCy"},"source":["**Hybrid WF-T5 Algorithm**"]},{"cell_type":"code","metadata":{"id":"7oLz3f4PFPzo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621249892057,"user_tz":-300,"elapsed":8147,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"d996ca34-811a-457f-8cbf-1f37ca5161ae"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 20.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 20.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 11.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 12.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 11.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 11.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 11.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 11.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 11.1MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WD_vnyLXZQzD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623387134397,"user_tz":-300,"elapsed":16817,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"a34993d4-27f3-4f77-ea99-02bca722d619"},"source":["!pip install transformers -q\n","!pip install wandb -q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.3MB 6.8MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 24.8MB/s \n","\u001b[K     |████████████████████████████████| 901kB 43.4MB/s \n","\u001b[K     |████████████████████████████████| 1.8MB 5.0MB/s \n","\u001b[K     |████████████████████████████████| 133kB 20.8MB/s \n","\u001b[K     |████████████████████████████████| 174kB 23.4MB/s \n","\u001b[K     |████████████████████████████████| 102kB 8.9MB/s \n","\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pzM1_ykHaFur"},"source":["# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsxnBrMgsM_o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623350801884,"user_tz":-300,"elapsed":477,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"d80d8b2d-bbf2-4cb4-8c20-6ed9f049e3f7"},"source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('stopwords')\n","import nltk\n","nltk.download('punkt')\n","\n","def _create_frequency_table(text_string) -> dict:\n","    \"\"\"\n","    \"\"\"\n","    stopWords = set(stopwords.words(\"english\"))\n","    words = word_tokenize(text_string)\n","    ps = PorterStemmer()\n","\n","    freqTable = dict()\n","    for word in words:\n","        word = ps.stem(word)\n","        if word in stopWords:\n","            continue\n","        if word in freqTable:\n","            freqTable[word] += 1\n","        else:\n","            freqTable[word] = 1\n","\n","    return freqTable\n","\n","\n","def _score_sentences(sentences, freqTable) -> dict:\n","    \"\"\"\n","    \"\"\"\n","\n","    sentenceValue = dict()\n","\n","    for sentence in sentences:\n","        word_count_in_sentence = (len(word_tokenize(sentence)))\n","        word_count_in_sentence_except_stop_words = 0\n","        for wordValue in freqTable:\n","            if wordValue in sentence.lower():\n","                word_count_in_sentence_except_stop_words += 1\n","                if sentence[:15] in sentenceValue:\n","                    sentenceValue[sentence[:15]] += freqTable[wordValue]\n","                else:\n","                    sentenceValue[sentence[:15]] = freqTable[wordValue]\n","\n","        if sentence[:15] in sentenceValue:\n","            sentenceValue[sentence[:15]] = sentenceValue[sentence[:15]] / word_count_in_sentence_except_stop_words\n","\n","        '''\n","        '''\n","\n","    return sentenceValue\n","\n","\n","def _find_average_score(sentenceValue) -> int:\n","    \"\"\"\n","    Find the average score from the sentence value dictionary\n","    \"\"\"\n","    sumValues = 0\n","    for entry in sentenceValue:\n","        sumValues += sentenceValue[entry]\n","\n","    # Average value of a sentence from original text\n","    average = (sumValues / len(sentenceValue))\n","\n","    return average\n","\n","\n","def _generate_summary(sentences, sentenceValue, threshold):\n","    sentence_count = 0\n","    summary = ''\n","\n","    for sentence in sentences:\n","        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n","            summary += \" \" + sentence\n","            sentence_count += 1\n","\n","    return summary\n","\n","\n","def run_summarization(text):\n","    # 1 Create the word frequency table\n","    freq_table = _create_frequency_table(text)\n","\n","    '''\n","    We already have a sentence tokenizer, so we just need \n","    to run the sent_tokenize() method to create the array of sentences.\n","    '''\n","\n","    # 2 Tokenize the sentences\n","    sentences = sent_tokenize(text)\n","\n","    # 3 Important Algorithm: score the sentences\n","    sentence_scores = _score_sentences(sentences, freq_table)\n","\n","    # 4 Find the threshold\n","    threshold = _find_average_score(sentence_scores)\n","\n","    # 5 Important Algorithm: Generate the summary\n","    summary = _generate_summary(sentences, sentence_scores, 0.7 * threshold) \n","\n","    return summary\n","\n","text_str='''\n","We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or “concept”-level, to a sentence-level model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.\n","'''\n","string = run_summarization(text_str)\n","print(string)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"," \n","Identification of different risk factors and early prediction of mortality for patients with heart failure are crucial for guiding clinical decision-making in Intensive care unit cohorts. In this paper, we developed a comprehensive risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF). Utilizing a novel split rule and stopping criterion, the proposed iRSF was able to identify more accurate predictors to separate survivors and nonsurvivors and thus improve discrimination ability. Based on the public MIMIC II clinical database with 8 059 patients, 32 risk factors, including demographics, clinical, laboratory information, and medications, were analyzed and used to develop the risk model for patients with heart failure. Compared with previous studies, more critical laboratory predictors were identified that could reveal difficult-to-manage comorbidities, including aspartate aminotransferase, alanine aminotransferase, total bilirubin, serum creatine, blood urea nitrogen, and their inherent effects on events; these were determined to be critical indicators for predicting heart failure mortality with the proposed iRSF. The experimental results showed that the developed risk model was superior to those used in previous studies and the conventional random survival forest-based model with an out-of-bag C-statistic value of 0.821. Therefore, the developed iRSF-based risk model could serve as a valuable tool for clinicians in heart failure mortality prediction.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mxg4VP83wTp_","executionInfo":{"status":"ok","timestamp":1623387061561,"user_tz":-300,"elapsed":23011,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"36f28dd1-f6c4-43c8-dfa0-e18130329256"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CjjbrIf9Wz4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623387220536,"user_tz":-300,"elapsed":25045,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"62b6cad2-c42c-4d33-cfe9-3faedde92ec9"},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","import torch\n","import re\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Text_Summarization/models/model_files\")\n","model = AutoModelWithLMHead.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Text_Summarization/models/model_files\")\n","\n","## Move to CUDA\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","preprocess_text = string.strip().replace(\"\\n\",\"\")\n","tokenized_text = tokenizer.encode(string, return_tensors=\"pt\").to(device)\n","\n","summary_ids = model.generate(\n","            tokenized_text,\n","            min_length=50,\n","            max_length=150, \n","            num_beams=4,\n","            no_repeat_ngram_size=2,\n","            #repetition_penalty=2.5, \n","            #length_penalty=1.0, \n","            early_stopping=True\n","        )\n","\n","output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","print (\"\\n\\nSummarized text: \\n\",output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:810: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","Summarized text: \n"," The Fifth generation (5G) network is projected to support large amount of data traffic and massive number of wireless connections.5G mobile network aims to address the limitations of previous cellular standards and be a prospective key enabler for future Internet of Things (IoT).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eXY3xYOhF1UU"},"source":["text1='''\n","The Fifth generation (5G) network is projected to support large amount of data traffic and massive number of wireless connections. Different data traffic has different Quality of Service (QoS) requirements. 5G mobile network aims to address the limitations of previous cellular standards (i.e., 2G/3G/4G) and be a prospective key enabler for future Internet of Things (IoT). 5G networks support a wide range of applications such as smart home, autonomous driving, drone operations, health and mission critical applications, Industrial IoT (IIoT), and entertainment and multimedia. Based on end users’ experience, several 5G services are categorized into immersive 5G services, intelligent 5G services, omnipresent 5G services, autonomous 5G services, and public 5G services. In this paper, we present a brief overview of 5G technical scenarios. We then provide a brief overview of accepted papers in our Special Issue on 5G mobile services and scenarios. Finally, we conclude this paper.\n","'''\n","ref1='''\n","5G network is projected to support large amount of data traffic and massive number of wireless connections. 5G networks support a wide range of applications such as smart home, autonomous driving, drone operations, health and mission critical applications.\n","'''\n","\n","text2='''\n","The modern technology demands the maintenance of the increasing data which are in structured and unstructured form. The text documents collected in the various platforms occupies a massive space in the architectural structure of the computer system both physically and virtually. Apparently the users demand the summarizing of the collected documents for easy access and usage. To enable this automatic text summarization came into phase. The automatic text summarization condenses the text documents into meaningful phrases and textual messages which helps the user to understand the conceptual ides behind each core values. The importance of automatic text summarization stands as a helping source in the growing data. This paper discusses the basic blocks of the automatic text summarization and its feature in identifying the intricate properties of the meaningful text through various approaches.\n","'''\n","ref2='''\n","Automatic text summarization condenses the text documents into meaningful phrases and textual messages. This paper discusses the basic blocks of the automatic text summarizing and its feature in identifying the intricate properties.\n","'''\n","\n","text3='''\n","We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or “concept”-level, to a sentence-level model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.\n","'''\n","ref3='''\n","We present a model for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or \"concept\"-level, to a sentence-level model. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables.\n","'''\n","\n","text4='''\n","In this paper, we propose a universal solution to web search and web browsing on handheld devices for visually impaired people. For this purpose, we propose (1) to automatically cluster web page results and (2) to summarize all the information in web pages so that speech-to-speech interaction is used efficiently to access information.\n","'''\n","ref4='''\n","In this paper, we propose a universal solution to web search and web browsing on handheld devices for visually impaired people.\n","'''\n","\n","text5='''\n","Automatic text summarization (ATS) is the process of generating a summary by condensing text document by a computer machine. In this paper, we explored voting-based extractive approaches for text summarization. The main issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize the quality of summary. Voting-based methods are sensitive to initial ranking process. We proposed reciprocal ranking-based sentence scoring approach that alleviates the feature weighting and initial ranking problem. The proposed approach uses a specific prominent set of features for initial ranking that further enhance the performance. Experimental results on Document Understating Conference 2002 data-set using ROUGE evaluation matrices shows that our proposed method performs better as compared to other voting-based methods.\n","'''\n","ref5='''\n","Automatic text summarization is the process of generating a summary by condensing text document by a computer machine. Main issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize the quality of summary.\n","'''\n","\n","text6='''\n","Vehicular Ad-Hoc Networks (VANET) are considered as a subset of Mobile Ad-Hoc Networks (MANET). VANET is mainly used for the construction of an intelligent transport system. VANET enables communication between the vehicles (V2V) and vehicles to infrastructure (V2I). VANET can be used to coordinate the traffic, improve safety measures, support the drivers for hassle-free driving. It plays a major role in building smart cities in the near future. VANET is vulnerable to a number of security issues among which the DoS attack is a major part. DoS attack in VANET involves a malicious node flooding a huge amount of traffic using spoofed identities. This, in turn, may disrupt the services of vehicles in the network. The detection of the attack becomes very difficult due to fake identities. The detection scheme uses a cuckoo filter and IP detection technique to detect the attack in the network. Once the attack is detected it generates a broadcast message to all the other vehicles that are present in the network.\n","'''\n","ref6='''\n","VANET is a subset of Mobile Ad-Hoc Networks (MANET) It enables communication between the vehicles (V2V) and vehicles to infrastructure. VANET can be used to coordinate the traffic, improve safety measures, support the drivers for hassle-free driving.\n","'''\n","\n","text7='''\n","Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures: (a) a logistic discriminant approach which learns the metric from a set of labelled image pairs (LDML) and (b) a nearest neighbour approach which computes the probability for two images to belong to the same class (MkNN). We evaluate our approaches on the Labeled Faces in the Wild data set, a large and very challenging data set of faces from Yahoo! News. The evaluation protocol for this data set defines a restricted setting, where a fixed set of positive and negative image pairs is given, as well as an unrestricted one, where faces are labelled by their identity. We are the first to present results for the unrestricted setting, and show that our methods benefit from this richer training data, much more so than the current state-of-the-art method. Our results of 79.3% and 87.5% correct for the restricted and unrestricted setting respectively, significantly improve over the current state-of-the-art result of 78.5%. Confidence scores obtained for face identification can be used for many applications e.g. clustering or recognition from a single training example. We show that our learned metrics also improve performance for these tasks.\n","'''\n","ref7='''\n","Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures. We show our methods benefit from richer training data, much more so than the current state-of-the-art method.\n","'''\n","\n","text8='''\n","Identification of different risk factors and early prediction of mortality for patients with heart failure are crucial for guiding clinical decision-making in Intensive care unit cohorts. In this paper, we developed a comprehensive risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF). Utilizing a novel split rule and stopping criterion, the proposed iRSF was able to identify more accurate predictors to separate survivors and nonsurvivors and thus improve discrimination ability. Based on the public MIMIC II clinical database with 8 059 patients, 32 risk factors, including demographics, clinical, laboratory information, and medications, were analyzed and used to develop the risk model for patients with heart failure. Compared with previous studies, more critical laboratory predictors were identified that could reveal difficult-to-manage comorbidities, including aspartate aminotransferase, alanine aminotransferase, total bilirubin, serum creatine, blood urea nitrogen, and their inherent effects on events; these were determined to be critical indicators for predicting heart failure mortality with the proposed iRSF. The experimental results showed that the developed risk model was superior to those used in previous studies and the conventional random survival forest-based model with an out-of-bag C-statistic value of 0.821. Therefore, the developed iRSF-based risk model could serve as a valuable tool for clinicians in heart failure mortality prediction.\n","'''\n","ref8='''\n","Early prediction of mortality for patients with heart failure is crucial for guiding clinical decision-making. In this paper, we developed a risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF) The developed iRSF-based risk model could serve as a valuable tool for clinicians.\n","'''\n","\n","text9='''\n","Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes (FDNB) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set.\n","'''\n","ref9='''\n","Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. In this study, by following preprocessing steps, a Feature Dependent Naives (FDNB) classification method is proposed.\n","'''\n","\n","text10='''\n","Extracting relevant feature and classification are significant in brain-computer interface (BCI) systems. Deep learning have achieved remarkable growth in many fields like speech recognition and computer vision. However, deep learning in biomedical field is yet to be fully utilized. In this paper, We propose a novel methodology for convolutional neural network (CNN) based motor imagery (MI) classification using new form of input. Continuous Wavelet Transform (CWT) is applied to the input Electroencephalography (EEG) signal to extract the features of MI. After transformation, we consider the real part and imaginary part of the transformed signal to exploit magnitude and phase information at the same time. This feature is fed to the CNN having one convolution layer, one max-pooling layer and one fully connected layer. The classification accuracy is tested on two public BCI datasets: BCI competition IV dataset IIb and BCI competition II dataset III. The proposed method shows increase in classification accuracy compared to other MI classification methods. The results show that the method using CNN with magnitude and phase based features can be better than other state-of-the-art approaches.\n","'''\n","ref10='''\n","Deep learning in biomedical field is yet to be fully utilized. Method using CNN with magnitude and phase based features can be better than other state-of-the-art approaches. The proposed method shows increase in classification accuracy compared to other MI classification methods.\n","'''\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"53AXkN8qVlQk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623387256343,"user_tz":-300,"elapsed":885,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"2c7793b2-fa71-46fc-b658-2217566d742e"},"source":["from nltk.translate.bleu_score import sentence_bleu\n","\n","candidate = output.split()\n","print('BLEU score -> {}'.format(sentence_bleu(ref1, output )))\n","\n","\n","print('Individual 1-gram: %f' % sentence_bleu(ref1, output, weights=(1, 0, 0, 0)))\n","print('Individual 2-gram: %f' % sentence_bleu(ref1, output, weights=(0, 1, 0, 0)))\n","print('Individual 3-gram: %f' % sentence_bleu(ref1, output, weights=(0, 0, 1, 0)))\n","print('Individual 4-gram: %f' % sentence_bleu(ref1, output, weights=(0, 0, 0, 1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU score -> 0.5410822690539396\n","Individual 1-gram: 0.092857\n","Individual 2-gram: 1.000000\n","Individual 3-gram: 1.000000\n","Individual 4-gram: 1.000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i03YmEpI5FzK","executionInfo":{"status":"ok","timestamp":1623351215486,"user_tz":-300,"elapsed":4328,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"90bc2ce0-db6d-462f-d992-85cad0b62c54"},"source":["!pip install rouge"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting rouge\n","  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEZ_P5H_De7F","executionInfo":{"status":"ok","timestamp":1623387266841,"user_tz":-300,"elapsed":3,"user":{"displayName":"Sarosh Abbas","photoUrl":"","userId":"02600718136745733509"}},"outputId":"ab6f9ac3-275d-44ab-f25a-99035199a815"},"source":["from rouge import Rouge \n","rouge = Rouge()\n","scores = rouge.get_scores(ref1, output)\n","print(scores)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[{'rouge-1': {'f': 0.4819277058615183, 'p': 0.5128205128205128, 'r': 0.45454545454545453}, 'rouge-2': {'f': 0.3950617234141137, 'p': 0.42105263157894735, 'r': 0.37209302325581395}, 'rouge-l': {'f': 0.4788732344614164, 'p': 0.5151515151515151, 'r': 0.4473684210526316}}]\n"],"name":"stdout"}]}]}