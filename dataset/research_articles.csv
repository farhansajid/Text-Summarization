headline,title,text,
"This paper looks at each level of the proposed framework from a service-oriented perspective. The proposed approach might facilitate the research and development of big data, big data analytics, data science, business intelligence, and business intelligence.",10 Bigs Big Data and Its Ten Big Characteristics,"This paper reveals ten big characteristics (10 Bigs) of big data and
explores their non-linear interrelationships through presenting a unified
framework of big data. The framework has three levels: fundamental level,
technological level, and socio-economic level. The fundamental level has four
big fundamental characteristics of big data. The technological level consists of
three big technological characteristics of big data. The socioeconomic level has
three big socioeconomic characteristics of big data. The paper looks at each
level of the proposed framework from a service-oriented perspective. The
proposed approach in this paper might facilitate the research and development
of big data, big data analytics, data science, business intelligence, and business
analytics.",
"5G network is projected to support large amount of data traffic and massive number of wireless connections. 5G networks support a wide range of applications such as smart home, autonomous driving, drone operations, health and mission critical applications.",5G Mobile Services and Scenarios Challenges and Solutions,"The Fifth generation (5G) network is projected to support large amount of data traffic and
massive number of wireless connections. Different data traffic has different Quality of Service (QoS) requirements. 5G mobile network aims to address the limitations of previous cellular standards (i.e., 2G/3G/4G) and be a prospective key enabler for future Internet of Things (IoT). 5G networks support a wide range of applications such as smart home, autonomous driving, drone operations, health and mission critical applications, Industrial IoT (IIoT), and entertainment and multimedia. Based on end users’ experience, several 5G services are categorized into immersive 5G services, intelligent 5G services, omnipresent 5G services, autonomous 5G services, and public 5G services. In this paper, we present a brief overview of 5G technical scenarios. We then provide a brief overview of accepted papers in our Special Issue on 5G mobile services and scenarios. Finally, we conclude this paper.",
"Transformative solutions are expected to drive the surge for accommodating a rapidly growing number of intelligent devices and services. 6G will fulfill the requirements of a fully connected world and provide ubiquitous wireless connectivity for all. The Internet of NanoThings, the Internet of BioNanoThings, and cell-free massive MIMO communication networks have also been discussed.",6G and Beyond The Future of Wireless Communications Systems,"The next generation of wireless communication networks, or 6G, will fulfill the requirements
of a fully connected world and provide ubiquitous wireless connectivity for all. Transformative solutions are expected to drive the surge for accommodating a rapidly growing number of intelligent devices and services. Major technological breakthroughs to achieve connectivity goals within 6G include: (i) a network operating at the THz band with much wider spectrum resources, (ii) intelligent communication environments that enable a wireless propagation environment with active signal transmission and reception, (iii) pervasive artificial intelligence, (iv) large-scale network automation, (v) an all-spectrum reconfigurable front-end for
dynamic spectrum access, (vi) ambient backscatter communications for energy savings, (vii) the Internet of Space Things enabled by CubeSats and UAVs, and (viii) cell-free massive MIMO communication networks. In this roadmap paper, use cases for these enabling techniques as well as recent advancements on related topics are highlighted, and open problems with possible solutions are discussed, followed by a development timeline outlining the worldwide efforts in the realization of 6G. Going beyond 6G, promising early-stage technologies such as the Internet of NanoThings, the Internet of BioNanoThings, and quantum communications, which are expected to have a far-reaching impact on wireless communications, have also
been discussed at length in this paper.",
"Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions. We report satisfactory categorization performances on a large set of 13 categories.",A Bayesian Hierarchical Model for Learning Natural Scene Categories,"We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions
as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.",
Bayesian surprise provides an intuitive way to identify surprising information from a summarization input. Method quantifies the degree to which pieces of information in the input change one's beliefs about the world.,A bayesian method to incorporate background knowledge during automatic text summarization,"In order to summarize a document, it is often useful to have a background set
of documents from the domain to serve as a reference for determining new and
important information in the input document. We present a model based on
Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus. Specifically,
the method quantifies the degree to which pieces of information in the input change
one’s beliefs’ about the world represented in the background. We develop systems for generic and update summarization based on this idea. Our method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus.",
"Inductive reasoning is an important educational practice but can be difficult for teachers to support in the classroom. In this paper, we describe a model to predict and simulate inductive reasoning of students for a categorization task. Using data gathered from 222 students categorizing three topics, we fnd that our model has a 75% accuracy.",A Bayesian Model of Individual Differences and Flexibility in Inductive Reasoning for Categorization of Examples,"Inductive reasoning is an important educational practice but can be difcult for teachers to support in the classroom due to the high level of preparation and classroom time needed to choose the teaching materials that challenge students’ current views. Intelligent tutoring systems can potentially facilitate this work for teachers by supporting the automatic adaptation of examples based on a student model of the induction process. However, current models of inductive reasoning usually lack two main characteristics helpful to adaptive learning environments, individual di?erences of students and tracing of students’ learning as they receive feedback. In this paper, we describe a model to predict and simulate inductive reasoning of students for a categorization task. Our approach uses a Bayesian model for describing the reasoning processes of students. This model allows us to predict students’ choices in categorization questions by accounting for their feature biases. Using data gathered from 222 students categorizing three topics, we fnd that our model has a 75% accuracy, which is 10% greater than a baseline model. Our model is a contribution to learning analytics by enabling us to assign different bias profles to individual students and tracking these profle changes over time through which we can gain a better understanding of students’ learning processes. This model may be relevant for systematically analysing students’ di?erences and evolution in inductive reasoning strategies while supporting the design of adaptive inductive learning environments.", 
"SK Telecom Company of South Korea recently introduced the concept of IoST to its business model. The company deployed IoST, which constantly generates data via the LoRa wireless platform. A system developed using the proposed architecture will be able to analyze and store IoST data.",A Big Data Analytics Architecture for the internet of small thing,"The SK Telecom Company of South Korea recently introduced the concept of IoST to its business model. The company deployed IoST, which constantly generates data via the LoRa wireless platform. The increase in data rates generated by IoST is escalating exponentially. After attempting to analyze and store the massive volume of IoST data using existing tools and technologies, the South Korean company realized the shortcomings immediately. The current article addresses some of the issues and presents a big data analytics architecture for its IoST. A system developed using the proposed architecture will be able to analyze and store IoST data efficiently while enabling better decisions. The proposed architecture is composed of four layers, namely the small things layer, infrastructure layer, platform layer, and application layer. Finally, a detailed analysis of a big data implementation of the IoST used to track humidity and temperature via Hadoop is presented as a proof of concept.", 
"The Canadian banking industry has become highly competitive due to threats and disruptions caused by direct competitors. The primary objective of this paper is to construct a predictive churn model by utilizing big data. By deploying the above systems, we were able to uncover a wealth of data associated with over 3 million customers' records.",A big data analytics model for customer churn prediction in the retiree segment,"Undoubtedly, the change in consumers’ choices and expectations, stemming from the emerging technology and also signifcant availability of di?erent products and services, created a highly competitive landscape in various customer service sectors, including the fnancial industry. Accordingly, the Canadian banking industry has also become highly competitive due to the threats and disruptions caused by not only direct competitors, but also new entrants to the market. The primary objective of this paper is to construct a predictive churn model by utilizing big data, including the structured archival data, integrated with unstructured data from sources such as online web pages, the number of website visits and phone conversation logs, for the frst time in the fnancial industry. It also examines the e?ect of di?erent aspects of customers’ behavior on churning decisions. The Datameer big data analytics tool on the Hadoop platform and predictive techniques using the SAS business intelligence system were applied to study the client retirement journey path and to create a churn prediction model. By deploying the above systems, we were able to uncover a wealth of data and information associated with over 3 million customers’ records within the retiree segment of the target bank, from 2011 to 2015.", 
The goal of Intelligent Transport Systems (ITS) is to enhance network performance of vehicular Ad hoc NETworks (VANETs) There are some security concerns including the need to establish trust among the connected peers. The 5G communication system is seen as the technology to cater for the challenges in VANets.,A Blockchain-SDN enabled Internet of Vehicles Environment for Fog Computing and 5G Networks,"The goal of Intelligent Transport Systems (ITS) is to enhance network performance of Vehicular Ad hoc NETworks (VANETs). Even though it presents new opportunities to the Internet of Vehicles (IoV) environment, there are some security concerns including the need to establish trust among the connected peers. The fifth generation (5G) communication system, which provides reliable and low-latency communication services, is seen as the technology to cater for the challenges in VANETs. The incorporation of software defined networks (SDN) also ensures an effective network management. However, there should be monitoring and reporting services provided in the IoV. Blockchain, which has decentralization, transparency and immutability as some of its properties, is designed to ensure trust in networking platforms. In that regard, this paper analyzes the combination of blockchain and SDN for the effective operation of VANET systems in 5G and fog computing paradigms. With managerial responsibilities shared between the blockchain and the SDN, it helps to relieve the pressure off the controller due to the ubiquitous processing that occurs. A trust-based model that curbs malicious activities in the network is also presented. Simulation results substantially guarantee an efficient network performance, while also ensuring there is trust among the entities.", 
An  improper ordering can confuse the reader and deteriorate the readability of a summary. We present a bottom-up approach to arrange sentences extracted for multi-document summarization. We define four criteria to capture the association and order of two textual segments (e.g. sentences) We evaluate the sentence orderings produced by the proposed method and numerous baselines using subjective gradings as well as automatic evaluation measures.,A bottom-up approach to sentence ordering for multi-document summarization,"Ordering information is a difficult but important task for applications generating natural language texts such as multi-document summarization, question answering, and concept-to-text generation. In multi-document summarization, information is selected from a set of source documents. However, improper ordering of information in a summary can confuse the reader and deteriorate the readability of the summary. Therefore, it is vital to properly order the information in multi-document summarization. We present a bottom-up approach to arrange sentences extracted for multi-document summarization. To capture the association and order of two textual segments (e.g. sentences), we define four criteria: chronology, topical-closeness, precedence, and succession. These criteria are integrated into a criterion by a supervised learning approach. We repeatedly concatenate two textual segments into one segment based on the criterion, until we obtain the overall segment with all sentences arranged. We evaluate the sentence orderings produced by the proposed method and numerous baselines using subjective gradings as well as automatic evaluation measures. We introduce the average continuity, an automatic evaluation measure of sentence ordering in a summary, and investigate its appropriateness for this task.", 
"This paper provides a brief survey of the basic concepts and algorithms used for Machine Learning. We discuss applications of machine learning algorithms in various fields. These include pattern recognition, sensor networks, anomaly detection, Internet of Things.",A Brief Survey of Machine Learning Methods and their Sensor and IoT Applications,"This paper provides a brief survey of the basic concepts and algorithms used for Machine Learning and its applications. We begin with a broader definition of machine learning and then introduce various learning modalities including supervised and unsupervised methods and deep learning paradigms. In the rest of the paper, we discuss applications of machine learning algorithms in various fields including pattern recognition, sensor networks, anomaly detection, Internet of Things (IoT) and health monitoring. In the final sections, we present some of the software tools and an extensive bibliography.", 
Text mining is the task of extracting meaningful information from text. The amount of text that is generated every day is increasing dramatically. Efficient and efficient techniques and algorithms are required to discover useful patterns.,"A brief survey of text mining Classification, clustering and extraction techniques","The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efcient and e?ective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained signifcant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classifcation and clustering. Additionally, we brie?y explain text mining in biomedical and health care domains.", 
A large amount of on-line information and lengthiness information can't fit for the mobile devices. We propose a method which collects original news text and extracts summary sentences from them automatically.,A Chinese Automatic Text Summarization system for mobile devices,"A large amount of on-line information and lengthiness information can’t fit for the mobile devices. In order to save this problem, we propose a method which collects original news text from on-line information and extracts summary sentences from them automatically. On this basis, we adopt WML(Wireless Markup Language) to build a news website for mobile devices browsing through the news summary. The system is mainly made up by Automatic News Collection and Auto Text Summarization. Our experimental results proved the effectiveness of the means.", 
The detection of spatial clusters and outliers is critical to a number of spatial data analysis techniques. This article proposes an exploratory procedure to detect and classify clusters and outlier. The procedure is fully implemented using free and open source geospatial software and libraries.,A classification technique for local multivariate clusters and outliers of spatial association,"The detection of spatial clusters and outliers is critical to a number of spatial data analysis techniques. Many techniques embed spatial clustering components with the aim of exploring spatial variability and patterns in a data set, caused by the spatial association that generally affects most spatial data. A frontier challenge in spatial data analysis is to extend techniques—originally designed for univariate analysis—to a multivariate context, in order to be able to cope with the increasing complexity and variety of modern spatial data. This article proposes an exploratory procedure to detect and classify clusters and outliers in a multivariate spatial data set. Cluster and outlier detection relies on recently introduced multivariate extensions of the well-established local indicators of spatial association statistics. Two new indicators are proposed enabling the classification of multivariate clusters and outliers, not directly achievable with any already established technique. The procedure is fully implemented using free and open source geospatial software and libraries. The raw source code is made available for future reviews and replications. Empirical results from early applications on both synthetic and real spatial data are discussed. Advantages and limitations of the introduced procedure are outlined according to the empirical results.", 
A high quality summary is a main goal and challenge for any automatic text summarization. We use cellular learning automata for calculating similarity of sentences and particle swarm optimization method for weighting to the features according to their importance. The results show that second method performs better than the first method and benchmark methods.,"A combinational method of fuzzy, particle swarm optimization and cellular learning automata for text summarization","A high quality summary is a main goal and challenge for any automatic text summarization. In this paper, a new method is introduced for automatic text summarization problem. We use cellular learning automata for calculating similarity of sentences, particle swarm optimization method for weighting to the features according to their importance and use fuzzy logic for scoring sentences. The cellular learning automata method concentrate on reducing the redundancy problems but particle swarm optimization and fuzzy logic methods centralized on the scoring technique of the sentences. We propose two methods, the first method is text summarization based cellular learning automata and the second method is text summarization based combination of fuzzy, particle swarm optimization and cellular learning automata. The results show that second method performs better than the first method and the benchmark methods.", 
"This paper provides a comparative study of commercial and academic automatic document summarization tools. Microsoft Word, SweSum, SHVOONG and Online Brevity Document Summarizer are compared.",A comparative study of automatic text summarization system performance,"This paper provides a comparative study of commercial and academic automatic document summarization tools: Microsoft Word, SweSum, SHVOONG and Online Brevity Document Summarizer. Also, these automatic systems are compared to human summarization techniques and analyzed, aiming to reveal which automatic summary achieves the best results in comparison with the human summarization techniques. Results of the experiment show that the most high-quality automatic summary was generated by Online Brevity Document Summarizer.", 
This paper attempts to find the most useful wavelet function among the existing members of the wavelet families for electroencephalogram signal (EEG) analysis. The EEGs considered for this study belong to both normal as well as abnormal signals like epileptic EEG. It was found that Coiflets 1 is the most suitable candidate among the wavelets considered in this study for accurate classification of the EEG signals.,A comparative study of wavelet families for EEG signal classification,"Over the past two decades, wavelet theory has been used for the processing of biomedical signals for feature extraction, compression and de-noising applications. However the question as to which wavelet family is the most suitable for analysis of non-stationary bio-signals is still prevalent among researchers. This paper attempts to find the most useful wavelet function among the existing members of the wavelet families for electroencephalogram signal (EEG) analysis. The EEGs considered for this study belong to both normal as well as abnormal signals like epileptic EEG. Important features such as energy, entropy and standard deviation at different sub-bands were computed using the wavelet functions—Haar, Daubechies (orders 2–10), Coiflets (orders 1–10), and Biorthogonal (orders 1.1, 2.4, 3.5, and 4.4). Feature vectors were used to model and train the Probabilistic Neural Network (PNN) and the classification accuracies were evaluated for each case. The results obtained from PNN classifier were compared with Support Vector Machine (SVM) classifier. From the statistical analysis, it was found that Coiflets 1 is the most suitable candidate among the wavelet families considered in this study for accurate classification of the EEG signals. In this work, we have attempted to improve the computing efficiency as it selects the most suitable wavelet function that can be used for EEG signal processing efficiently and accurately with lesser computational time.", 
"Document categorization is a technique where the category of a document is determined. Three well-known supervised learning techniques are compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset.",A Comparative Study on Different Types of Approaches to Bengali document Categorization,"Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Naïve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.", 
A vehicular ad hoc network (VANET) is a mobile network in which network nodes are vehicles. VANETs present a unique range of challenges and opportunities for routing protocols. Clustering can be used to improve routing scalability and reliability. It can also be used for accident or congestion detection and information dissemination.,A Comparative Survey of VANET Clustering Techniques,"A vehicular ad hoc network (VANET) is a mobile ad hoc network (MANET) in which network nodes are vehicles – most commonly road vehicles. VANETs present a unique range of challenges and opportunities for routing protocols due to the semi-organised nature of vehicular movements subject to the constraints of road geometry and rules, and the obstacles which limit physical connectivity in urban environments. In particular, the problems of routing protocol reliability and scalability across large urban VANETs are currently the subject of intense research. Clustering can be used to improve routing scalability and reliability in VANETs, as it results in the distributed formation of hierarchical network structures by grouping vehicles together based on correlated spatial distribution and relative velocity. In addition to the benefits to routing, these groups can serve as the foundation for accident or congestion detection, information dissemination and entertainment applications. This paper explores the design choices made in the development of clustering algorithms targeted at VANETs. It presents a taxonomy of the techniques applied to solve the problems of cluster head election, cluster affiliation and cluster management, and identifies new directions and recent trends in the design of these algorithms. Additionally, methodologies for validating clustering performance are reviewed, and a key shortcoming – the lack of realistic vehicular channel modelling – is identified. The importance of a rigorous and standardised performance evaluation regime utilising realistic vehicular channel models is demonstrated.", 
"The challenges of Android forensics include the complexity of the Android application and different procedures and tools for obtaining data. Manual, Logical and physical acquisition techniques are used to acquire data from an Android mobile device. The results of the analysis demonstrate that the technique can retrieve Contacts, photos, Videos, Call Logs, and SMSs.",A comparison study of android mobile forensics for retrieving files system,"A comparison study of the Android forensic field in terms of Android forensic process for acquiring and analysing an Android disk image is presented. The challenges of Android forensics, including the complexity of the Android application, different procedures and tools for obtaining data, difficulties with hardware set up, using expensive commercial tools for acquiring logical data that fail to retrieve physical data acquisition are described in this paper. To solve these challenges and achieve high accuracy and integrity in Android forensic processes, a new open source technique is investigated. Manual, Logical and physical acquisition techniques are used to acquire data from an Android mobile device (Samsung Android 4.2.2). The mobile phone is identified by taking photos of the device and its individual components, including the memory expansion card, and labelling them with identifying information. Following the manual acquisition, logical acquisition is conducted using the AFLogical application in the ViaExtract tool (by Now secure) installed on a Santoku Linux Virtual Machine. The image file is then created using the AccessData FTK imager tool for physical acquisition. Four tools are utilized to analyse recovered data: one using ViaExtract on a Santoku Linux Virtual Machine, two using the AccessData FTK Imager, and one using file carving in Autopsy on a Kali Linux Virtual Machine. The results of the analysis demonstrate that the technique can retrieve Contacts, photos, Videos, Call Logs, and SMSs. Also, the EaseUS Data Recovery Wizard Free tool is used for the recovery of files from the LOST.DIRon external memory.", 
"The huge amount of information available in digital media has increased the demand for simple, language-independent extractive summarization strategies. In this paper, we employ concepts and metrics of complex networks to select sentences for an extractive summary. When applied to a corpus of Brazilian Portuguese texts, some versions performed better than summarizers that do not employ deep linguistic knowledge.",A complex network approach to text summarization,"Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media, which has increased the demand for simple, language-independent extractive summarization strategies. In this paper, we employ concepts and metrics of complex networks to select sentences for an extractive summary. The graph or network representing one piece of text consists of nodes corresponding to sentences, while edges connect sentences that share common meaningful nouns. Because various metrics could be used, we developed a set of 14 summarizers, generically referred to as CN-Summ, employing network concepts such as node degree, length of shortest paths, d-rings and k-cores. An additional summarizer was created which selects the highest ranked sentences in the 14 systems, as in a voting system. When applied to a corpus of Brazilian Portuguese texts, some CN-Summ versions performed better than summarizers that do not employ deep linguistic knowledge, with results comparable to state-of-the-art summarizers based on expensive linguistic resources. The use of complex networks to represent texts appears therefore as suitable for automatic summarization, consistent with the belief that the metrics of such networks may capture important text features.", 
Automatic text summarization condenses the text documents into meaningful phrases and textual messages. This paper discusses the basic blocks of the automatic text summarizing and its feature in identifying the intricate properties.,A comprehensive analysis on extractive automatic text summarization,The modern technology demands the maintenance of the increasing data which are in structured and unstructured form. The text documents collected in the various platforms occupies a massive space in the architectural structure of the computer system both physically and virtually. Apparently the users demand the summarizing of the collected documents for easy access and usage. To enable this automatic text summarization came into phase. The automatic text summarization condenses the text documents into meaningful phrases and textual messages which helps the user to understand the conceptual ides behind each core values. The importance of automatic text summarization stands as a helping source in the growing data. This paper discusses the basic blocks of the automatic text summarization and its feature in identifying the intricate properties of the meaningful text through various approaches., 
Text summarization aims at getting the most important content in a condensed form from a given document. It is considered to be an effective way of tackling information overload. There exist lots of text summarization approaches which are based on Latent Semantic Analysis (LSA),A Comprehensive Method for Text Summarization Based on Latent Semantic Analysis,"Text summarization aims at getting the most important content in a condensed form from a given document while retains the semantic information of the text to a large extent. It is considered to be an effective way of tackling information overload. There exist lots of text summarization approaches which are based on Latent Semantic Analysis (LSA). However, none of the previous methods consider the term description of the topic. In this paper, we propose a comprehensive LSA-based text summarization algorithm that combines term description with sentence description for each topic. We also put forward a new way to create the term by sentence matrix. The effectiveness of our method is proved by experimental results. On the summarization performance, our approach obtains higher ROUGE scores than several well known methods.", 
Multiple disease risk prediction models have been developed. They use various patient characteristics to estimate the probability of outcomes over a certain period of time. They hold the potential to improve decision making and individualize care.,A comprehensive study on disease risk predictions in machine learning,"Over recent years, multiple disease risk prediction models have been developed. These models use various patient characteristics to estimate the probability of outcomes over a certain period of time and hold the potential to improve decision making and individualize care. Discovering hidden patterns and interactions from medical databases with growing evaluation of the disease prediction model has become crucial. It needs many trials in traditional clinical findings that could complicate disease prediction. A Comprehensive study on different strategies used to predict disease is conferred in this paper. Applying these techniques to healthcare data, has improvement of risk prediction models to find out the patients who would get benefit from disease management programs to reduce hospital readmission and healthcare cost, but the results of these endeavors have been shifted.", 
"Vehicular communication networks is a powerful tool that enables numerous vehicular data services and applications. The rapid growth in vehicles has also resulted in the vehicular network becoming heterogeneous, dynamic, and large-scale. This study presents an exhaustive review of previous works by classifying them based on based on wireless communication, particularly VANET.","A comprehensive survey Benefits, Services, Recent works, Challenges, Security and Use cases for SDN-VANET","Vehicular communication networks is a powerful tool that enables numerous vehicular data services and applications. The rapid growth in vehicles has also resulted in the vehicular network becoming heterogeneous, dynamic, and large-scale, making it hard to meet the strict requirements, such as extremely latency, high mobility, top security, and enormous connections of the fifth-generation network. Previous studies have shown that with the increase in the application of Software-Defined Networking (SDN) on Vehicular Adhoc Network (VANET) in industries, researchers have exerted considerable efforts to improve vehicular communications. This study presents an exhaustive review of previous works by classifying them based on based on wireless communication, particularly VANET. First, a concise summary of the VANET structure and SDN controller with layers and details of their infrastructure is provided. Second, a description of SDN-VANET applications in different wireless communications, such as the Internet of Things (IoT) and VANET is provided with concentration on the examination and comparison of SDN-VANET works on several parameters. This paper also provides a detailed analysis of the open issues and research directions accomplished while integrating the VANET with SDN. It also highlights the current and emerging technologies with use cases in vehicular networks to address the several challenges in the VANET infrastructure. This survey acts as a catalyst in raising the emergent robustness routing protocol, latency, connectivity and security issues of future SDN-VANET architectures.", 
"Network coding has been used in traditional and emerging wireless networks to overcome communications issues. It also plays an important role in the area of vehicular ad-hoc networks (VANETs) to meet the challenges like high mobility, rapidly changing topology, and intermittent connectivity. VANET's consist of network of vehicles in which they communicate with each other.",A comprehensive survey of network coding in vehicular ad-hoc networks,"Network coding is a data processing technique in which the flow of digital data is optimized in a network by transmitting a composite of two or more messages to make the network more robust. Network coding has been used in traditional and emerging wireless networks to overcome the communications issues of these networks. It also plays an important role in the area of vehicular ad-hoc networks (VANETs) to meet the challenges like high mobility, rapidly changing topology, and intermittent connectivity. VANETs consist of network of vehicles in which they communicate with each other to ensure road safety, free flow of traffic, and ease of journey for the passengers. It is now considered to be the most valuable concept for improving efficiency and safety of future transportation. However, this field has a lot of challenges to deal with. This paper presents a comprehensive survey of network coding schemes in VANETs. We have classified different applications like content distribution, multimedia streaming, cooperative downloading, data dissemination, and summarized other key areas of VANETs in which network coding schemes are implemented. This research work will provide a clear understanding to the readers about how network coding is implemented in these schemes in VANETs to improve performance, reduce delay, and make the network more efficient.", 
"Text Summarization was one of the key areas of research in the recent times. Efforts were put to create a system which was able to generate effective summaries. The paper compares all the prevailing systems, their shortcomings, and a combination of technologies.",A Comprehensive Survey on Extractive and Abstractive Techniques for Text Summarization,"Over the years as the technology advanced, the amount of data generated during the simulations and processing has been constantly increasing. Techniques for creating synopses of this massively generated data have been in the forefront of the research in the recent times. Text Summarization was one such aspect of the research which focused on representing the idea of the context in a short representation. Efforts were put to create a system which was able to generate effective summaries providing an overview of all the ideas represented by the article. Text Summarization techniques can be broadly classified into Extractive and Abstractive Text Summarization techniques. The paper compares all the prevailing systems, their shortcomings, and a combination of technologies used to achieve improved results. The paper also draws attention towards the state-of-the-art standardized datasets used in developing the summarization systems. The paper also focuses on testing parameters and techniques used to test the efficiency of the summarizing systems.", 
Automatic text summarization system helps in providing a quick summary of the information contained in the document. Some efficient work has been done for text summarizing on various languages. Among them there are a few works on Bengali language. It motivated us to do develop or modify a new or existing summarization technique for Bengali document(s),A Comprehensive Survey on Extractive Text Summarization Techniques,"Automated data collection tools and matured database technology lead to tremendous amounts of data stored in database, data warehouses and other data repositories. With the increasing amount of online information, it becomes extremely difficult to find relevant information to users. Information retrieval system usually returns a large amount of documents listed in the order of estimated relevance. It is not possible for users to read each document in order to find the useful one. Automatic text summarization system, one of the special data mining applications, helps in this task by providing a quick summary of the information contained in the document(s). Some efficient work has been done for text summarization on various languages. But among them there are a few works on Bengali language. It has thus motivated us to do develop or modify a new or existing summarization technique for Bengali document(s) and to provide us an opportunity to make some contribution in natural language processing. To do the same, we have surveyed and compared some techniques on extractive text summarization on various languages in this paper. The summarizations have done for single or multiple documents in different languages. Finally, a comparative nomenclature on the discussed single or multi-document summarization techniques has been conducted.", 
Summarization systems offer the possibility of finding the main points of texts. Different types of summary might be useful in various applications. This paper presents a taxonomy of summarization systems and defines the most important criteria.,A Comprehensive Survey on Text Summarization Systems,"Text summarization systems are among the most attractive research areas nowadays. Summarization systems offers the possibility of finding the main points of texts and so the user will spend less time on reading the whole document. Different types of summary might be useful in various applications and summarization systems can be categorized based on these types. This paper presents a taxonomy of summarization systems and defines the most important criteria for a summary which can be generated by a system. Additionally, different methods of text summarization as well as main steps for summarization process is discussed. we also go through main criteria for evaluating a text summarization.", 
Maintaining network connectivity is an important challenge for Vehicular Ad-hoc NETwork (VANET) in an urban scene. Most existing studies analyze end-to-end connectivity probability under a certain node distribution model. This work proposes a connectivity prediction-based dynamic clustering model for VANET in an Urban scene.,A connectivity prediction-based dynamic clustering model for VANET in an urban scene,"Maintaining network connectivity is an important challenge for Vehicular Ad-hoc NETwork (VANET) in an urban scene, which has more complex road conditions than highways and suburban areas. Most existing studies analyze end-to-end connectivity probability under a certain node distribution model, and reveal the relationship among network connectivity, node density, and a communication range. Because of various influencing factors and changing communication states, most of their results are not applicable to VANET in an urban scene. In this work, we propose a connectivity prediction-based dynamic clustering model for VANET in an urban scene. First, we introduce a connectivity prediction method according to features of a vehicle node and relative features among vehicle nodes. Then, we formulate a dynamic clustering model based on connectivity among vehicle nodes and vehicle node density. Finally, we present a dynamic clustering model-based routing method to realize stable communications among vehicle nodes. The experimental results show that the proposed connectivity prediction method can achieve lower error rate than the geographic routing based on predictive locations and multi-layer perceptron. The proposed routing method can achieve lower end-to-end latency and higher delivery rate than the greedy perimeter stateless routing and modified distributed and mobility-adaptive clustering-based methods.", 
"Academics receive incentives, tenures, or awards from the number of citations they receive. Use of citations for research/er evaluation purposes can give rise to unethical practices and manipulation. This study conducted a content-based citation analysis study for Turkish citations.",A content-based citation analysis study based on text categorization,"Publications and citations are important components for measuring research performance. Academics receive incentives, tenures, or awards from the number of citations they receive; however, the use of citations for research/er evaluation purposes can give rise to unethical practices and manipulation. Consequently, it is necessary to change the current approach to the use of citations. The main aim of this study was to conduct a content-based citation analysis study for Turkish citations. To achieve this aim, 423 peerreviewed articles, the associated 12,881 references, and 101,019 sentences published in library and information science literature in Turkey were thoroughly examined. The citations were divided into four main categories; citation meaning, citation purpose, citation shape, and citation array. Then, each category was further divided into sub-categories. A tagging process with inter-annotator agreement was conducted and citation categories for the citation sentences determined. Weka software was used to apply the text categorization methods. The automatic citation sentence classification achieved at least a 90% success rate for all citation classes, which proved that using computational linguistics to evaluate citation contexts developing new techniques was possible and gave more detailed results.", 
"Text summarization is the process of creating a shorter version of one or more text documents. Sentence scoring is the technique most used for extractive text summarization, today. This paper advocates the thesis that the quality of the summary obtained with combinations of sentence scoring methods depend on text subject.",A Context Based Text Summarization System,"Text summarization is the process of creating a shorter version of one or more text documents. Automatic text summarization has become an important way of finding relevant information in large text libraries or in the Internet. Extractive text summarization techniques select entire sentences from documents according to some criteria to form a summary. Sentence scoring is the technique most used for extractive text summarization, today. Depending on the context, however, some techniques may yield better results than some others. This paper advocates the thesis that the quality of the summary obtained with combinations of sentence scoring methods depend on text subject. Such hypothesis is evaluated using three different contexts: news, blogs and articles. The results obtained show the validity of the hypothesis formulated and point at which techniques are more effective in each of those contexts studied.", 
This paper describes the different steps which lead to the construction of the LIP6 extractive summarizer. The basic idea behind this system is to expand question and title keywords of each topic with their respective cluster terms.,A contextual query expansion approach by term clustering for robust text summarization,"This paper describes the different steps which lead to the construction of the LIP6 extractive summarizer. The basic idea behind this system is to expand question and title keywords of each topic with their respective cluster terms. Term clusters are found by unsupervised learning using a classification variant of the wellknown EM algorithm. Each sentence is then characterized by 4 features, each of which uses bag-of-words similarities between expanded topic title or questions and the current sentence. A final score of the sentences is found by manually tuning the weights of a linear combination of these features ; these weights are chosen in order to maximize the Rouge-2 AvF measure on the Duc 2006 corpus.", 
"Congestive Heart Failure (CHF) is a severe pathophysiological condition associated with high prevalence, high mortality rates, and sustained healthcare costs. Potential of applying Convolutional Neural Network (CNN) approaches to the automatic detection of CHF has been largely overlooked thus far. This study presents a CNN model that accurately identifies CHF on the basis of one raw electrocardiogram (ECG) heartbeat only.",A convolutional neural network approach to detect congestive heart failure,"Congestive Heart Failure (CHF) is a severe pathophysiological condition associated with high prevalence, high mortality rates, and sustained healthcare costs, therefore demanding efficient methods for its detection. Despite recent research has provided methods focused on advanced signal processing and machine learning, the potential of applying Convolutional Neural Network (CNN) approaches to the automatic detection of CHF has been largely overlooked thus far. This study addresses this important gap by presenting a CNN model that accurately identifies CHF on the basis of one raw electrocardiogram (ECG) heartbeat only, also juxtaposing existing methods typically grounded on Heart Rate Variability. We trained and tested the model on publicly available ECG datasets, comprising a total of 490,505 heartbeats, to achieve 100% CHF detection accuracy. Importantly, the model also identifies those heartbeat sequences and ECG’s morphological characteristics which are class-discriminative and thus prominent for CHF detection. Overall, our contribution substantially advances the current methodology for detecting CHF and caters to clinical practitioners’ needs by providing an accurate and fully transparent tool to support decisions concerning CHF detection.", 
The EEG signal is a medium to realize a BCI system due to its zero clinical risk and portable acquisition devices. We present a Convolutional Recurrent Attention Model (CRAM) that utilizes a convolutional neural network to encode the highlevel representation of EEG signals. The proposed model is capable of exploiting the underlying invariant EEG patterns across different subjects.,A Convolutional Recurrent Attention Model for Subject-Independent EEG Signal Analysis,"The EEG signal is a medium to realize a BCI system due to its zero clinical risk and portable acquisition devices. Current EEG-based BCI research usually requires a subjectspecific adaptation step before a BCI can be employed by a new user. In contrast, the subject-independent scenario, where a well-trained model can be directly applied to new users without pre-calibration, is particularly desired. Considering this critical gap, the focus in this paper is developing an effective EEG signal analysis adaptively applied to subject-independent settings. We present a Convolutional Recurrent Attention Model (CRAM) that utilizes a convolutional neural network to encode the highlevel representation of EEG signals and a recurrent attention mechanism to explore the temporal dynamics of the EEG signals as well as to focus on the most discriminative temporal periods. Extensive experiments on a benchmark multiclass EEG dataset containing four movement intentions indicate that the proposed model is capable of exploiting the underlying invariant EEG patterns across different subjects and generalizing the patterns to new subjects with better performance than a series of state-ofthe-art and baseline approaches by at least 8 percentage points. The implementation code is made publicly available.", 
"Recent efforts have been made on developing mobile recommender systems for taxi drivers using Taxi GPS traces. In the real world, the income of taxi drivers is strongly correlated with the effective driving hours. We propose to develop a cost-effective recommender system for drivers to find passengers. We develop a novel recursion strategy based on the special form of the net profit function for searching optimal candidate routes efficiently.",A Cost-Effective Recommender System for Taxi Drivers,"The GPS technology and new forms of urban geography have changed the paradigm for mobile services. As such, the abundant availability of GPS traces has enabled new ways of doing taxi business. Indeed, recent efforts have been made on developing mobile recommender systems for taxi drivers using Taxi GPS traces. These systems can recommend a sequence of pick-up points for the purpose of maximizing the probability of identifying a customer with the shortest driving distance. However, in the real world, the income of taxi drivers is strongly correlated with the effective driving hours. In other words, it is more critical for taxi drivers to know the actual driving routes to minimize the driving time before finding a customer. To this end, in this paper, we propose to develop a cost-effective recommender system for taxi drivers. The design goal is to maximize their profits when following the recommended routes for finding passengers. Specifically, we first design a net profit objective function for evaluating the potential profits of the driving routes. Then, we develop a graph representation of road networks by mining the historical taxi GPS traces and provide a Brute-Force strategy to generate optimal driving route for recommendation. However, a critical challenge along this line is the high computational cost of the graph based approach. Therefore, we develop a novel recursion strategy based on the special form of the net profit function for searching optimal candidate routes efficiently. Particularly, instead of recommending a sequence of pick-up points and letting the driver decide how to get to those points, our recommender system is capable of providing an entire driving route, and the drivers are able to find a customer for the largest potential profit by following the recommendations. This makes our recommender system more practical and profitable than other existing recommender systems. Finally, we carry out extensive experiments on a real-world data set collected from the San Francisco Bay area and the experimental results clearly validate the effectiveness of the proposed recommender system.", 
Artificial Intelligence (AI) has been used widely in medicine and health care sector. Doctors need accurate predictions for the outcomes of their patients' diseases. The study of existing predictive models based on machine learning methods is extremely active.,A Critical Review for Developing Accurate and Dynamic Predictive Models Using Machine Learning Methods in Medicine and Health Care,"Recently, Artificial Intelligence (AI) has been used widely in medicine and health care sector. In machine learning, the classification or prediction is a major field of AI. Today, the study of existing predictive models based on machine learning methods is extremely active. Doctors need accurate predictions for the outcomes of their patients’ diseases. In addition, for accurate predictions, timing is another significant factor that influences treatment decisions. In this paper, existing predictive models in medicine and health care have critically reviewed. Furthermore, the most famous machine learning methods have explained, and the confusion between a statistical approach and machine learning has clarified. A review of related literature reveals that the predictions of existing predictive models differ even when the same dataset is used. Therefore, existing predictive models are essential, and current methods must be improved.", 
Multi-document extractive summarization relies on the concept of sentence centrality. We propose a new approach under the Hub-Authority framework. We provide an evaluation of our method on DUC 2004 data.,A cue-based hub-authority approach for multi-document text summarization,"Multi-document extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Although some research has introduced the graph-based ranking algorithms such as PageRank and HITS into the text summarization, we propose a new approach under the Hub-Authority framework: in this paper. Our approach combines the text content with some cues such as ""cue phrase"", ""sentence length"" and ""first sentence"" and explores the sub-topics in the multi-documents by bringing the features of these sub-topics into graph-based sentence ranking algorithms. We provide an evaluation of our method on DUC 2004 data. The results show that our approach is an effective graph-ranking schema in multi-document generic text summarization.", 
"A data-driven learning framework is proposed for generating color based signatures. To obtain the features, a linear transformation is learned from the pixel values based on its reconstruction error. The approach is compared with several baselines such as histograms in RGB, HSV, YUV and Lab color spaces.",A DATA-DRIVEN COLOR FEATURE LEARNING SCHEME FOR IMAGE RETRIEVAL,"This paper addresses content based image retrieval based on color features. Several previous works have addressed color based image retrieval based on hand-crafted features. In this paper, a data-driven learning framework is proposed for generating color based signatures. To obtain the features, a linear transformation is learned from the pixel values based on its reconstruction error. Using this linear transformation, the original pixel values are transformed into a higher dimensional space. In the higher dimensional space, a dictionary is learned to obtain the sparse codes of the pixels. A max pooling strategy is used to obtain the dominant color features of a region and the final feature vector for an image is obtained by concatenating the pooled features. We evaluate our approach following the standard evaluation criteria for the INRIA Holidays and University of Kentucky Benchmark datasets. The approach is compared with several baselines such as histograms in RGB, HSV, YUV and Lab color spaces and several other color based features proposed for addressing this problem. Our approach shows competitive results on these datasets and outperforms all the baselines.", 
Message Passing Interface (MPI) libraries use message queues to guarantee correct message ordering between communicating processes. Message queues are in the critical path of MPI communications. Collective communications are widely used in MPI applications and they can have considerable impact on generating long message queues.,A Dedicated Message Matching Mechanism for Collective Communications,"The Message Passing Interface (MPI) libraries use message queues to guarantee correct message ordering between communicating processes. Message queues are in the critical path of MPI communications and thus, the performance of message queue operations can have signifcant impact on the performance of applications. Collective communications are widely used in MPI applications and they can have considerable impact on generating long message queues. In this paper, we propose a message matching mechanism that improves the message queue search time by distinguishing messages coming from point-to-point and collective communications and allocating separate queues for them. Moreover, it dynamically profles the impact of each collective call on message queues during the application runtime and uses this information to adapt the message queue data structure for each collective operation dynamically. The proposed approach can successfully reduce the queue search time while maintaining scalable memory consumption. The evaluation results show that we can obtain up to 5.5x runtime speedup for applications with long list traversals. Moreover, we can gain up to 15% and 45% queue search time improvement for applications with short and medium list traversals, respectively.", 
Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance.,A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series,"Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. It is traditionally performed by a sleep expert who assigns to each 30 s of signal a sleep stage, based on the visual inspection of signals such as electroencephalograms (EEG), electrooculograms (EOG), electrocardiograms (ECG) and electromyograms (EMG). We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features, that exploits all multivariate and multimodal Polysomnography (PSG) signals (EEG, EMG and EOG), and that can exploit the temporal context of each 30 s window of data. For each modality the first layer learns linear spatial filters that exploit the array of sensors to increase the signal-to-noise ratio, and the last layer feeds the learnt representation to a softmax classifier. Our model is compared to alternative automatic approaches based on convolutional networks or decisions trees. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance. Our study reveals a number of insights on the spatio-temporal distribution of the signal of interest: a good trade-off for optimal classification performance measured with balanced accuracy is to use 6 EEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one minute of data before and after each data segment offers the strongest improvement when a limited number of channels is available. As sleep experts, our system exploits the multivariate and multimodal nature of PSG signals in order to deliver state-of-the-art classification performance with a small computational cost.", 
Deep Learning algorithm was applied to classifying EEG data based on Motor Imagery task. The performance of the proposed DBN was tested with different combinations of hidden units and hidden layers on multiple subjects. The recognition accuracy results were compared with Support vector machine (SVM) and DBN classifier demonstrated better performance.,A Deep Learning Method for Classification of EEG Data Based on Motor Imagery,"Effectively extracting EEG data features is the key point in Brain Computer Interface technology. In this paper, aiming at classifying EEG data based on Motor Imagery task, Deep Learning (DL) algorithm was applied. For the classification of left and right hand motor imagery, firstly, based on certain single channel, a weak classifier was trained by deep belief net (DBN); then borrow the idea of Ada-boost algorithm to combine the trained weak classifiers as a more powerful one. During the process of constructing DBN structure, many RBMs (Restrict Boltzmann Machine) are stacked on top of each other by setting the hidden layer of the bottom layer RBM as the visible layer of the next RBM, and Contrastive Divergence (CD) algorithm was also exploited to train multilayered DBN effectively. The performance of the proposed DBN was tested with different combinations of hidden units and hidden layers on multiple subjects, the experimental results showed that the proposed method performs better with 8 hidden layers. The recognition accuracy results were compared with Support vector machine (SVM) and DBN classifier demonstrated better performance in all tested cases. There was an improvement of 4 – 6% for certain cases.", 
"Sleep disorder is a symptom of many neurological diseases that may significantly affect the quality of daily life. Traditional methods are time-consuming and involve the manual scoring of polysomnogram (PSG) signals. In this study, a flexible deep learning model is proposed using raw PSG signals to monitor sleep stages.",A Deep Learning Model for Automated Sleep Stages Classification Using PSG Signals,"Sleep disorder is a symptom of many neurological diseases that may significantly affect the quality of daily life. Traditional methods are time-consuming and involve the manual scoring of polysomnogram (PSG) signals obtained in a laboratory environment. However, the automated monitoring of sleep stages can help detect neurological disorders accurately as well. In this study, a flexible deep learning model is proposed using raw PSG signals. A one-dimensional convolutional neural network (1D-CNN) is developed using electroencephalogram (EEG) and electrooculogram (EOG) signals for the classification of sleep stages. The performance of the system is evaluated using two public databases (sleep-edf and sleep-edfx). The developed model yielded the highest accuracies of 98.06%, 94.64%, 92.36%, 91.22%, and 91.00% for two to six sleep classes, respectively, using the sleep-edf database. Further, the proposed model obtained the highest accuracies of 97.62%, 94.34%, 92.33%, 90.98%, and 89.54%, respectively for the same two to six sleep classes using the sleep-edfx dataset. The developed deep learning model is ready for clinical usage, and can be tested with big PSG data.", 
"The most dangerous sleep disorder is obstructive sleep apnea syndrome (OSAS), which can cause sudden death. Many important parameters related to the diagnosis and treatments of such sleep disorders are simultaneously examined. In this study, a decision support system was developed to determine OSAS patients. When the performance of the study was compared with other studies in published literature, it was seen that satisfactory results were obtained.",A deep learning-based decision support system for diagnosis of OSAS using PTT signals,"Sleep disorders, which negatively affect an individual’s daily quality of life, are a common problem for most of society. The most dangerous sleep disorder is obstructive sleep apnea syndrome (OSAS), which manifests itself during sleep and can cause the sudden death of patients. Many important parameters related to the diagnosis and treatments of such sleep disorders are simultaneously examined. This process is exhausting and time-consuming for experts and also requires experience; thus, it can cause difference of opinion among experts. Because of this, automatic sleep staging systems have been designed. In this study, a decision support system was developed to determine OSAS patients. In the developed decision support system, unlike in the available published literature, patient and healthy individual classification was performed using only the Pulse Transition Time (PTT) parameter rather than other parameters obtained from polysomnographic data like ECG (Electrocardiogram), EEG (Electroencephalography), carbon dioxide measurement and EMG (Electromyography). The suggested method can perform feature extraction from PTT signals by means of a deep-learning method. AlexNet and VGG-16, which are two Convolutional Neural Network (CNN) models, have been used for feature extraction. With the features obtained, patients and healthy individuals were classified by the Support Vector Machine (SVM) and the k-nearest neighbors (k-NN) algorithms. When the performance of the study was compared with other studies in published literature, it was seen that satisfactory results were obtained.", 
"The objective of this research is to design and implement a machine learning (ML) based technique that can predict cases of septic shock and extreme sepsis. The classification algorithms demonstrated a 93.84%, 93.22%, 95.25% accuracy, sensitivity and specificity. The pattern used for clinical detection led to a small but statistically significant increase in IV usage and lab tests.",A Deep Learning-Based Sepsis Estimation Scheme,"The objective of this research is to design and implement a machine learning (ML) based technique that can predict cases of septic shock and extreme sepsis and assess its effects on medical practice and the patients. The study is a retrospective cohort type, which is used to algorithmic deduction and validation, along with pre- and post-impact assessment. For non-ICU cases, the algorithm was deduced and validated for specific periods. The classifiers used for the study have been deduced and validated by employing electronic health records (EHR), which were silent initially but alerted the clinical personnel concerning the sepsis prediction. For training the classification system, the chosen patients should have had ICD and the latest codes concerning extreme sepsis or septic shock. Moreover, the patients should have had positive blood culture during their interaction with the hospital, where there were indications of either systolic blood pressure (SBP) or lactate levels. The classification algorithms demonstrated a 93.84%, 93.22%, 95.25% accuracy, sensitivity and specificity respectively. The pattern used for clinical detection, in the context of the alerting system, led to a small but statistically significant increase in IV usage and lab tests. The values used for the alerting system were found to have no statistically significant difference in the context of different ICU wards since data from the laboratory tests serve as the primary early indicator of septic shock by confirming the presence of toxins.", 
"Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately.",A Deep Reinforced Model for Abstractive Summarization,"Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit “exposure bias” – they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", 
Deep learning has emerged as a promising technique to automatically extract features of raw MI EEG signals and then classify them. A deep transfer convolutional neural network (CNN) framework based on VGG-16 is proposed for EEG signal classification. The experimental results show that the proposed framework improves the accuracy and efficiency performance of EEG signal classifying compared with traditional methods. The proposal is based on a V GG-16 CNN model pretrained on the ImageNet and a target CNN model.,A Deep Transfer Convolutional Neural Network Framework for EEG Signal Classification,"Nowadays, motor imagery (MI) electroencephalogram (EEG) signal classification has become a hotspot in the research field of brain computer interface (BCI). More recently, deep learning has emerged as a promising technique to automatically extract features of raw MI EEG signals and then classify them. However, deep learning-based methods still face two challenging problems in practical MI EEG signal classification applications: (1) Generally, training a deep learning model successfully needs a large amount of labeled data. However, most of the EEG signal data is unlabeled and it is quite difficult or even impossible for human experts to label all the signal samples manually. (2) It is extremely timeconsuming and computationally expensive to train a deep learning model from scratch. To cope with these two challenges, a deep transfer convolutional neural network (CNN) framework based on VGG-16 is proposed for EEG signal classification. The proposed framework consists of a VGG-16 CNN model pretrained on the ImageNet and a target CNN model which shares the same structure with VGG-16 except for the softmax output layer. The parameters of the pre-trained VGG-16 CNN model are directly transferred to the target CNN model used for MI EEG signal classification. Then, front-layers parameters in the target model are frozen, while later-layers parameters are fine-tuned by the target MI dataset. The target dataset is composed of time-frequency spectrum images of EEG signals. The performance of the proposed framework is verified on the public benchmark dataset 2b from the BCI competition IV. The experimental results show that the proposed framework improves the accuracy and efficiency performance of EEG signal classification compared with traditional methods, including support vector machine (SVM), artificial neural network (ANN), and standard CNN.", 
Text summarization is the method of explicitly making a shorter version of one or more text documents. It is a significant method of detecting related material from huge text libraries or from the Internet. It's essential to extract the information in such a way that the content should be of user's interest.,A Detail Survey on Automatic Text Summarization,"the document summarization is becoming essential as lots of information getting generated every day. Instead of going through the entire text document, it is easy to understand the text document fast and easily by a relevant summary. Text summarization is the method of explicitly making a shorter version of one or more text documents. It is a significant method of detecting related material from huge text libraries or from the Internet. It is also essential to extract the information in such a way that the content should be of user’s interest. Text summarization is conducted using two main methods extractive summarization and abstractive summarization. When method select sentences from word document and rank them on basis of their weight to generate summary then that method is called extractive summarization. Abstractive summarization method focuses on main concepts of the document and then expresses those concepts in natural language. Many techniques have been developed for summarization on the basis of these two methods. There are many methods those only work for specific language. Here we discuss various techniques based on abstractive and extractive text summarization methods and shortcomings of different methods.", 
System achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts.,"A Discriminatively Trained, Multiscale, Deformable Part Model","This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.", 
Tree trimming is the problem of extracting an optimal subtree from an input tree. Previous approaches require integer linear programming (ILP) solvers to obtain exact solutions. We propose a dynamic programming (DP) algorithm for tree trimming problems whose running time is O(NL log N),A dynamic programming algorithm for tree trimming-based text summarization,"Tree trimming is the problem of extracting an optimal subtree from an input tree, and sentence extraction and sentence compression methods can be formulated and solved as tree trimming problems. Previous approaches require integer linear programming (ILP) solvers to obtain exact solutions. The problem of this approach is that ILP solvers are black-boxes and have no theoretical guarantee as to their computation complexity. We propose a dynamic programming (DP) algorithm for tree trimming problems whose running time is O(NL log N), where N is the number of tree nodes and L is the length limit. Our algorithm exploits the zero-suppressed binary decision diagram (ZDD), a data structure that represents a family of sets as a directed acyclic graph, to represent the set of subtrees in a compact form; the structure of ZDD permits the application of DP to obtain exact solutions, and our algorithm is applicable to different tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems.", 
"Signal is a new security protocol and accompanying app that provides end-to-end encryption for instant messaging. The core protocol has recently been adopted by WhatsApp, Facebook Messenger, and Google Allo. Signal includes several uncommon security properties such as ""future secrecy""",A Formal Security Analysis of the Signal Messaging Protocol,"Signal is a new security protocol and accompanying app that provides end-to-end encryption for instant messaging. The core protocol has recently been adopted by WhatsApp, Facebook Messenger, and Google Allo among many others; the first two of these have at least 1 billion active users. Signal includes several uncommon security properties (such as “future secrecy” or “post-compromise security”), enabled by a novel technique called ratcheting in which session keys are updated with every message sent. Despite its importance and novelty, there has been little to no academic analysis of the Signal protocol. We conduct the first security analysis of Signal’s Key Agreement and Double Ratchet as a multi-stage key exchange protocol. We extract from the implementation a formal description of the abstract protocol, and define a security model which can capture the “ratcheting” key update structure. We then prove the security of Signal’s core in our model, demonstrating several standard security properties. We have found no major flaws in the design, and hope that our presentation and results can serve as a starting point for other analyses of this widely adopted protocol.", 
"As content grows, effort for discriminating and filtering increases too. Orthogonally, users employ each time smaller devices with reduced screens for web reviewing. This work presents a formal method for the preparation of text summaries based on latent semantic analysis.",A formal technique for text summarization from web pages by using latent semantic analysis,"Web is the more attractive media for information consulting of, practically, whatever theme; humanity considers the Web, in the facts, the standard source of information. However as content grows, effort for discriminating and filtering increases too. Orthogonally, users employ each time smaller devices with reduced screens for web reviewing. Both considerations suggest the neediness of software tools for information acquiring and reduction, i.e., text summarization. There are several methods for text summarization, however, majority of them are based on techniques who considere plain documents in contrast with tree like structures of web pages, other are settled on the existence of keywords ignoring relations among words. In this work we present a formal method for the preparation of text summaries based on latent semantic analysis (LSA), which exploits the implicit relationships between the words that appear in a common context. In this way, text summaries are enriched with a certain semantic flavor incorporated by LSA. Furthermore we prepare the text summary induced by the query of an user and retrieving text excerpts more semantically similar to user’s interest. Additionally we define a formula called semantic similarity which encapsulates the properties of LSA and determines the best text web page node for producing summaries.", 
"A new graph model for text processing applications is proposed in this paper. It relies on four dimensions (similarity, semantic similarity, coreference, discourse information) to create the graph. The model proposed here outperforms the current approaches both quantitatively and qualitatively.",A Four Dimension Graph Model for Automatic Text Summarization,"Text summarization is the process of automatically creating a shorter version of one or more text documents. In this context, word-based, sentence-based and graph-based methods approaches are largely used. Among these, graphbased methods for automatic text summarization produce summaries based on the relationships between sentences. These relationships may also support the creation of several text processing applications such as extractive and abstractive summaries, question-answering and information retrieval systems, among others. A new graph model for text processing applications is proposed in this paper. It relies on four dimensions (similarity, semantic similarity, coreference, discourse information) to create the graph. The rationale behind the proposal presented here is resorting to more dimensions than previous works, and taking into account coreference resolution, taking into account to the role of pronouns in connecting the sentences. Coreference was not used in any previous graphbased summarization technique. An experiment was performed using the TextRank algorithm with the presented approach, on the CNN corpus. The results show that the model proposed here outperforms the current approaches both quantitatively and qualitatively.", 
"Every day 2.5 quintillion bytes of data are created worldwide. It is a complicated task to manage and extract useful information from social media platforms. We have considered Twitter as our social media platform to filter the medical data. In this article, we have used the Open Biomedical Annotator (OBA) to annotate raw text.",A Framework for Automatic Categorization of Social Data Into Medical Domains,"Nowadays, everyone is surrounded by a large volume of data which is generated from multiple sources. Every day 2.5 quintillion bytes of data are created worldwide. It is a very complicated task to manage and extract useful information from social media platforms. As people are more interested in seeking online health guidance, the medical textual data created by patients, healthcare professionals, and medical staff are expanding fast. In order to address this challenge, an ontology-based approach is used to annotate the medical domain data in this article. We have considered Twitter as our social media platform to filter the medical data from the general category data. In this article, we have used the Open Biomedical Annotator (OBA), which is an ontology-based Web service that annotates raw text with biomedical ontology concepts based on their textual metadata. Domain-specific knowledge is gathered by using MedlinePlus Health Topics and National Drug Data File (NDDF) ontologies licensed under the U. S. National Library of Medicine (UMLS), which will filter out all medical related terms. Furthermore, these medical terms are stored in a medical term database, which will then be forwarded to the UMLS REST Application Programming Interface (API) that provides important entities, such as Concept Unique Identifier (CUIs), semantic relations, attributes, definitions, and much more under the subset of SNOMEDCT_US which is the most precise and comprehensive health terminology platform in the world.", 
We propose a framework for abstractive summarization of multi-documents. It aims to select contents of summary not from source sentences but from semantic representation of source documents. Results indicate that the proposed approach performs better than other summarization systems.,A framework for multi-document abstractive summarization based on semantic role labelling,"We propose a framework for abstractive summarization of multi-documents, which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents. In this framework, contents of the source documents are represented by predicate argument structures by employing semantic role labeling. Content selection for summary is made by ranking the predicate argument structures based on optimized features, and using language generation for generating sentences from predicate argument structures. Our proposed framework differs from other abstractive summarization approaches in a few aspects. First, it employs semantic role labeling for semantic representation of text. Secondly, it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text; and finally it ranks the predicate argument structures based on features weighted by genetic algorithm (GA). Experiment of this study is carried out using DUC-2002, a standard corpus for text summarization. Results indicate that the proposed approach performs better than other summarization systems.", 
"Schizophrenia is a severe and prolonged disorder of the human brain which disturbs the behavioral characteristics of an individual completely. In this paper, a computerized approach based on optimization and classification is done to analyze the classification of schizophrenia from Electroencephalography (EEG) signals. EEG can analyze a lot of brain disorders and is used to study the diseases of the brain in an in-depth manner.",A Framework for Schizophrenia EEG Signal Classification With Nature Inspired Optimization Algorithms,"One of the severe and prolonged disorder of the human brain which disturbs the behavioral characteristics of an individual completely such as interruption in the thinking process and speech is schizophrenia. It is a manifestation of many symptoms such as hallucinations, functional deterioration, disorganized speech and hearing sounds and speeches that are non-existent. In this paper, a computerized approach based on optimization and classification is done to analyze the classification of schizophrenia from Electroencephalography (EEG) signals. As EEG can analyze a lot of brain disorders and is used to study the diseases of the brain in an in-depth manner, it can be used to analyze the schizophrenia EEG signals. In this paper, three feature extraction techniques are employed such as Partial Least Squares (PLS) Non linear Regression technique, Expectation Maximization based Principal Component Analysis (EM-PCA) technique and Isometric Mapping (Isomap) technique. The extracted features are further optimized with four optimization algorithms such as Flower Pollination algorithm, Eagle strategy using different evolution algorithm, Backtracking search optimization algorithm and Group search optimization algorithm. The optimized values are then classified with varied versions of both Adaboost classifier and Naïve Bayesian Classifier. The individual results show that for normal cases, Isomap features when optimized with Backtracking search optimization algorithm and classified with Modest Adaboost classifier, a classification accuracy of 98.77% is obtained. The individual results show that for schizophrenia case, when Isomap features are optimized with Flower Pollination optimization algorithm and classified with Real Adaboost classifier, a classification accuracy of 98.77% is obtained.", 
Text summarization is a process of producing a concise version of text from one or more information sources. The most commonly used automatic evaluation metrics like Recall-Oriented Understudy for Gisting Evaluation (ROUGE) strictly rely on the overlapping n-gram units between reference and candidate summaries. This paper proposes a word embedding based automatic text summarization and evaluation framework.,A Framework for Word Embedding Based Automatic Text Summarization and Evaluation,"Text summarization is a process of producing a concise version of text (summary) from one or more information sources. If the generated summary preserves meaning of the original text, it will help the users to make fast and effective decision. However, how much meaning of the source text can be preserved is becoming harder to evaluate. The most commonly used automatic evaluation metrics like Recall-Oriented Understudy for Gisting Evaluation (ROUGE) strictly rely on the overlapping n-gram units between reference and candidate summaries, which are not suitable to measure the quality of abstractive summaries. Another major challenge to evaluate text summarization systems is lack of consistent ideal reference summaries. Studies show that human summarizers can produce variable reference summaries of the same source that can significantly affect automatic evaluation metrics scores of summarization systems. Humans are biased to certain situation while producing summary, even the same person perhaps produces substantially different summaries of the same source at different time. This paper proposes a word embedding based automatic text summarization and evaluation framework, which can successfully determine salient top-n sentences of a source text as a reference summary, and evaluate the quality of systems summaries against it. Extensive experimental results demonstrate that the proposed framework is effective and able to outperform several baseline methods with regard to both text summarization systems and automatic evaluation metrics when tested on a publicly available dataset.", 
"A frequent term based text summarization algorithm is designed and implemented in java. The algorithm works in three steps. The designed algorithm is implemented using open source technologies like java, DISCO, Porters stemmer etc. and verified over the standard text mining corpus.",A frequent term and semantic similarity based single document text summarization algorithm,"Text summarization is an important activity in the analysis of a high volume text documents. Text summarization has number of applications; recently number of applications uses text summarization for the betterment of the text analysis and knowledge representation. In this paper a frequent term based text summarization algorithm is designed and implemented in java. The designed algorithm works in three steps. In the first step the document which is required to be summarized is processed by eliminating the stop word and by applying the stemmers. In the second step term-frequent data is calculated from the document and frequent terms are selected, for these selected words the semantic equivalent terms are also generated. Finally in the third step all the sentences in the document, which are containing the frequent and semantic equivalent terms, are filtered for summarization. The designed algorithm is implemented using open source technologies like java, DISCO, Porters stemmer etc. and verified over the standard text mining corpus.", 
This paper introduces Common Spatial Patterns (CSP) as a step-by-step filter optimization algorithm. It then proposes a generalized type of the CSP which is not limited in a specific optimization constraint. The proposed algorithms are able to be more directly aimed at achieving better accuracy and stability.,A general framework to estimate spatial and spatio-spectral filters for EEG signal classification,"In this paper, a general framework is proposed for simultaneous design of spatial and spectral filters, which are used to extract discriminant features from EEG signals in Brain Computer Interfacing (BCI) systems. This paper introduces Common Spatial Patterns (CSP) as a step-by-step filter optimization algorithm, and then proposes a generalized type of the CSP which is not limited in a specific optimization constraint. Moreover, it is shown that how this generalization can be extended to a spatio-spectral filter estimation scheme. Then, two specific versions of the generalized CSP are proposed, where a specific target function and optimization constraint are used for estimating the spatial and spectral filters. Unlike the traditional CSP which is not very closely linked to the classification accuracy, the proposed algorithms are able to be more directly aimed at achieving better accuracy and stability. Experimental results obtained from applying the introduced methods on the recorded imagery signals from two datasets, demonstrate considerable improvement in the classification accuracy and stability compared to the standard CSP and other similar methods.", 
This paper proposes an extractive multi-document text summarization model based on genetic algorithm (GA) Results clarify the effectiveness of the proposed model when compared with another state-of-the-art model. Experiments are applied to ten topics from Document Understanding Conference DUC2002 datasets.,A genetic based optimization model for extractive multi-document text summarization,"Extractive multi-document text summarization – a summarization with the aim of removing redundant information in a document collection while preserving its salient sentences – has recently enjoyed a large interest in proposing automatic models. This paper proposes an extractive multi-document text summarization model based on genetic algorithm (GA). First, the problem is modeled as a discrete optimization problem and a specific fitness function is designed to effectively cope with the proposed model. Then, a binary-encoded representation together with a heuristic mutation and a local repair operators are proposed to characterize the adopted GA. Experiments are applied to ten topics from Document Understanding Conference DUC2002 datasets (d061j through d070f). Results clarify the effectiveness of the proposed model when compared with another state-of-the-art model.", 
"We introduce a model for extractive meeting summarization based on the hypothesis that utterances convey bits of information, or concepts. We use keyphrases as concepts weighted by frequency, and an integer linear program to determine the best set of utterances.",A global optimization framework for meeting summarization,"We introduce a model for extractive meeting summarization based on the hypothesis that utterances convey bits of information, or concepts. Using keyphrases as concepts weighted by frequency, and an integer linear program to determine the best set of utterances, that is, covering as many concepts as possible while satisfying a length constraint, we achieve ROUGE scores at least as good as a ROUGEbased oracle derived from human summaries. This brings us to a critical discussion of ROUGE and the future of extractive meeting summarization.", 
"Twitter is a very popular online social networking site, where hundreds of millions of tweets are posted every day by millions of users. Many of the tweets are likely to contain semantically identical information. It is difficult to read all the tweets containing identical or redundant information. We propose a graph-based approach for summarizing tweets.",A graph based clustering technique for tweet summarization,"Twitter is a very popular online social networking site, where hundreds of millions of tweets are posted every day by millions of users. Twitter is now considered as one of the fastest and most popular communication mediums, and is frequently used to keep track of recent events or news-stories. Whereas tweets related to a particular event / news-story can easily be found using keyword matching, many of the tweets are likely to contain semantically identical information. If a user wants to keep track of an event / news-story, it is difficult for him to have to read all the tweets containing identical or redundant information. Hence, it is desirable to have good techniques to summarize large number of tweets. In this work, we propose a graph-based approach for summarizing tweets, where a graph is first constructed considering the similarity among tweets, and community detection techniques are then used on the graph to cluster similar tweets. Finally, a representative tweet is chosen from each cluster to be included into the summary. The similarity among tweets is measured using various features including features based on WordNet synsets which help to capture the semantic similarity among tweets. The proposed approach achieves better performance than Sumbasic, an existing summarization technique.", 
This work presents an extraction based automatic text summarization algorithm. The methodology proposed involves constructing of a directed weighted graph out of the original text. A ranking algorithm is used to compute the most important sentences in the text.,A graph based ranking strategy for automated text summarization,Text summarization is a process of capturing the idea and line of thought from an original text and inculcating the same into a short coherent text. Automated text summarization aims to meet this objective of retaining all the key ideas instilled in the text while skipping upon the redundant and repetitive bits of information. The reduced text thus compiled must be coherent in itself in order to meet the semantic and syntactic organization of the language. This work presents an extraction based automatic text summarization algorithm. The methodology proposed involves constructing of a directed weighted graph out of the original text wherein each sentences is taken to be a node. The weights for each of the edges are determined by using a suitable distortion measure which analyses the semantic relation between the two adjacent nodes / sentences. A ranking algorithm is used to compute the most important sentences in the text and that should be present in the summary based on the weighted graph. This technique has been employed on multiple data sets and has performed well on the evaluation parameters laid down for such applications., 
"The Internet of Medical Things (IoMT) has an important role to play in reducing the mortality rate by the early detection of disease. Many studies have focused on heart disease diagnosis, but the accuracy of the findings is low. The proposed MSSO-ANFIS prediction model obtains an accuracy of 99.45 with a precision of 96.54, which is higher than the other approaches.",A Healthcare Monitoring System for the Diagnosis of Heart Disease in the IoMT Cloud Environment Using MSSO-ANFIS,"The IoT has applications in many areas such as manufacturing, healthcare, and agriculture, to name a few. Recently, wearable devices have become popular with wide applications in the health monitoring system which has stimulated the growth of the Internet of Medical Things (IoMT). The IoMT has an important role to play in reducing the mortality rate by the early detection of disease. The prediction of heart disease is a key issue in the analysis of clinical dataset. The aim of the proposed investigation is to identify the key characteristics of heart disease prediction using machine learning techniques. Many studies have focused on heart disease diagnosis, but the accuracy of the findings is low. Therefore, to improve prediction accuracy, an IoMT framework for the diagnosis of heart disease using modified salp swarm optimization (MSSO) and an adaptive neuro-fuzzy inference system (ANFIS) is proposed. The proposed MSSO-ANFIS improves the search capability using the Levy flight algorithm. The regular learning process in ANFIS is dependent on gradient-based learning and has a tendency to become trapped in local minima. The learning parameters are optimized utilizing MSSO to provide better results for ANFIS. The following information is taken from medical records to predict the risk of heart disease: blood pressure (BP), age, sex, chest pain, cholesterol, blood sugar, etc. The heart condition is identified by classifying the received sensor data using MSSO-ANFIS. A simulation and analysis is conducted to show that MSSA-ANFIS works well in relation to disease prediction. The results of the simulation demonstrate that the MSSO-ANFIS prediction model achieves better accuracy than the other approaches. The proposed MSSO-ANFIS prediction model obtains an accuracy of 99.45 with a precision of 96.54, which is higher than the other approaches.", 
"In this paper, we select 14 important clinical features, i.e., age, sex, chest pain type, cholesterol, fasting blood sugar, resting ecg, max heart rate, exercise induced angina, old peak, slope, number of vessels colored, thal and diagnosis of heart disease. We develop a prediction model using J48 decision tree for classifying heart disease.",A Heart Disease Prediction Model using Decision Tree,"In this paper, we develop a heart disease prediction model that can assist medical professionals in predicting heart disease status based on the clinical data of patients. Firstly, we select 14 important clinical features, i.e., age, sex, chest pain type, trestbps, cholesterol, fasting blood sugar, resting ecg, max heart rate, exercise induced angina, old peak, slope, number of vessels colored, thal and diagnosis of heart disease. Secondly, we develop an prediction model using J48 decision tree for classifying heart disease based on these clinical features against unpruned, pruned and pruned with reduced error pruning approach.. Finally, the accuracy of Pruned J48 Decision Tree with Reduced Error Pruning Approach is more better then the simple Pruned and Unpruned approach. The result obtained that which shows that fasting blood sugar is the most important attribute which gives better classification against the other attributes but its gives not better accuracy.", 
Bangla is one of the most taught and used language all over the world. Summarized text is the concise form of the given text. This paper deal with the summarization of Bangla text based on extractive method. A new efficient extractive summarization method is proposed in this work.,A heuristic approach of text summarization for Bengali documentation,"Automated Text Summarization is a technique of summarizing any document or text automatically. Summarized text is the concise form of the given text. In Natural language processing many text summarization techniques are available for English language, but only a few for Bangla language. Bangla is one of the most taught and used language all over the world. Most of the text summarization techniques are implemented in two different ways, known as abstractive or extractive approach. This paper deal with the summarization of Bangla text based on extractive method. A new efficient extractive summarization method is proposed in this work. The other summarization tools developed for Bangla language seems not much appropriate from application point of view. The proposed analysis models are applicable for Bangla text summarization. In the proposed approach, basic extractive summarization is applied with new proposed model and a set of Bangla text analysis rules derived from the heuristics. Every Bangla sentences and words from original text is analyzed properly with Bangla sentence clustering method. This work proposed a new type of sentence scoring processes for Bangla text summarization. In the evaluation of this technique, the system reflects good accuracy of results, comparing to that of the human generated summarized result and other Bangla text summarization tools.", 
"Configuration management tools (CMTs) are indispensable software for DevOps (Development and Operations) CMTs automate system deployment and configuration through CMT modules, reusable, shareable units of configuration code. There is no hierarchical categorization in all CMT repositories, which can limit the search scope in specified categories.",A Hierarchical Categorization Approach for Configuration Management Modules,"Configuration management tools, CMTs for short, are a set of indispensable software for DevOps (Development and Operations). CMTs automate system deployment and configuration through CMT modules, which are reusable, shareable units of configuration code. Therefore, thousands of CMT modules have been developed for various systems, and are still growing fast. Although CMT repositories usually provide keyword- and tag- based search, a large number of search results could prevent users from finding desired CMT modules. CMT modules could be managed in a hierarchical categorization, which can limit the search scope in specified categories, and thus help to improve search performance. Unfortunately, there is no hierarchical categorization in all CMT repositories. In this paper, we propose a hierarchical categorization approach for CMT modules. Our approach first extracts frequentlyused module tags as categories, and constructs the category hierarchy by mining the hierarchical relations among tags. We leverage online module profiles (names, descriptions and tags) as source information to do categorization. It trains a set of classifiers by taking TF-IDF (term frequency-inverse document frequency) of module profiles as features. Finally, our evaluation on more than 11,000 CMT modules shows that our approach could obtain 90 fine-grained and multi-layered categories, and does categorization for CMT modules with high precision (0.81), recall (0.88) and F-Measure (0.85).", 
Text summarization and sentiment classification both aim to capture the main ideas of the text but at different levels. We propose a hierarchical end-to-end model for joint learning of both. Experimental results on Amazon online reviews datasets show it achieves better performance than the strong baseline systems.,A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification,"Text summarization and sentiment classification both aim to capture the main ideas of the text but at different levels. Text summarization is to describe the text within a few sentences, while sentiment classification can be regarded as a special type of summarization which “summarizes” the text into a even more abstract fashion, i.e., a sentiment class. Based on this idea, we propose a hierarchical endto-end model for joint learning of text summarization and sentiment classification, where the sentiment classification label is treated as the further “summarization” of the text summarization output. Hence, the sentiment classification layer is put upon the text summarization layer, and a hierarchical structure is derived. Experimental results on Amazon online reviews datasets show that our model achieves better performance than the strong baseline systems on both abstractive summarization and sentiment classification.", 
"The approach incorporates domain knowledge, statistical features, and genetic algorithms to extract important points of Arabic political documents. The (AS DKGA) approach demonstrated promising results when summarizing Arab political documents with average F-measure of 0.605 at the compression ratio of 40%.",A Hybrid Approach for Arabic Text Summarization Using Domain Knowledge and Genetic Algorithms,"Text summarization is the process of producing a shorter version of a specific text. Automatic summarization techniques have been applied to various domains such as medical, political, news, and legal domains proving that adapting domain-relevant features could improve the summarization performance. Despite the existence of plenty of research work in the domain-based summarization in English and other languages, there is a lack of such work in Arabic due to the shortage of existing knowledge bases. In this paper, a hybrid, single-document text summarization approach (abbreviated as (ASDKGA)) is presented. The approach incorporates domain knowledge, statistical features, and genetic algorithms to extract important points of Arabic political documents. The ASDKGA approach is tested on two corpora KALIMAT corpus and Essex Arabic Summaries Corpus (EASC). The Recall-Oriented Understudy for Gisting Evaluation (ROUGE) framework was used to compare the automatically generated summaries by the ASDKGA approach with summaries generated by humans. Also, the approach is compared against three other Arabic text summarization approaches. The (ASDKGA) approach demonstrated promising results when summarizing Arabic political documents with average F-measure of 0.605 at the compression ratio of 40%.", 
"Automatic text summarization comes under the domains of natural language processing, machine learning and information retrieval. The proposed method is validated through experiments and the results are promising.",A hybrid approach for automatic document summarization,"Automatic text summarization come under the domains of natural language processing, machine learning and information retrieval. As the abundance of textual information grows so does the need for summarising it. Here we consider a method for single document summarization using natural language processing tools and an extractive approach based on sentence similarity and document context. The technique uses a weighted undirected graph based scoring on paragraphs and a word frequency based scoring system on the entire document to obtain summaries. The proposed method is validated through experiments and the results are promising.", 
Text summarization is a process of extracting content from a document and generating summary of that document. It is an active research topic in other fields like natural language processing and machine learning. This method is based on Support- Vector-Machine (SVM).,A hybrid approach for extractive document summarization using machine learning and clustering technique,"Usually, presence of the same information in multiple documents is the main problem faced in effective information access. Instead of this redundant information thus accessed or retrieved, users are interested in retrieving information that addresses one or other several aspects. In such situation, text summarization proves to be very useful. Not only in Information retrieval, but it is an extremely active research topic in other fields like natural language processing and machine learning. Text summarization is a process of extracting content from a document and generating summary of that document thus presenting important content to user in a relatively condensed form. In this paper, study of several extractive text summarization approaches is made and an effective text summarization method is proposed. This method is based on Support-Vector-Machine (SVM). Proposed system tries to improve the performance and quality of the summary generated by the clustering technique by cascading it with SVM.", 
"Self Organizing Maps (SOM) is an unsupervised method and Artificial Neural Networks (ANN) is a supervised method. Hybrid model uses Stochastic Gradient Descent update set of parameters in an iterative manner to minimize the cost function. This novel method has been implemented on different documents, which are publicly available on Opinosis Dataset.",A Hybrid Approach of Text Summarization Using Latent Semantic Analysis and Deep Learning,"In the current scenario of Information Technology, excessive and vast information is available on online resources but it is not always easy to find relevant and useful information. Along this issue, the paper is presented a method on extractive single document text summarization using Deep Learning method - SelfOrganizing Maps (SOM) which is an unsupervised method and Artificial Neural Networks (ANN) which is a supervised method. The work involves investigating the effect of adding mapped sentences from SOM visualization, and re-training the inputs on ANN for ranking the sentences. In individual experiment of the hybrid model, a different mapping of SOM is added to the ANN network as input vector. Hybrid model uses Stochastic Gradient Descent update set of parameters in an iterative manner to minimize the cost function. In addition, using back-propagation weight is being adjusted for the input vector. The empirical results show that the hybrid model using mapping clearly provides a comprehensive result and improves the F-score on average 5% on ROUGE-1, ROUGE-2, ROUGE-L and ROUGE-SU4. This novel method has been implemented on different documents, which are publicly available on Opinosis Dataset. The ROUGE toolkit has been used to evaluate summaries which are generated from the proposed model and other existing algorithms versus human generated summary.", 
"Research focuses on developing a hybrid automatic text summarization approach, KCS. KCS employs the K-mixture probabilistic model to establish term weights in a statistical sense. Sentences are ranked and extracted based on their connective strength (CS) values.",A hybrid approach to automatic text summarization,"Automatic text summarization is to compress an original document into an abridged version by extracting almost all of the essential concepts with text mining techniques. This research focuses on developing a hybrid automatic text summarization approach, KCS, to enhancing the quality of summaries. KCS employs the K-mixture probabilistic model to establish term weights in a statistical sense, and further identifies the term relationships to derive the connective strength (CS) of nouns. Sentences are ranked and extracted based on their CS values. We conduct two experiments to justify the proposed approach. The quality of extracted summary is examined by its capability of increasing text classification accuracy. The results show that our proposed approach, KCS, performs best among all approaches considered. It implies that KCS can extract more representative sentences from the document and its feasibility in text summarization applications is thus justified.", 
"Starlet-H is a hybrid summarizer that combines natural language generation and salient sentence selection techniques. It receives as input textual reviews with associated rated topics and produces as output a natural language document. Using extractive summarization techniques, it selects salient quotes from the input reviews.",A hybrid approach to multi-document summarization of opinions in reviews,"We present a hybrid method to generate summaries of product and services reviews by combining natural language generation and salient sentence selection techniques. Our system, STARLET-H, receives as input textual reviews with associated rated topics, and produces as output a natural language document summarizing the opinions expressed in the reviews. STARLET-H operates as a hybrid abstractive/extractive summarizer: using extractive summarization techniques, it selects salient quotes from the input reviews and embeds them into an automatically generated abstractive summary to provide evidence for, exemplify or justify positive or negative opinions. We demonstrate that, compared to extractive methods, summaries generated with abstractive and hybrid summarization approaches are more readable and compact.", 
"We present a hybrid approach to the problem of Arabic text summarization. Our approach focuses on segment extraction and ranking using heuristic methods. We use a tokenizer, a stemmer and other statistical tools borrowed from traditional information retrieval. The summarization system was tested by 1200 human evaluators.",A Hybrid Arabic Text Summarization Technique Based on Text Structure and Topic Identification,"We present a hybrid approach to the problem of Arabic text summarization. Our approach focuses on segment extraction and ranking using heuristic methods that assign weighted scores to segments of text. Also, we use a text categorization system and the Arabic WordNet to identify the thematic structure of the input text in order to select the most relevant sentences obtained from the statistical analysis process. We use a tokenizer, a stemmer and other statistical tools borrowed from traditional information retrieval to identify relevant segments in the text. The source document is segmented into its major units (title, paragraphs and lines) and then, text-lines are interpreted to extract relevant segments for inclusion in the summary. The summarization system was tested by 1200 human evaluators, who were each given a copy of a newspaper article and a system-generated summary and asked to classify them as “rejected,” ”not-related,” “satisfactory,” “good,” or “accepted.” 76.92% of the summaries were judged to be “good” or “accepted” and 92.34% were judged to be “satisfactory,” or “good,” or “accepted.” These results confirm the viability of using this hybrid approach to tackle the problem of Arabic text summarization.", 
"Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem. We calculate scores for sentences in document clusters based on their latent characteristics.",A Hybrid Hierarchical Model for Multi-Document Summarization,"Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ?7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.", 
"This work proposes an approach that uses statistical tools to improve content selection in multi-document automatic text summarization. The method uses a trainable summarizer, which takes into account several features. The effectiveness of this technique is measured using the ROUGE score.",A hybrid machine learning model for multi-document summarization,"This work proposes an approach that uses statistical tools to improve content selection in multi-document automatic text summarization. The method uses a trainable summarizer, which takes into account several features: the similarity of words among sentences, the similarity of words among paragraphs, the text format, cue-phrases, a score related to the frequency of terms in the whole document, the title, sentence location and the occurrence of non-essential information. The effect of each of these sentence features on the summarization task is investigated. These features are then used in combination to construct text summarizer models based on a maximum entropy model, a naive-Bayes classifier, and a support vector machine. To produce the final summary, the three models are combined into a hybrid model that ranks the sentences in order of importance. The performance of this new method has been tested using the DUC 2002 data corpus. The effectiveness of this technique is measured using the ROUGE score, and the results are promising when compared with some existing techniques.", 
Automatic text summarization is aimed to create a brief outline of a given text covering the important points in the text. It can be generic or query specific. This paper attempts to combine the individual results of various methods to give a better assessment of the relationship between the sentences.,A hybrid method for query based automatic summarization system,Automatic text summarization is one of the research goals of Natural Language Processing which relieves humans from studying each and every line in a text document to understand the underlying concepts in it. Automatic text summarization is aimed to create a brief outline of a given text covering the important points in the text. Automatic text summarization can be generic or query specific. This paper is focused on Query specific text summarization where a summary of the given text is constructed based on the given query. Query specific text summarization is based on the calculation of the relationship between sentences in the text document and the query given. Several statistical techniques and linguistic techniques have been developed to find the relationship between the given query and the sentences in the document. These methods when used alone could not give desired accuracy in the results. In this paper a sentence scoring method is defined based on existing sentence scoring methods. It attempts to combine the individual results of these methods to give a better assessment of the relationship between the sentences., 
This paper aims to generate text summarization by combining TF-IDF and Text Rank algorithm. It aims to achieve better results than the existing methods. The summary is evaluated using the Rouge metric.,A Hybrid Model for Summarizing Text Documents Using Text Rank Algorithm and Term Frequency,"The amount of data available online in today's world is largely unstructured and very difficult to summarize the entire content and change it into the appropriate form of information. The volume of unstructured data is demanding the need for automatic systems which are capable of compacting information from different records into a simple, concise description. This paper aims to generate text summarization by combining TF-IDF and Text Rank algorithm to achieve better results than the existing methods. The summary is evaluated using the Rouge metric.", 
Vehicular ad-hoc Networks (VANETs) are predominantly gaining higher attention in the recent times both in the academic and industrial feld. The open communication environment of VANET enables the vehicle drivers to access the ubiquitous services.,A Hybrid Security Framework for the Vehicular Communications in VANET,"Vehicular ad-hoc Networks (VANETs) are predominantly gaining higher attention in the recent times both in the academic and industrial feld. The vehicular communications that intend to take place in the wireless medium support several safety applications of the vehicle passengers. The open communication environment of VANET enables the vehicle drivers to access the ubiquitous services which have become one of the main reasons for the increase of security vulnerabilities on vehicular communications. As a result, this paper emphasizes the need of securing vehicular communications in VANET and proposes a hybrid security framework. The novel approach of this framework lies in combining RSA and AES algorithms to form hybrid cryptography and this hybrid cryptography is used to secure the vehicular communications in VANET. The simulation results have validated the performance and the security levels of this hybrid cryptographic method when applied to VANET.", 
"5G driven VANETs need flexible and scalable resource allocation strategies. Current networks are designed with fixed resource allocation to a cell regardless of traffic conditions. The proposed approach will help network service providers to implement a customer-centric network infrastructure, depending on dynamic customer needs.",A Hybrid-Fuzzy Logic Guided Genetic Algorithm (H-FLGA) Approach for Resource Optimization in 5G VANETs,"To support diversified quality of service demands and dynamic resource requirements of users in 5G driven VANETs, network resources need flexible and scalable resource allocation strategies. Current heterogeneous vehicular networks are designed and deployed with a connection-centric mindset with fixed resource allocation to a cell regardless of traffic conditions, static coverage, and capacity. In this paper, we propose a hybrid-fuzzy logic guided genetic algorithm(H-FLGA) approach for the software defined networking controller, to solve a multi-objective resource optimization problem for 5G driven VANETs. Realizing the service oriented view, the proposed approach formulates five different scenarios of network resource optimization in 5G VANETs. Furthermore, the proposed fuzzy inference system is used to optimize weights of multi-objectives, depending on the type of service requirements of customers. The proposed approach shows the minimized value of multi-objective cost function when compared with the GA. The simulation results show the minimized value of end-to-end delay as compared to other schemes. The proposed approach will help the network service providers to implement a customer-centric network infrastructure, depending on dynamic customer needs of users.", 
"With the rapid growth of the World Wide Web, information overload is becoming a problem for an increasingly large number of people. There is a need for an effective and powerful tool that can automatically summarize text. In this paper, we present a keyphrase based approach to single document summarization.",A Keyphrase-Based Approach to Text Summarization for English and Bengali Documents,"With the rapid growth of the World Wide Web, information overload is becoming a problem for an increasingly large number of people. Since summarization helps human to digest the main contents of a text document very rapidly, there is a need for an effective and powerful tool that can automatically summarize text. In this paper, we present a keyphrase based approach to single document summarization that extracts frst a set of keyphrases from a document, use the extracted keyphrases to choose sentences from the document and fnally form an extractive summary with the chosen sentences. We view keyphrases (single or multi-word) as the important concepts and we assume that an extractive summary of a document is an elaboration of the important concepts contained in the document to some permissible extent and it is controlled by the given summary length. We have tested our proposed keyphrase-based summarization approach on two different datasets: one for English and another for Bengali. The experimental results show that the performance of the proposed system is comparable to some state-of-the art summarization systems.", 
This paper describes an efficient algorithm for language independent generic extractive summarization for single document. The algorithm is based on structural and statistical (rather than semantic) factors. We show that the method performs equally well regardless of the language.,A language independent approach to multilingual text summarization,"This paper describes an efficient algorithm for language independent generic extractive summarization for single document. The algorithm is based on structural and statistical (rather than semantic) factors. Through evaluations performed on a single-document summarization for English, Hindi, Gujarati and Urdu documents, we show that the method performs equally well regardless of the language. The algorithm has been applied on DUC data for English documents and various newspaper articles for other languages with corresponding stop words list and modified stemmer. The results of summarization have been compared with DUC 2002 data using degree of representativeness. For other languages, the degree of representativeness we get is highly encouraging.", 
"EEG is the most common signal source for noninvasive BCI applications. In such applications, the EEG signal needs to be decoded and translated into appropriate actions. A recently emerging EEG decoding approach is deep learning with Convolutional or Recurrent Neural Networks (CNNs, RNNs) Here we present a novel framework for the large-scale evaluation of different deep-learning architectures on different EEG datasets.",A large-scale evaluation framework for EEG deep learning architectures,"EEG is the most common signal source for noninvasive BCI applications. For such applications, the EEG signal needs to be decoded and translated into appropriate actions. A recently emerging EEG decoding approach is deep learning with Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many different architectures already published. Here we present a novel framework for the large-scale evaluation of different deep-learning architectures on different EEG datasets. This framework comprises (i) a collection of EEG datasets currently including 100 examples (recording sessions) from six different classification problems, (ii) a collection of different EEG decoding algorithms, and (iii) a wrapper linking the decoders to the data as well as handling structured documentation of all settings and (hyper-) parameters and statistics, designed to ensure transparency and reproducibility. As an applications example we used our framework by comparing three publicly available CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow ConvNet, and two versions of EEGNet. We also show how our framework can be used to study similarities and differences in the performance of different decoding methods across tasks. We argue that the deep learning EEG framework as described here could help to tap the full potential of deep learning for BCI applications.", 
Matching texts in highly inflected languages such as Arabic by simple stemming strategy is unlikely to perform well. We present a strategy for automatic text matching technique for for inflectional languages.,A Lemma Based Evaluator for Semitic Language Text Summarization Systems,"Matching texts in highly inflected languages such as Arabic by simple stemming strategy is unlikely to perform well. In this paper, we present a strategy for automatic text matching technique for for inflectional languages, using Arabic as the test case. The system is an extension of ROUGE test in which texts are matched on token's lemma level. The experimental results show an enhancement of detecting similarities between different sentences having same semantics but written in different lexical forms.", 
"Word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques.",A LITERATURE STUDY OF EMBEDDINGS ON SOURCE CODE,"Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.", 
"Long Short-Term Memory (LSTM) networks are introduced in epileptic seizure prediction, expanding the use of deep learning algorithms with convolutional neural networks. The evaluation is performed using long-term EEG recordings from the open CHB-MIT Scalp EEG database. The proposed methodology is able to predict all 185 seizures.",A Long Short-Term Memory deep learning network for the prediction of epileptic seizures using EEG signals,"The electroencephalogram (EEG) is the most prominent means to study epilepsy and capture changes in electrical brain activity that could declare an imminent seizure. In this work, Long Short-Term Memory (LSTM) networks are introduced in epileptic seizure prediction using EEG signals, expanding the use of deep learning algorithms with convolutional neural networks (CNN). A pre-analysis is initially performed to find the optimal architecture of the LSTM network by testing several modules and layers of memory units. Based on these results, a two-layer LSTM network is selected to evaluate seizure prediction performance using four different lengths of preictal windows, ranging from 15 min to 2 h. The LSTM model exploits a wide range of features extracted prior to classification, including time and frequency domain features, between EEG channels cross-correlation and graph theoretic features. The evaluation is performed using long-term EEG recordings from the open CHB-MIT Scalp EEG database, suggest that the proposed methodology is able to predict all 185 seizures, providing high rates of seizure prediction sensitivity and low false prediction rates (FPR) of 0.11–0.02 false alarms per hour, depending on the duration of the preictal window. The proposed LSTM-based methodology delivers a significant increase in seizure prediction performance compared to both traditional machine learning techniques and convolutional neural networks that have been previously evaluated in the literature.", 
"In this paper, we present an approach that leverages neural language models and deep learning techniques in combination with standard classification approaches for product matching and categorization. We use structured product data as supervision for training feature extraction models. To minimize the need for lots of data for supervision, we use neural word embeddings from large quantities of publicly available product data.",A method for automatic text summarization based on rhetorical analysis and topic modeling,"This article describes the original method of automatic summarization of scientific and technical texts based on rhetorical analysis and using topic modeling. The proposed method combines the use of a linguistic knowledge base and machine learning. For the detection of key terms, we used topic modeling. First, unigram topic models containing only one-word terms are constructed. Further, these models are extended by adding multiword terms. The most significant fragments of the original document are determined in the process of rhetorical analysis with the help of discursive markers. When evaluating the importance of text fragments, keywords, multiword terms, and scientific lexicon characterizing scientific and technical texts are also taken into account. A linguistic knowledge base has been created to store information about the markers and scientific lexicon. The experiments showed that this method is effective, needs a comparatively small amount of training data and can be adapted to processing texts of different subject fields in other languages.", 
Four modern systems of automatic text summarization are tested on the basis of a model vocabulary composed by subjects.,A method for evaluating modern systems of automatic text summarization,Four modern systems of automatic text summarization are tested on the basis of a model vocabulary composed by subjects. Distribution of terms of the vocabulary in the source text is compared with their distribution in summaries of different length generated by the systems. Principles for evaluation of the efficiency of the current systems of automatic text summarization are described., 
"In this paper, a semantic relatedness based query focused text summarization technique is introduced to find relevant information from a single text document. The method extracts the related sentences according to the query.",A Method for Semantic Relatedness Based Query Focused Text Summarization,"In this paper, a semantic relatedness based query focused text summarization technique is introduced to find relevant information from single text document. This semantic relatedness measure extracts the related sentences according to the query. The query focused text summarization approach can work on short query when the query does not contain enough information. Better summaries are produced by this method with increased number of query related sentences included. Experiments and evaluation are done on DUC 2005 and 2006 datasets and results show significant performance.", 
The aim of this research is to stem words from Persian documents to make their use more efficient in text summarization. Compound of existing techniques in the words network was used to create a Persian database. The algorithm used for summarization is based on statistical techniques.,A method for stemming and eliminating common words for Persian text summarization,"tract: With high increasing documents and electronic texts in Persian language, the use of fast methods to achieve texts through huge sets of documents is highly crucial. Persian text summarization which shows the main concept of a text in minimum size is an effective solution. One of the steps in Persian text summarization is to stem and eliminate common words. The aim of this research is to stem words from Persian documents to make their use more efficient in text summarization, the present method is to eliminate words and stem keywords. The compound of existing techniques in the words network was used to create a Persian database using the Dehkhoda dictionary. The algorithm used for summarization is based on statistical techniques. In this method each sentence is given an important weight, sentences with higher weight are used for summarization. By comparing the results of other algorithms on Persian texts we concluded that our technique extracts the root of the existing words with more precision.", 
"This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment. It uses a new method for measuring agreement, Relevance-Prediction, which compares subjects' judgments on summaries with their own judgments on full text documents.","A methodology for extrinsic evaluation of text summarization, does ROUGE correlate","This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement, Relevance-Prediction, which compares subjects’ judgments on summaries with their own judgments on full text documents. We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the benefits of summarization. We found positive correlations between ROUGE scores and two different summary types, where only weak or negative correlations were found using other agreement measures. However, we show that ROUGE may be sensitive to the choice of summarization style. We discuss the importance of these results and the implications for future summarization evaluations.", 
The task of automatic misogyny identification and categorization has not received as much attention as other natural language tasks. This work addresses this sentence classification task from a representation learning perspective. We set new state-of-the-art for the task with our finetuned BERT. All our code is open source for easy reproducibility.,A Metric Learning Approach to Misogyny Categorization,"The task of automatic misogyny identification and categorization has not received as much attention as other natural language tasks have, even though it is crucial for identifying hate speech in social Internet interactions. In this work, we address this sentence classification task from a representation learning perspective, using both a bidirectional LSTM and BERT optimized with the following metric learning loss functions: contrastive loss, triplet loss, center loss, congenerous cosine loss and additive angular margin loss. We set new state-of-the-art for the task with our finetuned BERT, whose sentence embeddings can be compared with a simple cosine distance, and we release all our code as open source for easy reproducibility. Moreover, we find that almost every loss function performs equally well in this setting, matching the regular cross entropy loss.", 
"A two-stage sentences selection method for text summarization is proposed. To discover all topics the sentences set is clustered by using k-means method. At the second stage, optimum selection of sentences is proposed to cover all topics.",A Model for Text Summarization,"Text summarization is a process for creating a concise version of document(s) preserving its main content. In this paper, to cover all topics and reduce redundancy in summaries, a two-stage sentences selection method for text summarization is proposed. At the first stage, to discover all topics the sentences set is clustered by using k-means method. At the second stage, optimum selection of sentences is proposed. From each cluster the salient sentences are selected according to their contribution to the topic (cluster) and their proximity to other sentences in cluster to avoid redundancy in summaries until the appointed summary length is reached. Sentence selection is modeled as an optimization problem. In this study, to solve the optimization problem an adaptive differential evolution with novel mutation strategy is employed. With a test on benchmark DUC2001 and DUC2002 data sets, the ROUGE value of summaries got by the proposed approach demonstrated its validity, compared to the traditional methods of sentence selection and the top three performing systems for DUC2001 and DUC2002.", 
"The paper lays emphasis on TextRank algorithm, a graph based approach used to tackle the automatic article summarization problem.",A Modification to Graph Based Approach for Extraction Based Automatic Text Summarization,"The paper lays emphasis on TextRank algorithm, a graph based approach used to tackle the automatic article summarization problem and proposing a variation to the similarity function used to compute scores during sentence extraction. The paper also emphasizes on the role of title of an article (if provided) in extracting an optimal, normalized score for each sentence.", 
New multidocument multi-lingual text summarization technique based on singular value decomposition and hierarchical clustering is proposed. The system has been successfully tested on summarizing several Persian document collections.,A Multi-Document Multi-Lingual Automatic Summarization System,"In this paper, a new multidocument multi-lingual text summarization technique, based on singular value decomposition and hierarchical clustering, is proposed. The proposed approach relies on only two resources for any language: a word segmentation system and a dictionary of words along with their document frequencies. The summarizer initially takes a collection of related documents, and transforms them into a matrix; it then applies singular value decomposition to the resulted matrix. After using a binary hierarchical clustering algorithm, the most important sentences of the most important clusters form the summary. The appropriate place of each chosen sentence is determined by a novel technique. The system has been successfully tested on summarizing several Persian document collections.", 
Text summarization is an important way of finding relevant information in large text libraries or in the Internet. This paper presents a multi-document summarization system that concisely extracts the main aspects of a set of documents. A new sentence clustering algorithm based on a graph model makes use of statistic similarities and linguistic treatment.,A multi-document summarization system based on statistics and linguistic treatment,"The massive quantity of data available today in the Internet has reached such a huge volume that it has become humanly unfeasible to efficiently sieve useful information from it. One solution to this problem is offered by using text summarization techniques. Text summarization, the process of automatically creating a shorter version of one or more text documents, is an important way of finding relevant information in large text libraries or in the Internet. This paper presents a multi-document summarization system that concisely extracts the main aspects of a set of documents, trying to avoid the typical problems of this type of summarization: information redundancy and diversity. Such a purpose is achieved through a new sentence clustering algorithm based on a graph model that makes use of statistic similarities and linguistic treatment. The DUC 2002 dataset was used to assess the performance of the proposed system, surpassing DUC competitors by a 50% margin of f-measure, in the best case.", 
Cross-Language Text Summarization generates summaries in a language different from the language of the source documents. We propose a compressive framework to generate cross-language summaries. An automatic evaluation showed that our method outperformed extractive state-of-art CLTS methods.,A Multi-Task Learning Framework for Abstractive Text Summarization,"Cross-Language Text Summarization generates summaries in a language different from the language of the source documents. Recent methods use information from both languages to generate summaries with the most informative sentences. However, these methods have performance that can vary according to languages, which can reduce the quality of summaries. In this paper, we propose a compressive framework to generate cross-language summaries. In order to analyze performance and especially stability, we tested our system and extractive baselines on a dataset available in four languages (English, French, Portuguese, and Spanish) to generate English and French summaries. An automatic evaluation showed that our method outperformed extractive state-of-art CLTS methods with better and more stable ROUGE scores for all languages.", 
Renal dysfunction is one of the most common complications of heart failure. Timely prediction of renal dysfunction can help medical staffs intervene early to avoid catastrophic consequences. The proposed MT-DWNN model achieves better prediction performance on renal dysfunction in HF patients than conventional models.,A Multi-Task Neural Network Architecture for Renal Dysfunction Prediction in Heart Failure Patients With Electronic Health Records,"Renal dysfunction, which is associated with bad clinical outcomes, is one of the most common complications of heart failure (HF). Timely prediction of renal dysfunction can help medical staffs intervene early to avoid catastrophic consequences. In this paper, we proposed a multi-task deep and wide neural network (MT-DWNN) for predicting fatal complications during hospitalization. The algorithm was tested on a dataset collected from Chinese PLA General Hospital, which contains 35,101 hospitalizations with HF diagnosis during the last 18 years, and 2,478 hospitalizations with a diagnosis of renal dysfunction. For the renal dysfunction task, the AUC of the proposed method is 0.9393, which is a significant improvement (p < 0:01) compared to that of conventional methods, while that of single task deep neural networks is 0.9370, that of random forest is 0.9360, and that of logistic regression is 0.9233. The experimental results show that the proposed MT-DWNN model achieves better prediction performance on renal dysfunction in HF patients than conventional models.", 
Cross-Language Text Summarization generates summaries in a language different from the language of the source documents. We propose a compressive framework to generate cross-language summaries. An automatic evaluation showed that our method outperformed extractive state-of-art CLTS methods.,A Multilingual Study of Compressive Cross-Language Text Summarization,"Cross-Language Text Summarization generates summaries in a language different from the language of the source documents. Recent methods use information from both languages to generate summaries with the most informative sentences. However, these methods have performance that can vary according to languages, which can reduce the quality of summaries. In this paper, we propose a compressive framework to generate cross-language summaries. In order to analyze performance and especially stability, we tested our system and extractive baselines on a dataset available in four languages (English, French, Portuguese, and Spanish) to generate English and French summaries. An automatic evaluation showed that our method outperformed extractive state-of-art CLTS methods with better and more stable ROUGE scores for all languages.", 
"In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence.",A Neural Attention Model for Abstractive Sentence Summarization,"Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.", 
The number of electronic documents as a media of business and academic information has increased tremendously after the introduction of the World Wide Web. Study aims to develop a text summarization system that incorporates learning ability. System is able to learn to classify sentences when well trained with sufficient text samples.,A neural-based text summarization system,"The number of electronic documents as a media of business and academic information has increased tremendously after the introduction of the World Wide Web. Ever since, instances where users being overloaded with too much electronic textual information are inevitable. The users may only be interested in shorter versions of text documents but are overloaded with lengthy texts. The objective of the study is to develop a text summarization system that incorporates learning ability by combining a statistical approach, keywords extraction, and neural network with unsupervised learning. The system is able to learn to classify sentences when well trained with sufficient text samples. Users with strong background in writing English summaries have subjectively evaluated the outputs of the text summarization system based on contents. With the average contents score of 83.03%, the system is regarded to have produced an effective summary with most of the important contents of the original text extracted without compromising the summary’s readability.", 
Good weighting methodologies are supposed to be more important than the feature selection process. In this paper we aim to explore a new algorithm for using GA in term weighting for text summarization process.,A new algorithm for term weighting in text summarization process,"The importance of good weighting methodology in information retrieval methods – the method that affects the most useful features of a document or query representative - is examined. Good weighting methodologies are supposed to be more important than the feature selection process. Weighting features is the thing that many information retrieval systems are regarding as being of minor importance as compared to find the features; but the experiments suggest that weighting is noticeably more important than feature selection. There are different methods for the term weighting such as TF*IDF and Information Gain Ratio which have been used in information retrieval systems. In this paper we aim to explore a new algorithm for using GA in term weighting for text summarization process and then by deploying it as an appropriate developed prototype, the outcomes are analyzed and some conclusions for Information Retrieval are considered.", 
This paper is concerned with a two stage procedure for analysis and classification of electroencephalogram (EEG) signals for schizophrenic patients and twenty age-matched control participants. Most of the selected channels are located in the prefrontal and temporal lobes confirming neuropsychological and neuroanatomical findings.,A new approach for EEG signal classification of schizophrenic and control participants,"This paper is concerned with a two stage procedure for analysis and classification of electroencephalogram (EEG) signals for twenty schizophrenic patients and twenty age-matched control participants. For each case, 20 channels of EEG are recorded. First, the more informative channels are selected using the mutual information techniques. Then, genetic programming is employed to select the best features from the selected channels. Several features including autoregressive model parameters, band power and fractal dimension are used for the purpose of classification. Both linear discriminant analysis (LDA) and adaptive boosting (Adaboost) are trained using tenfold cross validation to classify the reduced feature set and a classification accuracy of 85.90% and 91.94% is obtained by LDA and Adaboost, respectively. Another interesting observation from the channel selection procedure is that most of the selected channels are located in the prefrontal and temporal lobes confirming neuropsychological and neuroanatomical findings. The results obtained by the proposed approach are compared with a one stage procedure, the principal component analysis (PCA)-based feature selection, utilizing only 100 features selected from all channels. It is illustrated that the two stage procedure consisting of channel selection followed by feature reduction gives a more enhanced results in an efficient computation time.", 
"This paper proposes an extraction-based hybrid model for a single text document summarization. The model is depending on the linear combination of statistical measures like sentence position, TF-IDF, aggregate similarity, centroid, and sentiment analysis.",A New Approach for Single Text Document Summarization,"This paper proposes an extraction-based hybrid model for a single text document summarization. The hybrid model is depending on the linear combination of statistical measures like sentence position, TF-IDF, aggregate similarity, centroid, and sentiment analysis. Our idea to include sentiment analysis for salient sentence extraction is derived from the concept that emotion plays an important role in communication to effectively convey any message; hence, it can play vital role in text document summarization. As we know for any sentence, emotions (calling sentiments) may be negative, positive, or neutral. Sentence which has strong sentiment are more important for us which may be either negative or positive.", 
Summarization is a process of giving the shorter version of a text document. The main aim is to reduce the body of the text and maintaining coherence and avoiding redundancy.,A New Approach for Text Summarizer,"Text Summarization is a process of giving the shorter version of a text document. For the research scholars who want to do research on a particular domain have to search a lot of documents. It is very difficult and it takes a lot of time to go through the domain, in this case there is a chance of missing some key word in those documents. To get rid of this problem it is better to have a summary of a document. Summarizer gives the summary of a paper. The summarizer can be developed using some algorithms like Sentence Position, Sentence resemblance to the title, Lexical Similarity etc. The main aim is to reduce the body of the text and maintaining coherence and avoiding redundancy.", 
MUSE is a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. It performs better than the best known multilingual approach (TextRank1) in both English and Hebrew.,A new Approach to Improving Multilingual Summarization using a Genetic Algorithm,"Automated summarization methods can be defined as “language-independent,” if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. stateof-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.", 
"In this article, Deep Auto-Encoder and Discrete Wavelet Transform (DWT) are combined for the detection of epilepsy from EEG signals. 128 features were obtained by taking the A5 parameter. The proposed method gives satisfactory results when compared with the common methods.",A New Automatic Epilepsy Serious Detection Method by Using Deep Learning Based on Discrete Wavelet Transform,"In this article, Deep Auto-Encoder and Discrete Wavelet Transform (DWT) are combined for the detection of epilepsy from EEG signals. In the first stage, DWT was applied to analyze the EEG signal and 128 features were obtained by taking the A5 parameter. In the second stage, deep automatic encoders are used to obtain high level and sensitive features from the A5. In addition, these features are classified into two groups: normal and abnormal. Finally, the two auto-encoders and SoftMax stacked and trained by using backpropagation algorithm to improve the classification accuracy. The proposed method gives satisfactory results when compared with the common methods presented in this file.", 
This paper proposes a novel methodology to generate an extractive text summary from a corpus of documents. We propose a heuristic method which uses the Latent Dirichlet Allocation technique to identify the optimum number of independent topics. The use of topic modeling ensures that all the important content is captured in the extracted summary.,A New Automatic Multi-document Text Summarization using Topic Modeling,"This paper proposes a novel methodology to generate an extractive text summary from a corpus of documents. Unlike most existing methods, our approach is designed in such a way that the final generated summary covers all the important topics from a corpus of documents. We propose a heuristic method which uses the Latent Dirichlet Allocation technique to identify the optimum number of independent topics present in the corpus. Some of the sentences are identified as the important sentences from each independent topic using a set of word and sentence level features. In order to ensure that the final summary is coherent, we suggest a novel technique to reorder the sentences based on sentence similarity. The use of topic modeling ensures that all the important content from the corpus of documents is captured in the extracted summary which in turn strengthen the summary. Experimental results show that the proposed approach is promising.", 
"Gravitation Model (GM) is proposed to solve the class-imbalanced classification problem. In training phase, each class is weighted by a mass factor, which can be learned from the training data. In testing phase, a new document will be assigned to a class with the max gravitational force.",A new Centroid-Based Classification model for text categorization,"The automatic text categorization technique has gained significant attention among researchers because of the increasing availability of online text information. Therefore, many different learning approaches have been designed in the text categorization field. Among them, the widely used method is the Centroid-Based Classifier (CBC) due to its theoretical simplicity and computational efficiency. However, the classification accuracy of CBC greatly depends on the data distribution. Thus it leads to a misfit model and also has poor classification performance when the data distribution is highly skewed. In this paper, a new classification model named as Gravitation Model (GM) is proposed to solve the class-imbalanced classification problem. In the training phase, each class is weighted by a mass factor, which can be learned from the training data, to indicate data distribution of the corresponding class. In the testing phase, a new document will be assigned to a particular class with the max gravitational force. The performance comparisons with CBC and its variants based on the results of experiments conducted on twelve real datasets show that the proposed gravitation model consistently outperforms CBC together with the Class-Feature-Centroid Classifier (CFC). Also, it obtains the classification accuracy competitive to the DragPushing (DP) method while it maintains a more stable performance. Thus, the proposed gravitation model is proved to be less over-fitting and has higher learning ability than CBC model.", 
"Evaluation of automatic text summarization is a challenging task due to the difficulty of calculating similarity of two texts. In this paper, we define a new dissimilarity measure – compression Dissimilarity. We propose a new automatic evaluating method based on compression dissimilarities.",A new evaluation measure using compression dissimilarity on text summarization,"Evaluation of automatic text summarization is a challenging task due to the difficulty of calculating similarity of two texts. In this paper, we define a new dissimilarity measure – compression dissimilarity to compute the dissimilarity between documents. Then we propose a new automatic evaluating method based on compression dissimilarity. The proposed method is a completely “black box” and does not need preprocessing steps. Experiments show that compression dissimilarity could clearly distinct automatic summaries from human summaries. Compression dissimilarity evaluating measure could evaluate an automatic summary by comparing with high-quality human summaries, or comparing with its original document. The evaluating results are highly correlated with human assessments, and the correlation between compression dissimilarity of summaries and compression dissimilarity of documents can serve as a meaningful measure to evaluate the consistency of an automatic text summarization system.", 
An algorithm based on the graph theory is introduced to select the most important sentences of the document. The results show that considering simultaneous different criteria generate a summary which is more similar to human one. The algorithm is based on an algorithm that weights nodes and edges in a graph.,A New Graph-Based Algorithm for Persian Text Summarization,"Nowadays, with increasing volume of electronic text information, the need for production of summary systems becomes essential. Summary systems capture and summarize the most important concepts of the documents and help the user to go through the main points of the text faster and make the processing of information much easier. An important class of such systems is the ones that produce extractive summaries. This summary is produced by selecting most important parts of the document without doing any modification on the main text. One approach for producing this kind of summary is using the graph theory. In this paper a new algorithm based on the graph theory is introduced to select the most important sentences of the document. In this algorithm the nodes and edges will be assigned with different weights and then the final weight of each one will be defined by combining these values. This final weight indicates the importance of the sentence and the probability of appearing this sentence in the final summary. The results show that considering simultaneous different criteria generate a summary which is more similar to human one.", 
"In graph-based extractive text summarization techniques, the weight assigned to the edges of the graph is the crucial parameter for the sentence ranking. In this paper, we propose a new graph- based summarization technique that takes into account the similarity among the individual sentences as well as the similarity between the sentences and the overall document. The evaluation results of the proposed method demonstrate a signifcant improvement of the summary quality.",A new graph-based extractive text summarization using keywords or topic modeling,"In graph-based extractive text summarization techniques, the weight assigned to the edges of the graph is the crucial parameter for the sentence ranking. The weights associated with the edges are based on the similarity between sentences (nodes). Most of the graph-based techniques use the common words based similarity measure to assign the weight. In this paper, we propose a new graph-based summarization technique, which, besides taking into account the similarity among the individual sentences, also considers the similarity between the sentences and the overall (input) document. While assigning the weight among the edges of the graph, we consider two attributes. The frst attribute is the similarity among the nodes, which forms the edges of the graph. The second attribute is the weight given to a component that represents how much the particular edge is similar to the topics of the overall document for which we incorporate the topic modeling. Along with these modifcations, we use the semantic measure to fnd the similarity among the nodes. The evaluation results of the proposed method demonstrate a signifcant improvement of the summary quality over the existing text summarization techniques.", 
The importance of text summarization grows rapidly as the amount of information increases exponentially. This paper presents a new hybrid summarization technique that combines statistical properties of documents with Farsi linguistic features. The originality of the technique lies on the use of term co-occurrence property.,A New Hybrid Farsi Text Summarization Technique Based on Term Co-Occurrence and Conceptual Property of the Text,"The importance of text summarization grows rapidly as the amount of information increases exponentially. This paper presents a new hybrid summarization technique that combines statistical properties of documents with Farsi linguistic features. The originality of the technique lies on the use of term co-occurrence property of the text. It could detect the number of subjects. The proposed technique summarizes the document in proportion to the subject treated in a document. It considers the conceptual property of the text algorithm and based on word synonymy prevents similar sentences to be included in the summary. It also preserves the cohesion of the summarized text. Our results show better performance in comparison with FarsiSum, well known Farsi Summarizer, which is based only on the heuristic property of the text and do not consider the Farsi challenges.", 
The prevalence of hypertensive heart disease has increased annually. It has seriously endangered the safety of human life. The XGBSVM hybrid model can predict whether hypertensive patients will develop heart disease within three years.,A New Hybrid XGBSVM Model Application for Hypertensive Heart Disease,"The changes in people’s life rhythm and improvement in material levels that happened in recent years increased the number of people suffering from high blood pressure in the world. Therefore, as a cardiac complication of hypertension, the prevalence of hypertensive heart disease has increased annually, it has seriously endangered the safety of human life, and the effective prediction of hypertensive heart disease has become a worldwide problem. This paper uses the newly proposed XGBSVM hybrid model to predict whether hypertensive patients will develop hypertensive heart disease within three years. The final experiment proves that through this model, hypertensive patients can learn their risk of hypertensive heart disease within 3 years and then undergo targeted preventive treatment, thereby reducing the psychological, physiological and economic burden. This paper confirms that the machine learning can be successfully applied in the biomedical field, with strong real-world significance and research value.", 
Image classification is a challenging problem of computer vision. This paper proposes a new image classification approach through a tree-structured feature set. The proposed method incorporates both global image features and local region-based features. Experimental results show that this approach performs better than conventional approaches.,A new image classification technique using tree-structured regional features,"Image classification is a challenging problem of computer vision. Conventional image classification methods use flat image features with fixed dimensions, which are extracted from a whole image. Such features are computationally effective but are crude representation of the image content. This paper proposes a new image classification approach through a tree-structured feature set. In this approach, the image content is organized in a two-level tree, where the root node at the top level represents the whole image and the child nodes at the bottom level represent the homogeneous regions of the image. The tree-structured representation combines both the global and the local features through the root and the child nodes. The tree-structured feature data are then processed by a two-level self-organizing map (SOM), which consists of an unsupervised SOM for processing image regions and a supervising concurrent SOM (CSOM) classifier for the overall classification of images. The proposed method incorporates both global image features and local region-based features to improve the performance of image classification. Experimental results show that this approach performs better than conventional approaches.", 
Text summarization technique deals with the compression of large document into shorter version. Technique based on local and global properties of words and identifying significant words. Extraction based text summarization involves selecting sentences of high rank from document.,A New Method of Text Summarization,"Today, the tremendous information is available on the internet, is difficult to get the information fast and most efficiently. Text summarization technique deals with the compression of large document into shorter version. it is a process that reduces the size of the text document and extract significant sentences from a text document. This is a novel technique for text summarization. The technique based on local and global properties of words and identifying significant words. The local property of word can be considered the sum of normalized term frequency multiplied by normalized number of sentences containing that word. Global property can be thought of as maximum semantic similarity between a word and title words. Extraction based text summarization involves selecting sentences of high rank from document based on word and sentence features and put them together to generate summary.", 
"A brain-computer interface (BCI) system allows direct communication between the brain and the external world. Common spatial pattern (CSP) has been used effectively for feature extraction of data used in BCI systems. We propose a new automated filter tuning approach for motor imagery electroencephalography (EEG) signal classification, which automatically finds the filter parameters for optimal performance.",A new parameter tuning approach for enhanced motor imagery EEG signal classification,"A brain-computer interface (BCI) system allows direct communication between the brain and the external world. Common spatial pattern (CSP) has been used effectively for feature extraction of data used in BCI systems. However, many studies show that the performance of a BCI system using CSP largely depends on the filter parameters. The filter parameters that yield most discriminating information vary from subject to subject and manually tuning of the filter parameters is a difficult and timeconsuming exercise. In this paper, we propose a new automated filter tuning approach for motor imagery electroencephalography (EEG) signal classification, which automatically and flexibly finds the filter parameters for optimal performance. We have evaluated the performance of our proposed method on two public benchmark datasets. Compared to the existing conventional CSP approach, our method reduces the average classification error rate by 2.89% and 3.61% for BCI Competition III dataset IVa and BCI Competition IV dataset I, respectively. Moreover, our proposed approach also achieved lowest average classification error rate compared to state-of-the-art methods studied in this paper. Thus, our proposed method can be potentially used for developing improved BCI systems, which can assist people with disabilities to recover their environmental control. It can also be used for enhanced disease recognition such as epileptic seizure detection using EEG signals.", 
"This paper proposes a new text similarity measure and mathematical model for automatic text summarization. Model consists of two stages. For detection of topics the sentences in document collection are clustered. At the second stage, the model generates a summary by extracting relevant sentences from each cluster.",A new similarity measure and mathematical model for text summarization,"This paper proposes a new text similarity measure and mathematical model for automatic text summarization. Model consists of two stages. At the first stage, for detection of topics the sentences in document collection are clustered. At the second stage, the model generates a summary by extracting relevant sentences from each cluster. For clustering of sentences the kmeans algorithm is utilized. Sentence selection process is formalized as an optimization problem. To select relevant sentences from each cluster and avoid redundancy in the summary this model uses both the sentence-to-cluster relation and the sentence-to-sentence relation. To solve the optimization problem a differential evolution algorithm with adaptive mutation strategy is developed.", 
A novel summarization scheme based on metaheuristic optimization is introduced that generates a summary by extracting salient and relevant sentences from a collection of documents. The proposed work generates optimal combinations of sentence scoring methods.,A novel approach for text summarization using optimal combination of sentence scoring methods,"In this paper, a novel multi-document summarization scheme based on metaheuristic optimization is introduced that generates a summary by extracting salient and relevant sentences from a collection of documents. The proposed work generates optimal combinations of sentence scoring methods and their respective optimal weights to extract the sentences with the help of a metaheuristic approach known as teaching–learning-based optimization. In addition, the proposed scheme is compared to two summarization methods that use different metaheuristic approaches. The experimental results show the efficacy of the proposed summarization scheme.", 
This paper provides a novel hybrid model for Arabic text summarization. It combines Rhetorical Structure Theory (RST) and Vector Space Model (VSM) The proposed model uses RST to discover the most significant paragraphs based on functional and semantic criteria.,A novel Arabic text summarization model based on rhetorical structure theory and vector space model,"This paper provides a novel hybrid model for Arabic text summarization, combining Rhetorical Structure Theory (RST) and Vector Space Model (VSM). The proposed model uses RST to discover the most significant paragraphs based on functional and semantic criteria. To overcome the limitation of RST sensitivity to text size, the proposed model uses VSM for ranking the significant paragraphs based on the cosine similarity feature. The proposed model’s output summary is tested on three categories of 212 news articles with different size-ranges. The statistical results show that the proposed model improves the average precision of the output text summary over using RST only, without losing the advantages of summarization by RST. Such advantages ranges from the ability of extracting semantics behind the text, which lays out an output summary not out of context, without the lack of cohesion and with non broken anaphoric references.", 
How to alleviate information overload is the main concern of research on natural language processing. This study proposes a novel approach based on recent Bayesian topic models. The proposed model incorporates the concepts of n-grams into hierarchically latent topics.,A novel contextual topic model for multi-document summarization,"Information overload becomes a serious problem in the digital age. It negatively impacts understanding of useful information. How to alleviate this problem is the main concern of research on natural language processing, especially multi-document summarization. With the aim of seeking a new method to help justify the importance of similar sentences in multi-document summarizations, this study proposes a novel approach based on recent hierarchical Bayesian topic models. The proposed model incorporates the concepts of n-grams into hierarchically latent topics to capture the word dependencies that appear in the local context of a word. The quantitative and qualitative evaluation results show that this model has outperformed both hLDA and LDA in document modeling. In addition, the experimental results in practice demonstrate that our summarization system implementing this model can significantly improve the performance and make it comparable to the state-of-the-art summarization systems.", 
"Emotion recognition based on electroencephalogram (EEG) signal is attracting more and more attention. We propose an end-to-end model which is based on Convolutional Neural Networks (CNNs) In order to represent the EEG signals better, the original channels of EEG are firstly rearranged by Pearson Correlation Coefficient.",A Novel Convolutional Neural Networks for Emotion Recognition Based on EEG Signal,"Emotion recognition based on electroencephalogram (EEG) signal is attracting more and more attention. Many feature engineering based models have been investigated. However, these models require a lot of effort for manually designing feature set. And these features can be hardly transformed among different problems. To reduce the manual effort on features used in EEG-based recognition and improve the performance, we propose an end-to-end model which is based on Convolutional Neural Networks (CNNs). In order to represent the EEG signals better, the original channels of EEG are firstly rearranged by Pearson Correlation Coefficient and the rearranged EEGs are fed into CNN. experiments were carried on DEAP dataset. The experimental results on the DEAP dataset show that the proposed method achieves 77.98% accuracy on the Valence recognition and 72.98% on the Arousal recognition.", 
Deep learning approaches have been widely used in many fields to extract features and classify various types of data successfully. This work proposes a novel approach that combines deep learning and data augmentation for EEG classification. We applied the empirical mode decomposition on the EEG frames and mixed their intrinsic mode functions to create new artificial EEG frames.,A Novel Deep Learning Approach with Data Augmentation to Classify Motor Imagery Signals,"Brain-computer interface provides a new communication bridge between human mind and devices, depending largely on the accurate classification and identification of non-invasive EEG signals. Recently, deep learning approaches have been widely used in many fields to extract features and classify various types of data successfully. However, the deep learning approach requires massive data to train its neural networks, and the amount of data impacts greatly on the quality of the classifiers. This work proposes a novel approach that combines deep learning and data augmentation for EEG classification. We applied the empirical mode decomposition on the EEG frames and mixed their intrinsic mode functions to create new artificial EEG frames, followed by transforming all EEG data into tensors as inputs of the neural network by complex Morlet wavelets. We proposed two neural networks—convolutional neural network and wavelet neural network—to train the weights and classify two classes of motor imagery signals. The wavelet neural network is a new type of neural network using wavelets to replace the convolutional layers. The experimental results show that the artificial EEG frames substantially improve the training of neural networks, and both two networks yield relatively higher classification accuracies compared to prevailing approaches. Meanwhile, we also verified the performance of our new proposed wavelet neural network model in the classification of steady-state visual evoked potentials.", 
"Electroencephalogram (EEG) signal based emotion recognition has attracted more attention in recent years. Traditional approaches often lack the high-level features and the generalization ability is poor. In this paper, we propose a novel model for multi-subject emotion classification.",A Novel Deep-Learning based Framework for Multi-Subject Emotion Recognition,"Electroencephalogram (EEG) signal based emotion recognition, as a challenging pattern recognition task, has attracted more and more attention in recent years and widely used in medical, Affective Computing and other fields. Traditional approaches often lack of the high-level features and the generalization ability is poor, which are difficult to apply to the practical application. In this paper, we proposed a novel model for multi-subject emotion classification. The basic idea is to extract the high-level features through the deep learning model and transform traditional subject-independent recognition tasks into multi-subject recognition tasks. Experiments are carried out on the DEAP dataset, and our results demonstrate the effectiveness of the proposed method.", 
"There are various methods to extract feature from EEG signals but the effective feature selection is an issue. In this paper, a novel effective. feature selection based on Statistical-Principal. Components Analysis (S-PCA) and wavelet transform (WT) features is proposed. The results indicate an improvement of the classification performance in comparison with current methods.",A novel ensemble local graph structure based feature extraction network for EEG signal analysis,"Electroencephalogram (EEG) signals have been extensively utilized to identify brain disorders such as epilepsy. In this study, a novel feature extraction network based on local graph structure (LGS) is utilized for EEG signal classification. The aim of this work is to create a framework which utilize ensemble of LGS that uses logically extended LGS, symmetric LGS, vertical LGS, vertical symmetric LGS, zigzag horizontal LGS, zigzag horizontal middle LGS, zigzag vertical LGS and zigzag vertical middle LGS. By using these LGS methods with discrete wavelet transform (DWT), a novel ensemble feature extraction network is formed. In this framework, LGSs are utilized for feature extraction and 2D-DWT is utilized for pooling. In the feature reduction phase, two widely known feature reduction techniques, namely ReliefF and neighborhood component analysis (NCA) are used together. Five different benchmark classifiers are employed to present the strength of the proposed ensemble feature extraction framework. In the experiments, two publicly available EEG datasets have been employed to test the proposed ensemble LGS feature extraction based multilevel EEG signal classification method. The proposed ensemble LGS method achieved 97.20% and 98.67% success rate for these datasets. Six cases were also examined to comprehensively evaluate the used Bonn dataset. Results clearly illustrated the success of the ensemble LGS based EEG classification method.", 
"Intelligent Transportation Systems have gained significant attention among Internet of Things applications. As part of smart cities, smart mobility initiatives offer new opportunities for intelligent transportation systems to maximize the utilization of time-sensitive data. The main contribution of this study is to improve vehicular Ad hoc Network performance in real-time using a geographically distributed computing architecture based on fog technology.",A novel geographically distributed architecture based on fog technology for improving Vehicular Ad hoc Network (VANET) performance,"Intelligent Transportation Systems have gained significant attention among Internet of Things applications due to its specific features and its high capability to promote the innovation of the automotive industries. As part of smart cities, smart mobility initiatives offer new opportunities for intelligent transportation systems to maximize the utilization of the time-sensitive data that are streaming out of different sensory transport resources to support “newcasting” instead of “forecasting” technique. As a result, efficient information dissemination has become the new production factor, notably in terms of mitigating traffic congestion, maximizing bandwidth utilization, and reducing transmission power consumption over the network. Cloud Computing and its counterparts have been established as relatively stable environments for proving a wide number of tackled solutions. However, the limitations of network bandwidth as well as the rapid expansion into the data transfer rate are still the bottlenecks of vehicular networks. The main contribution of this study is to improve Vehicular Ad hoc Network performance in real-time using a geographically distributed computing architecture based on fog technology. This architecture presents a set of novel techniques from different points of view. These novelties include, (i) a flexible registration methodology for improving the navigation process among mobile vehicles; (ii) a generic distributed mechanism to adjust the communication range and the network connectivity; and (iii) a new mathematical model to ensure transmission reliability through establishing powerful communication channels between the vehicular entities and fog layer. The effectiveness of the proposed architecture is evaluated using several performance metrics such as throughput, delay time, and jitter. The experimental results reveal that the worthiness of the proposed architecture to meet the quality of service requirements is more than other state-of-the-art techniques in the literature review.", 
"Punjabi is an official language of Punjab State in India. There are very few linguistic resources available for Punjabi. The proposed summarization system is hybrid of conceptual-, statistical-, location- and linguistic-based features. Results of proposed system are compared with different baseline systems.",A Novel Hybrid Text Summarization System for Punjabi Text,"Text summarization is the task of shortening text documents but retaining their overall meaning and information. A good summary should highlight the main concepts of any text document. Many statistical-based, location-based and linguistic-based techniques are available for text summarization. This paper has described a novel hybrid technique for automatic summarization of Punjabi text. Punjabi is an official language of Punjab State in India. There are very few linguistic resources available for Punjabi. The proposed summarization system is hybrid of conceptual-, statistical-, location- and linguistic-based features for Punjabi text. In this system, four new locationbased features and two new statistical features (entropy measure and Z score) are used and results are very much encouraging. Support vector machine-based classifier is also used to classify Punjabi sentences into summary and non-summary sentences and to handle imbalanced data. Synthetic minority over-sampling technique is applied for over-sampling minority class data. Results of proposed system are compared with different baseline systems, and it is found that F score, Precision, Recall and ROUGE-2 score of our system are reasonably well as compared to other baseline systems. Moreover, summary quality of proposed system is comparable to the gold summary.", 
"In Internet of Medical Things (IoMT) environment, feature selection is an efficient way of identifying the most discriminant health-related features from the original feature-set. Here, a new fuzzified version of discernibility matrix has been proposed to determine a subset of features, which provides the best classification accuracy. The empirical results obtained from our experiments in this paper is competitive in terms of accuracy and outperformed the other popular t-test, Kullback–Leibler Divergence (KLD) and Gini index based feature selection techniques.",A novel machine learning based feature selection for motor imagery EEG signal classification in Internet of medical things environment,"In Internet of Medical Things (IoMT) environment, feature selection is an efficient way of identifying the most discriminant health-related features from the original feature-set. Feature selection not only finds the best informative features, but also helps in reducing the overall dimensions of the given dataset. In this paper, the actual feature-set is obtained from Brain Computer Interface (BCI) Competition-II Dataset-III motor-imagery electroencephalogram (EEG) signal using the Adaptive Autoregressive (AAR) feature extraction technique. Based on the order (number of AR coefficients) of the AAR algorithm, two variants of datasets have been generated: 12 (order = 6 per electrode) and 24 (order = 12 per electrode) AAR features datasets. Here, a new fuzzified version of discernibility matrix has been proposed to determine a subset of features, which provides the best classification accuracy. In order to find the best feature subset, various types of dissimilarity measures have been used and compared with one another in our proposed fuzzy discernibility matrix (FDM) based feature selection technique. We have implemented the proposed algorithm on the given datasets using both the holdout technique as well as the 10-fold cross-validation in our study. The performances of the selected featuresubsets are evaluated based on accuracies using the Support Vector Machine (SVM) and Ensemble variants of classifiers. The empirical results obtained from our experiments in this paper is competitive in terms of accuracy and outperformed the other popular t-test, Kullback–Leibler Divergence (KLD), Bhattacharyya distance and Gini index based feature selection techniques. Our proposed FDM based feature selection algorithm using holdout technique provides 80% and 78.57% accuracies for the 12 and 24 features AAR datasets respectively. The results obtained in the holdout technique with only 50% of the best discriminant features are even better than the performances obtained while using the original feature-sets (without using any feature selection technique). Again, it gives 78.57% and 75.57% mean-accuracies from 5 × 10-fold cross-validations using only 6and 12 most discriminant AAR features from the actual 12&24 features-sets respectively.", 
"Motor imagery (MI) is where users imagine limb movements to control the system. It has immense potential for its applicability in gaming, neuro-prosthetics and neuro-rehabilitation. The proposed method generated an accuracy of 96.54% and is highly accurate.",A novel method of motor imagery classification using eeg signal,"A subject of extensive research interest in the Brain Computer Interfaces (BCIs) niche is motor imagery (MI), where users imagine limb movements to control the system. This interest is owed to the immense potential for its applicability in gaming, neuro-prosthetics and neuro-rehabilitation, where the user’s thoughts of imagined movements need to be decoded. Electroencephalography (EEG) equipment is commonly used for keeping track of cerebrum movement in BCI systems. The EEG signals are recognized by feature extraction and classification. The current research proposes a Hybrid-KELM (Kernel Extreme Learning Machine) method based on PCA (Principal Component Analysis) and FLD (Fisher's Linear Discriminant) for MI BCI classification of EEG data. The performance and results of the method are demonstrated using BCI competition dataset III, and compared with those of contemporary methods. The proposed method generated an accuracy of 96.54%.", 
Text summarization is a process that reduces the size of the text document and extracts significant sentences from a text document. We present a novel technique for text summarization. The originality of technique lies on exploiting local and global properties of words and identifying significant words.,A novel method of significant words identification in text summarization,"Text summarization is a process that reduces the size of the text document and extracts significant sentences from a text document. We present a novel technique for text summarization. The originality of technique lies on exploiting local and global properties of words and identifying significant words. The local property of word can be considered as the sum of normalized term frequency multiplied by its weight and normalized number of sentences containing that word multiplied by its weight. If local score of a word is less than local score threshold, we remove that word. Global property can be thought of as maximum semantic similarity between a word and title words. Also we introduce an iterative algorithm to identify significant words. This algorithm converges to the fixed number of significant words after some iterations and the number of iterations strongly depends on the text document. We used a two-layered backpropagation neural network with three neurons in the hidden layer to calculate weights. The results show that this technique has better performance than MS-word 2007, baseline and Gistsumm summarizers.", 
This method based on clustering of sentences. The generated summary can contain the main contents of different topics. The clustering method satisfies as much homogeneity within each cluster as well,A Novel Partitioning-Based Clustering Method and Generic Document Summarization,In this paper is proposed the generic summarization method that extracts the most relevance sentences from the source document to form a summary. This method based on clustering of sentences. The specificity of this approach is that the generated summary can contain the main contents of different topics as many as possible and reduce its redundancy at the same time. The clustering method satisfies as much homogeneity within each cluster as well as much separability between the clusters as possible., 
"The Internet of Things (IoT) has particular applications in public safety as well as other domains such as smart cities, health monitoring, smart homes and environments. As the numbers of devices connected to the Internet is expanding, the threat to confidentiality and security is increasing. The aim of this paper is design a typical network security model for cooperative virtual networks in the IoT era.",A Novel Security Model for Cooperative Virtual Networks in the IoT Era,"The Internet of Things (IoT) has particular applications in public safety as well as other domains such as smart cities, health monitoring, smart homes and environments, smart industry, and various types of pervasive systems. The attacker can simply attack the IoT device in such applications, because it is randomly distributed, dynamic topology and not reliable due to energy and communication limitation. Moreover, the threat to confidentiality and security is increasing as the number of devices connected in IoT is increasing. As the numbers of devices connected to the Internet is expanding, the threat to confidentiality and security is increasing. The aim of this paper is design a typical network security model for cooperative virtual networks in the IoT era. This paper presents and discusses network security vulnerabilities, threats, attacks and risks in switches, firewalls and routers, in addition to a policy to mitigate those risks. The paper provides the fundamentals of secure networking system including firewall, router, AAA server and VLAN technology. It presents a novel security model to defense the network from internal and external attacks and threats in the IoT Era. A testbed is built to investigate the proposed model, and the performed assessment show an effective security performance with a good network performance.", 
EEG signals capture information from central nervous system and are closely related with our brain activities. Special properties of EEG dataset have brought difficulties for conventional machine learning methods. Authors develop novel semi-supervised deep structured framework to overcome these difficulties. experiments conducted on real EEG dataset show superiority of our method over multiple baselines for the affective state recognition.,A Novel Semi-supervised Deep Learning Framework for Affective State Recognition on EEG Signals,"Nowadays the rapid development in the area of human-computer interaction has given birth to a growing interest on detecting different affective states through smart devices. By using the modern sensor equipment, we can easily collect electroencephalogram (EEG) signals, which capture the information from central nervous system and are closely related with our brain activities. Through the training on EEG signals, we can make reasonable analysis on people’s affection, which is very promising in various areas. Unfortunately, the special properties of EEG dataset have brought difficulties for conventional machine learning methods. The main reasons lie in two aspects: the small set of labeled samples and the noisy channel problem. To overcome these difficulties and successfully identify the affective states, we come up with a novel semi-supervised deep structured framework. Compared with previous deep learning models, our method is more adapted to the EEG classification problem. We first adopt a two-level procedure, which involves both supervised label information and unsupervised structure information to jointly make decision on channel selection. And then, we add a generative Restricted Boltzmann Machine (RBM) model for the classification task, and use the training objectives of generative learning and unsupervised learning to jointly regularize the discriminative training. Finally, we extend it to the active learning scenario, which solves the costly labeling problem. The experiments conducted on real EEG dataset have shown both the convincing result on critical channel selection and the superiority of our method over multiple baselines for the affective state recognition.", 
"Motor imagery electroencephalogram (MI-EEG) signals are widely used in brain-computer interface (BCI) systems. There are few effective deep learning algorithms applied to BCI systems, particularly for MI based BCI. We propose an algorithm that combines continuous wavelet transform (CWT) and a simplified convolutional neural network.",A Novel Simplified Convolutional Neural Network Classification Algorithm of Motor Imagery EEG Signals Based on Deep Learning,"Left and right hand motor imagery electroencephalogram (MI-EEG) signals are widely used in brain-computer interface (BCI) systems to identify a participant intent in controlling external devices. However, due to a series of reasons, including low signal-to-noise ratios, there are great challenges for efficient motor imagery classification. The recognition of left and right hand MI-EEG signals is vital for the application of BCI systems. Recently, the method of deep learning has been successfully applied in pattern recognition and other fields. However, there are few effective deep learning algorithms applied to BCI systems, particularly for MI based BCI. In this paper, we propose an algorithm that combines continuous wavelet transform (CWT) and a simplified convolutional neural network (SCNN) to improve the recognition rate of MI-EEG signals. Using the CWT, the MI-EEG signals are mapped to time-frequency image signals. Then the image signals are input into the SCNN to extract the features and classify them. Tested by the BCI Competition IV Dataset 2b, the experimental results show that the average classification accuracy of the nine subjects is 83.2%, and the mean kappa value is 0.651, which is 11.9% higher than that of the champion in the BCI Competition IV. Compared with other algorithms, the proposed CWT-SCNN algorithm has a better classification performance and a shorter training time. Therefore, this algorithm could enhance the classification performance of MI based BCI and be applied in real-time BCI systems for use by disabled people.", 
"Paper presents a new algorithm for the classification of multiclass EEG signals. Algorithm involves applying the optimum allocation technique to select representative samples that reflect an entire database. Results show very high classification performances for each class, and confirm the consistency of the proposed method in each repeated experiment.",A novel statistical algorithm for multiclass EEG signal classification,"This paper presents a new algorithm for the classification of multiclass EEG signals. This algorithm involves applying the optimum allocation technique to select representative samples that reflect an entire database. This research investigates whether the optimum allocation is suitable to extract representative samples depending on their variability within the groups in the input EEG data. It also assesses whether these samples are efficient for the multiclass least square support vector machine (MLS-SVM) to classify EEG signals. The performances of the MLS-SVM with four different output coding approaches: minimum output codes (MOC), error correcting output codes (ECOC), One vs One (1vs1) and One vs All (1vsA), are evaluated with a benchmark epileptic EEG database. To test the consistency, all experiments are repeated ten times with the same classifying parameters in each classification process. The results show very high classification performances for each class, and also confirm the consistency of the proposed method in each repeated experiment. In addition, the performances by the optimum allocation based MLS-SVM method are compared with the four existing reference methods using the same database. The outcomes of this research demonstrate that the optimum allocation is very effective and efficient for extracting the representative patterns from the multiclass EEG data, and the MLS-SVM is also very well fitted with the optimum allocation technique for the EEG classification.", 
The need for tools for automatic summarization of Web documents has become very critical. Text summarization is an important activity in the analysis of a high volume text documents. The summarizer proposed generates a summary based on the calculated Sentence Weight (SW).,A Novel Technique for Efficient Text Document Summarization as a Service,"Due to an exponential growth in the generation of web data, the need for tools and mechanisms for automatic summarization of Web documents has become very critical. Web data can be accessed from multiple sources, for e.g. on different Web pages, which makes searching for relevant pieces of information a difficult task. Therefore, an automatic summarizer is vital towards reducing human effort. Text summarization is an important activity in the analysis of a high volume text documents and is currently a major research topic in Natural Language Processing. It is the process of generation of the summary of an input document by extracting the representative sentences from it. In this paper, we present a novel technique for generating the summarization of domainspecific text from a single Web document by using statistical NLP techniques on the text in a reference corpus and on the web document. The summarizer proposed generates a summary based on the calculated Sentence Weight (SW), the rank of a sentence in the document’s content, the number of terms and the number of words in a sentence, and using term frequency in the input corpus.", 
Text summarization is a reduction of original text to summarized text by selecting what is important in the source. News articles on sports and politics from online Hindi newspapers were used as input to the system. The system achieves an average precision of 73% over multiple Hindi documents.,A novel technique for multidocument Hindi text summarization,"A text summary is a reduction of original text to summarized text by selecting what is important in the source. Over a period of years the World Wide Web has expanded so that tremendous amount of data is created and available online. Text summarization is needed when people want a gist of a particular topic from one or more sources of information available online. Taking into consideration the above problem a novel technique for multi document, extractive text summarization is proposed. Also considering the common language in India being Hindi, a summarizer for the same language is built. News articles on sports and politics from online Hindi newspapers were used as input to the system. Fuzzy inference engine was used for the extraction process using eleven important features of the text. The system achieves an average precision of 73% over multiple Hindi documents.", 
"Summarization is the task of producing a shorter version of a document while preserving its originality. Summarization models attempts in compressing information which includes text, images and several other information which could be stored in a document source.",A performance study of text summarization model using heterogeneous data sources,"Text summary generation has potential significance in recent years due to rapid growth of online information. Document summarization is the task of producing a shorter version of a document while preserving its originality. Summarization models attempts in compressing information which includes text, images and several other information which could be stored in a document source. This provides challenges for obtaining quality summaries as compared with human made summary. Though several models were designed and presented by several research groups and by individuals, it is hard to find a unique model for documents with different types of data source. This paper presents a performance study of text summarization model using heterogeneous data sources. The results obtained leads to promising results with generic summaries obtained from summarization model designed for our research study.", 
"Automatic text summarization aims to sieve relevant information in documents by creating shorter versions of the text. Most techniques and tools available are designed only for the English language, which is a severe restriction. This paper proposes a language independent summarization platform for 25 different languages.",A platform for language independent summarization,"The text data available on the Internet is not only huge in volume, but also in diversity of subject, quality and idiom. Such factors make it infeasible to efficiently scavenge useful information from it. Automatic text summarization is a possible solution for efficiently addressing such a problem, because it aims to sieve the relevant information in documents by creating shorter versions of the text. However, most of the techniques and tools available for automatic text summarization are designed only for the English language, which is a severe restriction. There are multilingual platforms that support, at most, 2 languages. This paper proposes a language independent summarization platform that provides corpus acquisition, language classification, translation and text summarization for 25 different languages.", 
Face verification involves determining whether a pair of facial images belong to the same or different subjects. We propose a principled transfer learning approach for merging plentiful source- domain data with limited samples from some target domain of interest. We later use principles from convex analysis to recast our algorithm as an equivalent structured rank minimization problem.,A Practical Transfer Learning Algorithm for Face Verification,"Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many important applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merging plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ideally performs nearly as well as if rich target-domain data were present. Based upon a surprisingly simple generative Bayesian model, our approach combines a KL-divergence-based regularizer/prior with a robust likelihood function leading to a scalable implementation via the EM algorithm. As justification for our design choices, we later use principles from convex analysis to recast our algorithm as an equivalent structured rank minimization problem leading to a number of interesting insights related to solution structure and feature-transform invariance. These insights help to both explain the effectiveness of our algorithm as well as elucidate a wide variety of related Bayesian approaches. Experimental testing with challenging datasets validate the utility of the proposed algorithm.", 
"In this paper, the model is proposed to predict the heart disease detection by using data mining techniques. The data mining algorithm uses the Logistic Regression model and Neural Network model. The web application can be support for the user, who wants to diagnose heart disease.",A Predictive Model for Heart Disease Detection Using Data Mining Techniques,"In this paper, the model is proposed to predict the heart disease detection by using data mining techniques. The data mining algorithm uses the Logistic Regression model and Neural Network model. The dataset of this paper uses the heart disease data at the University of California Irvine (UCI). There are a total of 303 Instances and 75 Attributes in the United States. The evaluation criteria using the confusion matrix table such as accuracy, precision, recall and F-Measure. The results show that the Logistic Regression model is better performance than Neural Network model. The Logistic Regression model has 95.45% precision and 91.65% accuracy. The web application can be support for the user, who wants to diagnose heart disease detection.", 
"Unsupervised leaning is a popular method for classify unlabeled dataset. This paper presents in-deep study for three unsupervised classifiers. The three classifiers are evaluated using three significant metrics, which are classification accuracy, classification speed and memory consuming.","A Preliminary Performance Evaluation of K-means, KNN and EM Unsupervised Machine Learning Methods for Network Flow Classification","Unsupervised leaning is a popular method for classify unlabeled dataset i.e. without prior knowledge about data class. Many of unsupervised learning are used to inspect and classify network flow. This paper presents in-deep study for three unsupervised classifiers, namely: K-means, K-nearest neighbor and Expectation maximization. The methodologies and how it’s employed to classify network flow are elaborated in details. The three classifiers are evaluated using three significant metrics, which are classification accuracy, classification speed and memory consuming. The K-nearest neighbor introduces better results for accuracy and memory; while K-means announce lowest processing time.", 
A positive coronavirus patient can be traced through CDRA and contact tracing. The technique can track the path traversed by the patient and collect the cell numbers of all those people who have met with the patient. A COVID-19 patient is geo tagged and alerts are sent if any violation of isolation is done by the patients.,A Privacy-Preserved and Cost-Efficient Control Scheme for Coronavirus Outbreak Using Call Data Record and Contact Tracing,"Coronavirus or COVID-19, which has been declared pandemic by the World Health Organization, has incurred huge losses to the lives of people throughout the world. Although, the scientists, researchers and doctors are working round the clock to develop a vaccine for COVID-19, it may take a year or two to make a safe and effective vaccine available for the world. In current circumstances, a solution must be developed to control or stop the spread of the virus. For this purpose, a novel technique based on call data record analysis (CDRA) and contact tracing is proposed that can effectively control the coronavirus outbreak. A positive coronavirus patient can be traced through CDRA and contact tracing. The technique can track the path traversed by the patient and collect the cell numbers of all those people who have met with the patient. Keeping in tact the privacy of this group of people, when a test result of a person comes positive among the group, then he/she must be isolated and same CDRA and contact tracing procedures are adopted for that person. A COVID- 19 patient is geo tagged and alerts are sent if any violation of isolation is done by the patient. Moreover, the general public is informed in advance to avoid the path followed by the patients. This privacy preserved and cost effective mechanism is not only capable to control the coronavirus outbreak but also helps in isolating the patient in his/her house.", 
"This article presents a complete way to evaluate this type of systems efficiently. The objective evaluation is mainly done automatically, using established and proven metrics or frameworks. The subjective evaluation is based directly on the opinion of people. The obtained general results will provide valuable information about the completeness and coherence.",A Proposed Methodology for Subjective Evaluation of Video and Text Summarization,"To evaluate a system that automatically summarizes video files (image and audio), it should be taken into account how the system works and which are the part of the process that should be evaluated, as two main topics to be evaluated can be differentiated: the video summary and the text summary. So, in the present article it is presented a complete way in order to evaluate this type of systems efficiently. With this objective, the authors have performed two types of evaluation: objective and subjective (the main focus of this paper). The objective evaluation is mainly done automatically, using established and proven metrics or frameworks, but it may need in some way the participation of humans, while the subjective evaluation is based directly on the opinion of people, who evaluate the system by answering a set of questions, which are then processed in order to obtain the targeted conclusions. The obtained general results from both evaluation systems will provide valuable information about the completeness and coherence, as well as the correctness of the generated summarizations from different points of view, as the lexical, semantical, etc. perspective. Apart from providing information about the state of the art, it will be presented an experimental proposal too, including the parameters of the experiment and the evaluation methods to be applied.", 
Text summarization task is still an active area of research in natural language preprocessing. Several methods that have been proposed in the literature have presented mixed success. Method developed in a multi-document Arabic text summarization is based on extractive summary.,A proposed textual graph based model for arabic multi-document summarization,"Text summarization task is still an active area of research in natural language preprocessing. Several methods that have been proposed in the literature to solve this task have presented mixed success. However, such methods developed in a multi-document Arabic text summarization are based on extractive summary and none of them is oriented to abstractive summary. This is due to the challenges of Arabic language and lack of resources. In this paper, we present a minimal languagedependent processing abstractive Arabic multi-document summarizer. The proposed model is based on textual graph to remove multi-document redundancy and generate coherent summary. Firstly, the original text, highly redundant and related multidocument, will be converted into textual graph. Next, graph traversal with structural rules will be applied to concatenate related sentences to single ones. Finally, unwanted and less weighted phrases will be removed from the summarized sentences to generate final summary. Preliminary results show that the proposed method has achieved promising results for multidocument summarization.", 
"COVID-19 ( 2019 Novel Coronavirus) has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The situation has made it difficult to access accurate, on-demand information regarding the disease. With the advancements in the field of natural language processing, it has become possible to design chatbots that can automatically answer consumer questions. The authors say such models are rarely applied and evaluated in the healthcare domain.",A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19,"COVID-19 (2019 Novel Coronavirus) has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf (Term Frequency - Inverse Document Frequency), Bidirectional Encoder Representations from Transformers (BERT), Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT), and Universal Sentence Encoder (USE) to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot,we created a user-friendly interactive web application to be hosted online and made its source code available free of charge to anyone interested in running it locally, online, or just for experimental purposes. Overall, our work has yielded significant results in both designing a chatbot that produces high-quality responses to COVID-19-related questions and comparing several embedding generation techniques.", 
"Quantum machine learning (QML) is devoted to devising and implementing quantum algorithms that could enable machine learning faster than that of classical computers. In this paper, a hierarchic quantum mechanics-based framework is investigated to implement both the feature extraction and classification.",A quantum mechanics-based framework for EEG signal feature extraction and classification,"Quantum machine learning (QML) is an emerging research field, which is devoted to devising and implementing quantum algorithms that could enable machine learning faster than that of classical computers. In this paper, a hierarchic quantum mechanics-based framework is investigated to implement both the feature extraction and classification in the electroencephalogram (EEG) signal. Firstly, the classical EEG signal dataset is prepared as a quantum state while the sign of the data point is preserved. The prepared quantum state is then evolved with the quantum wavelet packet transformation (QWPT) and the wavelet packet energy entropy (WPEE) feature is extracted as the input of the subsequent quantum classifier. We finally propose the improved quantum support vector machine with the arbitrary nonlinear kernel, which is employed to predict the label of the EEG signal. The complexity analysis indicates that the proposed framework provides exponential speedup over the same structured classical counterpart. Besides, the quantitative experimental results verify the feasibility and validity.", 
"This paper presents a graph based method to find query specific multi-document summarization. System is divided into two stages, off-line and on-line. Experimental results for multi- document scenarios are encouraging.","A Query Specific Graph Based Approach to Multi-document Text Summarization, Simultaneous Cluster and Sentence Ranking","Recently the focus of query independent summary is shifted to query specific document summarization. This paper presents a graph based method to find query specific multi-document summarization. Our system is divided into two stages, off-line and on-line. We construct document as graph by considering paragraph as nodes in off-line stage. Edge scores are represented node similarities. In online stage, query specific weight are calculated and assigned to node. We then perform keyword search on the document graph and search a minimum top spanning tree for finding relevant nodes that satisfy the keyword search. Resultant summary looks coherent due to simultaneous cluster and sentence ranking. Experimental results for multi-document scenarios are encouraging.", 
"In this paper we propose a new user query based text summarization technique that makes use of Unified Medical Language System. We compare our method with keyword-only approach, and our ontologybased method performs clearly better.",A Query-Based Medical Information Summarization System Using Ontology Knowledge,"As huge amounts of knowledge are created rapidly, effective information access becomes an important issue. Especially for critical domains, such as medical and financial areas, efficient retrieval of concise and relevant information is highly desired. In this paper we propose a new user query based text summarization technique that makes use of Unified Medical Language System, an ontology knowledge source from National Library of Medicine. We compare our method with keyword-only approach, and our ontologybased method performs clearly better. Our method also shows potential to be used in other information retrieval areas.", 
"In this paper, we propose a deep learning approach to tackle the automatic summarization tasks. We incorporate topic information into the convolutional sequence-to-sequence (ConvS2S) model and use self-critical sequence training (SCST) for optimization.",A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization,"In this paper, we propose a deep learning approach to tackle the automatic summarization tasks by incorporating topic information into the convolutional sequence-to-sequence (ConvS2S) model and using self-critical sequence training (SCST) for optimization. Through jointly attending to topics and word-level alignment, our approach can improve coherence, diversity, and informativeness of generated summaries via a biased probability generation mechanism. On the other hand, reinforcement training, like SCST, directly optimizes the proposed model with respect to the non-differentiable metric ROUGE, which also avoids the exposure bias during inference. We carry out the experimental evaluation with state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets. The empirical results demonstrate the superiority of our proposed method in the abstractive summarization.", 
"Summarization corpora are numerous but fragmented, making it challenging for researchers to pinpoint the corpora most suited to a given summarization task. Each corpus is organized differently, which makes it time-consuming to experiment a new summarization algorithm on many corpora. More large-scale corpora for summarization are needed. Agreeing on a data standard would be beneficial to the field.",A Repository of Corpora for Summarization,"Summarization corpora are numerous but fragmented, making it challenging for researchers to efficiently pinpoint corpora most suited to a given summarization task. In this paper, we introduce a repository containing corpora available to train and evaluate automatic summarization systems. We also present an overview of the main corpora with respect to the different summarization tasks, and identify various corpus parameters that researchers may want to consider when choosing a corpus. Lastly, as the recent successes of artificial neural networks for summarization have renewed the interest in creating large-scale corpora for summarization, we survey which corpora are used in neural network research studies. We come to the conclusion that more large-scale corpora for summarization are needed. Furthermore, each corpus is organized differently, which makes it time-consuming for researchers to experiment a new summarization algorithm on many corpora, and as a result studies typically use one or very few corpora. Agreeing on a data standard for summarization corpora would be beneficial to the field.", 
"Summarization corpora are numerous but fragmented, making it challenging for researchers to pinpoint the corpora most suited to a given summarization task. Each corpus is organized differently, which makes it time-consuming to experiment a new summarization algorithm on many corpora. More large-scale corpora for summarization are needed. Agreeing on a data standard would be beneficial to the field.","A Review of Applications, Characteristics and Challenges in Vehicular Ad Hoc Networks (VANETs)","The density of traffic is increasing daily in the world. As a result, congestion, accidents, and pollution are also increasing. Vehicular ad hoc network VANET, a subclass of mobile ad hoc networks MANETs, is a promising approach for future intelligent transportation system its. These networks have no fixed infrastructure and instead rely on the vehicles themselves to provide network functionality. In this review paper, we present an overview of the concept of vehicular ad hoc networks, applications, characteristics. also, we will provide discuss some of the issues and challenges in VANET.", 
"EEG signals have now been popularly used in a wide variety of applications such as seizure detection/prediction, motor imagery classification, mental task classification, emotion classification, sleep state classification, and drug effects diagnosis. Efficient channel selection algorithms are needed with varying importance from one application to another. Signal processing tools such as time- domain analysis, power spectral estimation, and wavelet transform have been used for feature extraction.",A review of channel selection algorithms for EEG signal processing,"Digital processing of electroencephalography (EEG) signals has now been popularly used in a wide variety of applications such as seizure detection/prediction, motor imagery classification, mental task classification, emotion classification, sleep state classification, and drug effects diagnosis. With the large number of EEG channels acquired, it has become apparent that efficient channel selection algorithms are needed with varying importance from one application to another. The main purpose of the channel selection process is threefold: (i) to reduce the computational complexity of any processing task performed on EEG signals by selecting the relevant channels and hence extracting the features of major importance, (ii) to reduce the amount of overfitting that may arise due to the utilization of unnecessary channels, for the purpose of improving the performance, and (iii) to reduce the setup time in some applications. Signal processing tools such as time-domain analysis, power spectral estimation, and wavelet transform have been used for feature extraction and hence for channel selection in most of channel selection algorithms. In addition, different evaluation approaches such as filtering, wrapper, embedded, hybrid, and human-based techniques have been widely used for the evaluation of the selected subset of channels. In this paper, we survey the recent developments in the field of EEG channel selection methods along with their applications and classify these methods according to the evaluation approach.", 
Text summarization is one of the typical tasks of text mining. It is among most attractive research areas now-a-days. This paper gives a review of text summarization and defines the criteria for summary generation.,A Review of Text Summarization,"The excessive use of internet and online technologies has caused a rapid growth of electronic data. When a data is being accessed from such a huge repository of e-documents, hundreds and thousands of documents are retrieved. For a user, it is impossible to read all the retrieved documents. Also, these documents contain redundant information. The problem is termed as Information Overload. Text summarization addresses this problem by producing the summary of related documents. Text summarization is one of the typical tasks of text mining. It is among most attractive research areas now-a-days. This paper gives a review of text summarization and defines the criteria for summary generation.", 
Electroencephalography (EEG) has been a staple method for identifying certain health conditions in patients since its discovery. All the primary methods used in machine learning have been applied in some form in EEG classification. Supervised learning methods are on average of higher accuracy than their unsupervised counterparts.,A Review on Machine Learning for EEG Signal Processing in Bioengineering,"Electroencephalography (EEG) has been a staple method for identifying certain health conditions in patients since its discovery. Due to the many different types of classifiers available to use, the analysis methods are also equally numerous. In this review, we will be examining specifically machine learning methods that have been developed for EEG analysis with bioengineering applications. We reviewed literature from 1988 to 2018 to capture previous and current classification methods for EEG in multiple applications. From this information, we are able to determine the overall effectiveness of each machine learning method as well as the key characteristics. We have found that all the primary methods used in machine learning have been applied in some form in EEG classification. This ranges from Naive-Bayes to Decision Tree/Random Forest, to Support Vector Machine (SVM). Supervised learning methods are on average of higher accuracy than their unsupervised counterparts. This includes SVM and KNN. While each of the methods individually is limited in their accuracy in their respective applications, there is hope that the combination of methods when implemented properly has a higher overall classification accuracy. This paper provides a comprehensive overview of Machine Learning applications used in EEG analysis. It also gives an overview of each of the methods and general applications that each is best suited to.", 
Vehicular ad-hoc network (VANET) could be used for traffic security and efficiency on roads. Once deployed they could bring a new environment to drivers. Privacy in real time environment makes privacy a real challenge.,A Review on Privacy Preserving Authentication in VANETs,"The potential growth in Mobile ad-hoc Network triggered the Vehicular ad-hoc network (VANET) for traffic security and efficiency on roads, because once deployed they could bring a new environment to drivers. Vehicular communication in real time environment makes privacy a real challenge, which might affect the large scale deployment of VANETs. Researchers have proposed many solutions to these problems. This paper provides a detailed study on different privacy preserving authentication algorithms in vehicular communication. We start the paper with an introduction to system architecture and the requirements needed. Detailed discussion on different algorithms comes afterwards.", 
"With large number of documents on the web, there is a increasing need to retrieve the best relevant document. Similarity between words, sentences, paragraphs and documents is an important component in various tasks. This paper describes different types of similarity like lexical similarity, semantic similarity etc.",A Review on Text Similarity Technique used in IR and its Application,"With large number of documents on the web, there is a increasing need to be able to retrieve the best relevant document. There are different techniques through which we can retrieve most relevant document from the large corpus. Similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and text summarization. Text similarity means user’s query text is matched with the document text and on the basis on this matching user retrieves the most relevant documents. Text similarity also plays an important role in the categorization of text as well as document. We can measure the similarity between sentences, words, paragraphs and documents to categorize them in an efficient way. On the basis of this categorization, we can retrieve the best relevant document corresponding to user’s query. This paper describes different types of similarity like lexical similarity, semantic similarity etc.", 
"In recent years, an enormous amount of text data from diversified sources has been emerged day-by-day. This huge amount of data carries essential information and knowledge that needs to be effectively summarized to be useful. The challenges discussed are generic and applicable to every possible scenario in text summarization.",A Review on Text Summarization Techniques,"In recent years, an enormous amount of text data from diversified sources has been emerged day-by-day. This huge amount of data carries essential information and knowledge that needs to be effectively summarized to be useful. Hence, the main contribution of this paper is twofold. We first introduce some concepts related to extractive text summarization and then provide a systematic analysis of various text summarization techniques. In particular, some challenges in extractive summarization of single as well as multiple documents are introduced. The problems focus on the textual assessment and similarity measurement between the text documents are addressed. The challenges discussed are generic and applicable to every possible scenario in text summarization. Then, existing state-of-the-art of extractive summarization techniques are discussed that focus on the identified challenges.", 
Electroencephalogram (EEG) signal analysis is widely used for human-computer interaction and neurological disease diagnosis. Transfer learning is applied in this field to transfer the knowledge learnt in one domain into a different but related domain. This paper describes four main methods of transfer learning and explores their practical applications in EEG signal analysis.,A review on transfer learning in EEG signal analysis,"Electroencephalogram (EEG) signal analysis, which is widely used for human-computer interaction and neurological disease diagnosis, requires a large amount of labeled data for training. However, the collection of substantial EEG data could be difficult owing to its randomness and non-stationary. Moreover, there is notable individual difference in EEG data, which affects the reusability and generalization of models. For mitigating the adverse effects from the above factors, transfer learning is applied in this field to transfer the knowledge learnt in one domain into a different but related domain. Transfer learning adjusts models with small-scale data of the task, and also maintains the learning ability with individual difference. This paper describes four main methods of transfer learning and explores their practical applications in EEG signal analysis in recent years. Finally, we discuss challenges and opportunities of transfer learning and suggest areas for further study.", 
"We present a model for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or ""concept""-level, to a sentence-level model. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables.",A Scalable Global Model for Summarization,"We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or “concept”-level, to a sentence-level model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.", 
"Security and privacy issues may affect the large-scale deployment of VANETs. Researchers have proposed many solutions to these issues. Privacy preserving methods are reviewed, and the tradeoff between security and privacy is discussed.",A Security and Privacy Review of VANETs,"Vehicular ad hoc networks (VANETs) have stimulated interest in both academic and industry settings because, once deployed, they would bring a new driving experience to drivers. However, communicating in an open-access environment makes security and privacy issues a real challenge, which may affect the large-scale deployment of VANETs. Researchers have proposed many solutions to these issues. We start this paper by providing background information of VANETs and classifying security threats that challenge VANETs. After clarifying the requirements that the proposed solutions to security and privacy problems in VANETs should meet, on the one hand, we present the general secure process and point out authentication methods involved in these processes. Detailed survey of these authentication algorithms followed by discussions comes afterward. On the other hand, privacy preserving methods are reviewed, and the tradeoff between security and privacy is discussed. Finally, we provide an outlook on how to detect and revoke malicious nodes more efficiently and challenges that have yet been solved.", 
"Text summarization and sentiment classification, in NLP, are two main tasks implemented on text analysis. A Self-Attentive Hierarchical model for jointly improving text Summarization and Sentiment Classification (SAHSSC) is proposed in this paper. Experimental results evaluated on SNAP amazon online review datasets show that our model outperforms the state-of-the-art baselines by a considerable margin.",A Self-Attentive Hierarchical Model for Jointly Improving Text Summarization and Sentiment Classification,"Text summarization and sentiment classification, in NLP, are two main tasks implemented on text analysis, focusing on extracting the major idea of a text at different levels. Based on the characteristics of both, sentiment classification can be regarded as a more abstractive summarization task. According to the scheme, a Self-Attentive Hierarchical model for jointly improving text Summarization and Sentiment Classification (SAHSSC) is proposed in this paper. This model jointly performs abstractive text summarization and sentiment classification within a hierarchical end-to-end neural framework, in which the sentiment classification layer on top of the summarization layer predicts the sentiment label in the light of the text and the generated summary. Furthermore, a self-attention layer is also proposed in the hierarchical framework, which is the bridge that connects the summarization layer and the sentiment classification layer and aims at capturing emotional information at text-level as well as summary-level. The proposed model can generate a more relevant summary and lead to a more accurate summary-aware sentiment prediction. Experimental results evaluated on SNAP amazon online review datasets show that our model outperforms the state-of-the-art baselines on both abstractive text summarization and sentiment classification by a considerable margin.", 
"The sentences, as individual textual units, usually are too short for major text processing techniques to provide appropriate performance. In this study, we propose a semantic method for implementing an extractive multi-document summarizer system. The proposed framework learns the semantic representation of words from a set of given documents via word2vec method. It expands each sentence through an innovative method with the most informative and the least redundant words related to the main topic of sentence.","A semantic approach to extractive multi-document summarization, Applying sentence expansion for tuning of conceptual densities","Today, due to a vast amount of textual data, automated extractive text summarization is one of the most common and practical techniques for organizing information. Extractive summarization selects the most appropriate sentences from the text and provide a representative summary. The sentences, as individual textual units, usually are too short for major text processing techniques to provide appropriate performance. Hence, it seems vital to bridge the gap between short text units and conventional text processing methods. In this study, we propose a semantic method for implementing an extractive multi-document summarizer system by using a combination of statistical, machine learning based, and graph-based methods. It is a language-independent and unsupervised system. The proposed framework learns the semantic representation of words from a set of given documents via word2vec method. It expands each sentence through an innovative method with the most informative and the least redundant words related to the main topic of sentence. Sentence expansion implicitly performs word sense disambiguation and tunes the conceptual densities towards the central topic of each sentence. Then, it estimates the importance of sentences by using the graph representation of the documents. To identify the most important topics of the documents, we propose an inventive clustering approach. It autonomously determines the number of clusters and their initial centroids, and clusters sentences accordingly. The system selects the best sentences from appropriate clusters for the fnal summary with respect to information salience, minimum redundancy, and adequate coverage. A set of extensive experiments on DUC2002 and DUC2006 datasets was conducted for investigating the proposed scheme. Experimental results showed that the proposed sentence expansion algorithm and clustering approach could considerably enhance the performance of the summarization system. Also, comparative experiments demonstrated that the proposed framework outperforms most of the state-of-the-art summarizer systems and can impressively assist the task of extractive text summarization.", 
"In this paper we propose a new user query based text summarization technique. It makes use of WordNet, a general knowledge source from Princeton University. Our summarization system is specially tuned to summarize medical documents.",A Semantic Free-text Summarization System Using Ontology Knowledge,"As huge amounts of knowledge are created rapidly, effective information access becomes an important issue. Especially for critical domains, such as medical and financial areas, efficient retrieval of concise and relevant information is highly desired. In this paper we propose a new user query based text summarization technique that makes use of WordNet, a general knowledge source from Princeton University. Our summarization system is specially tuned to summarize medical documents by integrating Unified Medical Language System, a medical ontology knowledge source from National Library of Medicine. We participated in the Document Understanding Conference 2007 Main Task and ranked in the middle tier of 32 systems.", 
"Natural Language Processing and Computational Linguistics applications involve the generation of new texts based on some existing texts. How to automatically and accurately assess quality of these applications has been a problem for decades. In this paper, we present some preliminary results on one especially useful and challenging problem.",A Semantic QA-Based Approach for Text Summarization Evaluation,"Many Natural Language Processing and Computational Linguistics applications involve the generation of new texts based on some existing texts, such as summarization, text simplification and machine translation. However, there has been a serious problem haunting these applications for decades, that is, how to automatically and accurately assess quality of these applications. In this paper, we will present some preliminary results on one especially useful and challenging problem in NLP system evaluation – how to pinpoint content differences of two text passages (especially for large passages such as articles and books). Our idea is intuitive and very different from existing approaches. We treat one text passage as a small knowledge base, and ask it a large number of questions to exhaustively identify all content points in it. By comparing the correctly answered questions from two text passages, we will be able to compare their content precisely. The experiment using 2007 DUC summarization corpus clearly shows promising results.", 
New query-based extractive summary methodology is put forward. The approach makes use of phrasal decomposition of the text where each sentence is ascribed a scoring function. The scoring function is expressed as a convex combination of a set of features that are extracted beforehand.,"A Semantic Summarization System, University of Birmingham at TAC 2008","Text summarization of document or multi-documents has been acknowledged as one of the most challenging tasks in information system community because of the rich semantic structure of the language and the subjectivity inherent to the summarization task. In this paper, a new query-based extractive summary methodology is put forward. The approach makes use of phrasal decomposition of the text where each sentence is ascribed a scoring function, which will then be used to identify the most relevant sentences in the sequel. The scoring function is expressed as a convex combination of a set of features that are extracted beforehand from the (multi) document(s). Besides, the scoring function includes a semantic similarity evaluation where the WordNet taxonomy is used in conjunction with a variety of other extracted features, as a basis to construct the sentence-sentence semantic similarity. The system architecture as well as its linguistics processing parts are described. Finally, we present the results of our participation in TAC 2008 with possible perspectives.", 
"Summarization is one of the important Natural Language processing applications. In this paper, we propose a new Arabic query-based text summarization model. The model accepts both user query and Arabic document and then generates the extractive summary. We apply the Latent Semantic Analysis technique and exploiting the Arabic WordNet (AWN) ontology.",A Semantic Text Summarization Model for Arabic Topic-Oriented,"In the era of data overloading, Text Summarization systems (TSs) is one of the important Natural Language processing applications. These systems provide a concise form for the input document(s). According to the type of output summary, Text Summarization can be classified into extractive and abstractive. While the extractive text summarization is the process of identifying the important sections of the input text and producing them verbatim, the abstractive text summarization produces a new material in a generalized form. To facilitate the topic-oriented summarization, current research efforts focus on query-based text summarization, which summarizes the input document according to the user query. Although, the Arabic language is one of the Semitic languages and is spoken by 422 million people, there are very limited research efforts in Arabic query-based text summarization. In this paper, we propose a new Arabic query-based text summarization model. The model accepts both user query and Arabic document and then generates the extractive summary. The proposed model generates the extractive summary for the input document semantically by applying the Latent Semantic Analysis technique and exploiting the Arabic WordNet (AWN) ontology. Finally, to show the importance of the proposed model, a case study is presented.", 
Text summarization is an area that supports the cause of information retrieval systems. This paper discusses on the relevance of using traditional stoplists for text summarization. A sentence scoring mechanism has been developed by combining the above methodologies with semantic analysis.,A sentence scoring method for extractive text summarization based on Natural language queries,"The developments in storage devices and computer networks have given the scope for the world to become a paperless community, for example Digital news paper systems and digital library systems. A paperless community is heavily dependent on information retrieval systems. Text summarization is an area that supports the cause of information retrieval systems by helping the users to get their needed information. This paper discusses on the relevance of using traditional stoplists for text summarization and the use of Statistical analysis for sentence scoring. A new methodology is proposed for implementing the stoplist concept and statistical analysis concept based on parts of speech tagging. A sentence scoring mechanism has been developed by combining the above methodologies with semantic analysis. This sentence scoring method has given good results when applied to find out the relation between natural language queries and the sentences in a document.", 
In paper text summarization represented as a sentence scoring and selection process. The process is modeled as a multi-objective optimization problem. ,A sentence selection model and HLO algorithm for extractive text summarization,In paper text summarization represented as a sentence scoring and selection process. The process is modeled as a multi-objective optimization problem. The proposed model attempts to find balance between coverage and redundancy in a summary. For solving the optimization problem a human learning optimization algorithm is utilized., 
"Sentence-based extractive summarization aims at automatically generating shorter versions of texts. Paper proposes novel approach, based on abstract argumentation, to select the sentences in a text that are to be included in the summary. It also proposes a new strategy for similarity assessment among sentences, adopting a different similarity measure.",A Similarity-Based Abstract Argumentation Approach to Extractive Text Summarization,"Sentence-based extractive summarization aims at automatically generating shorter versions of texts by extracting from them the minimal set of sentences that are necessary and sufficient to cover their content. Providing effective solutions to this task would allow the users to save time in selecting the most appropriate documents to read for satisfying their information needs or for supporting their decision-making tasks. This paper proposes 2 contributions: (i) it defines a novel approach, based on abstract argumentation, to select the sentences in a text that are to be included in the summary; (ii) it proposes a new strategy for similarity assessment among sentences, adopting a different similarity measure than those traditionally exploited in the literature. The effectiveness of the proposed approach was confirmed by experimental results obtained on the English subset of the benchmark MultiLing2015 dataset.", 
"This study aims to develop a speller system based on a bipolar single-channel electroencephalogram with sufficient accuracy. The proposed system consists of a custom-designed headset, a new virtual keyboard with 58 characters, special symbols, and digits. It uses a five-target steady-state visual evoked potential (SSVEP)-based BCI utilizing a one-dimensional Convolutional Neural Network.",A Single-Channel SSVEP-Based BCI Speller using Deep Learning,"This study aims to develop a speller system based on a bipolar single-channel electroencephalogram (EEG) with sufficient accuracy. The proposed system consists of a custom-designed headset, a new virtual keyboard with 58 characters, special symbols, and digits, and a five-target steady-state visual evoked potential (SSVEP)-based BCI utilizing one-dimensional Convolutional Neural Network (1-D CNN) for SSVEP frequency detection. The deep learning model is implemented and trained under training mode before applying in the operation mode of the system. To validate the proposed model, we acquire the training dataset with numerous testing conditions including different frequency resolutions of the feature and different time window lengths of analysis. Two types of features based on frequency domain are investigated to compare their performances in terms of classification accuracy of the model. The experimental results from eight subjects shows that, on average, the proposed model can classify five-class SSVEP data with a high accuracy of 99.2%. The proposed BCI is then employed in an online experiment of spelling the word “SPELLER” using a 2-s time window. Consequently, the system achieves an average accuracy of 97.4% and an information transfer rate of 49 ± 7.7 bpm, showing the practicality and feasibility of implementing a reliable single-channel SSVEP-based speller utilizing 1-D CNN.",  
"Software-defined networking (SDN) is considered as one of the main enabler technologies of 5G that is expected to propel the penetration of vehicular networks. The rapid development of wireless technology has generated an avalanche demand for bandwidth-intensive applications. In this paper, we propose an SDN based incentive caching mechanism for a 5G-enabled vehicular network.",A Stackelberg Game Approach for Incentive V2V Caching in Software-Defined 5G-enabled VANET,"Software-defined networking (SDN) is considered as one of the main enabler technologies of 5G that is expected to propel the penetration of vehicular networks. The rapid development of wireless technology has generated an avalanche demand for bandwidth-intensive applications (e.g., video-ondemand, streaming video, etc.) causing an exponential increase in mobile data traffic. Moreover, the use of edge caching technique enhances network resource utilization and reduce backhaul traffic. Many incentive mechanisms have been developed to encourage caching actors to enhance the caching process. In this paper, we propose an SDN based incentive caching mechanism for a 5G-enabled vehicular network. Our caching strategy consists of a small base station (SBS) that encourages mobile vehicles equipped with embarked caches to store and share its popular contents using vehicle to vehicle (V2V) communication. SBS aims to offload the cellular core links and reduce traffic congestion, where cache-enabled vehicles compete to earn more SBS reward. The interaction between the SBS and the cache-enabled vehicles is formulated using a Stackelberg game with a non-cooperative sub-game to model the conflict between cache-enabled vehicles. The SBS acts first as a leader by announcing the number of popular contents that it wants to cache and the cache-enabled vehicles respond after by the optimal number of contents they accept to cache and the corresponding caching price. Two optimization problems are investigated and the Stackelberg equilibrium is derived. The simulation results demonstrated the efficiency of our game theoretical based incentive V2V caching strategy.", 
"Coronary arteriongraphy (CAG) is an accurate invasive technique for the diagnosis of coronary heart disease (CHD) However, its invasive procedure is not appropriate for the detection of CHD in the annual physical examination. In this study, a two level stacking based model is designed in which level 1 is base-level and level 2 is metalevel.",A Stacking-Based Model for Non-Invasive Detection of Coronary Heart Disease,"Coronary arteriongraphy (CAG) is an accurate invasive technique for the diagnosis of coronary heart disease (CHD). However, its invasive procedure is not appropriate for the detection of CHD in the annual physical examination. With the successful application of machine learning (ML) in various fields, our goal is to perform selective integration of multiple ML algorithms and verify the validity of feature selection methods with personal clinical information commonly seen in the annual physical examination. In this study, a two level stacking based model is designed in which level 1 is base-level and level 2 is metalevel. The predictions of base-level classifiers is selected as the input of meta-level. The pearson correlation coefficient and maximum information coefficient are first calculated to find the classifier with the lowest correlation. Then enumeration algorithm is used to find the best combining classifiers which acquire the best result in the end. The Z-Alizadeh Sani CHD dataset which we use consists of 303 cases verified by CAG. Experimental results demonstrate that the proposed model obtains an accuracy, sensitivity and specificity of 95.43%, 95.84%, 94.44%, respectively for the detection of CHD. The proposed method can effectively aid clinicians to detect those with normal coronary arteries from those with CHD.", 
There are no formal procedures or models for digital data acquisition to which courts of law can refer. This paper proposes a model that is standardised in that it can enable digital forensic investigators to follow a uniform approach. It can be applied in both law enforcement and corporate investigations.,A standardised data acquisition process model for digital forensic investigations,"Similar to traditional evidence, courts of law do not assume that digital evidence is reliable if there is no evidence of some empirical testing regarding the theories and techniques pertaining to its production. Courts take a careful notice of the way in which digital evidence has been acquired and stored. In contrast with traditional crimes for which there are well-established standards and procedures upon which courts can rely, there are no formal procedures or models for digital data acquisition to which courts of law can refer. A standardised data acquisition process model is needed to enable digital forensic investigators to follow a uniform approach, and to assist courts of law in determining the reliability of digital evidence presented to them. This paper proposes a model that is standardised in that it can enable digital forensic investigators in following a uniform approach, and that is generic in that it can be applied in both law enforcement and corporate investigations. To carry out the research presented in the paper, the design science research process (DSRP) methodology proposed by Peffers et al. (2006) has been followed.", 
"Automatic Document Summarization is to compress an original document into a summarized version by extracting almost all of the essential concepts. This research focuses on developing a statistical automatic text summarization approach, K-mixture probabilistic model, to enhancing the quality of summaries.",A Statistical Approach for Automatic Text Summarization by Extraction,"Automatic Document Summarization is a highly interdisciplinary research area related with computer science as well as cognitive psychology. This Summarization is to compress an original document into a summarized version by extracting almost all of the essential concepts with text mining techniques. This research focuses on developing a statistical automatic text summarization approach, Kmixture probabilistic model, to enhancing the quality of summaries. KSRS employs the K-mixture probabilistic model to establish term weights in a statistical sense, and further identifies the term relationships to derive the semantic relationship significance (SRS) of nouns. Sentences are ranked and extracted based on their semantic relationship significance values. The objective of this research is thus to propose a statistical approach to text summarization. We propose a K-mixture semantic relationship significance (KSRS) approach to enhancing the quality of document summary results. The K-mixture probabilistic model is used to determine the term weights. Term relationships are then investigated to develop the semantic relationship of nouns that manifests sentence semantics. Sentences with significant semantic relationship, nouns are extracted to form the summary accordingly.", 
"In this work, we use Hidden Markov Models (HMM), Conditional Random Field (CRF), Gaussian Mixture Models (GMM) and Mathematical Methods of Statistics (MMS) for Chinese and Japanese text summarization. The results show that HMM, CRF and GMM have remarkable increases than MMS.",A study on cross-language text summarization using supervised methods,"In this work, we use Hidden Markov Models (HMM), Conditional Random Field (CRF), Gaussian Mixture Models (GMM) and Mathematical Methods of Statistics (MMS) for Chinese and Japanese text summarization. The purpose of this work is to study the applicability of mentioned three trainable models for cross-language text summarization. For model training, we use several training features such as sentence position, sentence centrality, number of Name Entity and so on. For model testing, Chinese on-line news and Japanese news are used as test data which are extracted from web pages. We evaluate each model by measuring the precision at the compression rate 10%, 20% and 30%. MMS is a baseline method. The results show that HMM, CRF and GMM have remarkable increases than MMS on both Chinese and Japanese text summarization by using the same training features. Especially, GMM model make a best performance in all tests.", 
"Summarization is commonly classified into two types, extractive and abstractive. Summarization by abstraction needs understanding of the original text and then generating the summary which is semantically related. Ontology is one among the approach used for getting abstractive summary for a specific domain.",A Study on Ontology Based Abstractive Summarization,"With widespread use of Internet and the emergence of information aggregation on a large scale, a quality text summarization is essential to effectively condense the information. Automatic summarization systems condense the documents by extracting the most relevant facts. Summarization is commonly classified into two types, extractive and abstractive. Summarization by abstraction needs understanding of the original text and then generating the summary which is semantically related. Abstractive summarization requires the understanding of complex natural language processing tasks. There are many methods adopted for abstractive summarization. Ontology is one among the approach used for getting abstractive summary for a specific domain. In this paper, we discuss about various works carried out using ontology for abstractive text summarization.", 
This paper presents a text summarizer for Bangla. It uses some extraction methods for text summarization.,A study on text summarization techniques and implement few of them for Bangla language,"Text summarization is the technique which automatically creates an abstract or summary of a text. The technique has been developed for many years. So a survey has been done on different summarization techniques. No work in this area has been done for Bangla language. This paper presents a text summarizer for Bangla, which uses some extraction methods for text summarization.", 
Text Summarization is the process of identifying and extracting the most vital information in a document. It has been seen as an effective method for dealing with increasing amount of information on the Internet. Genetic Programming was used to evolve the function that ranks the sentences in a documents.,A Study on the Use of Genetic Programming for Automatic Text Summarization,"Text Summarization is the process of identifying and extracting the most vital information in a document. It has been seen as an effective method for dealing with increasing amount of information on the Internet nowadays. In this paper, we present an application of Genetic Programming to the problem of Automatic Text Summarization. Genetic Programming was used to evolve the function that ranks the sentences in a document based on their importance. The summary was extracted by selecting the sentences that have the highest rankings. The experiment was conducted on a number of Vietnamese news documents. The result showed that the summaries created by Genetic Programming are better than those created by a number of statistic based methods and even by human (non-experts).", 
"Automatic text summarization is the process of automatically reducing the length of documents. Due to the flood of digital text-based information, there is a great demand for summarization systems. In this paper, we investigate a number of word-embedding based approaches for sentence representation.",A Study on the Use of Word Embeddings and PageRank for Vietnamese Text Summarization,"Automatic text summarization is the process of automatically reducing the length of documents without losing the primary ideas. Due to the flood of digital text-based information, there is a great demand for summarization systems. In this paper, we investigate a number of word-embedding based approaches for sentence representation which are combined with the PageRank algorithm to select sentences for summary construction. We compare these new methods with a range of other current approaches to summarization. While the same summarization approaches can generally be applied across different languages, we target Vietnamese because of the relative lack of previous work in this space and also because it provides a good example of a language which generally requires word segmentation. Our experiments find that a word-embedding and graph based approach is an effective strategy for Vietnamese summarization and that word segmentation is not necessary for achieving good summarization results.", 
"System retrieves and summarizes scientific documents for a given information need. System ingested 270,000 papers and its summarization module aims to generate concise yet detailed summaries. System was validated with human experts.",A Summarization System for Scientific Documents,"We present a novel system providing summaries for Computer Science publications. Through a qualitative user study, we identified the most valuable scenarios for discovery, exploration and understanding of scientific documents. Based on these findings, we built a system that retrieves and summarizes scientific documents for a given information need, either in form of a free-text query or by choosing categorized values such as scientific tasks, datasets and more. Our system ingested 270,000 papers, and its summarization module aims to generate concise yet detailed summaries. We validated our approach with human experts.", 
This paper presents a Machine Learning-based approach to Arabic text summarization which uses AdaBoost. This technique is employed to predict whether a new sentence is likely to be included in the summary. This approach was compared against other Machine Learning approaches.,A Supervised Approach to Arabic Text Summarization Using AdaBoost,"In recent years, research in text summarization has become very active for many languages. Unfortunately, looking at the effort devoted to Arabic text summarization, we find much fewer attention paid to it. This paper presents a Machine Learning-based approach to Arabic text summarization which uses AdaBoost. This technique is employed to predict whether a new sentence is likely to be included in the summary or not. In order to evaluate the approach, we have used a corpus of Arabic articles. This approach was compared against other Machine Learning approaches and the results obtained show that the approach we suggest using AdaBoost outperforms other existing approaches.", 
"Text summarization has become a hot topic, it has attracted experts in data mining and natural language processing field. In this paper, we present a Vietnamese text summarization method based on sentence extraction approach using neural network for learning.",A supervised learning method combine with dimensionality reduction in Vietnamese text summarization,"The World Wide Web has brought us a vast amount of online information. When we search with a keyword, data feedback from many different websites and the user cannot read all the information. So that, text summarization has become a hot topic, it has attracted experts in data mining and natural language processing field. For Vietnamese, some methods of text summarization based on that have been proposed for English also bring some significant results. However, still remain some difficult problems to treat with the Vietnamese language processing, typical in this is the Vietnamese text segmentation tool and text summarization corpus. In this paper, we present a Vietnamese text summarization method based on sentence extraction approach using neural network for learning combine reducing dimensional features to overcome the cost when building term sets and reduce the computational complexity. The experimental results show that our method is really effective in reducing computational complexity, and is better than some methods that have been proposed previous.", 
Text summarization is used to compress source text into a diminished version conserving its information content and overall meaning. Text summarization methods can be classified into extractive and abstractive summarization. This paper gives comparative study of various text summarization techniques.,A SURVEY AUTOMATIC TEXT SUMMARIZATION,"Text summarization is compress the source text into a diminished version conserving its information content and overall meaning. Because of the great amount of the information we are provided it and thanks to development of Internet Technologies, text summarization has become an important tool for interpreting text information. Text summarization methods can be classified into extractive and abstractive summarization. An extractive summarization method involves selecting sentences of high rank from the document based on word and sentence features and put them together to generate summary. The importance of the sentences is decided based on statistical and linguistic features of sentences. An abstractive summarization is used to understanding the main concepts in a given document and then expresses those concepts in clear natural language. In this paper, gives comparative study of various text summarization techniques.", 
"Autonomous Cyber Reasoning System (CRS) has recently attracted extensive attention from both industry and academia. Utilizing automated system to detect, exploit and patch software vulnerabilities seems so attractive because of its scalability and cost-efficiency compared with the human expert based solution.","A Survey of Automatic Software Vulnerability Detection, Exploitation and Patching Techniques","With the success of the Cyber Grand Challenge (CGC) sponsored by DARPA, the topic of Autonomous Cyber Reasoning System (CRS) has recently attracted extensive attention from both industry and academia. Utilizing automated system to detect, exploit and patch software vulnerabilities seems so attractive because of its scalability and cost-efficiency compared with the human expert based solution. In this paper, we give an extensive survey of former representative works related to the underlying technologies of a CRS, including vulnerability detection, exploitation and patching. As an important supplement, we then review several pioneer studies that explore the potential of machine learning technologies in this field, and point out that the future development of Autonomous CRS is inseparable from machine learning.", 
"This paper concentrates on survey and performance analysis of automatic text summarizers for Marathi language. Summaries are of two types: Abstractive summaries and Extractive summary. Various linguistic features for selecting important sentences in summary are: Marathi headlines identification, identification of lines just next to headlines. Weights of features are determined using mathematical regression.",A Survey of Automatic Text Summarization System for Different Regional Language in India,"Automatic text summarization is technique of compressing the original text into shorter form which will provide same meaning and information as provided by original text. The brief summary produced by summarization system allows readers to quickly and easily understand the content of original documents without having to read each individual document. The overall motive of text summarization is to convey the meaning of text by using less number of words and sentences. Summaries are of two types: Abstractive summaries and Extractive summaries. Extractive summaries involve extracting relevant sentences from the source text in proper order. The relevant sentences are extracted by applying statistical and language dependent features to the input text. On the other hand, abstractive text summaries are made by applying natural language understanding. This system comprises of two main steps: Pre Processing and Processing phase. Pre Processing phase represents the Marathi text in structured way. In processing phase, different features deciding the importance of sentences are determined and calculated. Some of the statistical features are Marathi keywords identification, relative sentence length feature and numbered data feature. Various linguistic features for selecting important sentences in summary are: Marathi headlines identification, identification of lines just next to headlines, identification of Marathi-nouns, identification of Marathi-proper-nouns, identification of common-English-Marathi-nouns, identification of Marathi-cue-phrases and identification of title-keywords in sentences. Scores of sentences are determined from sentence-feature-weight equation. Weights of features are determined using mathematical regression. This paper concentrates on survey and performance analysis of automatic text summarizers for Marathi language.", 
"In the era of Big Data, textual data is rapidly growing and is available in many different languages. In the fast-moving world, it's difficult to read all the text-content. Automatic text summarization is a technique which compresses large text to a shorter text which includes the important information.",A survey of automatic text summarization techniques for Indian and foreign languages,"Today in the era of Big Data, textual data is rapidly growing and is available in many different languages. In the fast-moving world, it’s difficult to read all the text-content. Hence, the need for text summarization is being in the spotlight. Automatic text summarization is a technique which compresses large text to a shorter text which includes the important information. There are two types of summaries: Extractive summaries and Abstractive summaries. Extractive summaries are produced by extracting the whole sentences from the source text. Abstractive summaries are produced by reformulating sentences of the source text. Several text summarization techniques have been proposed in past years for English and various European languages but there are very few techniques that can be found for native languages of India. This paper presents a survey of text summarization techniques for various Indian and foreign languages like English, European, etc. Also, an approach for summarizing Hindi text using machine learning technique has been proposed. We have also described few challenges which are still under research.", 
Survey of relevant data mining techniques which are involved in risk prediction of heart disease. This paper provides a quick and easy understanding of various prediction models in data mining and helps to find best model for further work. These techniques are chosen based on their efficiency in the literature.,A Survey of Data Mining Techniques on Risk Prediction Heart Disease,"Comparison of classification techniques in Data mining to find the best technique for creating risk prediction model of heart disease at minimum effort. In Data mining, different methods used to find risk prediction of heart disease. There are two types of model used in analysis of data. First one is applying single model to various heart data and another one is applying combined model to the data. The combined model also known as hybrid model. This paper provides a quick and easy understanding of various prediction models in data mining and helps to find best model for further work. This is unique approach because various techniques listed and expressed in bar chart to understand accuracy level of each. These techniques are chosen based on their efficiency in the literature. In previous studies of different researcher expressed their effort on finding best approach for risk prediction model and here we found best model by comparing those researcher’s findings as survey. This survey helps to understand the recent techniques involved in risk prediction of heart disease at classification in data mining. Survey of relevant data mining techniques which are involved in risk prediction of heart disease provides best prediction model as hybrid approach comparing with single model approach.", 
"Brain Computer Interfacing (BCI) is a communication system which uses commands to interact with outside world. BCI's have extensive applications such as to classify motor imagery tasks, and to detect abnormalities and loss of functions of brain.",A Survey of Deep Learning and Traditional Approaches for EEG Signal Processing and Classification,"Processing of Electroencephalography (EEG) signals help to detect the specific patterns in a person’s ongoing brain activity and then translate these patterns into control command. Brain Computer Interfacing (BCI) is a communication system which uses these commands to interact with outside world. BCI’s have extensive applications such as to classify motor imagery tasks, and to detect abnormalities and loss of functions of brain. Selection and optimization of the efficient processing technique to extract discriminative features from the EEG signals is a key to develop high performance and robust system. The main purpose of the feature selection and reduction process is threefold: by extracting relevant features computational complexity can be reduced, it improve performance by avoiding over fitting which may be caused due to irrelevant features, selected features will take less time to process. Some novel methods such as time-domain analysis, power spectral estimation, and wavelet transform have been widely used for the evaluation of the selected subset of features. In this paper first, we survey comparative evaluation of various recent techniques for feature selection and classification applied for BCI at each stage. Second, the use of deep learning techniques for brain signal classification is explored.", 
"Low power and long range machine-to-machine (M2M) communication techniques are expected to provide ubiquitous connections for the wireless devices. The design of the LPWA techniques features low cost, low data rate, long communication range, and low power consumption. The paper also presents a summary of the research directions for improving the performance of the surveyed M2M technologies.",A Survey of Enabling Technologies of Low Power and Long Range Machine-to-Machine Communications,"Low power and long range machine-to-machine (M2M) communication techniques are expected to provide ubiquitous connections for the wireless devices. In this paper, three major low power and long range M2M solutions are surveyed. The first type of solutions is referred to as the low power wide area (LPWA) network. The design of the LPWA techniques features low cost, low data rate, long communication range, and low power consumption. The second type of solutions is the IEEE 802.11ah which features higher data rates using a wider bandwidth than the LPWA-based solutions. The third type of solutions is operated under the cellular network infrastructure. Based on the analysis of the pros and cons of the enabling technologies of the surveyed M2M solutions, as well as the corresponding deployment strategies, the gaps in knowledge are identified. The paper also presents a summary of the research directions for improving the performance of the surveyed low power and long range M2M communication technologies.", 
"There is a massive enthusiasm concerning the generation of automatic text summary frameworks. This survey highlights, for the first time, how the swarm intelligence (SI) optimization techniques are performed to solve the text summarization task efficiently. The research will enthuse researchers to further consider the various types of SI when solving the summarization tasks.",A survey of multiple types of text summarization with their satellite contents based on swarm intelligence optimization algorithms,"Due to the tremendous increment of data on the web, extracting the most important data as a conceptual brief would be valuable for certain users. Therefore, there is a massive enthusiasm concerning the generation of automatic text summary frameworks to constitute abstracts automatically from the text, web, and social network messages associated with their satellite content. This survey highlights, for the first time, how the swarm intelligence (SI) optimization techniques are performed to solve the text summarization task efficiently. Additionally, a convincing justification of why SI, especially Ant Colony Optimization (ACO), has been presented. Unfortunately, three types of text summarization tasks using SI indicate bit utilizing in the literature when contrasted with the other summarization techniques as machine learning and genetic algorithms, in spite of the fact that there are seriously promising outcomes of the SI methods. On the other hand, it has been noticed that the summarization task with multiple types has not been formalized as a multiobjective optimization (MOO) task before, despite that there are many objectives which can be considered. Moreover, the SI was not employed before to support the real-time summary approaches. Thus, a new model has been proposed to be adequate for achieving many objectives and to satisfy the real-time needs. Eventually, this study will enthuse researchers to further consider the various types of SI when solving the summarization tasks, particularly, in the short text summarization (STS) field.", 
"An autonomous system is needed to automate operations such as product or brand recognition, stock tracking and planogram matching. This survey classifies and compares all existing works with the aim to guide researchers working on merchandising.",A Survey of Product Recognition in Shelf Images,"Nowadays, merchandising is one of the significant method which allows to increase the sales. Therefore, activities such as monitoring the number of products on the shelves, completing the missing products and matching the planogram continuously have become important. An autonomous system is needed to automate operations such as product or brand recognition, stock tracking and planogram matching. In the literature, it is seen that many studies have been carried out in order to address this issue. This survey classifies and compares all existing works with the aim to guide researchers working on merchandising.", 
"Measuring the similarity between words, sentences, paragraphs and documents is an important component in various tasks. This survey discusses the existing works on text similarity through partitioning them into three approaches",A Survey of Text Similarity Approaches,"Measuring the similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and text summarization. This survey discusses the existing works on text similarity through partitioning them into three approaches; String-based, Corpus-based and Knowledge-based similarities. Furthermore, samples of combination between these similarities are presented.", 
Numerous approaches for identifying important content for automatic text summarization have been developed to date. In this chapter we give a broad overview of existing approaches based on these distinctions. We also point out some of the peculiarities of the task of summarization which have posed challenges to machine learning approaches.,A Survey of Text Summarization Techniques,"Numerous approaches for identifying important content for automatic text summarization have been developed to date. Topic representation approaches first derive an intermediate representation of the text that captures the topics discussed in the input. Based on these representations of topics, sentences in the input document are scored for importance. In contrast, in indicator representation approaches, the text is represented by a diverse set of possible indicators of importance which do not aim at discovering topicality. These indicators are combined, very often using machine learning techniques, to score the importance of each sentence. Finally, a summary is produced by selecting sentences in a greedy approach, choosing the sentences that will go in the summary one by one, or globally optimizing the selection, choosing the best set of sentences to form a summary. In this chapter we give a broad overview of existing approaches based on these distinctions, with particular attention on how representation, sentence scoring or summary selection strategies alter the overall performance of the summarizer. We also point out some of the peculiarities of the task of summarization which have posed challenges to machine learning approaches for the problem, and some of the suggested solutions.", 
"There is a great demand for summarizing text documents to provide a representative substitute for the original documents. Several generic text summarization algorithms have been developed, each with its own advantages and disadvantages. Some algorithms are particularly good for. summarizing short documents but not for long ones. Others perform well in identifying. and summarizing single-topic documents but their precision degrades sharply with. multi-topic papers.",A survey of unstructured text summarization techniques,"Due to the explosive amounts of text data being created and organizations increased desire to leverage their data corpora, especially with the availability of Big Data platforms, there is not usually enough time to read and understand each document and make decisions based on document contents. Hence, there is a great demand for summarizing text documents to provide a representative substitute for the original documents. By improving summarizing techniques, precision of document retrieval through search queries against summarized documents is expected to improve in comparison to querying against the full spectrum of original documents. Several generic text summarization algorithms have been developed, each with its own advantages and disadvantages. For example, some algorithms are particularly good for summarizing short documents but not for long ones. Others perform well in identifying and summarizing single-topic documents but their precision degrades sharply with multi-topic documents. In this article we present a survey of the literature in text summarization. We also surveyed some of the most common evaluation methods for the quality of automated text summarization techniques. Last, we identified some of the challenging problems that are still open, in particular the need for a universal approach that yields good results for mixed types of documents.", 
Document summarization means retrieved short and important text from the source document. Plenty of techniques have been developed on English summarization and other Indian languages.,A Survey of Various Methods for Text Summarization,"Document summarization means retrieved short and important text from the source document. In this paper, we studied various techniques. Plenty of techniques have been developed on English summarization and other Indian languages but very less efforts have been taken for Hindi language. Here, we discusses various techniques in which so many features are included such as time and memory consumption, efficiency, accuracy, ambiguity, redundancy.", 
"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world. Current demand for Machine-Type Communications (MTC) has resulted in a variety of communication technologies. 5G mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT.",A Survey on 5G Networks for the Internet of Things Communication Technologies and Challenges,"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. The current demand for Machine-Type Communications (MTC) has resulted in a variety of communication technologies with diverse service requirements to achieve the modern IoT vision. More recent cellular standards like Long-Term Evolution (LTE) have been introduced for mobile devices but are not well suited for low-power and low data rate devices such as the IoT devices. To address this, there is a number of emerging IoT standards. Fifth Generation (5G) mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT. In this paper, the state-of-the-art of the IoT application requirements along with their associated communication technologies are surveyed. Additionally, the 3rd Generation Partnership Project (3GPP) cellular-based LowPower Wide Area (LPWA) solutions to support and enable the new service requirements for Massive to Critical IoT use cases are discussed in detail, including Extended Coverage Global System for Mobile Communications for the Internet of Things (EC-GSM-IoT), enhanced Machine-Type Communications (eMTC), and Narrowband-Internet of Things (NB-IoT). Furthermore, 5G New Radio (NR) enhancements for new service requirements and enabling technologies for the IoT are introduced. This paper presents a comprehensive review related to emerging and enabling technologies with main focus on 5G mobile networks that is envisaged to support the exponential traffic growth for enabling the IoT. The challenges and open research directions pertinent to the deployment of Massive to Critical IoT applications are also presented in coming up with an efficient context-aware congestion control (CACC) mechanism.", 
"Text summarization is the part of Information Retrieval system which comes under the area of Text Mining. This is the most popular application for information compression. In present time, the growth of data increases hugely on World Wide Web or on user's desktops.",A Survey on A hybrid method for query based automatic summarization system for text,"Text summarization is the part of Information Retrieval system which comes under the area of Text Mining. This is the most popular application for information compression. Text summarization is a process of generating a summary by reducing the size of original document and concern important information of original document. There is arising a need to provide high quality summary in less time because in present time, the growth of data increases hugely on World Wide Web or on user’s desktops so Multi-Document summarization is the best tool for making summary in less time. This paper presents a survey of existing techniques with the trinket highlighting the need of intelligent Multi-Document summarizer.",   
"Summarization is classified mainly into two techniques Abstract and Extract. Summarization area was broadly spread over different research fields, Natural Language Processing (NLP), Machine Learning and Semantics.",A survey on abstractive summarization techniques,"Text Summarization solves climacteric problems in furnishing information to the necessities of user. Due to explosive growth of digital data on internet, information floods are the results to the user queries. This makes user impractical to read entire documents and select the desirables. To this problem summarization is a novel approach which surrogates the original document by not deviating from the theme helps the user to find documents easily. Summarization area was broadly spread over different research fields, Natural Language Processing (NLP), Machine Learning and Semantics etc… Summarization is classified mainly into two techniques Abstract and Extract. This article gives a deep review of Abstract summarization techniques.", 
"Summarization is the task of extracting salient information from the original text document. It is very difficult for humans to understand and interpret the content of the text. This paper collectively summarizes and deciphers the various methodologies, challenges and issues of abstractive summarization.",A survey on abstractive text summarization,"Text Summarization is the task of extracting salient information from the original text document. In this process, the extracted information is generated as a condensed report and presented as a concise summary to the user. It is very difficult for humans to understand and interpret the content of the text. In this paper, an exhaustive survey on abstractive text summarization methods has been presented. The two broad abstractive summarization methods are structured based approach and semantic based approach. This paper collectively summarizes and deciphers the various methodologies, challenges and issues of abstractive summarization. State of art benchmark datasets and their properties are being explored. This survey portrays that most of the abstractive summarization methods produces highly cohesive, coherent, less redundant summary and information rich.", 
"Textual content on the web, in particular, is growing at an exponential rate. The ability to decipher through such a massive amount of data to extract useful information is a significant undertaking. Text summarization systems intent to assist with content reduction keeping the relevant information and filtering the non-relevant parts of the text. This paper offers an in-depth introduction to automatic text summarization.",A Survey on Automatic Text Summarization,"Text summarization endeavors to produce a summary version of a text, while maintaining the original ideas. The textual content on the web, in particular, is growing at an exponential rate. The ability to decipher through such a massive amount of data to extract useful information is a significant undertaking, and requires an automatic mechanism to aid with the extant repository of information. The text summarization systems intent to assist with content reduction keeping the relevant information and filtering the non-relevant parts of the text. In terms of the input, there are two fundamental approaches among the text summarization systems. The first approach summarizes a single document. In other words, the system takes one document as an input and produces a summary version as its output. An alternative approach is to take several documents as its input and produce a single summary document as its output. In terms of output, the summarization systems are also categorized into two major types. One approach would be to extract exact sentences from the original document to build the summary output. An alternative would be a more complex approach, in which the rendered text is a rephrased version of the original document. This paper will offer an in-depth introduction to automatic text summarization. We also mention some evaluation techniques to evaluate the quality of automatic text summarization.", 
"The increasing volume of textual information on any topic requires its compression to allow humans to digest it. These challenges have led to new developments in the area of Natural Language Processing (NLP) and Information Retrieval (IR) Despite some progress over recent years with several solutions for information extraction and text summarization, the problems of generating consistent narrative summaries are still unresolved.",A survey on evaluation of summarization methods,"The increasing volume of textual information on any topic requires its compression to allow humans to digest it. This implies detecting the most important information and condensing it. These challenges have led to new developments in the area of Natural Language Processing (NLP) and Information Retrieval (IR) such as narrative summarization and evaluation methodologies for narrative extraction. Despite some progress over recent years with several solutions for information extraction and text summarization, the problems of generating consistent narrative summaries and evaluating them are still unresolved. With regard to evaluation, manual assessment is expensive, subjective and not applicable in real time or to large collections. Moreover, it does not provide re-usable benchmarks. Nevertheless, commonly used metrics for summary evaluation still imply substantial human e?ort since they require a comparison of candidate summaries with a set of reference summaries. The contributions of this paper are three-fold. First, we provide a comprehensive overview of existing metrics for summary evaluation. We discuss several limitations of existing frameworks for summary evaluation. Second, we introduce an automatic framework for the evaluation of metrics that does not require any human annotation. Finally, we evaluate the existing assessment metrics on a Wikipedia data set and a collection of scientifc articles using this framework. Our fndings show that the majority of existing metrics based on vocabulary overlap are not suitable for assessment based on comparison with a full text and we discuss this outcome.", 
"Condensation of document information from the text according to the query is primarily a concerned matter due to the rapid growth of information. Text summarization is considered to be a challenging task and a paramount research area these days. In this paper, an effort has been made to go through the various extractive based approaches for query-based text summarization.",A survey on existing extractive techniques for query-based text summarization,"Condensation of document information from the text according to the query is primarily a concerned matter due to the rapid growth of information. In fact, there is not usually enough time to scan through the contents and understand each document and make decision based on the queries. Hence, there is a great demand for query based summarization of text documents. Therefore, text summarization is considered to be a challenging task and a paramount research area these days. Moreover, query-based text summarization is a significant problem, which has innumerable applications. It becomes increasingly important to extensively study the di?erent mechanisms that can provide an effective and a short summary. Therefore, in this paper, an effort has been made to go through the various extractive based approaches for query-based text summarization so that time and effort can be reduced to find a useful summary for the users as specified by their need.", 
"Text Summarization is the process of obtaining salient information from an authentic text document. In this work, a comprehensive review of extractive text summarization process methods has been ascertained. The implication of sentences is determined based on linguistic and statistical features.",A survey on extractive text summarization,"Text Summarization is the process of obtaining salient information from an authentic text document. In this technique, the extracted information is achieved as a summarized report and conferred as a concise summary to the user. It is very crucial for humans to understand and to describe the content of the text. Text Summarization techniques are classified into abstractive and extractive summarization. The extractive summarization technique focuses on choosing how paragraphs, important sentences, etc., produces the original documents in precise form. The implication of sentences is determined based on linguistic and statistical features. In this work, a comprehensive review of extractive text summarization process methods has been ascertained. In this paper, the various techniques, populous benchmarking datasets and challenges of extractive summarization have been reviewed. This paper interprets extractive text summarization methods with a less redundant summary, highly adhesive, coherent and depth information.", 
Heart disease is the one of the most common disease. We used different attributes which can relate to this heart diseases well to locate the better method to predict. We additionally used algorithms for prediction.,A SURVEY ON HEART DISEASE PREDICTION USING MACHINE LEARNING,"Heart disease is the one of the most common disease. This disease is quite common now days we used different attributes which can relate to this heart diseases well to locate the better method to predict and we additionally used algorithms for prediction. This survey paper explores a different models based on such algorithms and techniques and analyse their performance. Models based on supervised learning algorithms, Support Vector Machines (SVM), K-Nearest Neighbour (KNN), NaïveBayes, Decision Trees (DT), Random Forest (RF) and ensemble models are discovered very mainstream among the researchers.", 
"Main feature of MEC is to push mobile computing, network control and storage to the network edges. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. Research aims to seamlessly merge the two disciplines of wireless communications and mobile computing. Advancements in these directions will facilitate the transformation of M EC from theory to practice.",A Survey on Mobile Edge Computing The Communication Perspective,"Driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing, from the centralized mobile cloud computing toward mobile edge computing (MEC). The main feature of MEC is to push mobile computing, network control and storage to the network edges (e.g., base stations and access points) so as to enable computation-intensive and latency-critical applications at the resource-limited mobile devices. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The promised gains of MEC have motivated extensive efforts in both academia and industry on developing the technology. A main thrust of MEC research is to seamlessly merge the two disciplines of wireless communications and mobile computing, resulting in a wide-range of new designs ranging from techniques for computation offloading to network architectures. This paper provides a comprehensive survey of the state-of-the-art MEC research with a focus on joint radio-and-computational resource management. We also discuss a set of issues, challenges, and future research directions for MEC research, including MEC system deployment, cache-enabled MEC, mobility management for MEC, green MEC, as well as privacy-aware MEC. Advancements in these directions will facilitate the transformation of MEC from theory to practice. Finally, we introduce recent standardization efforts on MEC as well as some typical MEC application scenarios.", 
"Vehicular ad hoc networks (VANETs) are becoming the most promising research topic in intelligent transportation systems. They provide information to deliver comfort and safety to both drivers and passengers. But unique characteristics of VANets make security, privacy, and trust management challenging issues.","A Survey on Recent Advances in Vehicular Network Security, Trust, and Privacy","Vehicular ad hoc networks (VANETs) are becoming the most promising research topic in intelligent transportation systems, because they provide information to deliver comfort and safety to both drivers and passengers. However, unique characteristics of VANETs make security, privacy, and trust management challenging issues in VANETs’ design. This survey article starts with the necessary background of VANETs, followed by a brief treatment of main security services, which have been well studied in other fields. We then focus on an in-depth review of anonymous authentication schemes implemented by five pseudonymity mechanisms. Because of the predictable dynamics of vehicles, anonymity is necessary but not sufficient to thwart tracking an attack that aims at the drivers’ location profiles. Thus, several location privacy protection mechanisms based on pseudonymity are elaborated to further protect the vehicles’ privacy and guarantee the quality of location-based services simultaneously. We also give a comprehensive analysis on various trust management models in VANETs. Finally, considering that current and near-future applications in VANETs are evaluated by simulation, we give a much-needed update on the latest mobility and network simulators as well as the integrated simulation platforms. In sum, this paper is carefully positioned to avoid overlap with existing surveys by filling the gaps and reporting the latest advances in VANETs while keeping it self-explained.", 
3GPP has proposed related standards with the fifth generation mobile communication technology (5G) This paper makes a large number of contributions to the security aspects of 3GPP 5G networks. We focus on the support of massive Internet of Things (IoT) devices and Device to Device (D2D) communication.,A Survey on Security Aspects for 3GPP 5G Networks,"With the continuous development of mobile communication technologies, Third Generation Partnership Project (3GPP) has proposed related standards with the fifth generation mobile communication technology (5G), which marks the official start of the evolution from the current Long Term Evolution (LTE) system to the next generation mobile communication system (5GS). This paper makes a large number of contributions to the security aspects of 3GPP 5G networks. Firstly, we present an overview of the network architecture and security functionality of the 3GPP 5G networks. Subsequently, we focus on the new features and techniques including the support of massive Internet of Things (IoT) devices, Device to Device (D2D) communication, Vehicle to Everything (V2X) communication, and network slice, which incur the huge challenges for the security aspects in 3GPP 5G networks. Finally, we discuss in detail the security features, security requirements or security vulnerabilities, existing security solutions and some open research issues about the new features and techniques in 3GPP 5G network.", 
"VANETs are comprised of smart vehicles and roadside units (RSUs) and on-board units (OBUs) which communicate through unreliable wireless media. They have diverse system concerns and security difficulties in getting the accessibility of ubiquitous availability, secure communication, interchanges, and reputation management system. By their fluctuation in nature, they are genuinely defenseless against assaults, which may result in life-jeopardizing circumstances.","A survey on security attacks in VANETs Communication, applications and challenges","Over the past few decades, the intelligent transportation system (ITS) have emerged with new technologies and becomes the data-driven ITS, because the substantial amount of data is assembled from the multiple sources. Vehicular Ad hoc networks (VANETs), are a particular case of ad hoc networks that are used in the smart ITS. VANETs have become one of the most, encouraging, promising, and fastest-growing subsets of the mobile ad hoc networks (MANETs). They are comprised of smart vehicles and roadside units (RSUs) and on-board units (OBUs) which communicate through unreliable wireless media. Other than lacking infrastructure, delivering entities move with different increasing speeds. Thus, this delays establishing reliable end-to-end communication paths and having efficient data transfer. In this manner, VANETs have diverse system concerns and security difficulties in getting the accessibility of ubiquitous availability, secure communication, interchanges, and reputation management system. Which influence the trust in collaboration and arrangement between the portable system. By their fluctuation in nature, they are genuinely defenseless against assaults, which may result in life-jeopardizing circumstances. In this survey, we provide an extensive overview of the ITS and the evolution of ITS to VANETs. We provide the details of VANETs, discussed the privacy and security attacks in VANETs with their applications and challenges. We address the effectiveness of VANETs and cloud computing with architecture and related privacy and security issues. We also examined the communication protocols for each network layer with the relevant attacks occurred at each layer. We also discussed the potential benefits of the different proposed techniques related to VANETs, application, and challenges in details. In the end, we provide a conclusion with some open and emerging issues in VANETs.", 
Sentence fusion is currently used in automatic text summarization and question answering systems. It is also used in natural language generation systems in the name of sentence aggregation. In this paper we discuss about the research done on sentence fusion.,A survey on sentence fusion techniques of abstractive text summarization,"Sentence fusion is one of the tasks of automatic text summarization that has wide spread applications in the field of computer science. It is currently used in automatic text summarization and question answering systems. It is also used in natural language generation systems in the name of sentence aggregation. The task of sentence fusion poses various challenges like deciding whether two sentences can be combined or not, which part of a sentence should be selected for combination, how these parts can be combined and how to fit the combinations in a grammatically correct sentence structure. In this paper we discuss about the research done on sentence fusion and various challenges that are yet to be met.", 
Scalability and precision are of importance for the deployment of code review tools. Traditional tools can only detect some security flaws automatically with high false positive and false negative. Various machine learning approaches have been developed to learn and detect flaws and vulnerabilities. We believe machine learning and its branches will become out-standing in source code review.,A Survey on Source Code Review Using Machine Learning,"Source code review constrains software system security sufficiently. Scalability and precision are of importance for the deployment of code review tools. However, traditional tools can only detect some security flaws automatically with high false positive and false negative by tedious reviewing largescale source code. Various flaws and vulnerabilities show specific characteristic in source code. Machine learning systems founded feature matrixes of source code as input, including variables, functions and files, generating ad-hoc label by distinguish or generation methodologies to review source code automatically and intelligently. Source code, whatever the programming language, is text information in nature. Both secure and vulnerable feature can be curved from source code. Fortunately, a variety of machine learning approaches have been developed to learn and detect flaws and vulnerabilities in intelligent source code security review. Combination of code semantic and syntactic feature contribute to the optimation of false positive and false negative during source code review. In this paper, we give the review of literature related to intelligent source code security review using machine learning methods. It illustrate the primary evidence of approaching ML in source code security review. We believe machine learning and its branches will become out-standing in source code review.", 
"The past decade has seen the development of different wireless access technologies for vehicle-to-everything (V2X) communications. An extensive set of related use cases have been drafted, each with its own requirements. The choice of the future enabling technology is not so easy to predict and mostly depends on mandatory laws at the international level.",A Survey on the Roadmap to Mandate on Board Connectivity and Enable V2V-Based Vehicular Sensor Networks,"Vehicles will soon be connected and will be interacting directly with each other and with the road infrastructure, bringing substantial benefits in terms of safety and traffic efficiency. The past decade has seen the development of different wireless access technologies for vehicle-to-everything (V2X) communications and an extensive set of related use cases have been drafted, each with its own requirements. In this paper, focusing on short-range communications, we analyze the technical and economic motivations that are driving the development of new road users’ connectivity, discussing the international intentions to mandate on board devices for V2X communication. We also go in depth with the enabling wireless access technologies, from IEEE 802.11p to short-range Cellular-V2X and other complementary technologies, such as visible light communication (VLC) and millimeterWaves, up to hybrid communication and 5G. We conclude our survey with some performance comparison in urban realistic scenarios, underlying that the choice of the future enabling technology is not so easy to predict and mostly depends on mandatory laws at the international level.", 
The vision of Internet of Things (IoT) is to enable systems across the globe to share data using advanced communication technologies. Future personalized and connected healthcare is one of the promising area to see the benefits of IoT.,A Survey on the Roles of Communication Technologies in IoT-based Personalized Healthcare Applications,"The vision of Internet of Things (IoT) is to enable systems across the globe to share data using advanced communication technologies. With the recent technological advancements, IoT-based solutions are no longer a challenging vision. IoT will offer numerous, and potentially revolutionary, benefits to today’s digital world. Future personalized and connected heathcare is one of the promising area to see the benefits of IoT. This paper surveys emerging healthcare applications, including detailed technical aspects required for the realization of a complete endto-end solution for each application. The survey explores the key application-specific requirements from the perspective of communication technologies. Furthermore, a detailed exploration from the existing to the emerging technologies and standards that would enable such applications is presented, highlighting the critical consideration of short-range and long-range communications. Finally, the survey highlights important open research challenges and issues specifically related to IoT-based future healthcare systems.", 
"From the last decade, it is observed that there is a never-before-seen positive flux in the quantity of textual information. This has created a necessity of doing extensive research in the field of Automatic Text Summarization. This survey tends to present a go through upon some of the most relevant approaches for summarization.",A Survey Paper on Text Summarization Methods,"From the last decade, it is observed that there is a never-before-seen positive flux in the quantity of textual information that a single document or even multiple documents present to us. This has created a necessity of doing extensive research in the field of Automatic Text Summarization in the field of Natural Language Processing (NLP), while this call for research work was made in 1950, the exponential growth of computing power in the 21st century has allowed unconventional methods to work better in this field. This survey tends to present a go through upon some of the most relevant approaches for summarization from surface to rhetorical, also classifying the techniques based on Single and Multiple Documents used as an input, the aim is to present a good read to the readers, young and budding linguists, about various existing methods used for text summarization.", 
There has been a great amount of work on query-independent summarization of documents. We present a method to create query-specific summaries. We identify the most query-relevant fragments and combine them using the semantic associations within the document. We also evaluate efficient algorithms that support computing summaries in interactive time.,A System for Query-Specific Document Summarization,"There has been a great amount of work on query-independent summarization of documents. However, due to the success of Web search engines query-specific document summarization (query result snippets) has become an important problem, which has received little attention. We present a method to create query-specific summaries by identifying the most query-relevant fragments and combining them using the semantic associations within the document. In particular, we first add structure to the documents in the preprocessing stage and convert them to document graphs. Then, the best summaries are computed by calculating the top spanning trees on the document graphs. We present and experimentally evaluate efficient algorithms that support computing summaries in interactive time. Furthermore, the quality of our summarization method is compared to current approaches using a user survey.", 
The text summarization task has gained much importance because of the large amount of online data. This paper presents an automatic process for text assessment that relies on fuzzy rules on a variety of extracted features to find the most important information. The proposed approach can benefit development and use of future expert systems able to automatically assess writing.,A text summarization method based on fuzzy rules and applicable to automated assessment,"In the last two decades, the text summarization task has gained much importance because of the large amount of online data, and its potential to extract useful information and knowledge in a way that could be easily handled by humans and used for a myriad of purposes, including expert systems for text assessment. This paper presents an automatic process for text assessment that relies on fuzzy rules on a variety of extracted features to find the most important information in the assessed texts. The automatically produced summaries of these texts are compared with reference summaries created by domain experts. Differently from other proposals in the literature, our method summarizes text by investigating correlated features to reduce dimensionality, and consequently the number of fuzzy rules used for text summarization. Thus, the proposed approach for text summarization with a relatively small number of fuzzy rules can benefit development and use of future expert systems able to automatically assess writing. The proposed summarization method has been trained and tested in experiments using a dataset of Brazilian Portuguese texts provided by students in response to tasks assigned to them in a Virtual Learning Environment (VLE). The proposed approach was compared with other methods including a naive baseline, Score, Model and Sentence, using ROUGE measures. The results show that the proposal provides better f-measure (with 95% CI) than aforementioned methods.", 
Automatic text summarization is an essential tool in this era of information overloading. In this paper we present an automatic extractive Arabic summarization system where the user can cap the size of the final summary. It is a direct system where no machine learning is involved.,A text summarizer for Arabic,"Automatic text summarization is an essential tool in this era of information overloading. In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. It is a direct system where no machine learning is involved. We use a two pass algorithm where in pass one, we produce a primary summary using Rhetorical Structure Theory (RST); this is followed by the second pass where we assign a score to each of the sentences in the primary summary. These scores will help us in generating the final summary. For the final output, sentences are selected with an objective of maximizing the overall score of the summary whose size should not exceed the user selected limit. We used Rouge to evaluate our system generated summaries of various lengths against those done by a (human) news editorial professional. Experiments on sample texts show our system to outperform some of the existing Arabic summarization systems including those that require machine learning.", 
Steady-State Visual Evoked Potential (SSVEP) is one of the popular methods of brain-computer interfacing. Main challenge is finding an appropriate intermediate representation to facilitate the classification task. We propose a deep learning model that uses a hybrid architecture based on Convolutional and Recurrent Neural Networks to classify SSVEP signals in the time domain directly.,A TIME DOMAIN CLASSIFICATION OF STEADY-STATE VISUAL EVOKED POTENTIALS USING DEEP RECURRENT-CONVOLUTIONAL NEURAL NETWORKS,"Steady-State Visual Evoked Potential (SSVEP) is one of the popular methods of brain-computer interfacing (BCI). It is used to translate the Electroencephalogram (EEG) signals into actions or choices. The main challenge in processing the SSVEP signal recognition is finding an appropriate intermediate representation to facilitate the classification task afterwards. In the literature, frequency domain analysis was extensively adopted as an intermediate representation for SSVEP classification. In this presented paper, we propose a deep learning model that uses a hybrid architecture based on Convolutional and Recurrent Neural Networks to classify SSVEP signals in the time domain directly. We achieved accuracy 93.59% compared to 87.40% for the state-of-the-art method: canonical correlation analysis in the frequency domain. The proposed architecture facilitates the real-time classification of SSVEP signals in the time domain for realtime applications such as robot cars and exoskeletons.", 
"Text summarization research is far from what people want, especially in abstractive summarization. A high-quality summarization system needs to focus on the topic content of the document and the similarity between the summary and the source document. We propose a topic information fusion and semantic relevance for text summarization based on Fine-tuning BERT(TIF-SR).",A Topic Information Fusion and Semantic Relevance for Text Summarization,"With the continuous development of deep learning, pre-trained models have achieved sound effects in the field of natural language processing. However, text summarization research is far from what people want, especially in abstractive summarization. A high-quality summarization system needs to focus on the topic content of the document and the similarity between the summary and the source document. In this paper, we propose a topic information fusion and semantic relevance for text summarization based on Fine-tuning BERT(TIF-SR). Primarily, considering the critical role of topic information in summary generation, we extract topic keywords and fusion them with source documents as part of the input. Secondly, make the summary closer to the source document by calculating the semantic similarity between the generated summary and the source document, the quality of the abstract is improved. The experimental data indicate that the ROUGE index and readability have improved in this model, so these shreds of evidence suggest that the method proposed by our model is sufficient.", 
"Comments in the code are a primary source for system documentation. They are crucial for the work of software maintainers, as a basis for code traceability. This paper presents an approach, based on topic modeling, for analyzing comments consistency to the source code.",A Topic Modeling Approach To Evaluate The Comments Consistency To Source Code,"A significant amount of source code in software systems is made up of comments, parts of the code that are ignored by the compiler. Comments in the code are a primary source for system documentation. These are crucial for the work of software maintainers, as a basis for code traceability, for maintenance activities, but also for the use of the code itself as a library or framework in other projects. Although many software developers consider comments important, existing approaches to software quality analysis mainly disregard code comments and focus only on source code. This paper presents an approach, based on topic modeling, for analyzing the comments consistency to the source code. A model was provided to analyze the quality of comments in terms of consistency since comments should be consistent with the source code they refer to. The results show a similarity in the trend of topic distribution and it emerges that almost all classes are associated with no more than 3 topics.", 
"Most text automatic summarization algorithms are targeted for documents of relatively short length. This paper proposes a topic modeling based approach to extractive automatic summarizing. It aims to achieve a good balance among compression ratio, summarization quality and machine readability.",A topic modeling based approach to novel document automatic summarization,"Most of existing text automatic summarization algorithms are targeted for multi-documents of relatively short length, thus difficult to be applied immediately to novel documents of structure freedom and long length. In this paper, aiming at novel documents, we propose a topic modeling based approach to extractive automatic summarization, so as to achieve a good balance among compression ratio, summarization quality and machine readability. First, based on topic modeling, we extract the candidate sentences associated with topic words from a preprocessed novel document. Second, with the goals of compression ratio and topic diversity, we design an importance evaluation function to select the most important sentences from the candidate sentences and thus generate an initial novel summary. Finally, we smooth the initial summary to overcome the semantic confusion caused by ambiguous or synonymous words, so as to improve the summary readability. We evaluate experimentally our proposed approach on a real novel dataset. The experiment results show that compared to those from other candidate algorithms, each automatic summary generated by our approach has not only a higher compression ratio, but also better summarization quality.", 
Mobile system summarizes Turkish Wikipedia text. System selects the sentences due to structural features of Turkish language and semantic features of the sentences. Performance evaluation is made based on judgments of human experts.,A Turkish Wikipedia Text Summarization System for Mobile Devices,"Today Wikipedia provides a very large and reliable domain-independent encyclopedic repository. With this study a mobile system which summarizes Turkish Wikipedia text is presented. The presented system selects the sentences due to structural features of Turkish language and semantic features of the sentences. The performance evaluation is made based on judgments of human experts. The results are tested due to precision and recall values of a ranked sentence list and it is concluded that, the summarization results are promising.", 
Most Chinese text summarization algorithms use the sequence-to-sequence model. This model is prone to the problems of unknown words and incomplete content generation. We propose a new two-stage automatic text. summarization method using keyword information and adversarial learning.,A Two-stage Chinese text summarization algorithm using keyword information and adversarial learning,"At present, most Chinese text summarization algorithms use the sequence-to-sequence model, but this model is prone to the problems of unknown words and incomplete content generation. To address these problems, we propose a new two-stage automatic text summarization method using keyword information and adversarial learning in this paper. On the one hand, the proposed method integrates the keyword information into the sequence-to-sequence model. The main information and keywords of the article are considered simultaneously through the attention mechanism to improve the information of summary generation. On the other hand, adversarial learning is introduced into the proposed model to avoid the problem that the semantic vector after passing through the encoder cannot save the context information better. Experiments are carried out on the Chinese dataset LCSTS, and the comparison results show that the proposed method has advantages in abstractive summarization.", 
This paper presents a variable dimension particle swarm optimization (VDiPSO) based model for extractive text summarization. The proposed model clusters the sentences on the basis of their similarities using VDiPSO. The experiments are performed on the DUC 2002 dataset.,A Variable Dimension Optimization Approach for Text Summarization,"Text summarization is the process of transfiguring a large document information into a clear and concise form. This paper presents a variable dimension particle swarm optimization (VDiPSO) based model for extractive text summarization. At first, the proposed model clusters the sentences on the basis of their similarities using VDiPSO. Every feasible solution of clustering is expressed as a particle in the swarm. Each dimension in the particle represents a cluster center. Using the metaheuristic approach, we derive the optimal number of clusters. After that, the most important text features are used to score the sentences of each cluster. Finally, top 30% high scored sentences from each clusters are extracted to generate summary. The implementation of the proposed approach has been done in Python 3.5 in Anaconda environment. The experiments are performed on the DUC 2002 dataset using the co-selection based performance parameters. We show empirically that the proposed hybrid approach is viable and effective for extractive text summarization.", 
"Static analysis tools can detect source code defects at an early phase during the software development process. They need to be inspected manually by developers, which is inevitable and costly. A large proportion of alarms are found to be false positives. We propose a defect identification model based on machine learning to classify the reported alarms.",A variable-level automated defect identification model based on machine learning,"Static analysis tools, automatically detecting potential source code defects at an early phase during the software development process, are diffusely applied in safety-critical software fields. However, alarms reported by the tools need to be inspected manually by developers, which is inevitable and costly, whereas a large proportion of them are found to be false positives. Aiming at automatically classifying the reported alarms into true defects and false positives, we propose a defect identification model based on machine learning. We design a set of novel features at variable level, called variable characteristics, for building the classification model, which is more fine-grained than the existing traditional features. We select 13 base classifiers and two ensemble learning methods for model building based on our proposed approach, and the reported alarms classified as unactionable (false positives) are pruned for the purpose of mitigating the effort of manual inspection. In this paper, we firstly evaluate the approach on four open-source C projects, and the classification results show that the proposed model achieves high performance and reliability in practice. Then, we conduct a baseline experiment to evaluate the effectiveness of our proposed model in contrast to traditional features, indicating that features at variable level improve the performance significantly in defect identification. Additionally, we use machine learning techniques to rank the variable characteristics in order to identify the contribution of each feature to our proposed model.", 
Vehicular Ad-hoc Network (VANET) is a significant component of intelligent transportation system. It facilitates vehicles to share sensitive information and corporate with others. This paper proposes an anti-attack trust management scheme in VANET called AATMS.,AATMS An Anti-Attack Trust Management Scheme in VANET,"Vehicular Ad-hoc Network (VANET) is a significant component of intelligent transportation system, which facilitates vehicles to share sensitive information and corporate with others. However, due to its unique characteristics, such as openness, dynamic topology and high mobility, VANET suffers from various attacks. This paper proposes an anti-attack trust management scheme in VANET called AATMS to evaluate the trustworthiness of vehicles. With the help of AATMS, vehicles in VANET can avoid malicious vehicles and cooperate with trusted vehicles. The idea of AATMS is mainly inspired by TrustRank algorithm, which is used to combat web spams. In this paper, we calculate local trust and global trust, which indicate the local and global trust relationships among vehicles. First, Bayesian inference is adopted to calculate local trust of vehicles based on historical interactions. Then we select a small set of seed vehicles according to local trust and some social factors. Once we identify the reputable seed vehicles, we use the local trust link structure of vehicles to evaluate the global trust of all vehicles. The simulation results show that AATMS can efficiently identify trustworthy and untrustworthy vehicles in VANET even under malicious attacks.", 
"Text summarization aims to offer a highly condensed and valuable information that expresses the main ideas of the text. In this work, we put forward a new generative model based on convolutional sequencing architecture. We also equip our model with a copying mechanism to deal with the rare or unseen words.",Abstract Text Summarization with a Convolutional Seq2seq Model,"Abstract text summarization aims to offer a highly condensed and valuable information that expresses the main ideas of the text. Most previous researches focus on extractive models. In this work, we put forward a new generative model based on convolutional seq2seq architecture. A hierarchical CNN framework is much more efficient than the conventional RNN seq2seq models. We also equip our model with a copying mechanism to deal with the rare or unseen words. Additionally, we incorporate a hierarchical attention mechanism to model the keywords and key sentences simultaneously. Finally we verify our model on two real-life datasets, GigaWord and DUC corpus. The experiment results verify the effectiveness of our model as it outperforms state-of-the-art alternatives consistently and statistical significantly.", 
"Text summarization is considered as a challenging task in the NLP community. The availability of datasets for the task of multilingual text summarizing is rare. In this work, we build an abstract text summarizer for the German language using the state-of-the-art ""Transformer"" model.","Abstract Text Summarization, A Low Resource Challenge","Text summarization is considered as a challenging task in the NLP community. The availability of datasets for the task of multilingual text summarization is rare, and such datasets are difficult to construct. In this work, we build an abstract text summarizer for the German language text using the state-of-the-art “Transformer” model. We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language. To generate synthetic data, the Common Crawl (German) dataset is exploited, which covers different domains. The synthetic data is effective for the low resource condition and is particularly helpful for our multilingual scenario where availability of summarizing data is still a challenging issue. The data are also useful in deep learning scenarios where the neural models require a large amount of training data for utilization of its capacity. The obtained summarization performance is measured in terms of ROUGE and BLEU score. We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 (R1 F1) on the development and test sets, respectively, compared to the system which does not rely on data augmentation.", 
"Seq2Seq learning has recently been used for abstractive and extractive summarization. In this study, we propose that Seq2seq models should be started with contextual information at the first time-step of the input to obtain better summaries. The output summaries are more document centric, than being generic, overcoming one of the major hurdles of using generative models.",Abstractive and Extractive Text Summarization using Document Context Vector and Recurrent Neural Networks,"Sequence to sequence (Seq2Seq) learning has recently been used for abstractive and extractive summarization. In current study, Seq2Seq models have been used for eBay product description summarization. We propose a novel Document-Context based Seq2Seq models using RNNs for abstractive and extractive summarizations. Intuitively, this is similar to humans reading the title, abstract or any other contextual information before reading the document. This gives humans a high-level idea of what the document is about. We use this idea and propose that Seq2Seq models should be started with contextual information at the first time-step of the input to obtain better summaries. In this manner, the output summaries are more document centric, than being generic, overcoming one of the major hurdles of using generative models. We generate document-context from user-behavior and seller provided information. We train and evaluate our models on human-extracted-golden-summaries. The document-contextual Seq2Seq models outperform standard Seq2Seq models. Moreover, generating human extracted summaries is prohibitively expensive to scale, we therefore propose a semi-supervised technique for extracting approximate summaries and using it for training Seq2Seq models at scale. Semi-supervised models are evaluated against human extracted summaries and are found to be of similar efficacy. We provide side by side comparison for abstractive and extractive summarizers (contextual and non-contextual) on same evaluation dataset. Overall, we provide methodologies to use and evaluate the proposed techniques for large document summarization. Furthermore, we found these techniques to be highly effective, which is not the case with existing techniques.", 
This work proposes to add an attention mechanism on output sequence to avoid repetitive contents. We applied our model to the public dataset provided by NLPCC 2017 shared task3. The evaluation results show that our system achieved the best ROUGE performance among all the participating teams.,Abstractive Document Summarization via Neural Model with Joint Attention,"Due to the difficulty of abstractive summarization, the great majority of past work on document summarization has been extractive, while the recent success of sequence-to-sequence framework has made abstractive summarization viable, in which a set of recurrent neural networks models based on attention encoder-decoder have achieved promising performance on short-text summarization tasks. Unfortunately, these attention encoder-decoder models often suffer from the undesirable shortcomings of generating repeated words or phrases and inability to deal with out-of-vocabulary words appropriately. To address these issues, in this work we propose to add an attention mechanism on output sequence to avoid repetitive contents and use the subword method to deal with the rare and unknown words. We applied our model to the public dataset provided by NLPCC 2017 shared task3. The evaluation results show that our system achieved the best ROUGE performance among all the participating teams and is also competitive with some state-of-the-art methods.", 
"Recent progress has been made to abstractive sentence summarization using neural models. Attempts on document summarization are still in a primitive stage, and evaluation results are worse than extractive methods on benchmark datasets. We propose a novel graph-based attention mechanism in the sequence-to-sequence framework.",Abstractive document summarization with a graph-based attentional neural model,"Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.", 
"Text summarization is one of the famous problems in natural language processing and deep learning. The goal is to create a short, fluent and understandable abstractive summary of a text document. We have used a bi-directional RNN with LSTM's and attention model in decoding layer. We applied the sequence to sequence model to generate a short summary of food descriptions.",Abstractive method of text summarization with sequence to sequence RNNs,"Text summarization is one of the famous problems in natural language processing and deep learning in recent years. Generally, text summarization contains a short note on a large text document. Our main purpose is to create a short, fluent and understandable abstractive summary of a text document. For making a good summarizer we have used amazon fine food reviews dataset, which is available on Kaggle. We have used reviews text descriptions as our input data, and generated a simple summary of that review descriptions as our output. To assist produce some extensive summary, we have used a bi-directional RNN with LSTM’s in encoding layer and attention model in decoding layer. And we applied the sequence to sequence model to generate a short summary of food descriptions. There are some challenges when we working with abstractive text summarizer such as text processing, vocabulary counting, missing word counting, word embedding, the efficiency of the model or reduce value of loss and response machine fluent summary. In this paper, the main goal was increased the efficiency and reduce train loss of sequence to sequence model for making a better abstractive text summarizer. In our experiment, we’ve successfully reduced the training loss with a value of 0.036 and our abstractive text summarizer able to create a short summary of English to English text.", 
Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network which generates a summary of an input sentence. Our experiments show that the model significantly outperforms the state-of-the-art method on the Gigaword corpus.,Abstractive sentence summarization with attentive recurrent neural networks,Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence. The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation. Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets. Our experiments show that the model significantly outperforms the recently proposed state-of-the-art method on the Gigaword corpus while performing competitively on the DUC-2004 shared task., 
This paper proposes a selective reinforced sequence-to-sequence (i.e. Seq2Seq) attention model for abstractive social media text summarization. We add a selective gate after the encoder module for better filtering out invalid information. Evaluations on a famous social media data set demonstrate that our model outperforms most of famous baseline models.,Abstractive social media text summarization using selective reinforced Seq2Seq attention model,"Abstractive text summarization aims to generate a brief version of a given sentence while attempting to express its main meaning. Although some models based on the sequence-to-sequence framework have achieved remarkable results recently, there are still many problems that cannot be ignored. In this paper, we propose a selective reinforced sequence-to-sequence (i.e. Seq2Seq) attention model for abstractive social media text summarization. We add a selective gate after the encoder module for better filtering out invalid information. Specifically, we combine the cross-entropy and reinforcement learning policy for optimizing the ROUGE score directly. Evaluations on a famous social media data set (i.e. LCSTS ) demonstrate that our model outperforms most of famous baseline models, and the proposed model is 2.6%, 2.1% and 2.5% higher than the basic Seq2Seq attention model on the F1 score of ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The source code will be available.", 
"Focus of automatic text summarization research has exhibited a gradual shift from extractive methods to abstractive methods in recent years, owing in part to advances in neural methods.","Abstractive Summarization, A Survey of the State of the Art","The focus of automatic text summarization research has exhibited a gradual shift from extractive methods to abstractive methods in recent years, owing in part to advances in neural methods. Originally developed for machine translation, neural methods provide a viable framework for obtaining an abstract representation of the meaning of an input text and generating informative, fluent, and human-like summaries. This paper surveys existing approaches to abstractive summarization, focusing on the recently developed neural approaches.", 
This work proposes a novel framework for enhancing abstractive text summarization. It uses the combination of deep learning techniques along with semantic data transformations. The overall approach is evaluated on two popular datasets with encouraging results.,Abstractive text summarization based on deep learning and semantic content generalization,"This work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations. Initially, a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form. Subsequently, a methodology is proposed which transforms the aforementioned generalized summary into human-readable form, retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words. The overall approach is evaluated on two popular datasets with encouraging results.", 
"Most of the graph-based extractive methods represent sentence as bag of words. This work presents a semantic graph approach with improved ranking algorithm for abstractive summarization of multi-documents. Experiment of this research is accomplished using DUC-2002, a standard dataset for document summarization. Experimental findings signify that the proposed approach shows superior performance than other summarization approaches.",Abstractive Text Summarization based on Improved Semantic Graph Approach,"The goal of abstractive summarization of multi-documents is to automatically produce a condensed version of the document text and maintain the significant information. Most of the graph-based extractive methods represent sentence as bag of words and utilize content similarity measure, which might fail to detect semantically equivalent redundant sentences. On other hand, graph based abstractive method depends on domain expert to build a semantic graph from manually created ontology, which requires time and effort. This work presents a semantic graph approach with improved ranking algorithm for abstractive summarization of multi-documents. The semantic graph is built from the source documents in a manner that the graph nodes denote the predicate argument structures (PASs)—the semantic structure of sentence, which is automatically identified by using semantic role labeling; while graph edges represent similarity weight, which is computed from PASs semantic similarity. In order to reflect the impact of both document and document set on PASs, the edge of semantic graph is further augmented with PAS-to-document and PAS-to-document set relationships. The important graph nodes (PASs) are ranked using the improved graph ranking algorithm. The redundant PASs are reduced by using maximal marginal relevance for re-ranking the PASs and finally summary sentences are generated from the top ranked PASs using language generation. Experiment of this research is accomplished using DUC-2002, a standard dataset for document summarization. Experimental findings signify that the proposed approach shows superior performance than other summarization approaches.", 
We explore to what extent knowledge about the pre-trained language model is beneficial for abstractive summarization. We experiment with conditioning the encoder and decoder of a Transformer-based neural model on the BERT language model. We additionally train our models on the SwissText dataset to demonstrate usability on German. Both models outperform the baseline in ROUGE scores on two datasets.,Abstractive Text Summarization based on Language Model Conditioning and Locality Modeling,"We explore to what extent knowledge about the pre-trained language model that is used is beneficial for the task of abstractive summarization. To this end, we experiment with conditioning the encoder and decoder of a Transformer-based neural model on the BERT language model. In addition, we propose a new method of BERT-windowing, which allows chunk-wise processing of texts longer than the BERT window size. We also explore how locality modeling, i. e., the explicit restriction of calculations to the local context, can affect the summarization ability of the Transformer. This is done by introducing 2-dimensional convolutional self-attention into the first layers of the encoder. The results of our models are compared to a baseline and the state-of-the-art models on the CNN/Daily Mail dataset. We additionally train our model on the SwissText dataset to demonstrate usability on German. Both models outperform the baseline in ROUGE scores on two datasets and show its superiority in a manual qualitative analysis.", 
"There are two main approaches to summarization namely extractive and abstractive method. In this work, we compare the performance of above two models using ROUGE and BLEU score",Abstractive Text Summarization Using Artificial Intelligence,"Text summarization is the process of creating concise summary of text. There are two main approaches to summarization namely extractive and abstractive method. Most of the system summaries use extractive method. Amongst few abstractive models available there are two models namely sequence to sequence and LSTM bidirectional model. In this work, we compare the performance of above two models using ROUGE and BLEU score on Amazon reviews and CNN news dataset.", 
Text Summarization is the task of constructing summary sentences by merging facts from different source sentences. It is very difficult and time consuming for human beings to manually summarize large documents of text. We propose an LSTM-CNN based ATS framework (ATSDL) that can construct new sentences by exploring more fine-grained fragments than sentences.,Abstractive text summarization using LSTM-CNN based deep learning,"Abstractive Text Summarization (ATS), which is the task of constructing summary sentences by merging facts from different source sentences and condensing them into a shorter representation while preserving information content and overall meaning. It is very difficult and time consuming for human beings to manually summarize large documents of text. In this paper, we propose an LSTM-CNN based ATS framework (ATSDL) that can construct new sentences by exploring more fine-grained fragments than sentences, namely, semantic phrases. Different from existing abstraction based approaches, ATSDL is composed of two main stages, the first of which extracts phrases from source sentences and the second generates text summaries using deep learning. Experimental results on the datasets CNN and DailyMail show that our ATSDL framework outperforms the state-of-the-art models in terms of both semantics and syntactic structure, and achieves competitive results on manual linguistic quality evaluation.", 
"In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks. We propose several novel models that address critical problems in summarization. We also propose a new dataset consisting of multi-sentence summaries.",Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,"In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.", 
"Recently, abstractive text summarization has achieved success in switching from linear models via sparse and handcrafted features to nonlinear neural network models via dense inputs. This success comes from the application of deep learning models on natural language processing tasks.",Abstractive Text Summarization using Transfer Learning,"Recently, abstractive text summarization has achieved success in switching from linear models via sparse and handcrafted features to nonlinear neural network models via dense inputs. This success comes from the application of deep learning models on natural language processing tasks where these models are capable of modeling intricate patterns in data without handcrafted features. In this work, the text summarization problem has been explored using Sequence-to-sequence recurrent neural networks and Transfer Learning with a Unified Text-to-Text Transformer approaches. Experimental results showed that the Transfer Learning-based model achieved considerable improvement for abstractive text summarization.", 
Multi-head attention summarization (MHAS) model can consider the previously predicted words when generating new words. It can learn the internal structure of the article by adding self-attention layer to the traditional encoder and decoder. Experimental results show that our proposed model outperforms the previous extractive and abstractive models.,Abstractive Text Summarization with Multi-Head Attention,"In this paper, we present a novel sequence-tosequence architecture with multi-head attention for automatic summarization of long text. Summaries generated by previous abstractive methods have the problems of duplicate and missing original information commonly. To address these problems, we propose a multi-head attention summarization (MHAS) model, which uses multi-head attention mechanism to learn relevant information in different representation subspaces. The MHAS model can consider the previously predicted words when generating new words to avoid generating a summary of redundant repetition words. And it can learn the internal structure of the article by adding self-attention layer to the traditional encoder and decoder and make the model better preserve the original information. We also integrate the multi-head attention distribution into pointer network creatively to improve the performance of the model. Experiments are conducted on CNN/Daily Mail dataset, which is a long text English corpora. Experimental results show that our proposed model outperforms the previous extractive and abstractive models.", 
This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model. The DailyMail dataset is extended by collecting images and captions from the Web. Experiments show our model outperforms the neural abstractive and extractive text summarization methods that do not consider images.,Abstractive text-image summarization using multi-modal attentional hierarchical rnn,"Rapid growth of multi-modal documents on the Internet makes multi-modal summarization research necessary. Most previous research summarizes texts or images separately. Recent neural summarization research shows the strength of the Encoder-Decoder model in text summarization. This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model to summarize a text document and its accompanying images simultaneously, and then to align the sentences and images in summaries. A multi-modal attentional mechanism is proposed to attend original sentences, images, and captions when decoding. The DailyMail dataset is extended by collecting images and captions from the Web. Experiments show our model outperforms the neural abstractive and extractive text summarization methods that do not consider images. In addition, our model can generate informative summaries of images.", 
"Study: Semantic segmentation method segregates regions that only contain objects from the same class. In contrast, the fetal heart may contain multiple objects, such as the atria, ventricles, valves, and aorta. We demonstrate a strong correlation between the predicted septal defects and ground truth as a mean average precision (mAP) The model used has a high potential to help cardiologists complete the initial screening for fetal congenital heart disease.",Accurate Detection of Septal Defects With Fetal Ultrasonography Images Using Deep Learning-Based Multiclass Instance Segmentation,"Accurate screening for septal defects is important for supporting radiologists’ interpretative work. Some previous studies have proposed semantic segmentation and object detection approaches to carry out fetal heart detection; unfortunately, the models could not segment different objects of the same class. The semantic segmentation method segregates regions that only contain objects from the same class. In contrast, the fetal heart may contain multiple objects, such as the atria, ventricles, valves, and aorta. Besides, blurry boundaries (shadows) or a lack of consistency in the acquisition ultrasonography can cause wide variations. This study utilizes Mask-RCNN (MRCNN) to handle fetal ultrasonography images and employ it to detect and segment defects in heart walls with multiple objects. To our knowledge, this is the first study involving a medical application for septal defect detection using instance segmentation. The use of MRCNN architecture with ResNet50 as a backbone and a 0.0001 learning rate allows for two times faster training of the model on fetal heart images compared to other object detection methods, such as FasterRCNN (FRCNN). We demonstrate a strong correlation between the predicted septal defects and ground truth as a mean average precision (mAP). As shown in the results, the proposed MRCNN model achieves good performance in multiclass detection of the heart chamber, with 97.59% for the right atrium, 99.67% for the left atrium, 86.17% for the left ventricle, 98.83% for the right ventricle, and 99.97% for the aorta. We also report competitive results for the defect detection of holes in the atria and ventricles via semantic and instance segmentation. The results show that the mAP for MRCNN is about 99.48% and 82% for FRCNN. We suggest that evaluation and prediction with our proposed model provide reliable detection of septal defects, including defects in the atria, ventricles, or both. These results suggest that the model used has a high potential to help cardiologists complete the initial screening for fetal congenital heart disease.", 
"An EEG emotional feature learning and classification method using deep Convolution Neural Network (CNN) was proposed based on temporal features, frequential features and their combinations of EEG signals. The experimental results showed that the deep CNN models which require no feature engineering achieved the best recognition performance on temporal and frequency combined features.",Accurate EEG-based Emotion Recognition on Combined Features Using Deep Convolutional Neural Networks,"In order to improve the accuracy of emotional recognition by end-to-end automatic learning of emotional features in spatial and temporal dimensions of electroencephalogram (EEG), an EEG emotional feature learning and classification method using deep Convolution Neural Network (CNN) was proposed based on temporal features, frequential features and their combinations of EEG signals in DEAP dataset. The shallow machine learning models including Bagging tree (BT), Support vector machine (SVM), linear discriminant analysis (LDA) and Bayesian linear discriminant analysis (BLDA) models and deep CNN models were used to make emotional binary classification experiments on DEAP datasets in valence and arousal dimensions. The experimental results showed that the deep CNN models which require no feature engineering achieved the best recognition performance on temporal and frequency combined features in both valence and arousal dimensions, which is 3.58% higher than the performance of the best traditional BT classifier in valence dimension and 3.29% higher than that of BT classifier in arousal dimension.", 
"Code summarization is the task of creating short, natural language descriptions of source code. In recent years, automatic code summarization has become a high value target of research. In this paper, we advocate for a special emphasis on action word prediction as an important stepping stone problem.",Action Word Prediction for Neural Source Code Summarization,"Source code summarization is the task of creating short, natural language descriptions of source code. Code summarization is the backbone of much software documentation such as JavaDocs, in which very brief comments such as “adds the customer object” help programmers quickly understand a snippet of code. In recent years, automatic code summarization has become a high value target of research, with approaches based on neural networks making rapid progress. However, as we will show in this paper, the production of good summaries relies on the production of the action word in those summaries: the meaning of the example above would be completely changed if “removes” were substituted for “adds.” In this paper, we advocate for a special emphasis on action word prediction as an important stepping stone problem towards better code summarization – current techniques try to predict the action word along with the whole summary, and yet action word prediction on its own is quite difficult. We show the value of the problem for code summaries, explore the performance of current baselines, and provide recommendations for future research.", 
We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. We show that adding parts leads to top-performing results for both tasks.,Actions and Attributes from Wholes and Parts,"We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. We observe that for deeper networks parts are less significant. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-ofthe-art person detection system.", 
Bayesian classifiers with Gaussian mixture models are adopted as the decision rule to classify electroencephalogram (EEG) signals. The stochastic approximation method is used as the specific gradient descent method for updating the parameters of mean values.,ADAPTIVE EEG SIGNAL CLASSIFICATION USING STOCHASTIC APPROXIMATION METHODS,"Classification of time-varying electrophysiological signals is an important problem in the development of brain-computer interfaces (BCIs). Designing adaptive classifiers is a potential way to address this task. In this paper, Bayesian classifiers with Gaussian mixture models (GMMs) are adopted as the decision rule to classify electroencephalogram (EEG) signals. The stochastic approximation method (SAM) is used as the specific gradient descent method for updating the parameters of mean values and covariance matrices in the distribution of GMMs, where the parameters are simultaneously updated in a batch mode. Experimental results using data from a BCI show that the stochastic approximation method is effective for EEG classification tasks.", 
"One challenge in the current research of brain–computer interfaces (BCIs) is how to classify time-varying electroencephalographic (EEG) signals as accurately as possible. We propose an adaptive feature extractor, namely adaptive common spatial patterns (ACSP).",Adaptive feature extraction for EEG signal classification,"One challenge in the current research of brain–computer interfaces (BCIs) is how to classify time-varying electroencephalographic (EEG) signals as accurately as possible. In this paper, we address this problem from the aspect of updating feature extractors and propose an adaptive feature extractor, namely adaptive common spatial patterns (ACSP). Through the weighed update of signal covariances, the most discriminative features related to the current brain states are extracted by the method of multi-class common spatial patterns (CSP). Pseudo-online simulations of EEG signal classification with a support vector machine (SVM) classifier for multi-class mental imagery tasks show the effectiveness of the proposed adaptive feature extractor.", 
Vehicular Ad hoc NETworks (VANET) aims to enhance traffic safety by enabling frequent broadcasting of location information between vehicles. Many privacy schemes have adopted a silent period in which a vehicle stops sharing its locations for a period. A privacy simulator PREXT is used to evaluate and compare the performance of schemes.,Adjusted Location Privacy Scheme for VANET Safety Applications,"The primary aim of Vehicular Ad hoc NETworks (VANET) is to enhance traffic safety by enabling frequent broadcasting of location information between vehicles. In VANET safety applications, a vehicle requires to broadcast messages, which usually contain its location information, every (1-10 Hz) with other vehicles in its communication area (300m) to facilitate cooperative awareness. This would arise privacy issues because vehicles are vulnerable to tracking attacks via their locations. To prevent long-term linking, many privacy schemes have adopted a silent period in which a vehicle stops sharing its locations for a period. However, silent periods could have a negative impact on safety applications as an accident could have happened if a vehicle stop sharing its locations with other neighbours. Thus, in this paper, we first discuss three privacy schemes (RSP, SLOW and CAPS), which adopted silent periods but in different concepts. Then, we improve the privacy and safety level of CAPS. A privacy simulator PREXT is used to evaluate and compare the performance of schemes.", 
"This paper proposes a novel Adversarial Reinforcement Learning architecture for Chinese text summarization. In our model, we use a generator to generate summaries, a discriminator to distinguish between generated summaries and real ones, and reinforcement learning to evolve the generator. Experiments were run on two Chinese corpora, respectively consisting of long documents and short texts.",Adversarial Reinforcement Learning for Chinese Text Summarization,"This paper proposes a novel Adversarial Reinforcement Learning architecture for Chinese text summarization. Previous abstractive methods commonly use Maximum Likelihood Estimation (MLE) to optimize the generative models, which makes auto-generated summary less incoherent and inaccuracy. To address this problem, we innovatively apply the Adversarial Reinforcement Learning strategy to narrow the gap between the generated summary and the human summary. In our model, we use a generator to generate summaries, a discriminator to distinguish between generated summaries and real ones, and reinforcement learning (RL) strategy to iteratively evolve the generator. Besides, in order to better tackle Chinese text summarization, we use a character-level model rather than a word-level one and append Text-Attention in the generator. Experiments were run on two Chinese corpora, respectively consisting of long documents and short texts. Experimental Results showed that our model significantly outperforms previous deep learning models on rouge score.", 
"The next generation of mobile systems, 5G, will be the communication standard that accommodates the proliferation of the Internet of Things (IoT) Unmanned aerial vehicles (UAVs) are envisioned to support many applications in providing 5G connectivity to the IoT. The authors raise the problem of degrading the network spectrum with UAVs' management messages.",Aerial Control System for Spectrum Effciency in UAV-to-Cellular Communications,"The next generation of mobile systems, 5G, will be the communication standard that accommodates the proliferation of the Internet of Things (IoT). Unmanned aerial vehicles (UAVs) are envisioned to support many applications in providing 5G connectivity to the IoT, by extending highspeed connectivity from the sky to objects on the ground, or even by carrying onboard some IoT devices. However, given their critical nature, the management of UAVs induces high exchange of control messages with the ground control station, resulting in the crowded spectrum used by cellular networks. The authors raise the problem of degrading the network spectrum with UAVs’ management messages, and discuss the need for an efficient orchestration system. In this article, they propose a novel scheme, dubbed the aerial control system, which is based on separating the data plane from the control plane of UAVs, and pushing the latter to be performed in the air by UAVs. The solution provides an orchestration logic that takes advantage of the autonomous nature of UAVs to organize UAVs in one or several clusters. UAV-to-UAV communication enables spectrum reuse and avoids crowding the network with management messages, while dedicating more 5G spectrum for ensuring more bandwidth to the IoT through UAV-to-infrastructure communication.", 
"Affective states of a user provide important information for many applications such as, personalized information retrieval/delivery or intelligent human-computer interface design. Electroencephalogram (EEG) in particular, has been shown to be very effective in estimating a user's affective states. Two types of semi-supervised deep learning approaches were applied as application specific feature extractors.",Affective States Classification using EEG and Semi-supervised Deep Learning Approaches,"Affective states of a user provide important information for many applications such as, personalized information (e.g., multimedia content) retrieval/delivery or intelligent human-computer interface design. In recently years, physiological signals, Electroencephalogram (EEG) in particular, have been shown to be very effective in estimating a user’s affective states during social interaction or under video or audio stimuli. However, due to the large number of parameters associated with the neural expression of emotion, there is still a lot of unknowns on the specific spatial and spectral correlation of the EEG signal and the affective states expression. To investigate on such correlation, two types of semi-supervised deep learning approaches, stacked denoising autoencoder (SDAE) and deep belief networks (DBN), were applied as application specific feature extractors for the affective states classification problem using EEG signals. To evaluate the efficacy of the proposed semi-supervised approaches, a subject-specific affective states classification experiment were carried out on the DEAP database to classify 2-dimensional affect states. The DBN based model achieved averaged F1 scores of 86.67%, 86.60% and 86.69% for arousal, valence and liking states classification respectively, which has significantly improved the state-of-art classification performance. By examining the weight vectors at each layer, we were also able to gain insights on the spatial or spectral locations of the most discriminating features. Another main advantage of applying the semi-supervised learning methods is that only a small fraction of labeled data, e.g., 1/6 of the training samples, were used in this study.", 
Image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. In this paper we investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval. We show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities.,Aggregating Deep Convolutional Features for Image Retrieval,"Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It has also been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregation approaches developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptors. In this paper we investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides arguably the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.", 
"In this paper, we present an AHP technique for Persian Text Summarization. This proposed model uses analytical hierarchy as a base factor for an evaluation algorithm.",AHP techniques for Persian text summarization,"In recent years, there has been an increasing amount of information on the web. Some of essential resources to shorten text documents use summarization technologies. In this paper, we present an AHP technique for Persian Text Summarization. This proposed model uses analytical hierarchy as a base factor for an evaluation algorithm and improves the summarization quality of Persian language text. The weighting and combination methods are two main contributions of the proposed text evaluation algorithm.", 
"Software Defined Internet of Things (SD-IoT) Networks profits from centralized management and interactive resource sharing. But with the rapid growth in services and applications, it is vulnerable to possible attacks and faces severe security challenges. In this paper, we propose an AI-based two-stage intrusion detection empowered by software defined technology.",AI-based Two-Stage Intrusion Detection for Software Defined IoT Networks,"Software Defined Internet of Things (SD-IoT) Networks profits from centralized management and interactive resource sharing which enhances the efficiency and scalability of IoT applications. But with the rapid growth in services and applications, it is vulnerable to possible attacks and faces severe security challenges. Intrusion detection has been widely used to ensure network security, but classical detection means are usually signature-based or explicit-behavior-based and fail to detect unknown attacks intelligently, which are hard to satisfy the requirements of SD-IoT Networks. In this paper, we propose an AI-based two-stage intrusion detection empowered by software defined technology. It flexibly captures network flows with a globle view and detects attacks intelligently through applying AI algorithms. We firstly leverage Bat algorithm with swarm division and Differential Mutation to select typical features. Then, we exploit Random forest through adaptively altering the weights of samples using weighted voting mechanism to classify flows. Evaluation results prove that the modified intelligent algorithms select more important features and achieve superior performance in flow classification. It is also verified that intelligent intrusion detection shows better accuracy with lower overhead comparied with existing solutions.", 
This paper attempts to provide a background study of the various classical methods proposed by researchers for automatic text summarization. Attention is devoted to the most widely used algebraic methods called Singular Value Decomposition and Non-negative Matrix Factorization.,Algebraic reduction in automatic text summarization â€“ the state of the art,Various kinds of information that is available on a topic electronically has abundantly increased over the past years. It has led the information highway to a situation called “information overload” problem. Automatic text summarization technique mainly addresses this issue by the extraction of a shortened version of information from texts written about the same topic. Several algebraic reduction methods are used to identify and extract the semantically important texts in a document to summarize it automatically. This paper attempts to provide a background study of the various classical methods proposed by researchers for automatic text summarization. Special focus is given to the most widely used algebraic methods called Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF). This work sheds more light on the application of SVD and NMF techniques on automatic text summarization. Attention is also devoted in this work to analyze the advantages and disadvantages of each approach., 
"State-of-the-art on automatic text summarization mostly revolves around news articles. We suggest that considering a wider variety of tasks would lead to an improvement in the field. We report automatic alignment and summarization performances on public_meetings, a novel corpus of aligned public meetings.","Align then summarize, automatic alignment methods for summarization corpus creation","Summarizing texts is not a straightforward task. Before even considering text summarization, one should determine what kind of summary is expected. How much should the information be compressed? Is it relevant to reformulate or should the summary stick to the original phrasing? State-of-the-art on automatic text summarization mostly revolves around news articles. We suggest that considering a wider variety of tasks would lead to an improvement in the field, in terms of generalization and robustness. We explore meeting summarization: generating reports from automatic transcriptions. Our work consists in segmenting and aligning transcriptions with respect to reports, to get a suitable dataset for neural summarization. Using a bootstrapping approach, we provide pre-alignments that are corrected by human annotators, making a validation set against which we evaluate automatic models. This consistently reduces annotators’ efforts by providing iteratively better pre-alignment and maximizes the corpus size by using annotations from our automatic alignment models. Evaluation is conducted on public_meetings, a novel corpus of aligned public meetings. We report automatic alignment and summarization performances on this corpus and show that automatic alignment is relevant for data annotation since it leads to large improvement of almost +4 on all ROUGE scores on the summarization task.", 
"The objective of this paper is large scale object instance retrieval, given a query image. We start from VLAD, the state-of-the-art compact descriptor introduced by Jegou, for this purpose. We show that a simple change to the normalization method significantly improves retrieval performance.",All about VLAD,"The objective of this paper is large scale object instance retrieval, given a query image. A starting point of such systems is feature detection and description, for example using SIFT. The focus of this paper, however, is towards very large scale retrieval where, due to storage requirements, very compact image descriptors are required and no information about the original SIFT descriptors can be accessed directly at run time. We start from VLAD, the state-of-the art compact descriptor introduced by Jegou, for this purpose, and make three novel contributions: first, we show that a simple change to the normalization method significantly improves retrieval performance; second, we show that vocabulary adaptation can substantially alleviate problems caused when images are added to the dataset after initial vocabulary learning. These two methods set a new state=of-the-art over all benchmarks investigated here for both mid-dimensional (20k-D to 30k-D) and small (128-D) descriptors. Our third contribution is a multiple spatial VLAD representation, MultiVLAD, that allows the retrieval and localization of objects that only extend over a small part of an image (again without requiring use of the original image SIFT descriptors).", 
"This paper presents a conceptual of EEG analysis and classification of brainwaves signal for alpha and beta signals during Functional Electrical Stimulation, FES-assisted exercise. It also includes the criteria of the subject for both stroke patient and healthy person.",Alpha and Beta EEG Brainwave Signal Classification Technique A Conceptual Study,"This paper presents a conceptual of EEG analysis and classification of brainwaves signal for alpha and beta signals during Functional Electrical Stimulation, FES-assisted exercise. The characteristics of brainwave signals, data acquisition for electroencephalograph (EEG) signal and data session are identified. This paper also includes the criteria of the subject for both stroke patient and healthy person. The process of filtering the artifact and sampling the data were studied based on the established previous worked. In addition, a review on feature extraction for further classifying of brainwave signals stroke patients before and after performing FES-assisted exercised were also identified.", 
Ambient backscatter communication technology can support sustainable and independent communications. It can open up a whole new set of applications that facilitate the Internet of things (IoT) The authors propose a novel hybrid transmitter design by combining the advantages of both ambient backscattering and wireless powered communications.,Ambient Backscatter Assisted Wireless Powered Communications,"Ambient backscatter communication technology has been introduced recently, and is quickly becoming a promising choice for self-sustainable communication systems, as an external power supply or a dedicated carrier emitter is not required. By leveraging existing RF signal resources, ambient backscatter technology can support sustainable and independent communications and consequently open up a whole new set of applications that facilitate Internet of things (IoT). In this article, we study an integration of ambient backscatter with wireless powered communication networks (WPCNs). We first present an overview of backscatter communication systems with an emphasis on the emerging ambient backscatter technology. Then we propose a novel hybrid transmitter design by combining the advantages of both ambient backscatter and wireless powered communications. Furthermore, in the cognitive radio environment, we introduce a multiple access scheme to coordinate hybrid data transmissions. The performance evaluation shows that the hybrid transmitter outperforms traditional designs. In addition, we discuss open issues related to ambient backscatter networking.", 
"This paper proposes an approach, based on abstract argumentation, to select the sentences in a text that are to be included in its summary. The proposed approach obtained interesting experimental results on the English subset of the benchmark MultiLing 2015 dataset.",An Abstract Argumentation-Based Approach to Automatic Extractive Text Summarization,"Sentence-based extractive summarization aims at automatically generating shorter versions of texts by extracting from them the minimal set of sentences that are necessary and sufficient to cover their content. Providing effective solutions to this task would allow the users of Digital Libraries to save time in selecting documents that may be appropriate for satisfying their information needs or for supporting their decision-making tasks. This paper proposes an approach, based on abstract argumentation, to select the sentences in a text that are to be included in its summary. The proposed approach obtained interesting experimental results on the English subset of the benchmark MultiLing 2015 dataset.", 
"Latent Semantic Analysis or LSA uses a method of singular value decomposition of co-occurrence document-term matrix to derive a latent class model. Recent works have improved the standard LSA using method of probability distribution, regularization, sparseness constraint.",An adaptive Latent Semantic Analysis for text mining,"Latent Semantic Analysis or LSA uses a method of singular value decomposition of co-occurrence document-term matrix to derive a latent class model. Despite its success, there are some shortcomings in this technique. Recent works have improved the standard LSA using method of probability distribution, regularization, sparseness constraint. But there are still some other deficiencies. It is dealt with this paper, an adapted technique called hk-LSA based on reducing dimension of vector space and like-probabilistic relationships between document and latent-topic space is proposed. The adaptive technique overcomes some weak points of LSA such as processing density of orthogonal matrices, complexity in matrix decomposition, facing with alternative iteration algorithms, etc. The experiments show consistent and substantial improvements of the hk-LSA over LSA.", 
Heart disease is a term that assigns to a large number of medical conditions related to the heart. This paper aims at analyzing the various data mining techniques introduced in recent years for heart disease prediction.,An Analysis of Heart Disease Prediction using Different Data Mining Techniques,Heart disease is a term that assigns to a large number of medical conditions related to heart. These medical conditions describe the abnormal health conditions that directly influence the heart and all its parts. Heart disease is a major health problem in today’s time. This paper aims at analyzing the various data mining techniques introduced in recent years for heart disease prediction. The observations reveal that Neural networks with 15 attributes has outperformed over all other data mining techniques. Another conclusion from the analysis is that decision tree has also shown good accuracy with the help of genetic algorithm and feature subset selection., 
"Machine learning plays an ever-increasing role in advanced automotive functionality for driver assistance and autonomous operation. However, its adequacy from the perspective of safety certification remains controversial.",An Analysis of ISO 26262 Using Machine Learning Safely in Automotive Software,"Machine learning (ML) plays an ever-increasing role in advanced automotive functionality for driver assistance and autonomous operation; however, its adequacy from the perspective of safety certification remains controversial. In this paper, we analyze the impacts that the use of ML as an implementation approach has on ISO 26262 safety lifecycle and ask what could be done to address them. We then provide a set of recommendations on how to adapt the standard to accommodate ML.", 
"WhatsApp is mobile application which allows exchange of messages, videos, audio's and images via Smartphone. This paper focuses on",An Analysis of WhatsApp Forensics in Android Smartphones,"WhatsApp is mobile application which allows exchange of messages, videos, audio’s and images via Smartphone. The increased use of IM on Android phones has turned to be goldmine for mobile and computer forensic investigators. This paper focuses on conducting forensic data analysis by extracting useful information from WhatsApp and from similar applications installed on Android platform.", 
"SVM obtained the uppermost performance with the utmost classification accuracy of 97.07%. Whereas, NB and RF have achieved the second highest accuracy by prediction. Findings can help to reduce the existence of breast cancer disease through developing machine learning-based predictive system.",An Analysis On Breast Disease Prediction Using Machine Learning Approaches,"The central aspect of this study is to evaluate the different Machine learning classifier's performance for the prediction of breast cancer disease. In this work, we have used six supervised classification techniques for the classification of breast cancer disease. For example, SVM, NB, KNN, RF, DT, and LR used for the early prediction of breast cancer. Therefore, we evaluated breast cancer dataset through sensitivity, specificity, f1 measure, and total accuracy. The prediction performance of breast cancer analysis shows that SVM obtained the uppermost performance with the utmost classification accuracy of 97.07%. Whereas, NB and RF have achieved the second highest accuracy by prediction. Our findings can help to reduce the existence of breast cancer disease through developing a machine learning-based predictive system for early prediction.", 
Android forensics is one of the most studied topics in the mobile forensics literature. There does not appear to have a formal model that captures the activities undertaken during a forensic investigation. We adapt a widely used adversary model from the cryptographic literature to capture a forensic investigator's capabilities. We demonstrate the utility of the model using five popular Android social apps.,An Android Social App Forensics Adversary Model,"Android forensics is one of the most studied topics in the mobile forensics literature, partly due to the popularity of Android devices and apps. However, there does not appear to have a formal model that captures the activities undertaken during a forensic investigation. In this paper, we adapt a widely used adversary model from the cryptographic literature to formally capture a forensic investigator’s capabilities during the collection and analysis of evidentiary materials from mobile devices. We demonstrate the utility of the model using five popular Android social apps (Twitter, POF Dating, Snapchat, Fling and Pinterest). We recover various information of forensic interest, such as databases, user account information, sent-received images, profile pictures, contact lists, unviewed text messages. We are also able to determine when a notification was sent, a tweet was posted, as well as identifying the Facebook authentication token string used in the apps.", 
Pakistan is the only country which is greatly affected by terrorism after becoming a front line ally of United States. Joining the war against terrorism was the biggest mistake of Pakistan. Pakistan should keep the national interest in mind and become more careful.,AN APPRAISAL OF TERRORISM IN PAKISTAN,"Pakistan is the only country which is greatly affected by terrorism after becoming a front line ally of United States in war against terrorism. Pakistan had to join the war in order to bring an end to the international isolation it was facing after the nuclear tests of 1998 and to overcome the sanctions in order to get economic as well as military assistance. Joining the war against terrorism was the biggest mistake of Pakistan as it produced many short and long term repercussions. The consequences of these policies brought terrorism in Pakistan and in order to counter that terrorism the armed forces had fought many wars inside its territory, which resulted in institutional instability and raised social issues that remained steady to plague the society. The United States is very much concerned about the rise of terrorism and extremism in the country. Now the need of the hour is that Pakistan should keep the national interest in mind and become more careful in understanding the most tough security challenges faced by the country and it should not allow any other country to gain strategic advantage of the ongoing situation.", 
"Text Summarization is one of the mentionable research areas of Natural language processing. In this paper, a word2 vector approach has been discussed in the context of text summarization for the Bengali language.",An Approach for Bengali Text Summarization using Word2Vector,"Text Summarization is one of the mentionable research areas of Natural language processing. Several approaches have already been developed in this concern. Such as – Abstractive approach and extractive approach. Most recent recurrent neural network methods are producing much better results. Several mentionable research has already been discussed for English language summarizer, but a few have already done for the Bengali language. There are so many prerequisites for data analysis purpose -word2vector is one of them. Understanding the vector representation of any text leads the way to identify the key main points of that specific text and helps to measure the relationship of that text with other texts in similarity/dissimilarity. Generated matrix using word2vector can easily applicable for identifying top-ranked sentence/words, either domain specific or in general form. In this paper, a word2vector approach has been discussed in the context of text summarization for the Bengali language.", 
Automatic text summarization aims to reduce the document text size by building a brief and voluble summary that has the most important ideas in a document. Many approaches have been proposed to improve the automatic text summarizing results. This paper focuses on combining multiple graph-based approaches to improve results.,An Approach for Combining Multiple Weighting Schemes and Ranking Methods in Graph-Based Multi-Document Summarization,"Automatic text summarization aims to reduce the document text size by building a brief and voluble summary that has the most important ideas in that document. Through the years, many approaches were proposed to improve the automatic text summarization results; the graph-based method for sentence ranking is considered one of the most important approaches in this field. However, most of these approaches rely on only one weighting scheme and one ranking method, which may cause some limitations in their systems. In this paper, we focus on combining multiple graph-based approaches to improve the results of generic, extractive, and multi-document summarization. This improvement results in more accurate summaries, which could be used as a significant part of some natural language applications. We develop and experiment with two graph-based approaches that combine four weighting schemes and two ranking methods in one graph framework. To combine these methods, we propose taking the average of their results using the arithmetic mean and the harmonic mean. We evaluate our proposed approaches using DUC 2003 & DUC 2004 dataset and measure the performance using ROUGE evaluation toolkit. Our experiments demonstrate that using the harmonic mean in combining weighting schemes outperform the arithmetic mean and show a good improvement over the baselines and many state-of-the-art systems.", 
Research is going on for text summarization because of increasing information in the internet. The whole concept is to reduce or minimize the important information present in the documents. The procedure is manipulated by Restricted Boltzmann Machine (RBM) algorithm for better efficiency.,An approach for text summarization using deep learning algorithm,"Now days many research is going on for text summarization. Because of increasing information in the internet, these kind of research are gaining more and more attention among the researchers. Extractive text summarization generates a brief summary by extracting proper set of sentences from a document or multiple documents by deep learning. The whole concept is to reduce or minimize the important information present in the documents. The procedure is manipulated by Restricted Boltzmann Machine (RBM) algorithm for better efficiency by removing redundant sentences. The restricted Boltzmann machine is a graphical model for binary random variables. It consist of three layers input, hidden and output layer. The input data uniformly distributed in the hidden layer for operation. The experimentation is carried out and the summary is generated for three different document set from different knowledge domain. The f-measure value is the identifier to the performance of the proposed text summarization method. The top responses of the three different knowledge domain in accordance with the f-measure are 0.85, 1.42 and 1.97 respectively for the three document set.", 
"In this paper, we propose an approach to abstractive text summarization based on discourse rules, syntactic constraints, and word graph. Discourse rules are used to generate sentences from keywords. Word graph is used to represent word relations in the text.",An approach to abstractive text summarization,"Abstractive summarization is the technique of generating a summary of a text from its main ideas, not by copying verbatim most salient sentences from text. This is an important and challenge task in natural language processing. In this paper, we propose an approach to abstractive text summarization based on discourse rules, syntactic constraints, and word graph. Discourse rules and syntactic constraints are used in the process of generating sentences from keywords. Word graph is used in the sentence combination process to represent word relations in the text and to combine several sentences into one. Experimental results show that our approach is promising in solving the abstractive summarization task.", 
"Automatic summarization has been a hot topic in the field of natural language processing. In this paper, we introduce LexRank to Chinese texts to make up for the deficiency of LexRank. We propose an approach of extractive summarization for Chinese text based on the combination of spectral clustering and Lexrank.",An approach to automatic summarization for Chinese text based on the combination of spectral clustering and LexRank,"In the past half century, automatic summarization has been a hot topic in the field of natural language processing, and it will be paid more and more attention to with the rapid development of the mobile network technology. Most of the automatic summarization research today is on extractive summarization, which mainly ranks the sentences according to their simple heuristic features such as the frequency of words they contain, their position in the text or paragraph and so on. Inspired by the great performance of LexRank, we manage to introduce LexRank to Chinese texts. In order to make up the deficiency of LexRank, spectral clustering is adopted to process the component analysis. All in all, we propose an approach of extractive summarization for Chinese text based on the combination of spectral clustering and LexRank, which is of high coverage and low redundancy. It is demonstrated by experiments that our approach has been greatly improved compared to the original LexRank. In addition, our approach is easy to implement and robust to noise.", 
"Automatic summarization has been a hot topic in the field of natural language processing. In this paper, we introduce LexRank to Chinese texts to make up for the deficiency of LexRank. We propose an approach of extractive summarization for Chinese text based on the combination of spectral clustering and Lexrank.",An approach to automatic text summarization using simplified lesk algorithm and wordnet,"Text Summarization is a way to produce a text, which contains the significant portion of information of the original text(s). Different methodologies are developed till now depending upon several parameters to find the summary as the position, format and type of the sentences in an input text, formats of different words, frequency of a particular word in a text etc. But according to different languages and input sources, these parameters are varied. As result the performance of the algorithm is greatly affected. The proposed approach summarizes a text without depending upon those parameters. Here, the relevance of the sentences within the text is derived by Simplified Lesk algorithm and WordNet, an online dictionary. This approach is not only independent of the format of the text and position of a sentence in a text, as the sentences are arranged at first according to their relevance before the summarization process, the percentage of summarization can be varied according to needs. The proposed approach gives around 80% accurate results on 50% summarization of the original text with respect to the manually summarized result, performed on 50 different types and lengths of texts. We have achieved satisfactory results even upto 25% summarization of the original text.", 
Text Summarization is the procedure by which significant portions of a text are retrieved. This approach performs the summarization task by unsupervised learning methodology. The importance of a sentence in an input text is evaluated by the help of Simplified Lesk algorithm. The proposed approach gives best results upto 50% summarization of the original text.,An approach to automatic text summarization using WordNet,"Text Summarization is the procedure by which the significant portions of a text are retrieved. Most of the approaches perform the summarization based on some hand tagged rules, such as format of the writing of a sentence, position of a sentence in the text, frequency of few particular words in a sentence etc. But according to different input sources, these predefined constraints greatly affect the result. The proposed approach performs the summarization task by unsupervised learning methodology. The importance of a sentence in an input text is evaluated by the help of Simplified Lesk algorithm. As an online semantic dictionary WordNet is used. First, this approach evaluates the weights of all the sentences of a text separately using the Simplified Lesk algorithm and arranges them in decreasing order according to their weights. Next, according to the given percentage of summarization, a particular number of sentences are selected from that ordered list. The proposed approach gives best results upto 50% summarization of the original text and gives satisfactory result even upto 25% summarization of the original text.", 
"In this paper, we propose a practical approach for extracting the most relevant sentences from a document to form a summary. The idea of our approach is to obtain concepts of words based on HowNet.",An approach to concept-obtained text summarization,"In this paper, we propose a practical approach for extracting the most relevant sentences from the original document to form a summary. The idea of our approach is to obtain concepts of words based on HowNet, and use concept as feature, instead of word. We use conceptual vector space model to form a rough summarization, and then calculate degree of semantic similarity of sentence for reducing its redundancy. Experimental results show that our approach is effective and efficient, and performance of the system is reliable.", 
This paper describes an approach to generic Bengali text summarization using latent semantic analysis. We have compared our proposed approach with some state-of-the-art.,An Approach to Generic Bengali Text Summarization Using Latent Semantic Analysis,This paper describes an approach to generic Bengali text summarization using latent semantic analysis (LSA). Our proposed LSA based single document summarization method uses the latent semantic analysis technique to identify semantically important sentences. We have compared our proposed approach with some state-of-the art summarization approaches. The results demonstrate that our proposed Bengali text summarization approach is effective., 
We propose an efficient text summarization technique that involves two basic operations. The first operation involves finding coherent chunks in the document. The second operation involves ranking the text and picking the sentences that rank above a given threshold.,An Approach to Text Summarization,We propose an efficient text summarization technique that involves two basic operations. The first operation involves finding coherent chunks in the document and the second operation involves ranking the text in the individual coherent chunks and picking the sentences that rank above a given threshold. The coherent chunks are formed by exploiting the lexical relationship between adjacent sentences in the document. Occurrence of words through repetition or relatedness by sense relation plays a major role in forming a cohesive tie. The proposed text ranking approach is based on a graph theoretic ranking model applied to text summarization task., 
"Big data generated in the vehicular network is also an issue in the security of VANET. Cuckoo filter, as a space-efficient probabilistic data structure, is used in the authentication mechanism. In the second layer, a plausibility model by performing fuzzy logic is presented to cope with inaccurate information. All obtained results are validated through well-known evaluation measures.",An authentication and plausibility model for big data analytic under LOS and NLOS conditions in 5G-VANET,"The exchange of correct and reliable data among legitimate nodes is one of the most important challenges in vehicular ad hoc networks (VANETs). Malicious nodes and obstacles, by generating inaccurate information, have a negative impact on the security of 5G-VANET. The big data generated in the vehicular network is also an issue in the security of VANET. To this end, a security model based on authentication and plausibility is proposed to improve the safety of network named ‘AFPM’. In the first layer, an authentication mechanism using edge nodes along with 5G is proposed to deal with the illegitimate nodes who enter the network and broadcast wrong information. In the authentication mechanism, because of the growth of the connected vehicles to the edge nodes that lead to generating big data and hence the inappropriateness of the traditional data structures, cuckoo filter, as a space-efficient probabilistic data structure, is used. In the second layer, a plausibility model by performing fuzzy logic is presented to cope with inaccurate information. The plausibility model is based on detection of inconsistent data involved in the event message. The plausibility model not only tackles with inaccurate, incomplete, and inaccuracy data but also deals with misbehaviour nodes under both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. All obtained results are validated through well-known evaluation measures such as F-measure and communication overhead. The results presented in this paper demonstrate that the proposed security model possesses a better performance in comparison with the existing studies.", 
Study: Predictive system based on artificial neural network can accurately predict heart disease. The proposed model achieves the prediction accuracy of 93.33%. The obtained results are promising compared to the previously reported methods. The findings suggest that the proposed diagnostic system can be used by physicians.,An Automated Diagnostic System for Heart Disease Prediction Based on x 2 Statistical Model and Optimally Configured Deep Neural Network,"Different automated decision support systems based on artificial neural network (ANN) have been widely proposed for the detection of heart disease in previous studies. However, most of these techniques focus on the preprocessing of features only. In this paper, we focus on both, i.e., refinement of features and elimination of the problems posed by the predictive model, i.e., the problems of underfitting and overfitting. By avoiding the model from overfitting and underfitting, it can show good performance on both the datasets, i.e., training data and testing data. Inappropriate network configuration and irrelevant features often result in overfitting the training data. To eliminate irrelevant features, we propose to use ? 2 statistical model while the optimally configured deep neural network (DNN) is searched by using exhaustive search strategy. The strength of the proposed hybrid model named ? 2-DNN is evaluated by comparing its performance with conventional ANN and DNN models, another state of the art machine learning models and previously reported methods for heart disease prediction. The proposed model achieves the prediction accuracy of 93.33%. The obtained results are promising compared to the previously reported methods. The findings of the study suggest that the proposed diagnostic system can be used by physicians to accurately predict heart disease.", 
Early risk identification of an unexpected sudden cardiac death (SCD) is highly significant for timely intervention and increasing survival rate. We present an automated strategy for prediction of SCD with a high-level accuracy by using measurable arrhythmic markers. The automated strategy can predict SCD in less than one second with an average accuracy of 98.91%. The method could be more practical and efficient if applied in portable smart devices with real-time requirements in hospital settings or at home.,An Automated Strategy for Early Risk Identification of Sudden Cardiac Death by Using Machine Learning Approach on Measurable Arrhythmic Risk Markers,"Early risk identification of an unexpected sudden cardiac death (SCD) in a person who is suffering malignant ventricular arrhythmias is highly significant for timely intervention and increasing the survival rate. For this purpose, we have presented an automated strategy for prediction of SCD with a high-level accuracy by using measurable arrhythmic markers in this paper. The set of arrhythmic parameters includes three repolarization interval ratios, such as TpTe/QT, JTp/JTe, and TpTe/JTp and two conduction-repolarization markers, such as TpTe/QRS and TpTe/(QT×QRS). Each of them is calculated directly from the detected QRS complex waves and T-wave of electrocradiogram (ECG) signals. Then, all calculated markers are used for the automatical classification of normal and SCD risk groups by employing machine learning classifiers, such as k-nearest neighbor (KNN), decision tree (DT), Naive Bayes (NB), support vector machine (SVM), and random forest (RF). The effectiveness and usefulness of the proposed method is evaluated using a database of measured ECG data acquired from 28 SCD and 18 normal patients. For the automated strategy, the set of five arrhythmic risk markers can predict SCD in less than one second with an average accuracy of 98.91% (KNN), 98.70% (SVM), 98.99% (DT), 97.46% (NB), and 99.49% (RF) for 30 minutes before the occurrence of SCD. Moreover, a practical and straightforward SCD index (SCDI) through a judicious integration of these markers is also proposed by using the Student’s t-test. The obtained SCDIs are 1.2058 ± 0.0795 and 1.7619 ± 0.1902 for normal and SCD patients, respectively, which provide a sufficient discrimination degree with a p-value of 6.5061e-35. The present results show that both the automated classifier and the integrated SCDI can predict the SCD up to 30 minutes earlier, and that these predictions could be more practical and efficient if applied in portable smart devices with real-time requirements in hospital settings or at home.", 
"Epilepsy is a life-threatening and challenging neurological disorder, affecting a large number of people. Manual inspection of EEG brain signals is a time-consuming and laborious process, which puts a heavy burden on neurologists. To overcome this problem, we propose a system that is an ensemble of pyramidal one-dimensional convolutional neural network (P-1D-CNN) models. In almost all cases concerning epilepsy detection, it gives an accuracy of 99.1% and outperforms the state-of-the-art systems.",An automated system for epilepsy detection using EEG brain signals based on deep learning approach,"Epilepsy is a life-threatening and challenging neurological disorder, which is affecting a large number of people all over the world. For its detection, encephalography (EEG) is a commonly used clinical approach, but manual inspection of EEG brain signals is a time-consuming and laborious process, which puts a heavy burden on neurologists and affects their performance. Several automatic systems have been proposed using traditional approaches to assist neurologists, which perform well in detecting binary epilepsy scenarios e.g. normal vs. ictal, but their performance degrades in classifying ternary case e.g. ictal vs. normal vs. inter-ictal. To overcome this problem, we propose a system that is an ensemble of pyramidal one-dimensional convolutional neural network (P-1D-CNN) models. Though a CNN model learns the internal structure of data and outperforms hand-engineered techniques, the main issue is the large number of learnable parameters, whose learning requires a huge volume of data. To overcome this issue, P-1DCNN works on the concept of refinement approach and it involves 61% fewer parameters compared to standard CNN models and as such it has better generalization. Further to overcome the limitations of the small amount of data, we propose two augmentation schemes. We tested the system on the University of Bonn dataset, a benchmark dataset; in almost all the cases concerning epilepsy detection, it gives an accuracy of 99.1 ± 0.9% and outperforms the state-of-the-art systems. In addition, while enjoying the strength of a CNN model, P-1D-CNN model requires 61% less memory space and its detection time is very short (< 0.000481 s), as such it is suitable for real-time clinical setting. It will ease the burden of neurologists and will assist the patients in alerting them before the seizure occurs. The proposed P-1DCNN model is not only suitable for epilepsy detection, but it can be adopted in developing robust expert systems for other similar disorders.", 
"The existing garbage disposal system in India consists of unclassified waste collected from homes. Biodegradable waste is used to generate power, enrich soil and act as food to animals. This process does not harm the earth making it valuable, ecologically safe and helps us to protect our environment.",FRIENDLY WASTE SEGREGATION USING DEEP LEARNING,"Recent enforcement of law by the Indian government for the welfare of sanitation workers has raised the need for an automated system in waste management. The existing garbage disposal system in India consists of unclassified waste collected from homes which are then segregated at a station manually. This segregation of solid waste done by manual labor can bring about many health hazards for the waste sorters in addition to being less efficient, time consuming and not completely feasible due to their large amount. In our paper, we have proposed an automated recognition system using Deep learning algorithm in Artificial Intelligence to classify objects as biodegradable and non-biodegradable, where the system once trained with an initial dataset, can identify objects real-time and classify them almost accurately. Biodegradable waste is used to generate power, enrich soil and act as food to animals. This process does not harm the earth making it valuable, ecologically safe and helps us to protect our environment, rich ecosystem and human inhabitants in future.", 
"Reusing the previously generated plans instead of generating new plans for new queries is an efficient technique for query processing. We've introduced a multi-objective automatic query plan recommendation method, a combination of incremental DBSCAN and NSGA-II. By comparing different types of query workloads we've found that the introduced method outperforms the other well-known approaches.",An automatic clustering technique for query plan recommendation,"The query optimizer is responsible for identifying the most efficient Query Execution Plans (QEP’s). The distributed database relations may be kept in several places. These results in a dramatic increase in the number of alternative query’ plans. The query optimizer cannot exhaustively explore the alternative query plans in a vast search space at reasonable computational costs. Henceforth, reusing the previously generated plans instead of generating new plans for new queries is an efficient technique for query processing. To improve the accuracy of clustering, we’ve rewritten the queries to standardize their structures. Furthermore, TF representation schema has been used to convert the queries into vectors. In this paper, we’ve introduced a multi-objective automatic query plan recommendation method, a combination of incremental DBSCAN and NSGA-II. The quality of the results of incremental DBSCAN has been influenced by Minpts (minimum points) and Eps (epsilon). Two cluster validity indices, Dunn index and Davies–Bouldin index, have simultaneously been optimized to calculate the goodness of an answer. Comparative results have been shown against the incremental DBSCAN and K-means regarding an external cluster validity index, namely, the ARI. By comparing different types of query workloads, we’ve found that the introduced method outperforms the other well-known approaches.", 
Manual summarization requires much human effort and time. For this reason automatic text summarization is introduced which saves the legal expert time. The summarization task involves the identification of rhetorical roles presenting the sentences of a legal judgement document.,An Automatic Legal Document Summarization and Search Using Hybrid System,In this paper we propose a hybrid system for automatic text summarization and automatic search task related to legal documents in the legal domain. Manual summarization requires much human effort and time. For this reason automatic text summarization is introduced which saves the legal expert time. The summarization task involves the identification of rhetorical roles presenting the sentences of a legal judgement document. The search task involves the identification of related past cases as per the given legal query. For these two tasks we have introduced hybrid system which is the combination of different techniques. The techniques involved in our hybrid system are keyword or key phrase matching technique and case based technique. We have implemented and tested and required results are produced., 
Developers need to know more about characteristics and types of residual vulnerabilities in systems to adopt suitable countermeasures in current and next versions. We propose an automatic vulnerability classification framework based on conditions that activate vulnerabilities. We evaluate the effectiveness of the classification by analysing 580 software security defects of the Firefox project.,An Automatic Software Vulnerability Classification,"Security defects are common in large software systems because of their size and complexity. Although efficient development processes, testing, and maintenance policies are applied to software systems, there are still a large number of vulnerabilities that can remain, despite these measures. Developers need to know more about characteristics and types of residual vulnerabilities in systems to adopt suitable countermeasures in current and next versions. We propose an automatic vulnerability classification framework based on conditions that activate vulnerabilities with the goal of helping developers to design appropriate corrective actions (the most costly part of the development and maintenance phases). Different machine learning techniques (Random Forest, C4.5 Decision Tree, Logistic Regression, and Naive Bayes) are employed to construct a classifier with the highest F-measure in labelling an unseen vulnerability by the framework. We evaluate the effectiveness of the classification by analysing 580 software security defects of the Firefox project. The achieved results show that C4.5 Decision Tree is able to identify the category of unseen vulnerabilities with 69% F-measure.", 
This paper proposes an automated technique for single document summarization which combines content-based and graph-based approaches. We introduce the Hopfield Network algorithm as a technique for ranking text segments.,An Automatic Text Summarization Approach using Content-Based and Graph-Based Characteristics,"The continuing growth of World Wide Web and on-line text collections makes a large volume of information available to users. Automatic text summarization allows users to quickly understand documents. In this paper, we propose an automated technique for single document summarization which combines content-based and graph-based approaches and introduce the Hopfield Network algorithm as a technique for ranking text segments. A series of experiments are performed using the DUC collection and a Thai-document collection. The results show the superiority of the proposed technique over reference systems, in addition the Hopfield Network algorithm on undirected graph is shown to be the best text segment ranking algorithm in the study.", 
"This paper proposes a better approach for text summarization using lexical chaining and correlation of sentences. Lexical chains are created using Wordnet . The score of each Lexical chain is calculated based on keyword strength, Tf-idf & other features.",An automatic text summarization using lexical cohesion and correlation of sentences,"Due to substantial increase in the amount of information on the Internet, it has become extremely difficult to search for relevant documents needed by the users. To solve this problem, Text summarization is used which produces the summary of documents such that the summary contains important content of the document. This paper proposes a better approach for text summarization using lexical chaining and correlation of sentences. Lexical chains are created using Wordnet . The score of each Lexical chain is calculated based on keyword strength, Tf-idf & other features. The concept of using lexical chains helps to analyze the document semantically and the concept of correlation of sentences helps to consider the relation of sentence with preceding or succeeding sentence. This improves the quality of summary generated.", 
Machine text summarization is of necessary with the existing of the enormous amount of popular articles. This work evaluates the latent semantic analysis technique to summarize popular articles.,An automatic text summarization using text features and singular value decomposition for popular articles in Indonesia language,"The machine text summarization is of necessary with the existing of the enormous amount of popular articles. This work evaluates the latent semantic analysis technique to summarize popular articles in Indonesia language. The summarization performance are evaluated with respect to precision, recall, and F-measure. As results, the performance seems to be reasonably high particularly when the summarization level is 50%.",  
Automatic text summarization allows users to quickly understand documents. We propose an automated technique for single document summary extraction in Thai language. We introduce the Topic Sensitive PageRank algorithm for ranking text segments.,An Automatic Thai Text Summarization Using Topic Sensitive PageRank,"The continuing growth of World Wide Web and on-line text collections makes a large volume of information available to users. Automatic text summarization allows users to quickly understand documents. In this paper, we propose an automated technique for single document summary extraction in Thai language which combines content-based and graph-based features and introduce the Topic Sensitive PageRank algorithm as a technique for ranking text segments. A series of experiments are performed using a Thai document collection. The results show the superiority of the proposed technique over reference systems.", 
An EEG signal classification method based on sparse auto-encoders and support vector machine (SVM) is proposed to greatly reduce the sample rate and enhance the efficiency of the vision detection. The classification rates in this work outperform the current state-of-the-art EEG seizure detection methods.,An EEG Signal Classification Method based on Sparse Auto-Encoders and Support Vector Machine,"EEG signals, recording abnormal discharge of neurons in the brain, are widely used in epilepsy detection. In this paper, an EEG signal classification method based on sparse auto-encoders (SAE) and support vector machine (SVM) is proposed to greatly reduce the sample rate and enhance the efficiency of the vision detection. In practical application, sparse auto-encoder can get all the significant information at lower sample rate than sampled by Nyquist sampling principle. Due to this, it is widely used to extract higher layer features automatically. With the latter, it is used to obtain the high-dimensional pattern information of EEG signals, and map the input mode space into corresponding sparse space. This approach is precise enough to each sampling point rather than the conventional time window in the current researches and also has a better classification speed in comparison to other conventional methods. In order to ensure good classification rates (100%) for the EEG database, SVM is used to construct the generalized optimal classification hyper plane. Experimental result demonstrate that the classification rates in this work outperform the current state-of-the-art EEG seizure detection methods.", 
"Proposed approach is query based news article summarization. Results from web based on user query are filtered and refined and then result is directed to user. Keywords are the index terms that contain the most important information. The summarization technique identifies different features like thematic terms, named entity, title terms, numbers etc.",An Effective Approach for News Article Summarization,"Information on the World Wide Web and in other electronic form is increasing tremendously. Therefore there is a need for some form of information compression which can be achieved by various mining tasks like classification, clustering and summarization that help in understanding the information. Large amount of web content is news. News websites are daily overwhelmed with plenty of news articles. This paper presents an effective approach for single document news article summarization to help people obtain the most important information in the shortest time. The proposed approach is query based news article summarization. The results from web based on user query are filtered and refined and then result is directed to user. The technique used for summarization is keyword based extractive summarization. Keywords are the index terms that contain the most important information. The summarization technique identifies different features like thematic terms, named entity, title terms, numbers etc., that are relevant to news articles to construct keyword table. This knowledge base is then used to score sentences and then top ranked sentences are presented as summary to the user. For evaluation of summary generated, extrinsic technique by question answering system is used. The purpose of using this evaluation technique is to test if the summary can be used instead of original document while preserving the overall importance of the document i.e. can summary covers all the important information of the document.", 
Proposed method combines two consecutive sentences into a bi-gram pseudo sentence. contextual information is applied to statistical sentence-extraction techniques. The proposed method showed better performance than other sentence- Extraction methods in both single- and multi-document summarization.,An effective sentence-extraction technique using contextual information and statistical approaches for text summarization,"This paper proposes an effective method to extract salient sentences using contextual information and statistical approaches for text summarization. The proposed method combines two consecutive sentences into a bi-gram pseudo sentence so that contextual information is applied to statistical sentence-extraction techniques. Salient bi-gram pseudo sentences are first selected by the statistical sentence-extraction techniques, and then each selected bi-gram pseudo sentence is separated into two single sentences. The second sentence-extraction task for the separated single sentences is performed to make a final text summary. Because the proposed method uses the contextual information with the bi-gram pseudo sentences and combines the statistical sentence-extraction techniques effectively, it can achieve high performance. As a result, the proposed method showed better performance than other sentence-extraction methods in both single- and multi-document summarization.", 
"This study proposes an efficient neural network with convolutional layers to classify significantly class-imbalanced clinical data. The data is curated from the National Health and Nutritional Examination Survey (NHANES) The goal is to predict the occurrence of Coronary Heart Disease (CHD) Despite a high class imbalance in the NHANES dataset, the investigation confirms that our proposed CNN architecture has the classification power of 77% to correctly classify the presence of CHD.",An Efficient Convolutional Neural Network for Coronary Heart Disease Prediction,"This study proposes an efficient neural network with convolutional layers to classify significantly class-imbalanced clinical data. The data is curated from the National Health and Nutritional Examination Survey (NHANES) with the goal of predicting the occurrence of Coronary Heart Disease (CHD). While the majority of the existing machine learning models that have been used on this class of data are vulnerable to class imbalance even after the adjustment of class-specific weights, our simple two-layer CNN exhibits resilience to the imbalance with fair harmony in class-specific performance. Given a highly imbalanced dataset, it is often challenging to simultaneously achieve a high class 1 (true CHD prediction rate) accuracy along with a high class 0 accuracy, as the test data size increases. We adopt a two-step approach: first, we employ least absolute shrinkage and selection operator (LASSO) based feature weight assessment followed by majority-voting based identification of important features. Next, the important features are homogenized by using a fully connected layer, a crucial step before passing the output of the layer to successive convolutional stages. We also propose a training routine per epoch, akin to a simulated annealing process, to boost the classification accuracy. Despite a high class imbalance in the NHANES dataset, the investigation confirms that our proposed CNN architecture has the classification power of 77% to correctly classify the presence of CHD and 81.8% to accurately classify the absence of CHD cases on a testing data, which is 85.70% of the total dataset. This result signifies that the proposed architecture can be generalized to other studies in healthcare with a similar order of features and imbalances. While the recall values obtained from other machine learning methods, such as SVM and random forest, are comparable to that of our proposed CNN model, our model predicts the negative (Non-CHD) cases with higher accuracy. Our model architecture exhibits a way forward to develop better investigative tools, improved medical treatment and lower diagnostic costs by incorporating a smart diagnostic system in the healthcare system. The balanced accuracy of our model (79.5%) is also better than individual accuracies of SVM or random forest classifiers. The CNN classifier results in high specificity and test accuracy along with high values of recall and area under the curve (AUC).", 
"Conventional forward error correction (FEC) codes can not provide two lighting related features, dimming support and flicker mitigation. The proposed scheme can have a twice higher transmission efficiency than the existing schemes. At a dimming ratio of 25% or 75%, the coding gain of the proposed scheme is about 4.6dB and 1.4dB higher than that of the Reed-Solomon (RS) codes-based scheme.",An Efficient Flicker-free FEC Coding Scheme for Dimmable Visible Light Communication Based on Polar Codes,"Visible light communication (VLC) can provide short-range optical wireless communication together with illumination using LED lightings. Since conventional forward error correction (FEC) codes can not provide two lighting related features, dimming support and flicker mitigation, the existing coding schemes for reliable VLC usually rely on auxiliary coding techniques, which cause a complicated structure and a low transmission efficiency. In this paper, based on polar codes, an efficient and flicker-free FEC coding scheme for dimmable VLC is proposed to increase the transmission efficiency and simplify the coding structure. Taking advantages of polar codes’ recursive encoding structure, the proposed scheme can guarantee the equal probability and the short runs of 1s and 0s for arbitrary code rate without extra coding components. Numerical results show that the proposed scheme can have a twice higher transmission efficiency than the existing schemes. Furthermore, at a dimming ratio of 25% or 75%, the coding gain of the proposed scheme is about 4.6 dB and 1.4 dB higher than that of the Reed-Solomon (RS) codes-based scheme and the LDPC codes-based scheme, respectively.", 
Service discovery is known as one of the fundamental components in vehicular networks. A proper framework has been presented in this paper for the optimal service discovery in an urban area. The query success rate and overheads of updating service information improved by benefits of P2P network and infrastructure nodes.,An efficient infrastructure based service discovery in vehicular networks using P2P structures,"Service discovery is known as one of the fundamental components in vehicular networks. Intelligent transportation systems performance improves by the availability of efficient service discovery solutions in vehicular networks. Nonetheless, some challenges such as network partitioning and vehicular congestion reduce the efficiency of these components. Additionally, most of the presented solutions are suffering from the low query success rate and high overhead of service information updates. A proper framework has been presented in this paper for the optimal service discovery in an urban area. Considering related framework, a binary integer programming model has been proposed for optimal infrastructure placement in urban areas and a peer-to-peer (P2P) network which is created on infrastructure nodes. The query success rate and overheads of updating service information improved by benefits of P2P network and infrastructure nodes. In framework evaluation, an Infrastructure-based location service discovery mechanism has been presented in urban areas. The simulation results showed an improvement in performance of the given solution in comparison with the most recent other solutions.", 
"A patient monitoring scheme intended for heart patients utilizing IoT centered Deep Learning Modified Neural Network (DLMNN) is proposed to assist in the HD diagnosis, and medication is given accordingly. This proposed technique is executed via '3' steps: I)Authentication, ii) Encryption, and ii) Classification. The classified outcomes comprise '2'types of data: i) normal and ii) abnormal. It denotes the patient's heart condition and if the outcome is abnormal, an alert text is passed to the physician.",An Efficient IoT-Based Patient Monitoring and Heart Disease Prediction System Using Deep Learning Modified Neural Network,"The leading causes of death worldwide are chronic illnesses suchlike diabetes, Heart Disease (HD), cancer as well as chronic respiratory malady. It is remarkably intricate to diagnose HD with disparate symptoms or features. With the augmentation in popularity of smart wearable gadgets, a chance to render an Internet of Things (IoT) solution has turned out to be more. Unfortunately, the survival rates are low for the people suffering from sudden heart attacks. Consequently, a patient monitoring scheme intended for heart patients utilizing IoT centered Deep Learning Modified Neural Network (DLMNN) is proposed to assist in the HD diagnosis, and medication is given accordingly. This proposed technique is executed via ‘3’ steps: I) Authentication, ii) Encryption, and iii) Classification. First, by utilizing the substitution cipher (SC) together with the SHA-512, the heart patient of the specific hospital is authenticated. Subsequently, the wearable IoT sensor device, which is fixed to the patient’s body, concurrently transmits the sensor data to the cloud. This sensor data is encrypted and securely transmitted to the cloud utilizing the PDH-AES technique. After that, the encrypted data is finally decrypted, and by employing the DLMNN classifier, the classification is done. The classified outcomes comprise ‘2’types of data: i) normal and ii) abnormal. It denotes the patient’s heart condition and if the outcome is abnormal, an alert text is passed to the physician for treating the patient. The investigational outcomes are estimated and the DLMNN for HD diagnosis shows improvement as compared to existing algorithms. Additionally, the proposed PDH-AES used in support of secure data transmission results in the highest level of security i.e. 95.87%, and it is achieved in the lowest time for encryption along with decryption when weighted against the existent AES.", 
"High frequency components of EEG signal can provide accommodating data for enhancing the classification performance of the mental task-based BCI. Instead of using autoregressive (AR) parameters considering AR modeling of EEG data, reflection coefficients obtained from EEG signal are proposed as potential features. It is found that the proposed scheme can classify mental tasks with a very high level of accuracy as well as low time complexity.",An efficient scheme for mental task classification utilizing reflection coefficients obtained from autocorrelation function of EEG signal,"Classification of different mental tasks using electroencephalogram (EEG) signal plays an imperative part in various brain– computer interface (BCI) applications. In the design of BCI systems, features extracted from lower frequency bands of scalp-recorded EEG signals are generally considered to classify mental tasks and higher frequency bands are mostly ignored as noise. However, in this paper, it is demonstrated that high frequency components of EEG signal can provide accommodating data for enhancing the classification performance of the mental task-based BCI. Instead of using autoregressive (AR) parameters considering AR modeling of EEG data, reflection coefficients obtained from EEG signal are proposed as potential features. From a given frame of EEG data, reflection coefficients are directly extracted by using the autocorrelation values in a recursive fashion, which avoids matrix inversion and computation of AR parameters. Use of reflection coefficients not only provides an effective feature vector for EEG signal classification but also offers very low computational burden. Support vector machine classifier is deployed in leave-one-out cross-validation manner to carry out classification process. Extensive simulation is done on an openly accessible dataset containing five different mental tasks. It is found that the proposed scheme can classify mental tasks with a very high level of accuracy as well as low time complexity in contrast with some of the existing strategies.", 
"An automatic, generic, and extractive Arabic single document summarizing method aims at producing a sufficiently informative summary. The proposed extractive method evaluates each sentence based on a combination of statistical and semantic features. Two summarizing techniques including score-based and supervised machine learning were employed to produce the summary.",An efficient single document Arabic text summarization using a combination of statistical and semantic features,"The exponential growth of online textual data triggered the crucial need for an effective and powerful tool that automatically provides the desired content in a summarized form while preserving core information. In this paper, we propose an automatic, generic, and extractive Arabic single document summarizing method aiming at producing a sufficiently informative summary. The proposed extractive method evaluates each sentence based on a combination of statistical and semantic features in which a novel formulation is used taking into account sentence importance, coverage and diversity. Further, two summarizing techniques including score-based and supervised machine learning were employed to produce the summary and then assist leveraging the designed features. We demonstrate the effectiveness of the proposed method through a set of experiments under EASC corpus using ROUGE measure. Compared to some existing related work, the experimental evaluation shows the strength of the proposed method in terms of precision, recall, and F-score performance metrics.", 
This paper proposes an automatic method to generate an extractive summary of Vietnamese documents related to a common topic. It models text documents as weighted undirected graphs. Sentences are ranked according to their salient scores and selected based on Maximal marginal relevance.,An Efficient Vietnamese Text Summarization Approach Based on Graph Model,"This paper proposes an automatic method to generate an extractive summary of multiple Vietnamese documents which are related to a common topic by modeling text documents as weighted undirected graphs. It initially builds undirected graphs with vertices representing the sentences of documents and edges indicate the similarity between sentences. Then, by adopting PageRank algorithm, we can generate salient scores for sentences. Sentences are ranked according to their salient scores and selected based on Maximal marginal relevance to form the summaries. These summaries are combined and applied the same process one more time to form the final extractive summary of the document set. A series of experiments are performed on Vietnamese news articles. The results demonstrate the effectiveness of the proposed technique over reference systems.", 
"Electroencephalography (EEG) is the process of recording brain signals that generate due to a small amount of electric discharge in brain. In every minute, analysis of EEG signal can solve much neurological disorders like epilepsy.",An Empirical Analysis of Training Algorithms of Neural Networks-A Case Study of EEG Signal Classification Using Java Framework,"With the pace of modern lifestyle, about 40–50 million people in the world suffer from epilepsy—a disease with neurological disorder. Electroencephalography (EEG) is the process of recording brain signals that generate due to a small amount of electric discharge in brain. This may occur due to the information flow among several neurons. Therefore, in every minute, analysis of EEG signal can solve much neurological disorders like epilepsy. In this paper, a systematic procedure for analysis and classification of EEG signal is discussed for identification of epilepsy in a human brain. The analysis of EEG signal is made through a series of steps from feature extraction to classification. Feature extraction from EEG signal is done through discrete wavelet transform (DWT), and the classification task is carried out by MLPNN based on supervised training algorithms such as backpropagation, resilient propagation (RPROP), and Manhattan update rule. Experimental study in a Java platform confirms that RPROP trained MLPNN to classify EEG signal is promising as compared to back-propagation or Manhattan update rule trained MLPNN.", 
"TextRank has been used in a wide variety of commercial applications, including text classification, information retrieval and clustering. In these applications, the parameters of TextRank are set roughly, which might affect the effectiveness of returned results. In this work, we conduct an empirical study on TextRank, towards finding optimal parameter settings for keyword extraction.",An Empirical Study of TextRank for Keyword Extraction,"As a typical keyword extraction technology, TextRank has been used in a wide variety of commercial applications, including text classification, information retrieval and clustering. In these applications, the parameters of TextRank, including the co-occurrence window size, iteration number and decay factor, are set roughly, which might affect the effectiveness of returned results. In this work, we conduct an empirical study on TextRank, towards finding optimal parameter settings for keyword extraction. The experiments are done in Hulth2003 and Krapivin2009 datasets, which are two real datasets. We first remove the stop word by an open published English stop word list XPO6. And then, we extract the word stems by Porter Stemmer. Porter Stemmer is a tool which can find the stems of words with multiple variants, discard redundant information, strengthen the filtering effect, and extract the effective features of the text fully. We carry out extensive experiments to evaluate the effects of the parameters to keywords extraction, and evaluate the effectiveness of corresponding results by Precision, Recall and Accuracy. Experimental results show that TextRank shows the best performance when setting co-occurrence window size w D 3, iteration number t D 20, decay factor c D 0:9 and rank k D 10 respectively, and the results are independent of the text length.", 
"A significant amount of text to be analyzed nowadays is Web data, often from social media. How do standard pretrained language models generalize and capture peculiarities of short, informal and frequently automatically generated text found in social media? We focus on bot detection in Twitter as our evaluation task.",An Empirical study on Pre-trained Embeddings and Language Models for Bot Detection,"Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of downstream NLP tasks. Usually, such language models are learned from large and well-formed text corpora from e.g. encyclopedic resources, books or news. However, a significant amount of the text to be analyzed nowadays is Web data, often from social media. In this paper we consider the research question: How do standard pretrained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media? To answer this question, we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pretrained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.", 
"The work aims at automatic identification of various sleep stages like, sleep stages 1, 2, slow wave sleep (sleep stages 3 and 4), REM sleep and wakefulness from single channel EEG signal. The proposed ensemble SVM-based method could be used as an efficient and cost-effective method for sleep staging.",An ensemble system for automatic sleep stage classification using single channel EEG signal,"The present work aims at automatic identification of various sleep stages like, sleep stages 1, 2, slow wave sleep (sleep stages 3 and 4), REM sleep and wakefulness from single channel EEG signal. Automatic scoring of sleep stages was performed with the help of pattern recognition technique which involves feature extraction, selection and finally classification. Total 39 numbers of features from time domain, frequency domain and from non-linear analysis were extracted. After extraction of features, SVM based recursive feature elimination (RFE) technique was used to find the optimum number of feature subset which can provide significant classification performance with reduced number of features for the five different sleep stages. Finally for classification, binary SVMs were combined with one-against-all (OAA) strategy. Careful extraction and selection of optimum feature subset helped to reduce the classification error to 8.9% for training dataset, validated by k-fold cross-validation (CV) technique and 10.61% in the case of independent testing dataset. Agreement of the estimated sleep stages with those obtained by expert scoring for all sleep stages of training dataset was 0.877 and for independent testing dataset it was 0.8572. The proposed ensemble SVM-based method could be used as an efficient and cost-effective method for sleep staging with the advantage of reducing stress and burden imposed on subjects.", 
"Digital data has become an important aspect of machine learning and is present in huge volumes on the internet. Text summarization helps in condensing documents, and extract the important facts represented in it. In this paper, we compare different techniques to identify low and medium frequency words in hotel reviews.",An Evaluation of Word Frequency Techniques for Text Summarization Using Sentiment Analysis Approach,"Digital data has become an important aspect of machine learning and is present in huge volumes on the internet. To use this data efficiently, data handling and processing techniques are required to filter out information from documents and store them. An application of natural language processing, which helps in handling volumes of data, is text summarization. Text summarization helps in condensing documents, and extract the important facts represented in it. There are two techniques in text summarization: abstractive and extractive summarization. Extractive Summarization extracts keywords from the document and combines them to provide a semantically incorrect summary, whereas, Abstractive Summarization produces a semantically correct summary of the text. In this paper, we compare different techniques to identify low and medium frequency words in hotel reviews. We evaluate the techniques based on the correct identification of positive and negative words.",  
"This paper explores the development of the digital forensics process. It compares and contrasts four particular forensic methodologies, and proposes an abstract model. The model attempts to address some of the shortcomings of previous methodologies. It provides a consistent and standardized framework for digital forensic tool development.",An Examination of Digital Forensic Models,"Law enforcement is in a perpetual race with criminals in the application of digital technologies, and requires the development of tools to systematically search digital devices for pertinent evidence. Another part of this race, and perhaps more crucial, is the development of a methodology in digital forensics that encompasses the forensic analysis of all genres of digital crime scene investigations. This paper explores the development of the digital forensics process, compares and contrasts four particular forensic methodologies, and finally proposes an abstract model of the digital forensic procedure. This model attempts to address some of the shortcomings of previous methodologies, and provides the following advantages: a consistent and standardized framework for digital forensic tool development; a mechanism for applying the framework to future digital technologies; a generalized methodology that judicial members can use to relate technology to non-technical observers; and, the potential for incorporating nondigital electronic technologies within the abstraction.", 
This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of the proposed model.,An exploration of document impact on graph-based multi-document summarization,"The graph-based ranking algorithm has been recently exploited for multi-document summarization by making only use of the sentence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable. However, given a document set to be summarized, different documents are usually not equally important, and moreover, different sentences in a specific document are usually differently important. This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Various methods are employed to evaluate the two factors. Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model. Moreover, the results show the robustness of the proposed model.", 
"In this study, we propose an Arabic text summarization approach based on extractive graph-based approaches. Several basic units such as stem, word, and n-gram are applied in the summarization process. The Arabic document is represented as a graph and we used the shortest path algorithm to extract the summary.",An extractive graph-based Arabic text summarization approach,"In this study, we proposed an Arabic text summarization approach based on extractive graph-based approaches. In order to measure the efficiency of the proposed approach, several basic units such as stem, word, and n-gram are applied in the summarization process. The Arabic document is represented as a graph and we used the shortest path algorithm to extract the summary. Similarity between any two sentences is determined by ranking the sentences according to some statistical features. The final score is determined for each sentence using PageRank scoring, and finally, the sentences with high scores are included in the summary considering the compression ratio. The proposed approach is evaluated using EASC corpus, and intrinsic evaluation method. The results showed that the proposed approach achieved good results. Also, the results showed that the use of n-grams as a basic unit in summarization process achieved better results than the stem and word.", 
Big Data analytics helps enterprises and institutions to understand and identify the usability of large amount of data generated by their routine operations. Many types of actionable insights can be found from these type of semi-structured text data. In this paper we are proposing a recommendation system model for understanding and finding actionable insight.,An extractive text summarization approach for analyzing educational institution's review and feedback data,Big Data analytics helps the enterprises and institutions to understand and identify the usability of large amount of data generated by their routine operations. All most third forth part of these types of data is semi-structured text data. Many types of actionable insights can be found from these type of semi-structured text data that can help strategic management for making right decision. In this paper we are proposing a recommendation system model for understanding and finding actionable insight from the large amount of text data generated for an educational institution. Here we are discussing different type of data generation sources of this types of data as well as data cleaning processes required. The wordcloud for an educational institution is published that help strategic management for understanding the sentiment of different stack holders mainly students. Herewith we are identifying different types of findings from these sets of words that helps for betterment in the functionaries of an Educational Institution., 
"Automatic text summarization is an exertion of contriving the abridged form of a text document covering salient knowledge. For under-resourced languages such as Hindi, automatic text summarizing is still an unsolved problem. In this paper, we propose an extractive lexical knowledge-rich topic modeling text summarization approach for Hindi novels and stories.",An extractive text summarization approach using tagged-LDA based topic modeling,"Automatic text summarization is an exertion of contriving the abridged form of a text document covering salient knowledge. Numerous statistical, linguistic, rule-based, and position-based text summarization approaches have been explored for different rich-resourced languages. For under-resourced languages such as Hindi, automatic text summarization is a challenging task and still an unsolved problem. Another issue with such languages is the unavailability of corpus and the inadequacy of the processing tools. In this paper, we proposed an extractive lexical knowledge-rich topic modeling text summarization approach for Hindi novels and stories in which we implemented four independent variants using different sentence weighting schemes. We prepared a corpus of Hindi Novels and stories since the absence of a corpus. We used a smoothing technique for edifying and variety summaries followed by evaluating the efficacy of generated summaries against three metrics (gist diversity, retention ratio, and ROUGE score). The results manifest that the proposed model produces abridge, articulate and coherent summaries. To investigate the performance of the proposed model, we simulate the experiments on the English dataset as well. Further, we compare our models with the baselines and traditional topic modeling approach, where we show that the proposed model has confessed optimal results.", 
"This paper presents a method for text summarization which extracts important sentences from a single or multiple Bengali documents. The input document(s) should be pre-processed by tokenization, stemming operation and word score. K-means clustering algorithm has been applied to produce the final summary.",An extractive text summarization technique for Bengali document(s) using K-means clustering algorithm,"Text summarization, a field of data mining, is very important for developing various real-life applications. Many techniques have been developed for summarizing English text(s). But, a few attempts have been made for Bengali text because of its some multifaceted structure. This paper presents a method for text summarization which extracts important sentences from a single or multiple Bengali documents. The input document(s) should be pre-processed by tokenization, stemming operation etc. Then, word score is calculated by Term- Frequency/Inverse Document Frequency (TF/IDF) and sentence score is determined by summing up its constituent words’ scores with its position. Cue and skeleton words have also been considered to calculate the sentence score. For single or multiple documents, K-means clustering algorithm has been applied to produce the final summary. The experimental result shows satisfactory outputs in comparison to the existing approaches possessing linear run time complexity.", 
"Document summarization can be viewed as a reductive distilling of source text through content condensation. Words with high quantities of information are believed to carry more content and thereby importance. In a query-based summarization setting, the correlation between user queries and sentences to be scored is established.",An Extractive Text Summarizer Based on Significant Words,"Document summarization can be viewed as a reductive distilling of source text through content condensation, while words with high quantities of information are believed to carry more content and thereby importance. In this paper, we propose a new quantification measure for word significance used in natural language processing (NLP) tasks, and successfully apply it to an extractive text summarization approach. In a query-based summarization setting, the correlation between user queries and sentences to be scored is established from both the micro (i.e. at the word level) and the macro (i.e. at the sentence level) perspectives, resulting in an effective ranking formula. The experiments, both on a generic single document summarization evaluation, and on a query-based multi-document evaluation, verify the effectiveness of the proposed measures and show that the proposed approach achieves a state-of-the-art performance.", 
The aim of this study is to present an idea to combine Sequential Pattern Mining (SPM) and Deep Learning (DL) for better text summarization process and result. The findings of the study are presented as a logical design and mapping of current text representation that can be implemented.,An idea based on sequential pattern mining and deep learning for text summarization,"One of the Natural Language Processing (NLP) studies that has been widely researched is automatic text summarization. There are a lot of techniques and methods that are proposed for text summarization. However, not much attention has been given on the coherence and cohesion in text. The aim of this study is to present an idea to combine Sequential Pattern Mining (SPM) and Deep Learning (DL) for better text summarization process and result. In text summarization, it is important to produce understable and readable summary, and SPM as text representation extracting algorithm is capable to maintain the meaning of text by giving attention of the order of words appearance. Whereas DL is a popular and powerful machine learning technique widely used recently in various data mining studies. This study uses descriptive research methodology that collects all of the facts and information which are related to SPM and DL for text summarization, where NLP as the body of knowledge, SPM and DL as the method, and text summarization as the domain problem that need to be solved. The findings of the study are presented as a logical design and mapping of current text representation that can be implemented to further improve automatic text summarization results, in particular, to improve its coherence and cohesion.", 
"In this paper, we introduce how traditional software engineering can leverage immersive approaches for building, delivering and maintaining next-generation software applications. We present an augmented-reality based prototype for project managers, which provides contextual and immersive insights. We discuss important research questions that we are investigating further as part of our immersive software engineering research.",An Immersive Future for Sofware Engineering - Avenues and Approaches,"Software systems are increasingly becoming more intricate and complex, necessitating new ways to be able to comprehend and visualize them. At the same time, the nature of software engineering teams itself is changing with people playing more ?uid roles often needing seamless and contextual intelligence, for faster and better decisions. Moreover, the next-generation of software engineers will all be post-millennials, which may have totally di?erent expectations from their software engineering workplace. Thus, we believe that it is important to have a re-look at the way we traditionally do software engineering and immersive technologies have a huge potential here to help out with such challenges. However, while immersive technologies, devices and platforms, have matured in past few years, there has been very little research on studying how these technologies can in?uence software engineering. In this paper, we introduce how traditional software engineering can leverage immersive approaches for building, delivering and maintaining next-generation software applications. As part of our initial research, we present an augmented-reality based prototype for project managers, which provides contextual and immersive insights. Finally, we also discuss important research questions that we are investigating further as part of our immersive software engineering research.", 
"Brain signals were started to use in deception detection process from last few years. Electroencephalogram (EEG) signals can reveal many important features of our thought which make it as a better tool for deception detection. In this paper, we proposed a deep learning based classification of EEG signals for the given visual stimuli.",An Improved Approach for EEG Signal Classification using Autoencoder,"Brain signals were started to use in deception detection process from last few years. Electroencephalogram (EEG) signals can reveal many important features of our thought which make it as a better tool for deception detection. A number of experiments were done in terms of visual stimuli based EEG signals. The purpose of this paper is to improvise the existing methods in the classification of familiar and unfamiliar faces which can be used as a basic model in deception detection. In this paper, we proposed a deep learning based classification of EEG signals for the given visual stimuli. In this experiment, the subjects were shown by familiar and unfamiliar faces. After processing using Independent Component Analysis (ICA), the signal was fed to an autoencoder for classification. By training the model properly we got a mean accuracy of 82.21% which is far better than the models using conventional machine learning methods. Our model achieved the state of the art results for classification of familiar and unfamiliar EEG signals.", 
An approach for improving the accuracy of EEG signal classification is presented to detect epileptic seizures. The Artificial Neural Network (ANN) is trained by incorporating Levenberg-Marquardt(LM) training algorithm into the backpropagation algorithm that results in high classification accuracy. Experimental results reveal that the methodology will improve the clinical service of the EEG recording and also provide better decision making.,An Improved EEG Signal Classification Using Neural Network with the Consequence of ICA and STFT,"Signals of the Electroencephalogram (EEG) can reflect the electrical background activity of the brain generated by the cerebral cortex nerve cells. This has been the mostly utilized signal, which helps in effective analysis of brain functions by supervised learning methods. In this paper, an approach for improving the accuracy of EEG signal classification is presented to detect epileptic seizures. Moreover, Independent Component Analysis (ICA) is incorporated as a preprocessing step and Short Time Fourier Transform (STFT) is used for denoising the signal adequately. Feature extraction of EEG signals is accomplished on the basis of three parameters namely, Standard Deviation, Correlation Dimension and Lyapunov Exponents. The Artificial Neural Network (ANN) is trained by incorporating Levenberg-Marquardt(LM) training algorithm into the backpropagation algorithm that results in high classification accuracy. Experimental results reveal that the methodology will improve the clinical service of the EEG recording and also provide better decision making in epileptic seizure detection than the existing techniques. The proposed EEG signal classification using feed forward Backpropagation Neural Network performs better than to the EEG signal classification using Adaptive Neuro Fuzzy Inference System (ANFIS) classifier in terms of accuracy, sensitivity, and specificity.", 
This paper proposes an enhanced mixed feature-base and cluster-base approaches to produce a high qualified single-document summary. We used the Jaccard similarity measure to adjust the sentence clustering process instead of using the Normalized Google Distance (NGD) similarity measure. Both algorithms outperformed the standard baseline Microsoft Word Summarizer and Copernic methods.,An Improved Evolutionary Algorithm for Extractive Text Summarization,"The main challenge of extractive-base text summarization is in selecting the top representative sentences from the input document. Several techniques were proposed to enhance the process of selection such as feature-base, cluster-base, and graph-base methods. Basically, this paper proposed to enhance a previous work, and provides some limitations in the similarity calculation of that previous work. This paper proposes an enhanced mixed feature-base and cluster-base approaches to produce a high qualified single-document summary. We used the Jaccard similarity measure to adjust the sentence clustering process instead of using the Normalized Google Distance (NGD) similarity measure. In addition, this paper proposes a new real-to-integer values modulator instead of using the genetic mutation operator which was adopted in the previous work. The Differential Evolution (DE) algorithm is used for train and test the proposed methods. The DUC2002 dataset was preprocessed and used as a test bed. The results show that our proposed differential mutant presented a satisfied performance while the Genetic mutant proved to be the better. In addition, our analysis of NGD similarity scores showed that NGD was an inappropriate selection in the previous study as it performs successfully in a very big database such as Google. Our selection of Jaccard measure was fortunate and obtained superior results surpassed the NGD using the new proposed modulator and the genetic operator. In addition, both algorithms outperformed the standard baseline Microsoft Word Summarizer and Copernic methods.", 
An improved Faster R-CNN is proposed to retrieve the same object in different scenes with few training samples. The proposed SOR Faster RCNN is applied to our Coke cans dataset and three public image datasets. It has better identification performance than fine-tuned Faster R -CNN. It achieves much higher accuracy for detecting low-resolution images.,An Improved Faster R-CNN for Same Object Retrieval,"An improved Faster R-CNN (SOR Faster R-CNN) is proposed to retrieve the same object in different scenes with few training samples. By concatenating the feature maps of shallow and deep convolutional layers, the ability of RoI pooling to extract more detailed features is improved. In the training process, a pretrained CNN model is fine-tuned using a query image dataset so that the confidence score can identify an object proposal to the object level rather than the classification level. In the query process, we first select the ten images for which the object proposals have the closest confidence scores to the query object proposal. Then, the image for which the detected object proposal has the minimum cosine distance to the query object proposal is considered as the query result. The proposed SOR Faster RCNN is applied to our Coke cans dataset and three public image datasets, i.e., Oxford Buildings 5k, Paris Buildings 6k, and INS 13. The experimental results confirm that SOR Faster R-CNN has better identification performance than fine-tuned Faster R-CNN. Moreover, SOR Faster R-CNN achieves much higher accuracy for detecting low-resolution images than the fine-tuned Faster R-CNN on the Coke cans (0.094 mAP higher), Oxford Buildings (0.043 mAP higher), Paris Buildings (0.078 mAP higher) and INS 13 (0.013 mAP higher) datasets.", 
The process of text summarization is to identify the crux of the document. The present method is domain and language independent. It provides good harmonic mean and does not require any training or testing.,An improved key term weightage algorithm for text summarization using local context information and fuzzy graph sentence score,"The process of text summarization is to identify the crux of the document. In the proposed work, summarization is done using three different algorithms. They are sentence based key term weightage, the two way local context information scoring (LCIS) and the fuzzy graph sentence scoring (FGSS). They are used to improve the weight of the key terms, identify LCIS and the centroid of the document by calculating FGSS score respectively. This intelligent system assigning sentence based weightage to the key terms is found to be effective. The present method is domain and language independent. It provides good harmonic mean in comparison with the earlier studies and does not require any training or testing.", 
Proposal is to achieve the most reliable and substantial context or most relevant brief summary of the text. The extractive text summarization produces the short summary of a certain text which contains the most important information of original text.,An improved method of automatic text summarization for web contents using lexical chain with semantic-related terms,"Many researches have been converging on automatic text summarization as increasing of text documents due to the expansion of information diffusion constantly. The objective of this proposal is to achieve the most reliable and substantial context or most relevant brief summary of the text in extractive manner. The extractive text summarization produces the short summary of a certain text which contains the most important information of original text by extracting the set of sentences from the original document. This paper proposes an improved extractive text summarization method for documents by enhancing the conventional lexical chain method to produce better relevant information of the text using three distinct features or characteristics of keyword in a text. The keyword of the document is labeled using our previous work, transition probability distribution generator model which can learn the characteristics of the keyword in a document, and generates their probability distribution upon each feature.", 
"Ensemble learning is one of the principal current directions in the research of machine learning. In this paper, subspace ensembles for classification are explored which constitute an ensemble classifier system. The robustness and effectiveness of the proposed method is shown.",An Improved Random Subspace Method and Its Application to EEG Signal Classification,"Ensemble learning is one of the principal current directions in the research of machine learning. In this paper, subspace ensembles for classification are explored which constitute an ensemble classifier system by manipulating different feature subspaces. Starting with the nature of ensemble efficacy, we probe into the microcosmic meaning of ensemble diversity, and propose to use region partitioning and region weighting to implement effective subspace ensembles. An improved random subspace method that integrates this mechanism is presented. Individual classifiers possessing eminent performance on a partitioned region reflected by high neighborhood accuracies, are deemed to contribute largely to this region, and are assigned large weights in determining the labels of instances in this area. The robustness and effectiveness of the proposed method is shown empirically with the base classifier of linear support vector machines on the classification problem of EEG signals.", 
"Feature selection is beneficial for reducing the dimensionality of the problem. It leads to minimize the computational time and improve the performance of the categorization task. In this paper, we propose a new improved algorithm of the original Sine Cosine Algorithm (SCA) for feature selection. Unlike the SCA which focuses only on the best solution, our proposal takes into account two positions of the solution.",An improved sine cosine algorithm to select features for text categorization,"Bag of words model is commonly used for text categorization. The main problem of this model lies in the large number of involved features, which influences the categorization task performance. To deal with this problem, feature selection method is necessary. Feature selection is beneficial for reducing the dimensionality of the problem, it leads to minimize the computational time and improve the performance of the categorization task. In this paper, we propose a new improved algorithm of the original Sine Cosine Algorithm (SCA) for feature selection, which allows for better exploration in the search space. Unlike the SCA which focuses only on the best solution to generate a new solution, the new algorithm (ISCA) of our proposal takes into account two positions of the solution. (i), The position of the best solution found so far, and (ii), a given random position from the search space. This combination allows us to propose a simple algorithm which is able to avoid premature convergence and obtain very satisfactory performance. To validate the new ISCA algorithm, we carried out a series of experiments on nine text collection, where, we compared the experimental results with several search algorithms including the original SCA algorithm and some of its improved versions as well as the Moth-Flam Optimizer (MFO) algorithm. Moreover, from the state of the art, the Genetic Algorithm (GA) and the Ant Colony Optimization (ACO) are chosen in our comparative study. Our evaluation results demonstrate the high performance of our proposed ISCA algorithm which makes it very useful for text categorization problem.", 
"Feature selection is beneficial for reducing the dimensionality of the problem. It leads to minimize the computational time and improve the performance of the categorization task. In this paper, we propose a new improved algorithm of the original Sine Cosine Algorithm (SCA) for feature selection. Unlike the SCA which focuses only on the best solution, our proposal takes into account two positions of the solution.",An Improvised Extractive Approach to Hindi Text Summarization,"Text summarization is defined as a task of minimizing a text that is produced from one or more texts such that the actual significant information in the texts is not lost. A text summarization tool compresses the text and displays only the important content to the user. Using text summarization, decisions can be made in lesser time and the core of the document be understood. This paper emphasizes on an extractive approach and its implementation on Java. The extractive approach selects the significant sentences based on a thematic approach. Before selecting the thematic words the Hindi stop-words was removed and also the stemming process to retrieve the root words in the sentences under consideration. Stop-word elimination eliminates the semantically null words from the input document and stemming helps in clustering together words with the same radix term. The system is based on an algorithm for scoring the sentences based on occurrence of the radix of thematic words. The sentences with highest score are added to the summary. The generated summary is further processed based on removal of extraneous phrases from the previously selected summary sentences so as to bring the sentences closer to human generated summary. The testing of the accuracy of the system can be made by using a technique called The Expert Game. In expert game, experts underline and extract the most interesting or informative fragments of the text. The recall and precision of the system’s summary is measured against the human’s extract. Based on the testing, the system is found to be 85 % accurate.", 
The aim is to develop an advanced method to detect the drowsiness stage in electroencephalogram. The methods used are based on Machine Learning methodologies such as stacked autoencoder with softmax layers. Results obtained from 62 volunteers indicate 100% accuracy in drowsy/wakeful discrimination.,An Innovative Deep Learning Algorithm for Drowsiness Detection from EEG Signal,"The development of detection methodologies for reliable drowsiness tracking is a challenging task requiring both appropriate signal inputs and accurate and robust algorithms of analysis. The aim of this research is to develop an advanced method to detect the drowsiness stage in electroencephalogram (EEG), the most reliable physiological measurement, using the promising Machine Learning methodologies. The methods used in this paper are based on Machine Learning methodologies such as stacked autoencoder with softmax layers. Results obtained from 62 volunteers indicate 100% accuracy in drowsy/wakeful discrimination, proving that this approach can be very promising for use in the next generation of medical devices. This methodology can be extended to other uses in everyday life in which the maintaining of the level of vigilance is critical. Future works aim to perform extended validation of the proposed pipeline with a wide-range training set in which we integrate the photoplethysmogram (PPG) signal and visual information with EEG analysis in order to improve the robustness of the overall approach.", 
"Extractive summarization aims to produce a concise version of a document by extracting information-rich sentences from the original texts. The graph-based model is an effective and efficient approach to rank sentences since it is simple and easy to use. However, its performance depends heavily on good text representation. In this paper, an integrated graph model for extractive text summarization is proposed.",An Integrated Graph Model for Document Summarization,"Extractive summarization aims to produce a concise version of a document by extracting information-rich sentences from the original texts. The graph-based model is an effective and efficient approach to rank sentences since it is simple and easy to use. However, its performance depends heavily on good text representation. In this paper, an integrated graph model (iGraph) for extractive text summarization is proposed. An enhanced embedding model is used to detect the inherent semantic properties at the word level, bigram level and trigram level. Words with part-of-speech (POS) tags, bigrams and trigrams were extracted to train the embedding models. Based on the enhanced embedding vectors, the similarity values between the sentences were calculated in three perspectives. The sentences in the document were treated as vertexes and the similarity between them as edges. As a result, three different types of semantic graphs were obtained for every document, with the same nodes and different edges. These three graphs were integrated into one enriched semantic graph in a naive Bayesian fashion. After that, TextRank, which is a graph-based ranking algorithm, was applied to rank the sentences, before the top scored sentences were selected for the summary according to the compression rate. Evaluated on the DUC 2002 and DUC 2004 datasets, our proposed method shows competitive performance compared to the state-of-the-art methods.", 
Heart failure is considered one of the leading cause of death around the world. The diagnosis of heart failure is a challenging task especially in under-developed and developing countries. The proposed diagnostic system uses random search algorithm (RSA) for features selection and random forest model for heart failure prediction. It produces 3.3% higher accuracy than conventional random Forest model while using only 7 features.,An Intelligent Learning System Based on Random Search Algorithm and Optimized Random Forest Model for Improved Heart Disease Detection,"Heart failure is considered one of the leading cause of death around the world. The diagnosis of heart failure is a challenging task especially in under-developed and developing countries where there is a paucity of human experts and equipments. Hence, different researchers have developed different intelligent systems for automated detection of heart failure. However, most of these methods are facing the problem of overfitting i.e. the recently proposed methods improved heart failure detection accuracy on testing data while compromising heart failure detection accuracy on training data. Consequently, the constructed models overfit to the testing data. In order, to come up with an intelligent system that would show good performance on both training and testing data, in this paper we develop a novel diagnostic system. The proposed diagnostic system uses random search algorithm (RSA) for features selection and random forest model for heart failure prediction. The proposed diagnostic system is optimized using grid search algorithm. Two types of experiments are performed to evaluate the precision of the proposed method. In the first experiment, only random forest model is developed while in the second experiment the proposed RSA based random forest model is developed. Experiments are performed using an online heart failure database namely Cleveland dataset. The proposed method is efficient and less complex than conventional random forest model as it produces 3.3% higher accuracy than conventional random forest model while using only 7 features. Moreover, the proposed method shows better performance than five other state of the art machine learning models. In addition, the proposed method achieved classification accuracy of 93.33% while improving the training accuracy as well. Finally, the proposed method shows better performance than eleven recently proposed methods for heart failure detection.", 
"Sleep Apnea is a sleep disorder which causes stop in breathing for a short duration of time. Electroencephalogram (EEG) plays a vital role in detecting the sleep apnea by sensing and recording the brain's activities. This work employs features such as energy, entropy, and variance which are computed for each frequency band.",An Intelligent Sleep Apnea Classification System Based on EEG Signals,"Sleep Apnea is a sleep disorder which causes stop in breathing for a short duration of time that happens to human beings and animals during sleep. Electroencephalogram (EEG) plays a vital role in detecting the sleep apnea by sensing and recording the brain’s activities. The EEG signal dataset is subjected to filtering by using Infinite Impulse Response Butterworth Band Pass Filter and Hilbert Huang Transform. After pre-processing, the filtered EEG signal is manipulated for sub-band separation and it is fissioned into five frequency bands such as Gamma, Beta, Alpha, Theta, and Delta. This work employs features such as energy, entropy, and variance which are computed for each frequency band obtained from the decomposed EEG signals. The selected features are imported for the classification process by using machine learning classifiers including Support Vector Machine (SVM) with Kernel Functions, K-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). The performance measures such as accuracy, sensitivity, and specificity are computed and analyzed for each classifier and it is inferred that the Support Vector Machine based classification of sleep apnea produces promising results.", 
"This study proposes a scheme to identify insider threats in nuclear facilities through the detection of malicious intentions of potential insiders. Based on electroencephalography (EEG) signals, a classification model was developed to identify whether a subject has a malicious intention. By using EEG signals obtained while contemplating becoming an insider threat, the subject-wise model identified malicious intentions with 78.57% accuracy.",An Investigation of Insider Threat Mitigation Based on EEG Signal Classification,"This study proposes a scheme to identify insider threats in nuclear facilities through the detection of malicious intentions of potential insiders using subject-wise classification. Based on electroencephalography (EEG) signals, a classification model was developed to identify whether a subject has a malicious intention under scenarios of being forced to become an insider threat. The model also distinguishes insider threat scenarios from everyday conflict scenarios. To support model development, 21-channel EEG signals were measured on 25 healthy subjects, and sets of features were extracted from the time, time–frequency, frequency and nonlinear domains. To select the best use of the available features, automatic selection was performed by random-forest-based algorithms. The k-nearest neighbor, support vector machine with radial kernel, naïve Bayes, and multilayer perceptron algorithms were applied for the classification. By using EEG signals obtained while contemplating becoming an insider threat, the subject-wise model identified malicious intentions with 78.57% accuracy. The model also distinguished insider threat scenarios from everyday conflict scenarios with 93.47% accuracy. These findings could be utilized to support the development of insider threat mitigation systems along with existing trustworthiness assessments in the nuclear industry.", 
"Presently, analytics degree programs exhibit a growing trend to meet a strong market demand. Decision making, organization, communication, and structured data management are key to all job categories. Technical skills like statistics and programming skills are in most demand for DAs.",An investigation of skill requirements for business and data analytics positions A content analysis of job advertisements,"Presently, analytics degree programs exhibit a growing trend to meet a strong market demand. To explore the skill sets required for analytics positions, the authors examined a sample of online job postings related to professions such as business analyst (BA), business intelligence analyst (BIA), data analyst (DA), and data scientist (DS) using content analysis. They present a ranked list of relevant skills belonging to specific skills categories for the studied positions. Also, they conducted a pairwise comparison between DA and DS as well as BA and BIA. Overall, the authors observed that decision making, organization, communication, and structured data management are key to all job categories. The analysis shows that technical skills like statistics and programming skills are in most demand for DAs. The analysis is useful for creating clear definitions with respect to required skills for job categories in the business and data analytics domain and for designing course curricula for this domain.", 
Predicting heart disease is a complex task since it requires experience and advanced knowledge. Internet of Things (IoT) technology has lately been adopted in healthcare systems to collect sensor values for heart disease diagnosis and prediction. A smartwatch and heart monitor device that is attached to the patient monitors the blood pressure and electrocardiogram.,An IoT Framework for Heart Disease Prediction Based on MDCNN Classifier,"Nowadays, heart disease is the leading cause of death worldwide. Predicting heart disease is a complex task since it requires experience along with advanced knowledge. Internet of Things (IoT) technology has lately been adopted in healthcare systems to collect sensor values for heart disease diagnosis and prediction. Many researchers have focused on the diagnosis of heart disease, yet the accuracy of the diagnosis results is low. To address this issue, an IoT framework is proposed to evaluate heart disease more accurately using a Modified Deep Convolutional Neural Network (MDCNN). The smartwatch and heart monitor device that is attached to the patient monitors the blood pressure and electrocardiogram (ECG). The MDCNN is utilized for classifying the received sensor data into normal and abnormal. The performance of the system is analyzed by comparing the proposed MDCNN with existing deep learning neural networks and logistic regression. The results demonstrate that the proposed MDCNN based heart disease prediction system performs better than other methods. The proposed method shows that for the maximum number of records, the MDCNN achieves an accuracy of 98.2 which is better than existing classifiers.", 
This paper proposes an innovative graph-based text summarization model for generic single and multi-document summarization. The approach involves four unique processing stages. The empirical evaluation of the proposed model on a standard dataset showed the approach which outperformed the baseline comparators.,An Iterative Graph-Based Generic Single and Multi Document Summarization Approach Using Semantic Role Labeling and Wikipedia Concepts,"This paper proposes an innovative graph-based text summarization model for generic single and multi-document summarization. The approach involves four unique processing stages: parsing sentences semantically using Semantic Role Labeling (SRL), grouping semantic arguments while matching semantic roles to Wikipedia concepts, constructing a weighted semantic graph for each document and linking its sentences (nodes) through the semantic relatedness of the Wikipedia concepts. An iterative ranking algorithm is then applied to the document graphs to extract the most important sentences deemed as the summary. The empirical evaluation of the proposed summarization model on a standard dataset from the Document Understanding Conference (DUC) showed the effectiveness of the approach which outperformed the baseline comparators in terms of ROUGE scores.", 
Chain of custody of digital evidence in digital forensic field are today essential part of digital investigation process. Authors define taxonomy and use an ontological approach to manage chain of custody. Developed ontology can be used to further develop a set of standard and procedures for secure management with digital evidence.,An ontological approach to study and manage digital chain of custody of digital evidence,"Chain of custody of digital evidence in digital forensic field are today essential part of digital investigation process. In order the evidence to be accepted by the court as valid, chain of custody for digital evidence must be kept, or it must be known who exactly, when, where, why and how came into contact with evidence in each stage of the digital investigations process. This paper deals with digital evidence and chain of custody of digital evidence. Authors define taxonomy and use an ontological approach to manage chain of custody of digital evidence. The aim of this paper was to develop ontology to provide a new approach to study and better understand chain of custody of digital evidence . Additionally, developed ontology can be used as a method to further develop a set of standard and procedures for secure management with digital evidence.", 
Extractive text summarization aims to create a condensed version of one or more source documents by selecting the most informative sentences. We present an approach to sentence extraction that maps sentences to nodes of a hierarchical ontology. By considering ontology attributes we are able to improve the semantic representation of a sentence's information content.,An Ontology-Based Approach to Text Summarization,"Extractive text summarization aims to create a condensed version of one or more source documents by selecting the most informative sentences. Research in text summarization has therefore often focused on measures of the usefulness of sentences for a summary. We present an approach to sentence extraction that maps sentences to nodes of a hierarchical ontology. By considering ontology attributes we are able to improve the semantic representation of a sentence’s information content. The classifier that maps sentences to the taxonomy is trained using search engines and is therefore very flexible and not bound to a specific domain. In our experiments, we train an SVM classifier to identify summary sentences using ontology-based sentence features. Our experimental results show that the ontology-based extraction of sentences outperforms baseline classifiers, leading to higher Rouge scores of summary extracts.", 
Human resources play a critical role in the success of software projects. This work explores the use of ontologies in the building of a decision support system. Ontologies allow the system to discover semantic relatedness among new and previous software projects by means of its requirements specification.,An ontology-based approach with which to assign human resources to software projects,"Human resources play a critical role in the success of software projects. Ensuring the correct assignment of them to a specific project is, therefore, an immediate requirement for Software development organizations. Within this context, this work explores the use of ontologies in the building of a decision support system that will help human resources managers or project leaders to select those employees who are best suited to participating in a new software development project. Ontologies allow the system to discover semantic relatedness among new and previous software projects by means of its requirements specification. The system can, therefore, suggest those people who have participated on similar projects. We have proved the effectiveness of our approach by conducting an evaluation in a software development organization. Our findings confirm the success of our approach and reveal that it may bring considerable benefits to the software development process.", 
An optimal nonlinear feature extractor for extracting energy features under two different kinds of patterns is proposed. It carries out the simultaneous diagonalization of two signal covariance matrices in a high-dimensional kernel transformed space.,An optimal kernel feature extractor and its application to EEG signal classification,"An optimal nonlinear feature extractor for extracting energy features under two different kinds of patterns is proposed. It carries out the simultaneous diagonalization of two signal covariance matrices in a high-dimensional kernel transformed space, and thus promises to find features which are more discriminant, especially when the original data have nonlinear structures. Two operations, whitening transform and projection transform, are involved in kernel spaces. The mechanism of the feature extractor and its effectivity are shown with simulation data and the classification task of real electroencephalographic (EEG) signals.", 
"Heart disease might be the result of unhealthy food, mental stress, genetic issues, and a sedentary lifestyle. There are many advanced automated diagnosis systems for heart disease prediction. We introduce an optimally configured and improved deep belief network named OCI-DBN to solve these problems. The proposed method can improve the accuracy of heart disease predictions by up to 94.61%.",An Optimally Configured and Improved Deep Belief Network (OCI-DBN) Approach for Heart Disease Prediction Based on Ruzzoâ€“Tompa and Stacked Genetic Algorithm,"A rapid increase in heart disease has occurred in recent years, which might be the result of unhealthy food, mental stress, genetic issues, and a sedentary lifestyle. There are many advanced automated diagnosis systems for heart disease prediction proposed in recent studies, but most of them focus only on feature preprocessing, some focus on feature selection, and some only on improving the predictive accuracy. In this study, we focus on every aspect that may have an influence on the final performance of the system, i.e., to avoid overfitting and underfitting problems or to solve network configuration issues and optimization problems. We introduce an optimally configured and improved deep belief network named OCI-DBN to solve these problems and improve the performance of the system. We used the Ruzzo-Tompa approach to remove those features that are not contributing enough to improve system performance. To find an optimal network configuration, we proposed a stacked genetic algorithm that stacks two genetic algorithms to give an optimally configured DBN. An analysis of a RBM and DBN trained is performed to give an insight how the system works. Six metrics were used to evaluate the proposed method, including accuracy, sensitivity, specificity, precision, F1 score, and Matthew’s correlation coefficient. The experimental results are compared with other state-of-the-art methods, and OCI-DBN shows a better performance. The validation results assure that the proposed method can provide reliable recommendations to heart disease patients by improving the accuracy of heart disease predictions by up to 94.61%.", 
"Text summarization since the late 50's of the 20th century by the simple technical based on term frequency. Single syllable languages like Chinese, Vietnamese, Japanese, Thai, Mongolian and other ""native"" languages in Southeast and East Asia. We've experimented with 320 Vietnamese texts (equivalent to 11,670 Vietnamese sentences) show that our method is really effective.",An optimization text summarization method based on naive bayes and topic word for single syllable language,"Text summarization since the late 50’s of the 20th century by the simple technical based on term frequency and it applied for technical text summarization at IBM institute. During more than 50 years of development, text summarization is still a hot topic that attracting many researchers, scholars in the field of data mining and natural language processing proposals development of the text summarization system. For the English, there are some automatic text summary systems was built as SUMARIST, SWESUM,… But for single syllable languages like Chinese, Vietnamese, Japanese, Thai, Mongolian and other ""native"" languages in Southeast and East Asia. Amount people use single syllable language more than 60% of all language on the world. So that, processing of single syllable language is very necessary. However, it is very complex for language processing problem because it’s very hard to determine word or term based on white space and all word segmentation tools not reach 100% accuracy currently. In this paper, we propose a text summarization method based on Naïve Bayes algorithm and topic words set. We’ve experimented with 320 Vietnamese texts (equivalent to 11,670 Vietnamese sentences) show that our method is really effective; text summary is readable, understandable and closer with summary of the human.", 
"A model that generates a summary by paraphrasing a long text remains an open significant problem for natural language processing. We present an abstractive text summarization model, multi-layered attentional peephole convolutional LSTM (long short-term memory) (MAPCoL).",An Optimized Abstractive Text Summarization Model Using Peephole Convolutional LSTM,"Abstractive text summarization that generates a summary by paraphrasing a long text remains an open significant problem for natural language processing. In this paper, we present an abstractive text summarization model, multi-layered attentional peephole convolutional LSTM (long short-term memory) (MAPCoL) that automatically generates a summary from a long text. We optimize parameters of MAPCoL using central composite design (CCD) in combination with the response surface methodology (RSM), which gives the highest accuracy in terms of summary generation. We record the accuracy of our model (MAPCoL) on a CNN/DailyMail dataset. We perform a comparative analysis of the accuracy of MAPCoL with that of the state-of-the-art models in different experimental settings. The MAPCoL also outperforms the traditional LSTM-based models in respect of semantic coherence in the output summary.", 
Epilepsy is a brain disorder which affects around 65 million people around the world. This work is focused on the seizure prediction obtained from long-short time records using an optimized deep learning network model (ODLN) The proposed ODLN methodology reveals a notable increase in the performance rate of seizure prediction.,An optimized deep learning network model for EEG based seizure classifcation using synchronization and functional connectivity measures,"Epilepsy is a brain disorder related to alteration in the nervous system which affects around 65 million people among the world’s population. Few works are focused on prediction of seizure relied on deep learning approaches, but the capability of optimal design has no longer been absolutely exploited. This work is focused on the seizure prediction obtained from long-short time records using optimized deep learning network model (ODLN). In this paper, the synchronization patterns and its feasibility of distinguishing the pre-ictal from inter-ictal states are examined by utilizing the interaction graph model as a functional connectivity measure. An optimized deep learning network with short- long-term memory is computed for the prediction of epileptic seizures occurrences. For, the modelling of ODLN, pre-analysis is performed with three modules and memory layers. It is finalized from these results; a two-layer ODLN is optimum to perform the epileptic seizure prediction for four different window sizes from 15 to 120 min. The assessment is implemented on the CHB-MIT Scalp EEG data set, providing 100% sensitivity and low false prediction rate ranges from 0.10 to 0.02 for seizure prediction. The proposed ODLN methodology reveals a notable increase in the performance rate of seizure prediction when compared with existing machine learning and Convolutional neural networks methods.", 
About half of the people who develop heart failure (HF) die within five years of diagnosis. Researchers have developed several machine learning-based models for the early prediction of HF. This paper introduces an expert system that stacks two support vector machine (SVM) models. It shows better performance than the other state-of-the-art machine learning ensemble models.,An Optimized Stacked Support Vector Machines Based Expert System for the Effective Prediction of Heart Failure,"About half of the people who develop heart failure (HF) die within five years of diagnosis. Over the years, researchers have developed several machine learning-based models for the early prediction of HF and to help cardiologists to improve the diagnosis process. In this paper, we introduce an expert system that stacks two support vector machine (SVM) models for the effective prediction of HF. The first SVM model is linear and L1 regularized. It has the capability to eliminate irrelevant features by shrinking their coefficients to zero. The second SVM model is L2 regularized. It is used as a predictive model. To optimize the two models, we propose a hybrid grid search algorithm (HGSA) that is capable of optimizing the two models simultaneously. The effectiveness of the proposed method is evaluated using six different evaluation metrics: accuracy, sensitivity, specificity, the Matthews correlation coefficient (MCC), ROC charts, and area under the curve (AUC). The experimental results confirm that the proposed method improves the performance of a conventional SVM model by 3.3%. Moreover, the proposed method shows better performance compared to the ten previously proposed methods that achieved accuracies in the range of 57.85%–91.83%. In addition, the proposed method also shows better performance than the other state-of-the-art machine learning ensemble models.", 
"Cloud service providers and customers do not yet have any proper strategy or process that paves the way for a set procedure on how to investigate or go about the issues within the cloud. This paper provides an overview of the emerging field of cloud forensics and highlights its capabilities, strategy, investigation, challenges, and opportunities.","An overview of cloud forensics strategy capabilities, challenges, and opportunities","Cloud computing has become one of the most game changing technologies in the recent history of computing. It is gaining acceptance and growing in popularity. However, due to its infancy, it encounters challenges in strategy, capabilities, as well as technical, organizational, and legal dimensions. Cloud service providers and customers do not yet have any proper strategy or process that paves the way for a set procedure on how to investigate or go about the issues within the cloud. Due to this gap, they are not able to ensure the robustness and suitability of cloud services in relation to supporting investigations of criminal activity. Moreover, both cloud service providers and customers have not yet established adequate forensic capabilities that could assist investigations of criminal activities in the cloud. The aim of this chapter is to provide an overview of the emerging field of cloud forensics and highlight its capabilities, strategy, investigation, challenges, and opportunities. This paper also provides a detailed discussion in relation to strategic planning for cloud forensics.", 
Text Summarization is the process of creating a condensed form of text document. It is an important way to find relevant information precisely in large text in a short time.,An overview of Text Summarization techniques,Text Summarization is the process of creating a condensed form of text document which maintains significant information and general meaning of source text. Automatic text summarization becomes an important way of finding relevant information precisely in large text in a short time with little efforts. Text summarization approaches are classified into two categories: extractive and abstractive. This paper presents the comprehensive survey of both the approaches in text summarization.,  
"Text summarization is the process of automatically creating and condensing form of a given document. It is difficult for human beings to summarize manually large documents of text. Text mining and text summarization are close relationships, according to this study.",An overview on extractive text summarization,"With the increasing of online information and recourse texts, text summarization has become an essential and more favorite domain to preserve and show the main purpose of textual information. It is very difficult for human beings to summarize manually large documents of text. Text summarization is the process of automatically creating and condensing form of a given document and preserving its information content source into a shorter version with overall meaning. Nowadays text summarization is one of the most favorite research areas in natural language processing and could attracted more attention of NLP researchers. There are also much more close relationships between text mining and text summarization. According to difference requirements summary with respect to input text, established summarization systems should be created and classified based on the type of input text. In this study, at first, the topic of text mining and its relationship with text summarization are considered. Then a review has been done on some of the summarization approaches and their important parameters for extracting predominant sentences, identified the main stages of the summarizing process, and the most significant extraction criteria are presented. Finally, the most fundamental proposed evaluation methods are considered.", 
"In the age of information exploding, multi-document summarization is attracting particular attention. In this paper, we propose a document-level reconstruction framework named DocRebuild. It reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error.",An unsupervised multi-document summarization framework based on neural document model,"In the age of information exploding, multi-document summarization is attracting particular attention for the ability to help people get the main ideas in a short time. Traditional extractive methods simply treat the document set as a group of sentences while ignoring the global semantics of the documents. Meanwhile, neural document model is effective on representing the semantic content of documents in low-dimensional vectors. In this paper, we propose a document-level reconstruction framework named DocRebuild, which reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error. We also apply two strategies, sentence filtering and beamsearch, to improve the performance of our method. Experimental results on the benchmark datasets DUC 2006 and DUC 2007 show that DocRebuild is effective and outperforms the state-of-the-art unsupervised algorithms.", 
A modification of the Random Forest algorithm for the categorization of traffic situations is introduced in this paper. The proposed method is able to cluster data from any data source. The knowledge of traffic scenario clusters is crucial to accelerate the validation process.,An Unsupervised Random Forest Clustering Technique for Automatic Traffic Scenario Categorization,"A modification of the Random Forest algorithm for the categorization of traffic situations is introduced in this paper. The procedure yields an unsupervised machine learning method. The algorithm generates a proximity matrix which contains a similarity measure. This matrix is then reordered with hierarchical clustering to achieve a graphically interpretable representation. It is shown how the resulting proximity matrix can be visually interpreted and how the variation of the methods’ metaparameter reveals different insights into the data. The proposed method is able to cluster data from any data source. To demonstrate the methods’ potential, multiple features derived from a traffic simulation are used in this paper. The knowledge of traffic scenario clusters is crucial to accelerate the validation process. The clue of the method is that scenario templates can be generated automatically from actual traffic situations. These templates can be employed in all stages of the development process. The results prove that the procedure is well suited for an automatic categorization of traffic scenarios. Diverse other applications can benefit from this work.", 
"This paper classifies the sleep stages based on graph domain features from a single-channel electroencephalogram (EEG) signal. The accuracy and kappa coefficients of six-state classification are 87.5% and 0.81, respectively. It was found that the MDs of the deep sleep stage are higher than those on the awake and light sleep stages.",Analysis and Classification of Sleep Stages Based on Difference Visibility Graphs From a Single-Channel EEG Signal,"The existing sleep stages classification methods are mainly based on time or frequency features. This paper classifies the sleep stages based on graph domain features from a single-channel electroencephalogram (EEG) signal. First, each epoch (30 s) EEG signal is mapped into a visibility graph (VG) and a horizontal VG (HVG). Second, a difference VG (DVG) is obtained by subtracting the edges set of the HVG from the edges set of the VG to extract essential degree sequences and to detect the gait-related movement artifact recordings. The mean degrees (MDs) and degree distributions (DDs) P(k) on HVGs and DVGs are analyzed epoch-by-epoch from 14,963 segments of EEG signals. Then, the MDs of each DVG and HVG and seven distinguishable DD values of P(k) from each DVG are extracted. Finally, nine extracted features are forwarded to a support vector machine to classify the sleep stages into two, three, four, five, and six states. The accuracy and kappa coefficients of six-state classification are 87.5% and 0.81, respectively. It was found that the MDs of the VGs on the deep sleep stage are higher than those on the awake and light sleep stages, and the MDs of the HVGs are just the reverse.", 
This paper reviews all the features that use metrics and concept of complex network for scoring sentences. Quantitative and qualitative aspects were considered in our assessment performing on the DUC 2002 data sets.,Analysis of complex network methods for extractive automatic text summarization,"Automatic text summarization is an important research area in the domain of information systems. It aims to create a compressed version of documents, which should cover all the significant contents and overall meaning. In extractive text summarization, sentences are scored on various of features. A large number of features network based have been proposed by researchers in the past literatures. This paper reviews all the features that use metrics and concept of complex network for scoring sentences. The experiment results on single feature and combinations of various features we proposed are discussed. Quantitative and qualitative aspects were considered in our assessment performing on the DUC 2002 data sets.", 
"In this paper, an experimental evaluation of Mel-Frequency Cepstral Coefficient (MFCCs) for use in Electroencephalogram (EEG) signal classification is presented. The objective is to classify the given EEG signal into normal or abnormal that is based on the MFCC representation of EEG signal. ",ANALYSIS OF MFCC FEATURES FOR EEG SIGNAL CLASSIFICATION,"In this paper, an experimental evaluation of Mel-Frequency Cepstral Coefficients (MFCCs) for use in Electroencephalogram (EEG) signal classification is presented. The MFCC features are tested using CHB-MIT Scalp EEG Database. The objective is to classify the given EEG signal into normal or abnormal that is based on the MFCC representation of EEG signal. Initially, the QRS complex waves are detected using Pan Tompkins algorithm, and then the MFCC features are extracted. The performance of MFCC feature representation is analyzed in the context of an Artificial Neural Network (ANN) classification system in terms of sensitivity and specificity. The performance of EEG classification approach depends on the number of MFCC components used for the classification. When compared with 15 and 35 MFCC components, 25 MFCC components gives better result in terms of sensitivity (98%) and specificity (96%).", 
"VANET encompass the mobile nodes which are indicated as (MN), The vehicle networking system has drawn significant interest from academia and industry since its establishment. Its technology is used for military purposes by several foreign manufacturing and government sectors. There are certain loopholes or drawbacks like frequent network drop, high mobility which affected the overall QoS.",Analysis of Quality of Service in VANET,"VANET as Inter-vehicle communication is becoming a promising area of research, standardization, and growth with the ever-growing number of vehicles equipped with computer technology and wireless communication tools. Due to its erratic portability and the interrupted availability of a network, VANET is relatively incomprehensible for a strong end to end communication, In Vehicular Network the main advantage is that it communicates by using nodes. Because of its simple and basic method of communication, MANET is used most often in the military, just like data sharing between different computers. VANET and some modifications are identical to MANET.VANET encompass the mobile nodes which are indicated as (MN), The vehicle networking system has drawn significant interest from academia and industry since its establishment. Its technology is used for military purposes by several foreign manufacturing and government sectors, it also enables a wide range of applications, including collision prevention, security, blind crossing, dynamic route planning, traffic condition monitoring on a real-time basis, etc but there are certain loopholes or drawbacks like frequent network drop, high mobility which affected the overall QoS, In this paper, there are brief analyses on Quality of Service which include certain Routing Protocol like DIVERT, FBAODV, DSR and also to decrease the congestion control, there is a comparative study of the different protocol which evaluates in increasing the throughput and network efficiency.", 
This paper reviews features for sentence scoring. ROUGE-N is used to evaluate generated summary with abstractive summary of DUC 2002 dataset.,Analysis of Sentence Scoring Methods for Extractive Automatic Text Summarization,"Automatic text summarization is a major area of research in the domain of information systems. Most of the methods requires domain knowledge in order to produce a coherent and meaningful summary. In Extractive text summarization, sentences are scored on some features. A large number of feature based scoring methods have been proposed for extractive automatic text summarization by researchers. This paper reviews features for sentence scoring. The results on combinations of various features for scoring are discussed. ROUGE-N is used to evaluate generated summary with abstractive summary of DUC 2002 dataset.", 
Vehicular ad-hoc networks (VANETs) provide the essential infrastructure for intelligent transportation systems. The emergence of VANets has triggered a lot of research aimed at developing mathematical models. In this paper we consider the message propagation speed on the highway.,Analysis of the Message Propagation Speed in VANET with Disconnected RSUs,"Vehicular ad-hoc networks (VANETs), which are networks of communicating vehicles, provide the essential infrastructure for intelligent transportation systems. Thanks to the significant research efforts to develop the technological background of VANETs, intelligent transportation systems are nowadays becoming a reality. The emergence of VANETs has triggered a lot of research aimed at developing mathematical models in order to gain insight into the dynamics of the communication and to support network planning. In this paper we consider the message propagation speed on the highway, where messages can be exchanged not only between the vehicles, but also between the road-side infrastructure and the vehicles as well. In our scenario, alert messages are generated by a static message source constantly. Relying on an appropriately defined Markov renewal process, we characterize the message passing process between the road-side units, derive the speed of the message propagation, and provide the transient distribution of the distance where the message is available. Our results make it possible to determine the optimal distance between road-side units (RSUs) and to calculate the effect of speed restrictions on message propagation.", 
"Developers usually analyze source code to induce the bug cause, which is useful for bug understanding and localization. This paper aims at exploiting the relationship between bug causes and bug fixes to automatically classify bugs into their cause categories. We collected 2000 real-world bugs from two open source projects Mozilla and Radare2 to evaluate our approach.",Analyzing bug fix for automatic bug cause classification,"During the bug fixing process, developers usually need to analyze the source code to induce the bug cause, which is useful for bug understanding and localization. The bug fixes of historical bugs usually reflects the bug causes when fixing them. This paper aims at exploiting the corresponding relationship between bug causes and bug fixes to automatically classify bugs into their cause categories. First, we define the code-related bug classification criterion from the perspective of the cause of bugs. Then, we propose a new model to exploit the knowledge in the bug fix by constructing fix trees from the diff source code at Abstract Syntax Tree (AST) level, and representing each fix tree based on the encoding method of Tree-based Convolutional Neural Network (TBCNN). Finally, the corresponding relationship between bug causes and bug fixes is analyzed by automatically classifying bugs into their cause categories. We collected 2000 real-world bugs from two open source projects Mozilla and Radare2 to evaluate our approach. The experimental results show the existence of observational correlation between the bug fix and the cause of the historical bugs, and the proposed fix tree can effectively express the characteristics of the historical bugs for bug cause classification.", 
"This paper presents a detailed analysis of the use of crowdsourcing services for the Text Summarization task in the context of the tourist domain. In particular, our aim is to retrieve relevant information about a place or an object pictured in an image. The encouraging results obtained in the third and sixth experiments motivate us to strongly believe that crowdsourcing can be successfully employed.",Analyzing the capabilities of crowdsourcing services for text summarization,"This paper presents a detailed analysis of the use of crowdsourcing services for the Text Summarization task in the context of the tourist domain. In particular, our aim is to retrieve relevant information about a place or an object pictured in an image in order to provide a short summary which will be of great help for a tourist. For tackling this task, we proposed a broad set of experiments using crowdsourcing services that could be useful as a reference for others who want to rely also on crowdsourcing. From the analysis carried out through our experimental setup and the results obtained, we can conclude that although crowdsourcing services were not good to simply gather gold-standard summaries (i.e., from the results obtained for experiments 1, 2 and 4), the encouraging results obtained in the third and sixth experiments motivate us to strongly believe that they can be successfully employed for finding some patterns of behaviour humans have when generating summaries, and for validating and checking other tasks. Furthermore, this analysis serves as a guideline for the types of experiments that might or might not work when using crowdsourcing in the context of text summarization.", 
"Software developers experience a wide range of emotions, and anger deserves special attention, argue the authors. They argue that anger can serve as an onset for tools supporting collaborative software development. They build a classifier for anger direction based on a manually annotated gold standard of 723 sentences.",Anger and Its Direction in Collaborative Software,"Recent research has provided evidence that software developers experience a wide range of emotions. We argue that among those emotions anger deserves special attention as it can serve as an onset for tools supporting collaborative software development. This, however, requires a fine-grained model of the anger emotion, able to distinguish between anger directed towards self, others, and objects. Detecting anger towards self could be useful to support developers experiencing difficulties; detection of anger towards others might be helpful for community management; detecting anger towards objects might be helpful to recommend and prioritize improvements. As a first step towards automatic identification of anger direction, we built a classifier for anger direction, based on a manually annotated gold standard of 723 sentences that were obtained by mining comments in Apache issue reports.", 
"A method for identifying images containing categories of animals relies on four simple cues: text, color, shape and texture. The visual exemplars are obtained using a clustering method applied to text on web pages. The only supervision required is identifying which clusters of exemplars refer to which sense of a term.",Animals on the Web,"We demonstrate a method for identifying images containing categories of animals. The images we classify depict animals in a wide range of aspects, configurations and appearances. In addition, the images typically portray multiple species that differ in appearance (e.g. ukari’s, vervet monkeys, spider monkeys, rhesus monkeys, etc.). Our method is accurate despite this variation and relies on four simple cues: text, color, shape and texture. Visual cues are evaluated by a voting method that compares local image phenomena with a number of visual exemplars for the category. The visual exemplars are obtained using a clustering method applied to text on web pages. The only supervision required involves identifying which clusters of exemplars refer to which sense of a term (for example, “monkey” can refer to an animal or a bandmember). Because our method is applied to web pages with free text, the word cue is extremely noisy. We show unequivocal evidence that visual information improves performance for our task. Our method allows us to produce large, accurate and challenging visual datasets mostly automatically.", 
"Landslide susceptibility assessment has been conducted at the Pauri Garhwal area of Uttarakhand state, India. The area is affected by numerous landslides causing significant losses of life, infrastructure and property every year. The results show that the RF model has the highest predictive capability.","Application and Comparison of Decision Tree-Based Machine Learning Methods in Landside Susceptibility Assessment at Pauri Garhwal Area, Uttarakhand, India","Landslide susceptibility assessment has been conducted at the Pauri Garhwal area of Uttarakhand state, India, an area affected by numerous landslides causing significant losses of life, infrastructure and property every year. Decision tree-based machine learning methods, namely Random Forest (RF), Logistic Model Trees (LMT), Best First Decision Trees (BFDT) and Classification and Regression Trees (CART) have been used, and results are compared herein for proper spatial prediction of landslides. Analysis of the data has been done considering sixteen conditioning factors (i.e., slope angle, elevation, slope aspect, profile curvature, land cover, curvature, lithology, plan curvature, soil, distance to lineaments, lineament density, distance to roads, road density, distance to river, river density and rainfall), and 1295 historical landslide polygons. Models were validated and compared using Receiver Operating Characteristics (ROC) curve and statistical indices. The results show that the RF model has the highest predictive capability, followed by the LMT, BFDT and CART models, respectively, and indicate that although all four methods have shown good results, the performance of the RF method is the best for landslide spatial prediction.", 
Epilepsy is a neurological disorder affecting more than 50 million individuals in the world. Analysis of the electroencephalogram (EEG) is a powerful tool to assist neurologists for diagnosis and treatment. A new feature extraction method based on empirical mode decomposition (EMD) is proposed.,Application of empirical mode decomposition and artificial neural network for the classification of normal and epileptic EEG signals,"Epilepsy is a neurological disorder affecting more than 50 million individuals in the world. Analysis of the electroencephalogram (EEG) is a powerful tool to assist neurologists for diagnosis and treatment. In this paper a new feature extraction method based on empirical mode decomposition (EMD) is proposed. The EEG signal is decomposed into intrinsic mode functions (IMFs) by the EMD algorithm and four statistical parameters are calculated over these IMFs constituting the input feature vector to be fed to a multilayer perceptron neural network (MLPNN) classifier. Experimental results carried out on the publicly available Bonn dataset show that an accurate classification rate of 100% is achieved in the discrimination between normal and ictal EEG, and an accuracy of 97.7% is reached in the classification of interictal and ictal EEG signals. Our results are equivalent or outperform recent studies published in the literature.", 
"Maximum heart rate (MHR) is widely used in the prescription and monitoring of exercise intensity. Traditionally, MHR is predicted from an age-based formula, usually 220+age. In this paper, we use functional data analysis to create a new method to predict MHR.",Application of Functional Data Analysis for the Prediction of Maximum Heart Rate,"Maximum heart rate (MHR) is widely used in the prescription and monitoring of exercise intensity, and also as a criterion for the termination of sub-maximal aerobic fitness tests in clinical populations. Traditionally, MHR is predicted from an age-based formula, usually 220?age. These formulae, however, are prone to high predictive errors that potentially could lead to inaccurately prescribed or quantified training or inappropriate fitness test termination. In this paper, we used functional data analysis (FDA) to create a new method to predict MHR. It uses heart rate data gathered every 5 seconds during a low intensity, sub-maximal exercise test. FDA allows the use of all the information recorded by monitoring devices in the form of a function, reducing the amount of information needed to generalize a model, besides minimizing the curse of dimensionality. The functional data model created reduced the predictive error by more than 50% compared to current models within the literature. This new approach has important benefits to clinicians and practitioners when using MHR to test fitness or prescribe exercise.", 
"Study assesses forest-fire susceptibility (FFS) in Fars Province, Iran using machine-learning algorithms. BRT, GLM, and MDA have become important machine learning algorithms. The resulting FFS maps can enhance the e?ectiveness of planning and management of forest resources and ecological balances in this province. The accuracy results of the BRT (AUC = 88.90% and 88.2% and 85.6%) models are more effective than the GLM (A UC = 86.6% and 82.5%) model.",Application of learning vector quantization and different machine learning techniques to assessing forest fre influence factors and spatial modelling,"This study assesses forest-fire susceptibility (FFS) in Fars Province, Iran using three geographic information system (GIS)-based machine-learning algorithms: boosted regression tree (BRT), general linear model (GLM), and mixture discriminant analysis (MDA). Recently, BRT, GLM, and MDA have become important machine learning algorithms and their use has been enriched by application to various fields of research. A database of historical FFs identified using Landsat-8 OLI and MODIS satellite images (at 358 locations) and ten in?uencing factors (elevation, slope, topographical wetness index, aspect, distance from urban areas, annual mean temperature, land use, distance from road, annual mean rainfall, and distance from river) were input into a GIS. The 358 sites were divided into two sets for training (70%) and validation (30%). BRT, GLM, and MDA models were used to analyze the spatial relationships between the factors in?uencing FFs and the locations of fires to generate an FFS map. The prediction success of each modelled FFS map was determined with the help of the ROC curve, accuracy, overall accuracy, True-skill statistic (TSS), F-measures, corrected classify instances (CCI), and K-fold cross-validation (4-fold). The accuracy results of training and validation dataset in the BRT (AUC = 88.90% and 88.2%) and MDA (AUC = 86.4% and 85.6%) models are more e?ective than the GLM (AUC = 86.6% and 82.5%) model. Also, the outcome of the 4-fold measure confirmed the results from the other accuracy measures. Therefore, the accuracies of the BRT and MDA models are satisfactory and are suitable for FFS mapping in Fars Province. Finally, the well-accepted neural network application of learning-vector quantization (LVQ) reveals that land use, annual mean rainfall, and slope angle were the most useful determinants of FFS. The resulting FFS maps can enhance the e?ectiveness of planning and management of forest resources and ecological balances in this province.", 
"Three major crystal systems of silicate-based cathodes with Li–Si–(Mn, Fe, Co)–O compositions were predicted using wide range of classification algorithms in machine learning. The strong correlation between the crystal system and other physical properties of the cathodes was confirmed.",Application of machine learning methods for the prediction of crystal system of cathode materials in lithium-ion batteries,"The system of crystal structure has a major effect on the physical and chemical properties of Li-ion silicate cathodes. Hence, the prediction of crystal system has a vital importance to estimate many other properties of cathodes for applications in batteries. Three major crystal systems (monoclinic, orthorhombic and triclinic) of silicate-based cathodes with Li–Si–(Mn, Fe, Co)–O compositions were predicted using wide range of classification algorithms in machine learning. The calculations are based on the results of density functional theory calculations from Materials Project. The strong correlation between the crystal system and other physical properties of the cathodes was confirmed based on the feature evaluation in the statistical models. In addition, the parameters of various classification methods were optimized to obtain the best accuracy of prediction. Ensemble methods including random forests and extremely randomized trees provided the highest accuracy of prediction among other classification methods in the Monte Carlo cross validation tests.", 
New approach has been discussed to solve text summarization problem. It is known that the text summarization is an NP-Complete problem.,Application of Minimum Vertex Cover for Keyword-based Text Summarization Process,"In this article, a new approach has been discussed to solve text summarization problem using minimum vertex cover for weighted graph on the basis of keyword based approach. It is known that the text summarization problem is an NP-Complete problem.", 
Automatic Text Summarization has been shown to be useful for Natural Language Processing tasks such as Question Answering or Text Classification. We propose the generation of two types of summaries (generic and geographical) applying several compression rates to evaluate their effectiveness in the Geographical Information Retrieval task.,Application of Text Summarization techniques to the Geographical Information Retrieval task,"Automatic Text Summarization has been shown to be useful for Natural Language Processing tasks such as Question Answering or Text Classification and other related fields of computer science such as Information Retrieval. Since Geographical Information Retrieval can be considered as an extension of the Information Retrieval field, the generation of summaries could be integrated into these systems by acting as an intermediate stage, with the purpose of reducing the document length. In this manner, the access time for information searching will be improved, while at the same time relevant documents will be also retrieved. Therefore, in this paper we propose the generation of two types of summaries (generic and geographical) applying several compression rates in order to evaluate their effectiveness in the Geographical Information Retrieval task. The evaluation has been carried out using GeoCLEF as evaluation framework and following an Information Retrieval perspective without considering the geo-reranking phase commonly used in these systems. Although single-document summarization has not performed well in general, the slight improvements obtained for some types of the proposed summaries, particularly for those based on geographical information, made us believe that the integration of Text Summarization with Geographical Information Retrieval may be beneficial, and consequently, the experimental set-up developed in this research work serves as a basis for further investigations in this field.", 
"Blockchain is a distributed ledger capable of maintaining an immutable log of transactions happening in a network. In recent years, this technology has attracted significant scientific interest in research areas beyond the financial sector. In this context, the blockchain is seen as the missing link towards building a truly decentralized, trustless and secure environment for the IoT.",Applications of Blockchains in the Internet of Things A Comprehensive Survey,"The Blockchain technology has revolutionized the digital currency space with the pioneering cryptocurrency platform named Bitcoin. From an abstract perspective, a blockchain is a distributed ledger capable of maintaining an immutable log of transactions happening in a network. In recent years, this technology has attracted significant scientific interest in research areas beyond the financial sector, one of them being the Internet of Things (IoT). In this context, the Blockchain is seen as the missing link towards building a truly decentralized, trustless and secure environment for the IoT and, in this survey, we aim to shape a coherent and comprehensive picture of the current stateof-the-art efforts in this direction. We start with fundamental working principles of blockchains and how blockchain-based systems achieve the characteristics of decentralization, security, and auditability. From there, we build our narrative on the challenges posed by the current centralized IoT models, followed by recent advances made both in industry and research to solve these challenges and effectively use blockchains to provide a decentralized, secure medium for the IoT.", 
Wireless Sensor Networks (WSNs) have emerged as highly flexible and dynamic facets that are being deployed in almost every type of environment. WSN deployment in an urban environment is especially demanding due to its harsh and perverse channel conditions.,Applications of wireless sensor networks for urban areas A survey,"As new wireless technologies become more and more advance so does their expanse of applications. Among other new and innovative wireless networks, Wireless Sensor Networks (WSNs) have emerged as highly flexible and dynamic facets that are being deployed in almost every type of environment whether it is rural, suburban or urban in nature. The most adaptive and innovative research avenues are being considered in an urban environment, where WSN deployment is especially demanding due to its harsh and perverse channel conditions. We have chosen WSN deployment in an urban environment as linchpin of our research. As each application scenario is different from the other, therefore WSN solution for each application has to be adaptive and innovative. We have discussed each application of WSNs in urban areas in detail with all the problems related to it and in the end, technical solution to those problems has been discussed.", 
"This paper focuses on mining data from a database. Data mining techniques can be used to support digital forensic investigation. Digital forensic models are reviewed, concepts mapped onto experimental cases.",Applying data mining principles in the extraction of digital evidence,"Data mining in databases is part of the interdisciplinary field of knowledge discovery used for extracting patterns and relationships amongst the data sets from a data source. This paper focuses on mining data from a database. The study further identifies how data mining techniques can be used to support digital forensic investigation in a digital crime case. Digital forensic models are reviewed, concepts mapped onto experimental cases and tested on a Pentium (R) Core ™ 2 Duo CPU, 1.89 GHz with Windows XP Professional OS. The findings supported the benefits of integrating data mining principles in extracting data for digital evidence.", 
"In larger institutions, comprehensive corporate guidelines and requirements have to be followed. Large enterprises often develop numerous apps and lack an overview of development projects. For such systems an automatic categorization of artifacts is required. In this work we propose using a machine learning approach to categorize user stories.",Applying Machine Learning for Automatic User Story Categorization in Mobile Enterprises Application Development,"Mobile enterprise applications (apps) are developed in dynamic and complex environments. Hardware characteristics, operating systems and development tools are constantly changing. In larger institutions, comprehensive corporate guidelines and requirements have to be followed. In addition, larger enterprises often develop numerous apps and lack an overview of development projects. Because of the size of such companies, a comprehensive direct information exchange between developers is often not practicable. In this situation, IT support is necessary, for example to prevent unnecessary duplication of work in the development of software artifacts such as user stories, app screen designs or code features within the company. One approach to overcome these challenges is to support reusing results from previous projects by building systems to organize and analyse the knowledge base of enterprise app development projects. For such systems an automatic categorization of artifacts is required. In this work we propose using a machine learning approach to categorize user stories. The approach is evaluated on a set of user stories from real-world mobile enterprise application development projects. The results are promising and suggest that machine learning approaches can be beneficially applied to user story classification in large companies.", 
"Text summarization is the most challenging task in information retrieval especially for Arabic language. Summarization systems for Arabic are still not as mature as those for English. In this paper, we introduce a new method for Arabic text summarization based on graph theory and semantic similarity.",Arabic text summarization based on graph theory,"Automatic text summarization is a process of reducing the length of original document without a?ecting the content by extracting important information from huge amount of text data. The main goal is to facilitate the task of reading and searching information in large documents. Text summarization is the most challenging task in information retrieval especially for Arabic language. Unlike English and European languages, researches in Arabic text summarization are very few and still in their beginning. Summarization systems for Arabic are however still not as mature and as reliable as those developed for languages like English. In this paper, we introduce a new method for Arabic text summarization based on graph theory and semantic similarity between sentences to calculate importance of each sentence in document and most important sentences are extracted to generate document summary. In addition, because words sharing a root are semantically related, feature selection techniques based on the root can improves the semantic similarity between sentences and increases the weight of the semantic feature in the sentence. We will first review the related works in this field and especially in Arabic text summarization. Then we will present the architecture of our system, its components and its features. The last section will evaluate the system and compare it to other existing methods.", 
"Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise.",Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering,"Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.", 
"The current work investigates a developed automatic Arabic text summarization model. In this model, a technique of word root clustering is used as the major activity. Result obtained actually is considered promising and competitive to the verb/noun categorization ranking method.",Arabic text summarization model using clustering techniques,"the current work investigates a developed automatic Arabic text summarization model. In this model, a technique of word root clustering is used as the major activity. Unlike the previously presented systems of Arabic text summarization in the extract based design field, the current model adopts cluster weight of word roots instead of the word weight itself. The model is thoroughly illustrated through its different stages. Obviously, the general scheme follows traditional descriptive model of most of the system stages in literature with the exception of the ranking stage. This model with its developed technique has been subjected to a set of experiments. Various Arabic text examples are used for evaluation purposes. The efficiency of the summarization is calculated in terms of Precision and Recall measures. Result obtained actually is considered promising and competitive to the verb/noun categorization ranking method. This enhancement has been detected for Precision 76% and Recall 79% with the analogous values of 62% and 70% obtained in the verb/noun categorization method. The enhancement emerges in this tangible result is attributed to the implicit embedding of semantic capability of the developed model to expand the extract boundaries towards the abstract extremes of the design theme.", 
"This paper proposes an Arabic text summarization approach based on an aggregate similarity method originally proposed for the Korean language text. The proposed approach depends mainly on nouns as indicators of the importance of the sentences. To evaluate the proposed approach, a dataset of fifty documents is used and the performance of the approach is evaluated using the Recall and Precision measures.",ARABIC TEXT SUMMARIZATION USING AGGREGATE SIMILARITY,"This paper proposes an Arabic text summarization approach based on an aggregate similarity method which is originally proposed for the Korean language text. The proposed approach depends mainly on nouns as indicators of the importance of the sentences. Hence, the noun extraction process is the main process in the proposed approach. To do summarization of a given document, the document is segmented into sentences and then the sentences are tokenized into words. The noun extraction process is performed using fourteen noun extraction rules that are used as indication for the distinction of nouns from other non noun words. In the next step the frequencies for each noun in each sentence and in the whole document are computed and the sentence similarity between the noun frequency in the sentence and the document is calculated using the Inner Product measure. The summation of all similarities of every sentence represents an Aggregate similarity; the sentences that have the highest value of similarity are selected as the summary where the number of sentences that are selected is determined by a user defined threshold value. To evaluate the proposed approach, a dataset of fifty documents is used and the performance of the approach is evaluated using the Recall and Precision measures. The results obtained were 62% for Precision, 70 % for Recall, and 14% for the compression rate. As a conclusion, the result is acceptable according to the nature of the Arabic language which has rich vocabulary and complex grammar rules.", 
"In this study, we propose an extractive Arabic text summarizer based on a general-purpose architecture for Natural Language Generation (NLG) and Natural Language Understanding (NLU) We evaluate and extract the most important sentences at this document. We compare the efficiency between the proposed and other solutions to recommend what the best one is.",Arabic Text Summarization Using AraBERT Model Using Extractive Text Summarization Approach,"Recently, after the life of the individual changed and became more crowded with all the concerns of life, and with the diversity and the increasing of sources of knowledge on the Internet, it became difficult for us to read large texts and articles, so we are looking for the summaries of these texts before deciding dive deeply in reading. For this reason, it became urgent to provide tools to fulfill this function by extracting basic information while preserving the essence of the text. In this study, we proposed an extractive Arabic text summarizer based on a general-purpose architecture for Natural Language Generation (NLG) and Natural Language Understanding (NLU) like (AraBERT, BERT, XLNet, XLM, etc.) to summarize the Arabic document by evaluating and extracting the most important sentences at this document. Then, using the Rouge measure and human evaluation, we compared the efficiency between the proposed and other solutions to recommend what the best one we can use to summarize Arabic text and put our hands-on weak points to open the way for researchers to improve the approaches.", 
"Text summarization, along other tasks like text translation and sentiment analysis, used deep neural network models to enhance results. The new methods of text summarization are subject to a sequence?to?sequence framework of encoder–decoder model. Deep neural networks take advantage of big datasets to improve their results.",Arabic text summarization using deep learning approach,"Natural language processing has witnessed remarkable progress with the advent of deep learning techniques. Text summarization, along other tasks like text translation and sentiment analysis, used deep neural network models to enhance results. The new methods of text summarization are subject to a sequence?to?sequence framework of encoder–decoder model, which is composed of neural networks trained jointly on both input and output. Deep neural networks take advantage of big datasets to improve their results. These networks are supported by the attention mechanism, which can deal with long texts more efficiently by identifying focus points in the text. They are also supported by the copy mechanism that allows the model to copy words from the source to the summary directly. In this research, we are re?implementing the basic summarization model that applies the sequence?to?sequence framework on the Arabic language, which has not witnessed the employment of this model in the text summarization before. Initially, we build an Arabic data set of summarized article headlines. This data set consists of approximately 300 thousand entries, each consisting of an article introduction and the headline corresponding to this introduction. We then apply baseline summarization models to the previous data set and compare the results using the ROUGE scale.", 
"In this research, we propose the use of Firefly algorithm for the extraction of summaries of single Arabic documents. The proposed approach is compared with two evolutionary approaches that use genetic algorithms.",Arabic Text Summarization using Firefly Algorithm,"In this research, we propose the use of Firefly algorithm for the extraction of summaries of single Arabic documents. The proposed approach is compared with two evolutionary approaches that use genetic algorithms and harmony search. The EASC Corpus and the ROUGE toolkit are used for the evaluation of the proposed approach. Experimental results showed that the proposed approach achieved competitive and even higher ROUGE scores in comparison with the two state-of-the-art approaches.", 
LSA is a vectorial semantic form of analyzing relationships between a set of sentences. It is concerned with the word description as well as the sentence description for each concept or topic. LSA is implemented along with root representative and different weighting techniques. The optimal combination is specified and used as a proposed summarizer.,Arabic Text Summarization Using Latent Semantic Analysis,"The main objective of this paper is to address Arabic text summarization using latent semantic analysis technique. LSA is a vectorial semantic form of analyzing relationships between a set of sentences. It is concerned with the word description as well as the sentence description for each concept or topic. LSA creates the word by sentence semantic matrix of a document or documents. Each word in the matrix row is represented by word variations such as root, stem and original word. The root is empirically specified as the most effective word representative, where F-score of 63% is obtained at the same time an average ROUGE of 48.5% is obtained too. LSA is implemented along with root representative and different weighting techniques then the optimal combination is specified and used as a proposed summarizer for Arabic Text Summarization. Then the summarizer is implemented again, where the input documents are pre-processed by POS tagger. The summarizer performance and effectiveness are measured manually and automatically based on the summarization accuracy. Experimental results show that the summarizer obtains higher level of accuracy as compared to human summarizer. When the compression rate is 25% F-scores of 68% is obtained and an average ROUGE score of 59% is obtained as well, in terms of Arabic text summarization.", 
"About 50 million people are at risk of heart disease in the world. Method based on the analysis of 10-s ECG signal fragments is applied (on average, 13 times less classifications/analysis) Proposed method is efficient, fast (real-time classification), non-complex and simple to use, authors say. Results are one of the best results to date, and can be implemented in mobile devices.",Arrhythmia Detection Using Deep Convolutional Neural Network With Long Duration ECG Signals,"This article presents a new deep learning approach for cardiac arrhythmia (17 classes) detection based on long-duration electrocardiography (ECG) signal analysis. Cardiovascular disease prevention is one of the most important tasks of any health care system as about 50 million people are at risk of heart disease in the world. Although automatic analysis of the ECG signal is very popular, current methods are not satisfactory. The goal of our research was to design a new method based on deep learning to efficiently and quickly classify cardiac arrhythmias. Described research is based on 1000 ECG signal fragments from the MIT - BIH Arrhythmia database for one lead (MLII) from 45 persons. Approach based on the analysis of 10-s ECG signal fragments (not a single QRS complex) is applied (on average, 13 times less classifications/analysis). A complete end-to-end structure was designed instead of the handcrafted feature extraction and selection used in traditional methods. Our main contribution is to design a new 1D-Convolutional Neural Network model (1D-CNN). Proposed method is 1) efficient, 2) fast (real-time classification) 3) non-complex and 4) simple to use (combined feature extraction and selection, and classification in one stage). Deep 1D-CNN achieved a recognition overall accuracy of 17 cardiac arrhythmia disorders (classes) at a level of 91.33% and classification time per single sample of 0.015 sec. Compared to the current research, our results are one of the best results to date, and our solution can be implemented in mobile devices and cloud computing.", 
"Artex is another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence. Summaries are then generated by assembling the highest ranked sentences.",Artex is AnotheR TEXt summarizer,"This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.", 
"Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data. Popular AI techniques include machine learning methods for structured data and natural language processing for unstructured data.","Artificial intelligence in healthcare past, present and future","Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.", 
"To promote sustainability, smart production requires global perspectives of smart production application technology. A literature review on ML and AI empirical studies published in the last century was carried out to highlight the evolution of the topic before and after Industry 4.0. Eighty-two articles were reviewed and classified.","Artificial Intelligence and Machine Learning Applications in Smart Production Progress, Trends, and Directions","Adaptation and innovation are extremely important to the manufacturing industry. This development should lead to sustainable manufacturing using new technologies. To promote sustainability, smart production requires global perspectives of smart production application technology. In this regard, thanks to intensive research efforts in the field of artificial intelligence (AI), a number of AI-based techniques, such as machine learning, have already been established in the industry to achieve sustainable manufacturing. Thus, the aim of the present research was to analyze, systematically, the scientific literature relating to the application of artificial intelligence and machine learning (ML) in industry. In fact, with the introduction of the Industry 4.0, artificial intelligence and machine learning are considered the driving force of smart factory revolution. The purpose of this review was to classify the literature, including publication year, authors, scientific sector, country, institution, and keywords. The analysis was done using the Web of Science and SCOPUS database. Furthermore, UCINET and NVivo 12 software were used to complete them. A literature review on ML and AI empirical studies published in the last century was carried out to highlight the evolution of the topic before and after Industry 4.0 introduction, from 1999 to now. Eighty-two articles were reviewed and classified. A first interesting result is the greater number of works published by the USA and the increasing interest after the birth of Industry 4.0.", 
Automatic text summarization has played a critical role in helping people obtain key information from increasing huge data with the advantaged development of technology. We propose an AI system by applying deep learning to generate short summaries from the titles and abstracts of the Web of Science (WOS) database.,Artificial Intelligence for Automatic Text Summarization,"Automatic text summarization has played a critical role in helping people obtain key information from increasing huge data with the advantaged development of technology. In the past, few literatures are related to solve the problem of generating titles (short summaries) by using artificial intelligence (AI). The purpose of this study is that we proposed an AI approach for automatic text summarization. We developed an AI text summarization system architecture with three models, namely, statistical model, machine learning model, and deep learning model as well as evaluating the performance of three models. Essay titles and essay abstracts are used to train artificial intelligence deep learning model to generate the candidate titles and evaluated by ROUGE for performance evaluation. The contribution of this paper is that we proposed an AI automatic text summarization system by applying deep learning to generate short summaries from the titles and abstracts of the Web of Science (WOS) database.", 
The rise of super computing power and Big Data technologies appear to have empowered AI in recent years. The new generation of AI is rapidly expanding and has again become an attractive topic for research. This paper aims to identify the challenges associated with the use and impact of revitalized AI based systems for decision making.,"Artificial intelligence for decision making in the era of Big Data -evolution, challenges and research agenda","Artificial intelligence (AI) has been in existence for over six decades and has experienced AI winters and springs. The rise of super computing power and Big Data technologies appear to have empowered AI in recent years. The new generation of AI is rapidly expanding and has again become an attractive topic for research. This paper aims to identify the challenges associated with the use and impact of revitalized AI based systems for decision making and offer a set of research propositions for information systems (IS) researchers. The paper first provides a view of the history of AI through the relevant papers published in the International Journal of Information Management (IJIM). It then discusses AI for decision making in general and the specific issues regarding the interaction and integration of AI to support or replace human decision makers in particular. To advance research on the use of AI for decision making in the era of Big Data, the paper offers twelve research propositions for IS researchers in terms of conceptual and theoretical development, AI technology-human interaction, and AI implementation.", 
"The Internet of things (IoT) platform has played a significant role in improving road transport safety and efficiency. Such an IoT paradigm however, brings in strain on limited spectrum resources due to the need of continuous communication. Cognitive radio is a potential approach to alleviate the spectrum scarcity problem through opportunistic exploitation of the underutilized spectrum.",Artificial Intelligence Inspired Transmission Scheduling in Cognitive Vehicular Communications and Networks,"The Internet of things (IoT) platform has played a significant role in improving road transport safety and efficiency by ubiquitously connecting intelligent vehicles through wireless communications. Such an IoT paradigm however, brings in considerable strain on limited spectrum resources due to the need of continuous communication and monitoring. Cognitive radio (CR) is a potential approach to alleviate the spectrum scarcity problem through opportunistic exploitation of the underutilized spectrum. However, highly dynamic topology and time-varying spectrum states in CR-based vehicular networks introduce quite a few challenges to be addressed. Moreover, a variety of vehicular communication modes, such as vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V), as well as data QoS requirements pose critical issues on efficient transmission scheduling. Based on this motivation, in this paper, we adopt a deep Q-learning approach for designing an optimal data transmission scheduling scheme in cognitive vehicular networks to minimize transmission costs while also fully utilizing various communication modes and resources. Furthermore, we investigate the characteristics of communication modes and spectrum resources chosen by vehicles in different network states, and propose an efficient learning algorithm for obtaining the optimal scheduling strategies. Numerical results are presented to illustrate the performance of the proposed scheduling schemes.", 
"Artificial Muscle Intelligence with Deep Learning (AMIDL) system is proposed in this paper. Human thoughts captured using Electroencephalogram (EEG) sensors are transformed into body movements. The system also provides a feature for communicating human intentions as alert message to caregivers. The proposed system will be a great communication aid for paralyzed to express their thoughts and feelings with dear and near ones, thereby enhancing the quality of life.",Artificial Muscle Intelligence System With Deep Learning for Post-Stroke Assistance and Rehabilitation,"Stroke is one of the prime reasons for paralysis throughout the world caused due to impaired nervous system and resulting in disability to move the affected body parts. Rehabilitation is the natural remedy for recovering from paralysis and enhancing the quality of life. Brain Computer Interface (BCI) controlled assistive technology is the new paradigm, providing assistance and rehabilitation for the paralyzed. But, most of these devices are error prone and also hard to get continuous control because of the dynamic nature of the brain signals. Moreover, existing devices like exoskeletons brings additional burden on the patient and the caregivers and also results in mental fatigue and frustration. To solve these issues Artificial Muscle Intelligence with Deep Learning (AMIDL) system is proposed in this paper. AMIDL integrates user intentions with artificial muscle movements in an efficient way to improve the performance. Human thoughts captured using Electroencephalogram (EEG) sensors are transformed into body movements, by utilizing microcontroller and Transcutaneous Electrical Nerve Stimulation (TENS) device. EEG signals are subjected to pre-processing, feature extraction and classification, before being passed on to the affected body part. The received EEG signal is correlated with the recorded artificial muscle movements. If the captured EEG signal falls below the desired level, the affected body part will be stimulated by the recorded artificial muscle movements. The system also provides a feature for communicating human intentions as alert message to caregivers, in case of emergency situations. This is achieved by offline training of specific gesture and online gesture recognition algorithm. The recognized gesture is transformed into speech, thus enabling the paralyzed to express their feelings to the relatives or friends. Experiments were carried out with the aid of healthy and paralyzed subjects. The AMIDL system helped to reduce mental fatigue, miss-operation, frustration and provided continuous control. The thrust of lifting the exoskeleton is also reduced by using light weight wireless electrodes. The proposed system will be a great communication aid for paralyzed to express their thoughts and feelings with dear and near ones, thereby enhancing the quality of life.", 
This work investigates user attitudes towards personalized summaries. It uses a coarse-grained user model based on document aspects.,Aspect-Based Personalized Text Summarization,"This work investigates user attitudes towards personalized summaries generated from a coarse-grained user model based on document aspects. We explore user preferences for summaries at differing degrees of fit with their stated interests, the impact of length on user ratings, and the faithfulness of personalized and general summaries.", 
"Text summarization is the process of automatically creating a shorter version of one or more text documents. This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature. Three different datasets (News, Blogs and Article contexts) were evaluated.",Assessing sentence scoring techniques for extractive text summarization,"Text summarization is the process of automatically creating a shorter version of one or more text documents. It is an important way of finding relevant information in large text libraries or in the Internet. Essentially, text summarization techniques are classified as Extractive and Abstractive. Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. Abstractive summaries attempt to improve the coherence among sentences by eliminating redundancies and clarifying the contest of sentences. In terms of extractive summarization, sentence scoring is the technique most used for extractive text summarization. This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature. Three different datasets (News, Blogs and Article contexts) were evaluated. In addition, directions to improve the sentence extraction results obtained are suggested.", 
"Automatic text summarization can be useful to sieve relevant content from the Internet and digital libraries. However, it may not fully capture the informativeness of a text. A potential strategy to address this problem is the adoption of sentence simplification methods.",Assessing Sentence Simplification Methods Applied to Text Summarization,"Automatic text summarization is proving itself useful to sieve relevant content from the Internet and digital libraries with reduced human effort. Nevertheless, extractive summarization approaches have limitations, possibly not fully capturing the informativeness of a text. A potential strategy to address this problem is the adoption of sentence simplification methods. This work focuses on the evaluation of sentence simplification methods as a preprocessing step for extractive text summarization in order to answer the question of whether sentence simplification increases the informativeness of extractive summaries. Four different sentence simplification methods, two being simple filters and the other two performing rule-based transformations, are assessed here in order to point out the best method for such a purpose. Fifteen sentence scoring methods for summarization are applied in combination with the simplification methods to a corpus of 1,038 news articles in English. The results suggest that the transformation approaches, which take into account linguistic features and grammaticality, achieve the best performance.", 
"The volume of text data has been growing exponentially in the last years, mainly due to the Internet. Automatic Text Summarization has emerged as an alternative to help users find relevant information. This paper presents a comparative analysis of eighteen shallow sentence scoring techniques to compute the importance of a sentence.",Assessing shallow sentence scoring techniques and combinations for single and multi-document summarization,"The volume of text data has been growing exponentially in the last years, mainly due to the Internet. Automatic Text Summarization has emerged as an alternative to help users find relevant information in the content of one or more documents. This paper presents a comparative analysis of eighteen shallow sentence scoring techniques to compute the importance of a sentence in the context of extractive single and multi-document summarization. Several experiments were made to assess the performance of such techniques individually and applying different combination strategies. The most traditional benchmark on the news domain demonstrates the feasibility of combining such techniques, in most cases outperforming the results obtained by isolated techniques. Combinations that perform competitively with the state-of-the-art systems were found.", 
"Study explored classification of electroencephalography (EEG) signals to assess changes in neural activity. 15 participants acquired spatial knowledge via 60 navigation trials in a virtual environment. Time performance, perceived certainty, and EEG signal data were collected.",Assessment of changes in neural activity during acquisition of spatial knowledge using EEG signal classification,"This study explored the classification of electroencephalography (EEG) signals to assess changes in neural activity as individuals performed a training task in a virtual environment simulator. Commonly, task behavior and perception are used to assess a trainee’s ability to perform a task, however, changes in cognition are not usually measured and could be important to provide a true indication of an individual’s level of knowledge or skill. In this study, 15 participants acquired spatial knowledge via 60 navigation trials (divided into 10 blocks) in a novel virtual environment. Time performance, perceived certainty, and EEG signal data were collected. A significant increase in alpha power and classification accuracy of EEG data from block 1 against blocks 2-10 was observed and stabilized after block 7, while time performance and perceived certainty measures improved and stabilized after block 5 and 6, respectively. Results suggest that changes in neural activity, which may reflect an increase in cognitive efficiency, could provide additional insight beyond time performance and perceived certainty.", 
"Indoor environmental quality (IEQ) has an impact on health, cognitive performance and productivity. This study used environmental temperature, humidity, air pressure, and CO2 sensor data collected during 3.5–7 months in an office and a school facility. Self-reported data from 15 office workers and four teachers was used to train person-specific SVM classifier models.","Assessment of perceived indoor environmental quality, stress and productivity based on environmental sensor data and personality categorization","Indoor environmental quality (IEQ) has an in?uence on peoples’ health, cognitive performance and productivity in school and office environments. This study used environmental temperature, humidity, air pressure, and CO2 sensor data collected during 3.5–7 months in an office and a school facility to classify occupants’ perceptions of IEQ, stress and productivity in two classes, “negative” and “positive”. Self-reported data from 15 office workers and four teachers were used to train person-specific SVM classifier models. Relatively high accuracies were achieved in classifying IEQ (84%), stress (88%) and productivity (92%) using different combinations of environmental sensor data. Furthermore, the associations between the Big Five personality trait variables (neuroticism, extraversion, openness, agreeableness and conscientiousness) and negative experiences regarding stress, productivity and IEQ were investigated. Positive correlation was found between extroversion and co-occurring stress and IEQ problems, which suggest that more extroverted people more likely to be stressed by insufficient environmental quality or to be more sensitive to environmental factors when under stress. Overall, the results indicate that it is possible to measure and classify perceived IEQ, stress and productivity sufficiently accurately using inexpensive environmental sensors.", 
"An intelligent text summarization is one of the most challenging tasks in Natural language processing. This paper presents an automatic text summarizer for text documents using soft computing approach. It processes data through POS Tagger, NLP Parser, ambiguity removal, Semantic Representation, Sentence Reduction and Sentence Combination. The summarizer was tested on the standard DUC 2007 dataset as well as a corpus of hundred text documents.","ATSSC, Development of an approach based on soft computing for text summarization","Natural Language Processing (NLP) is a field of computer science and linguistics, concerned with the unique conversation between computers and human languages. It processes data through Lexical analysis, Syntax analysis, Semantic analysis, Discourse processing and Pragmatic analysis. An intelligent text summarization is one of the most challenging tasks in Natural language processing. It can be further used for applications like storytelling and question answering. This paper presents an automatic text summarizer for text documents using soft computing approach, consisting of SVO (Subject, Verb, and Object) Rules and Tag based training. This approach processes data through POS Tagger, NLP Parser, ambiguity removal, Semantic Representation, Sentence Reduction and Sentence Combination. At first, this paper defines the theme (title) of the document. After this operation, it preprocesses text document to perform pronominal reference resolution and text clustering. After these preprocessing operations it identifies and removes ambiguity from the language using parser. And then, it calculates the score for the sentences using the title of the document, Semantic Sentence Similarity utility and n-gram Co-Occurrence relations of the words in a particular sentence. At last, sentences are combined with the SVO Rules after providing tag based training for simple and complex sentences. The summarizer was tested on the standard DUC 2007 dataset as well as a corpus of hundred text documents of different domains created by us. DUC 2007 Update Task produced accuracy F-scores of 0.13523 (ROUGE-2) and 0.112561(ROUGE-SU4) for DUC 2007 documents and 0.4036 (ROUGE-2) and 0.3129 (ROUGE-SU4) for our corpus. Subjective evaluation was carried out by five language experts and twenty random individuals for system generated sample summaries.", 
"Text Summarization is condensing of text such that, redundant data are removed and important information is extracted. Opinions play a pivotal role in decision making in the society. In this paper, we propose a graph based technique that generates summaries of redundant opinions.","ATSSI, Abstractive Text Summarization using Sentiment Infusion","Text Summarization is condensing of text such that, redundant data are removed and important information is extracted and represented in the shortest way possible. With the explosion of the abundant data present on social media, it has become important to analyze this text for seeking information and use it for the advantage of various applications and people. From past few years, this task of automatic summarization has stirred the interest among communities of Natural Language Processing and Text Mining, especially when it comes to opinion summarization. Opinions play a pivotal role in decision making in the society. Other’s opinions and suggestions are the base for an individual or a company while making decisions. In this paper, we propose a graph based technique that generates summaries of redundant opinions and uses sentiment analysis to combine the statements. The summaries thus generated are abstraction based summaries and are well formed to convey the gist of the text.", 
"Recurrent sequence generators have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances.",Attention-Based Models for Speech Recognition,"Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", 
An attentive encoder-based summarization (AES) model is used to generate article summaries. It can generate a rich document representation by considering both the global information of a document and the relationships of sentences in the document. experimental results show that Bi-AES outperforms Uni-Aes.,Attentive Encoder-based Extractive Text Summarization,"In previous work on text summarization, encoder-decoder architectures and attention mechanisms have both been widely used. Attention-based encoder-decoder approaches typically focus on taking the sentences preceding a given sentence in a document into account for document representation, failing to capture the relationships between a sentence and sentences that follow it in a document in the encoder. We propose an attentive encoder-based summarization (AES) model to generate article summaries. AES can generate a rich document representation by considering both the global information of a document and the relationships of sentences in the document. A unidirectional recurrent neural network (RNN) and a bidirectional RNN are considered to construct the encoders, giving rise to unidirectional attentive encoder-based summarization (Uni-AES) and bidirectional attentive encoder-based summarization (Bi-AES), respectively. Our experimental results show that Bi-AES outperforms Uni-AES. We obtain substantial improvements over a relevant start-of-the-art baseline.", 
"The authors present two novel methods for face verification. Neither method requires costly, often brittle, alignment between image pairs, they say. Authors: Both methods produce compact visual descriptions, and work on real-world images. They say their methods improve on the current state-of-the-art for the LFW data set.",Attribute and Simile Classifiers for Face Verification,"We present two novel methods for face verification. Our first method – “attribute” classifiers – uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method – “simile” classifiers – removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set – termed PubFig – of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.", 
"AttSum is a novel summarization system that learns distributed representations for sentences and documents. It applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Without using any hand-crafted features, AttSum achieves competitive performance.","AttSum, Joint Learning of Focusing and Summarization with Neural Attention","Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. We also observe that the sentences recognized to focus on the query indeed meet the query need.", 
Electroencephalography (EEG) is one of the most promising methods in the field of Brain-Computer Interfaces (BCIs) One of the major challenges for EEG signal analysis is the small size of its datasets. This challenge can limit the performance of EEG signal classification models.,Augmenting The Size of EEG datasets Using Generative Adversarial Networks,"Electroencephalography (EEG) is one of the most promising methods in the field of Brain-Computer Interfaces (BCIs) due to its rich time-domain resolution and the availability of advanced and portable sensor technology. One of the major challenges for EEG signal analysis is the small size of its datasets as it is usually demanding for human subjects to perform lengthy experiments. Consequently, this challenge can limit the performance of EEG signal classification models. In this paper, we propose a novel generative adversarial network (GAN) model that can learn the statistical characteristics of the EEG signal and augment its datasets size to enhance the performance of classification models. Results show that the proposed model significantly outperforms other generative models on the utilized EEG dataset. Furthermore, it significantly enhances the performance of classification models working on small size EEG datasets after augmenting them with generated samples.", 
"VANET is vulnerable to various types of security attacks, the IoV structure should ensure security and efficient performance for vehicular communications. The authentication-based protocol (A-MAC) for smart vehicular communication is proposed. The scheme requires hash operations and uses cryptographic concepts to transfer messages between vehicles.",Authentication-Based Secure Data Dissemination Protocol and Framework for 5G-Enabled VANET,"The amalgamation of Vehicular Ad hoc Network (VANET) with the Internet of Things (IoT) leads to the concept of the Internet of Vehicles (IoV). IoV forms a solid backbone for Intelligent Transportation Systems (ITS), which paves the way for technologies that better explain about traffic efficiency and their management applications. IoV architecture is seen as a big player in different areas such as the automobile industry, research organizations, smart cities and intelligent transportation for various commercial and scientific applications. However, as VANET is vulnerable to various types of security attacks, the IoV structure should ensure security and efficient performance for vehicular communications. To address these issues, in this article, an authentication-based protocol (A-MAC) for smart vehicular communication is proposed along with a novel framework towards an IoV architecture model. The scheme requires hash operations and uses cryptographic concepts to transfer messages between vehicles to maintain the required security. Performance evaluation helps analyzing its strength in withstanding various types of security attacks. Simulation results demonstrate that A-MAC outshines other protocols in terms of communication cost, execution time, storage cost, and overhead.", 
"Autism Spectrum Disorder (ASD) is a neurodevelopmental that impact the social interaction and communication skills. The promising method to perform the classification is through a deep learning algorithm, which is currently a well-known and superior method in the pattern recognition field.",Autism spectrum disorder classification on electroencephalogram signal using deep learning algorithm,"Autism Spectrum Disorder (ASD) is a neurodevelopmental that impact the social interaction and communication skills. Diagnosis of ASD is one of the difficult problems facing researchers. This research work aimed to reveal the different pattern between autistic and normal children via electroencephalogram (EEG) by using the deep learning algorithm. The brain signal database used pattern recognition where the extracted features will undergo the multilayer perceptron network for the classification process. The promising method to perform the classification is through a deep learning algorithm, which is currently a well-known and superior method in the pattern recognition field. The performance measure for the classification would be the accuracy. The higher percentage means the more effectiveness for the ASD diagnosis. This can be seen as the ground work for applying a new algorithm for further development diagnosis of autism to see how the treatment is working as well in future.", 
"Modern organizations deals with terabytes of text, such as email, that often plays a significant role in their day-to-day operations. Identifying useful information from these data is quite difficult and requires some mechanism. One possible means is to use text categorization and summarization.",Auto text summarization with categorization and sentiment analysis,"In today’s world the volume of information is dramatically increasing, and the value of that information is growing fast. Modern organizations deals with terabytes of text, such as email, that often plays a significant role in their day-to-day operations. Even small and medium-sized organizations are dealing with growing volumes of text that require rapid access and meaningful analysis. Identifying useful information from these data is quite difficult and requires some mechanism. One possible means is to use text categorization and summarization. Text categorization is automatically arranging a set of documents into predefined categories and Summarization is a giving a condensed and precise depiction of input data such that the output includes the most significant concepts of the source. Sentiment analysis i.e. opinion mining states the use of NLP, text analysis and to identify and extract biased information in source materials.", 
"Arabic news articles in electronic collections are difficult to work with. Browsing by category is rarely supported. We developed tailored stemming (i.e., a new Arabic light stemmer) and automatic classification methods. We showed that our approach to stemming and classification is superior to state-of-the-art techniques.","Automated Arabic Text Classification with P-Stemmer, Machine Learning, and a Tailored News Article Taxonomy","Arabic news articles in electronic collections are difficult to work with. Browsing by category is rarely supported. While helpful machine learning methods have been applied successfully to similar situations for English news articles, limited research has been completed to yield suitable solutions for Arabic news. In connection with a QNRF funded project to build digital library community and infrastructure in Qatar, we developed software for browsing a collection of about 237K Arabic news articles, which should be applicable to other Arabic news collections as well. We designed a simple taxonomy for Arabic news stories that is suitable for the needs in Qatar and other nations, is compatible with the subject codes of the International Press Telecommunications Council, and was enhanced with the aid of a librarian expert as well as five Arabic-speaking volunteers. We developed tailored stemming (i.e., a new Arabic light stemmer) and automatic classification methods (the best being binary SVM classifiers) to work with the taxonomy. Using evaluation techniques commonly used in the information retrieval community, including 10-fold cross-validation and the Wilcoxon signed-rank test, we showed that our approach to stemming and classification is superior to state-of-the-art techniques.", 
A new technique has been proposed for Bangla text summarization. The system summarizes a single document at a time. Attributes like cue words and skeleton of the document are included in the process to make the summary more relevant to the content. The proposed technique was compared with summary of documents generated by humans.,Automated Bangla text summarization by sentence scoring and ranking,"In Natural Language Processing (NLP) the document summarization is an area that is getting interest of modern researchers. Though there are many techniques that have been proposed for English language but a few notable works have been done for Bangla text summarization. This paper deals with the development of an extraction based summarization technique which works on Bangla text documents. The system summarizes a single document at a time. Before creating the summary of a document, it is pre-processed by tokenization, removal of stop words and stemming. In the document summarization process, the countable features like word frequency and sentence positional value are used to make the summary more precise and concrete. Attributes like cue words and skeleton of the document are included in the process, which help to make the summary more relevant to the content of the document. The proposed technique has been compared with summary of documents generated by human professionals. The evaluation shows that 83.57% of summary sentences selected by the system agreed with those made by human.", 
"We target the problem of software bug reports classification. Our main aim is to build a classifier that is capable of classifying newly incoming bug reports into two predefined classes. This helps maintainers to quickly understand these bug reports and hence, allocate resources for each category.",Automated Classification of Software Bug Reports,"We target the problem of software bug reports classification. Our main aim is to build a classifier that is capable of classifying newly incoming bug reports into two predefined classes: corrective (defect fixing) report and perfective (major maintenance) report. This helps maintainers to quickly understand these bug reports and hence, allocate resources for each category. For this purpose, we propose a distinctive feature set that is based on the occurrences of certain keywords. The proposed feature set is then fed into a number of classification algorithms for building a classification model. The results of the proposed feature set achieved high accuracy in classification with SVM classification algorithm reporting an average accuracy of (93.1%) on three different open source projects.", 
"Software developers often misclassify an improvement request as a bug and vice versa. Automated classification of the submitted reports would be of great practical utility. In this paper, we analyze how machine learning techniques may be used to perform this task.",Automated classification of software issue reports using machine learning techniques an empirical study," Software developers, testers and customers routinely submit issue reports to software issue trackers to record the problems they face in using a software. The issues are then directed to appropriate experts for analysis and fixing. However, submitters often misclassify an improvement request as a bug and vice versa. This costs valuable developer time. Hence automated classification of the submitted reports would be of great practical utility. In this paper, we analyze how machine learning techniques may be used to perform this task. We apply different classification algorithms, namely naive Bayes, linear discriminant analysis, k-nearest neighbors, support vector machine (SVM) with various kernels, decision tree and random forest separately to classify the reports from three open-source projects. We evaluate their performance in terms of F-measure, average accuracy and weighted average F-measure. Our experiments show that random forests perform best, while SVM with certain kernels also achieve high performance.", 
"Depression affects large number of people across the world and it is considered as a global problem. It is a mood disorder which can be detected using electroencephalogram (EEG) signals. The manual detection of depression by analyzing the EEG signals requires lot of experience, tedious and time consuming.",Automated Depression Detection Using Deep Representation and Sequence Learning with EEG Signals,"Depression affects large number of people across the world today and it is considered as the global problem. It is a mood disorder which can be detected using electroencephalogram (EEG) signals. The manual detection of depression by analyzing the EEG signals requires lot of experience, tedious and time consuming. Hence, a fully automated depression diagnosis system developed using EEG signals will help the clinicians. Therefore, we propose a deep hybrid model developed using convolutional neural network (CNN) and long-short term memory (LSTM) architectures to detect depression using EEG signals. In the deep model, temporal properties of the signals are learned with CNN layers and the sequence learning process is provided through the LSTM layers. In this work, we have used EEG signals obtained from left and right hemispheres of the brain. Our work has provided 99.12% and 97.66% classification accuracies for the right and left hemisphere EEG signals respectively. Hence, we can conclude that the developed CNN-LSTM model is accurate and fast in detecting the depression using EEG signals. It can be employed in psychiatry wards of the hospitals to detect the depression using EEG signals accurately and thus aid the psychiatrists.", 
Epilepsy is a neural disorder that is associated with the central nervous system. Electroencephalograms (EEG) are widely used to detect epilepsy accurately. The interpretation of a particular type of abnormality using the EEG signal is a subjective affair and may vary from clinician-to-clinician. The proposed methodology focuses on automated detection of epilepsy using a novel stop-band energy filter bank.,Automated detection of abnormal EEG signals using localized wavelet filter banks,"Epilepsy is a neural disorder that is associated with the central nervous system (CNS) in which the brain activity sometimes becomes abnormal, which may lead to seizures, loss of awareness, unusual sensations, and behavior. Electroencephalograms (EEG) are widely used to detect epilepsy accurately. However, the interpretation of a particular type of abnormality using the EEG signal is a subjective affair and may vary from clinician-to-clinician. Visual inspection of the EEG signal by observing a change in frequency or amplitude in long-duration signals is an arduous task for the clinicians. It may lead to an erroneous classification of EEGs. The proposed methodology focuses on automated detection of epilepsy using a novel stop-band energy (SBE) minimized orthogonal wavelet filter bank. Using the wavelet decomposition, we obtain subbands (SBs) of EEG signals. Subsequently, fuzzy entropy, logarithmic of the squared norm, and fractal dimension are computed for each SB. The different combinations of the extracted features were supplied to various classifiers for the classification of normal and abnormal EEG signals. In the proposed method, we have used a single-channel EEG dataset of Temple University Hospital. The dataset is the most substantial EEG data publicly available, which contains an EEG recording of 2130 distinct subjects. Our proposed system obtained the highest classification accuracy (CACC) of 78.4% and 79.34% during training and evaluation using the SVM classifier. We achieved the highest F1-score of 0.88.", 
Malaria detection through microscopic examination of stained blood smears is a diagnostic challenge. This paper presents an automated analysis method for detection and staging of red blood cells infected by the malaria parasite Plasmodium falciparum. The method uses quantitative phase images of unstained cells to detect and stage malaria infection without staining or expert analysis. It has a high accuracy of up to 99.7% in detecting and staging infected cells.,Automated Detection of P. falciparum Using MachineLearningAlgorithms with Quantitative Phase Images of Unstained Cells,"Malaria detection through microscopic examination of stained blood smears is a diagnostic challenge that heavily relies on the expertise of trained microscopists. This paper presents an automated analysis method for detection and staging of red blood cells infected by the malaria parasite Plasmodium falciparum at trophozoite or schizont stage. Unlike previous efforts in this area, this study uses quantitative phase images of unstained cells. Erythrocytes are automatically segmented using thresholds of optical phase and refocused to enable quantitative comparison of phase images. Refocused images are analyzed to extract 23 morphological descriptors based on the phase information. While all individual descriptors are highly statistically different between infected and uninfected cells, each descriptor does not enable separation of populations at a level satisfactory for clinical utility. To improve the diagnostic capacity, we applied various machine learning techniques, including linear discriminant classification (LDC), logistic regression (LR), and k-nearest neighbor classification (NNC), to formulate algorithms that combine all of the calculated physical parameters to distinguish cells more effectively. Results show that LDC provides the highest accuracy of up to 99.7% in detecting schizont stage infected cells compared to uninfected RBCs. NNC showed slightly better accuracy (99.5%) than either LDC (99.0%) or LR (99.1%) for discriminating late trophozoites from uninfected RBCs. However, for early trophozoites, LDC produced the best accuracy of 98%. Discrimination of infection stage was less accurate, producing high specificity (99.8%) but only 45.0%-66.8% sensitivity with early trophozoites most often mistaken for late trophozoite or schizont stage and late trophozoite and schizont stage most often confused for each other. Overall, this methodology points to a significant clinical potential of using quantitative phase imaging to detect and stage malaria infection without staining or expert analysis.", 
"The purpose of this study was to develop an Automated Diagnostic Tool (ADT) to investigate and classify the EEG signal patterns into normal and schizophrenia classes. The ADT implements a sequence of events, such as EEG series splitting, non-linear features mining, t-test assisted feature selection, classification and validation.",Automated detection of schizophrenia using nonlinear signal processing methods,"Examination of the brain’s condition with the Electroencephalogram (EEG) can be helpful to predict abnormality and cerebral activities. The purpose of this study was to develop an Automated Diagnostic Tool (ADT) to investigate and classify the EEG signal patterns into normal and schizophrenia classes. The ADT implements a sequence of events, such as EEG series splitting, non-linear features mining, t-test assisted feature selection, classification and validation. The proposed ADT is employed to evaluate a 19-channel EEG signal collected from normal and schizophrenia class volunteers. A dataset was created by splitting the raw 19-channel EEG into a sequence of 6250 sample points, which was helpful to produce 1142 features of normal and schizophrenia class patterns. Non-linear feature extraction was then implemented to mine 157 features from each EEG pattern, from which 14 of the principal features were identified based on significance. Finally, a signal classification practice with Decision-Tree (DT), Linear-Discriminant analysis (LD), k-Nearest-Neighbour (KNN), Probabilistic-Neural Network (PNN), and Support-Vector-Machine (SVM) with various kernels was implemented. The experimental outcome showed that the SVM with Radial-Basis-Function (SVM-RBF) offered a superior average performance value of 92.91% on the considered EEG dataset, as compared to other classifiers implemented in this work.", 
"It is proven that EEG signals are the best markers for diagnosis of the epileptic seizures. In this paper, we used a deep learning technique based on multilayer perceptrons to improve the accuracy of seizure detection.",Automated EEG-Based Epileptic Seizure Detection Using Deep Neural Networks,"Millions of people around the world suffer from epilepsy. It is very important to provide a method to efficiently monitor the seizures and alert the caregivers to help patients. It is proven that EEG signals are the best markers for diagnosis of the epileptic seizures. In this paper, we used the frequency domain features (normalized in-band power spectral density) to extract information from EEG signals. We applied a deep learning technique based on multilayer perceptrons to improve the accuracy of seizure detection. The results indicate that our nonlinear technique is able to efficiently and automatically detect seizure and non-seizure episodes with an F-measure accuracy of around 95%.", 
"In this paper, a novel computer model is presented for EEG-based screening of depression using a deep neural network machine learning approach, known as Convolutional Neural Network (CNN) The proposed technique does not require a semi-manually-selected set of features to be fed into a classifier for classification. The model was tested using EEGs obtained from 15 normal and 15 depressed patients.",Automated EEG-based Screening of Depression Using Deep Convolutional Neural Network,"In recent years, advanced neurocomputing and machine learning techniques have been used for Electroencephalogram (EEG)-based diagnosis of various neurological disorders. In this paper, a novel computer model is presented for EEG-based screening of depression using a deep neural network machine learning approach, known as Convolutional Neural Network (CNN). The proposed technique does not require a semi-manually-selected set of features to be fed into a classifier for classification. It learns automatically and adaptively from the input EEG signals to differentiate EEGs obtained from depressive and normal subjects. The model was tested using EEGs obtained from 15 normal and 15 depressed patients. The algorithm attained accuracies of 93.5% and 96.0% using EEG signals from the left and right hemisphere, respectively. It was discovered in this research that the EEG signals from the right hemisphere are more distinctive in depression than those from the left hemisphere. This discovery is consistent with recent research and revelation that the depression is associated with a hyperactive right hemisphere. An exciting extension of this research would be diagnosis of different stages and severity of depression and development of a Depression Severity Index (DSI).", 
This paper describes an automated classification of emotions-labeled EEG signals using nonlinear higher order statistics and deep learning algorithm. The study is carried out with the web-available DEAP dataset that yields 82.01% average classification accuracy. The proposed algorithm has the potential for accurate and rapid recognition of human emotions.,Automated emotion recognition based on higher order statistics and deep learning algorithm,"The objective of this paper is online recognition of human emotions based on electroencephalogram (EEG) signals. The emotions are originated from the central and peripheral nervous systems. Hence, it can be adequately characterized by the EEG signal, as it directly reflects changes in the human emotional states. This paper describes an automated classification of emotions-labeled EEG signals using nonlinear higher order statistics and deep learning algorithm. The discrete wavelet transform is used to decompose the studied signal into sub-bands, known as rhythms of the EEG signal. The third-order cumulants (ToC) are used to explore the nonlinear dynamics of each sub-band signal in higher dimensional space. The data in the higher dimensional space contain repeated and redundant information due to presence of various symmetries in the ToC. Hence, an evolutionary data reduction technique, namely, the particle swarm optimization, is employed to get rid of irrelevant information. The long short-term memory based deep learning technique is used to retrieve the emotion variation from the optimized data corresponding to the labeled EEG signals. This study is carried out with the web-available DEAP dataset that yields 82.01% average classification accuracy with 10-fold cross-validation technique corresponding to four-labeled emotions classes. The achieved results have confirmed that the proposed algorithm has the potential for accurate and rapid recognition of human emotions.", 
"Summarization is a compressing technique of the original text to form a summary. A lot of work has been done in major languages like English, Chinese, etc. whereas less work on Hindi. In this paper, a comparative study is done for text summarization using Term Frequency – Inverse Document Frequency and TextRank algorithms, for the Hindi language.",AUTOMATED HINDI TEXT SUMMARIZATION USING TF-IDF AND TEXTRANK ALGORITHM,"Text Summarization is a compressing technique of the original text to form a summary which will provide the same meaning and information as provided by the original text. Summarizer helps in saving time and increasing efficiency. Summarization can be done using any of the two approaches: abstractive text summarization and extractive text summarization. Abstractive summarization refers to recreate the whole document in a few words or lines, which may include new words as well while extractive summarization refers to extract the important words or lines from the original document. A lot of work has been done in major languages like English, Chinese, etc. whereas less work has been done on Hindi. In this paper, a comparative study is done for text summarization using Term Frequency – Inverse Document Frequency and TextRank algorithms, for the Hindi language on a single document. The idea of using the extractive approach is to provide a summary by selecting the high ranking sentences from the input document. In Term Frequency – Inverse Document Frequency, a numerical value is assigned to each sentence and TextRank is a graph-based approach, which uses graphs for ranking the sentences.", 
"Comment-on (CON), a MEDLINE citation field, indicates previously published articles commented on by authors of a given article. Paper presents a general method for extracting ""citation sentences"" in the body text of online biomedical articles using a support vector machine (SVM)-based text summarization technique. A rule-based post-processing step is also introduced to further reduce false negative errors in detecting ""citations sentences"".",Automated Method for Extracting “Citation Sentences” from Online Biomedical Articles Using SVM-based Text Summarization Technique,"Comment-on (CON), a MEDLINE citation field, indicates previously published articles commented on by authors of a given article expressing possibly complimentary or contradictory opinions. Our idea of identifying the CON list for a given article is to first extract all “citation sentences” from the body text, and then to recognize the sentences (“CON sentences”) among these that mention CON articles and to analyze the corresponding bibliographic data in the reference section. As a preprocessing step for identifying the CON list, this paper presents a general method for extracting “citation sentences” in the body text of online biomedical articles using a support vector machine (SVM)-based text summarization technique. Input feature vectors for the SVM are created by combining four types of features: 1) word statistics representing how differently a word occurs in “citation sentences” compared to other sentences, and the existence of 2) author names, 3) publication years, and 4) citation tags in a sentence. A rule-based post-processing step is also introduced to further reduce false negative errors in detecting “citation sentences”. Experiments on a set of online biomedical articles show that a SVM with a RBF achieves good performance overall in terms of accuracy, precision, recall, and F-measure rates. Our experiments also show that errors in extracting “citation sentences” cause a minor degradation of performance in identifying CON sentences, but can be improved through the proposed rule-based post-processing.", 
"This paper represents our approach which we used in our Automated Text Summarization System known as MDSS. For sentence comparison, Jaccard’s coefficient is used to improve the worth and quality of the summarization. Resemblance exists between our algorithms and dynamic time warping.",Automated Multiple Related Documents Summarization via Jaccard's Coefficient,"Today, in the hasty advancement epoch of technology, allotting and gathering of information are imperative. Readers enthrall with an undersized edition of copious prolonged text documents. In this paper, we represent our approach which we used in our Automated Text Summarization System known as MDSS (Multiple Documents Summarization System). We elucidate a new fangled approach which is based on statistical (rather than semantic) factors. In contrast to single document summarization, the issues of compression, speediness, superfluous and passage opting are more decisive in multiple documents summarization. For sentence comparison, Jaccard?s coefficient is used to improve the worth and quality of the summarization. Resemblance exists between our algorithms and dynamic time warping. Our experimental domino effects indicate that it is useful and effectual to enhance the quality of multiple documents summarization via Jaccard?s coefficient. Our system MDSS is implemented in Java (jdk 1.6).", 
Summarization techniques can be very useful in improving the effectiveness of Web search. The proposed system incorporates the structure of the documents into the output summaries. The system also uses natural language processing techniques for summarization purposes.,Automated query-biased and structure-preserving text summarization on web documents,"Automatic summarization has become an important application recently due to the increased amount of information available on the Web. Summarization techniques can be very useful in improving the effectiveness of Web search. However, the available search engines, such as Google, only display short extracts under the search results, e.g. two lines of text fragments which consist of the query words and their surrounding text. In this paper, we investigate novel summarization techniques to improve the effectiveness of search engines. The proposed system incorporates the structure of the documents, namely the sectional hierarchy, into the output summaries. Different from the previous work, both the structural information and the content to be displayed in the summary are selected in a query-biased way. The system also uses natural language processing techniques for summarization purposes such as identification of phrases as better content carriers than single words.", 
"Previous automated methods such as ROUGE compare using fixed word ngrams, which are not ideal for a variety of reasons. This method is tested on DUC 2003, 2004, and 2005 systems and produces very good correlations with human judgments.",Automated Summarization Evaluation with Basic Elements,"As part of evaluating a summary automatically, it is usual to determine how much of the contents of one or more human produced ‘ideal’ summaries it contains. Previous automated methods such as ROUGE compare using fixed word ngrams, which are not ideal for a variety of reasons. In this paper we describe a framework in which summary evaluation measures can be instantiated and compared, and we implement a specific evaluation method using very small units of content, called Basic Elements, that address some of the shortcomings of ngrams. This method is tested on DUC 2003, 2004, and 2005 systems and produces very good correlations with human judgments.", 
The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. The Lexical cohesion structure of the text can be exploited to determine the importance of a sentence/phrase. Lexical chains are useful tools to analyze the lexical cohesion Structure in a text.,Automated text summarization base on lexicales chain and graph using of wordnet and wikipedia knowledge base,"The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval. With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Document summarization is a process of automatically creating a compressed version of a given document that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. The lexical cohesion structure of the text can be exploited to determine the importance of a sentence/phrase. Lexical chains are useful tools to analyze the lexical cohesion structure in a text .In this paper we consider the effect of the use of lexical cohesion features in Summarization, And presenting a algorithm base on the knowledge base. Ours algorithm at first find the correct sense of any word, Then constructs the lexical chains, remove Lexical chains that less score than other ,detects topics roughly from lexical chains, segments the text with respect to the topics and selects the most important sentences. The experimental results on an open benchmark datasets from DUC01 and DUC02 show that our proposed approach can improve the performance compared to sate-of-the-art summarization approaches.", 
"In a scientific work, an abstract always contains main information of an article including at least a researched problem, aim(s), methodology, and result of the study. People generally write manually by summarizing the article. This study is constructing automation for summarizing Indonesian articles as an alternative approach to an abstract.",Automated Text Summarization for Indonesian Article Using Vector Space Model,"In a scientific work, an abstract always contains main information of an article including at least a researched problem, aim(s), methodology, and result of the study. Writing an abstract requires a conscientious analysis since the contents would affect both the readers’ interestedness and disinterestedness on a particular or overall research topic. However, people generally write manually by summarizing the article. The aim of this study is constructing automation for summarizing Indonesian articles as an alternative approach to an abstract. This is involving two methods to summarize an article. A Term Frequency-inverse Document Frequency is used to get a keyword and weight terms, and a Vector Space Model is utilized to represent abstract text into a vector that used to identify the linkage of documents. From this method, the result of the summary can be generated from documents. Supporting this research, we used several journal articles written by a manual abstract. The results of this application show that the automatic summarization produces a paragraph which consists of more than three same sentences constantly as compared to manual paragraphing.", 
"The most well known and widely used technique for automated text summarization is sentence extraction technique. In this approach, a sentence is refined; unimportant words or phrases are omitted. A summarization tool has been developed based on the proposed approach.","Automated Text Summarization, Sentence Refinement Approach","Automated text summarization is a process of deriving a shorter version of a text document from an original text. The most well known and widely used technique for automated text summarization is sentence extraction technique. Using this technique, sentences are extracted based on certain features that have been decided. In this paper, a new technique called sentence refinement is introduced as an improvement of the technique. In this approach, a sentence is refined; unimportant words or phrases exist in the extracted sentences are omitted. A summarization tool has been developed based on the proposed approach. The tool was tested using English and Malay texts. Extrinsic and intrinsic measurement methods have been used in evaluating generated summaries. Results show the proposed approach is promising.", 
"Over- and under-sedation are common in the ICU, and contribute to poor ICU outcomes including delirium. Here we present a deep learning model based on a combination of convolutional and recurrent neural networks that automatically tracks both the level of consciousness and delirium using frontal EEG signals. The system achieves a median accuracy of 70% when allowing prediction to be within one RASS level difference across all patients.",Automated tracking of level of consciousness and delirium in critical illness using deep learning,"Over- and under-sedation are common in the ICU, and contribute to poor ICU outcomes including delirium. Behavioral assessments, such as Richmond Agitation-Sedation Scale (RASS) for monitoring levels of sedation and Confusion Assessment Method for the ICU (CAM-ICU) for detecting signs of delirium, are often used. As an alternative, brain monitoring with electroencephalography (EEG) has been proposed in the operating room, but is challenging to implement in ICU due to the differences between critical illness and elective surgery, as well as the duration of sedation. Here we present a deep learning model based on a combination of convolutional and recurrent neural networks that automatically tracks both the level of consciousness and delirium using frontal EEG signals in the ICU. For level of consciousness, the system achieves a median accuracy of 70% when allowing prediction to be within one RASS level difference across all patients, which is comparable or higher than the median technician–nurse agreement at 59%. For delirium, the system achieves an AUC of 0.80 with 69% sensitivity and 83% specificity at the optimal operating point. The results show it is feasible to continuously track level of consciousness and delirium in the ICU.", 
"Software vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We developed a fast and scalable vulnerability detection tool based on deep feature representation learning. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark.",Automated Vulnerability Detection in Source Code Using Deep Representation Learning,"Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a largescale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.", 
"As codebases for software projects get larger, the need for computer-aided program comprehension grows. Most research in this area is making use of pattern matching, which involves much human effort. This paper proposes to let go of defined patterns, and make use of simpler features.",Automatic Algorithm Recognition of Source-Code Using Machine Learning,"As codebases for software projects get larger, reaching ranges of millions of lines of code, the need for computer-aided program comprehension grows. We define one of the tasks of program comprehension to be algorithm recognition: given a piece of source-code from a file, identify the algorithm this code is implementing, such as brute-force or dynamic programming. Most research in this area is making use of pattern matching, which involves much human effort and is of questionable accuracy when the structure and semantics of programs change. Thus, this paper proposes to let go of defined patterns, and make use of simpler features, such as counts of variables and counts of different constructs to recognize algorithms. We then feed these features to a classification algorithm to predict the class or type of algorithm used in this source code. We show through experimental results that our proposed method achieves a good improvement over baseline.", 
"Clinicians have indicated that a sensitivity of 95% with specificity below 5% was the minimum requirement for clinical acceptance. This system automatically processes EEG records and classifies three patterns of clinical interest in brain activity that might be useful in diagnosing brain disorders. These algorithms are trained and evaluated using the Temple University Hospital EEG, which is the largest publicly available corpus of clinical EEG recordings.",Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures,"Brain monitoring combined with automatic analysis of EEGs provides a clinical decision support tool that can reduce time to diagnosis and assist clinicians in real-time monitoring applications (e.g., neurological intensive care units). Clinicians have indicated that a sensitivity of 95% with specificity below 5% was the minimum requirement for clinical acceptance. In this study, a high-performance automated EEG analysis system based on principles of machine learning and big data is proposed. This hybrid architecture integrates hidden Markov models (HMMs) for sequential decoding of EEG events with deep learning-based post-processing that incorporates temporal and spatial context. These algorithms are trained and evaluated using the Temple University Hospital EEG, which is the largest publicly available corpus of clinical EEG recordings in the world. This system automatically processes EEG records and classifies three patterns of clinical interest in brain activity that might be useful in diagnosing brain disorders: (1) spike and/or sharp waves, (2) generalized periodic epileptiform discharges, (3) periodic lateralized epileptiform discharges. It also classifies three patterns used to model the background EEG activity: (1) eye movement, (2) artifacts, and (3) background. Our approach delivers a sensitivity above 90% while maintaining a specificity below 5%. We also demonstrate that this system delivers a low false alarm rate, which is critical for any spike detection application.", 
"Text summarization has been a field of intensive research over the last 50 years, especially for English. Techniques and methodologies for Arabic text summarization are still immature due to the inherent complexity of the Arabic language. A survey would be a good basis for the design of an Arabic automatic text summarizing system.","Automatic Arabic Summarization, A survey of methodologies and systems","Text summarization has been a field of intensive research over the last 50 years, especially for commonly-used and relatively simple-grammar languages such as English. Moreover, the unprecedented growth in the amount of online information available in many languages to users and businesses, including news articles and social media, has made it difficult and time consuming for users to identify and consume sought after content. Hence, an automatic text summarization for various languages to generate accurate and relevant summaries from the huge amount of information available is essential nowadays. Techniques and methodologies for Arabic text summarization are still immature due to the inherent complexity of the Arabic language in terms of both structure and morphology. This paper describes the main challenges for Arabic text summarization and surveys the various methodologies and systems in the literature. This survey would be a good basis for the design of an Arabic automatic text summarization that combines the various “good” features of the existing systems and dismiss the “not-so-good” features.",  
This paper exposes a literature review of recent research works on Arabic text summarization. Current approaches used in this field are presented followed by a discussion about their limitations and the main challenges faced.,Automatic Arabic Text Summarization Approaches,"In recent years, automatic text summarization has seen renewed interest, and has been experiencing an increasing number of researches and products especially in English language. However, in Arabic language, little works and limited researches have been done in this field. This paper exposes a literature review of recent research works on Arabic text summarization. Current approaches used in this field are presented followed by a discussion about their limitations and the main challenges faced when dealing with such application. As a final point, a proposed approach to improve the quality of Arabic text summarization system is presented.",  
Only few extractive Arabic summarizers exist due to the lack of large collection in Arabic language. The goal is to study the capability of analogical proportions to represent the relationship between documents and their corresponding summaries. We suggest two algorithms to quantify the relevance/irrelevance of an extracted keyword from the input text to build its summary. The best-achieved results are ROUGE-1 = 0.96 and BLEU-1 : 0.65 corresponding to educational documents from EASC collection.,Automatic Arabic Text Summarization Using Analogical Proportions,"Automatic text summarization is the process of generating or extracting a brief representation of an input text. There are several algorithms for extractive summarization in the literature tested by using English and other languages datasets; however, only few extractive Arabic summarizers exist due to the lack of large collection in Arabic language. This paper proposes and assesses new extractive single-document summarization approaches based on analogical proportions which are statements of the form “a is to b as c is to d”. The goal is to study the capability of analogical proportions to represent the relationship between documents and their corresponding summaries. For this purpose, we suggest two algorithms to quantify the relevance/irrelevance of an extracted keyword from the input text, to build its summary. In the first algorithm, the analogical proportion representing this relationship is limited to check the existence/non-existence of the keyword in any document or summary in a binary way without considering keyword frequency in the text, whereas the analogical proportion of the second algorithm considers this frequency. We have assessed and compared these two algorithms with some language-independent summarizers (LexRank, TextRank, Luhn and LSA (Latent Semantic Analysis)) using our large corpus ANT (Arabic News Texts) and a small test collection EASC (Essex Arabic Summaries Corpus) by computing ROUGE (Recall Oriented Understudy for Gisting Evaluation) and BLEU (BiLingual Evaluation Understudy) metrics. The best-achieved results are ROUGE-1 = 0.96 and BLEU-1 = 0.65 corresponding to educational documents from EASC collection which outperform the best LexRank algorithm. The proposed algorithms are also compared with three other Arabic extractive summarizers, using EASC collection, and show better results in terms of ROUGE-1 = 0.75 and BLEU-1 = 0.47 for the first algorithm, and ROUGE-1 = 0.74 and BLEU-1 = 0.49 for the second one. Experimental results show the interest of analogical proportions for text summarization. In particular, analogical summarizers significantly outperform three among four language-independent summarizers in the case of BLEU-1 for ANT collection and they are not significantly outperformed by any other summarizer in the case of EASC collection.", 
"This paper propose a hybrid clustering method(partitioning and hierarchical) to group many Arabic documents into several clusters. Keyphrase extraction module is applied to extract important Keyphrases from each cluster, which helps identify the most important sentences. This model is designed for both single and multi-document Arabic text summarization.",Automatic Arabic text summarization using clustering and keyphrase extraction,"As the number of electronic documents increases rapidly, the need for faster techniques to assess the relevance of these documents emerges. A summary is a concise representation of underlying text. A full understanding of the document is essential to form an ideal summary. However, achieving full understanding is either difficult or impossible for computers. Therefore, selecting important sentences from the original text and presenting these sentences as a summary present the most common techniques in automated text summarization. This paper propose a hybrid clustering method(partitioning and hierarchical) to group many Arabic documents into several clusters .Then keyphrase extraction module is applied to extract important Keyphrases from each cluster, which helps identify the most important sentences and find similar sentences based on several similarity algorithms. It applied to extract one sentence from a group of similar sentences while ignoring the other similar sentences (i.e., sentences that have a greater similarity than the predefined threshold). This model is designed for both single and multi-document Arabic text summarization. The Recall Oriented Understudy for Gisting Evaluation (ROGUE) matrix used for the evaluation. For the summarization dataset, Essex Arabic Summaries Corpus was used. It has many topic based articles with multiple human summaries. This model achieved an accuracy of 80 % for single-document and 62% for multi-document summarization.", 
"This survey investigates several research studies that have been conducted in the field of Arabic text summarization. The literature in this field is fairly limited and relatively new compared to the available literature on other languages, such as English. One of the largest problems in Arabic summarization was the absence of Arabic gold standard summaries.","Automatic Arabic text summarization, a survey","This survey investigates several research studies that have been conducted in the field of Arabic text summarization. Specifically, it addresses summarization and evaluation methods, as well as the corpora used in those studies. The literature in this field is fairly limited and relatively new compared to the available literature on other languages, such as English. Therefore, there exists a great opportunity for further research in Arabic text summarization. In addition, one of the largest problems in Arabic summarization was the absence of Arabic gold standard summaries, although this situation is beginning to change, especially with the inclusion of Arabic language as a part of the corpora and tasks in the TAC 2011 MultiLing Pilot and ACL 2013 MultiLing Workshop. Finally, providing the required corpora and adopting them in Arabic summarization studies is an essential demand.", 
"This paper illustrates the implementation of term frequency and semantic sentence similarity based summarizing approaches to summarize a single Bangla document. Removing stopwords, noisy words, lemmatization, tokenization has been done beforehand. Both of these methods return a bunch of top-ranked sentences to create a summary.",Automatic Bangla Text Summarization Using Term Frequency and Semantic Similarity Approach,"With the increasing amount of data within the cloud, it is harder to get the expected one. This leads to the idea of text summarization. Automatic text summarization is a tool for summarizing textual data into a short and concise piece of information via which people can have the idea about the content. Several approaches are introduced but there are a little amount of work has been done on Bangla text summarizing techniques due to some different and multifaceted structure of Bangla language. This paper illustrates the implementation of term frequency and semantic sentence similarity based summarizing approaches to summarize a single Bangla document. Removing stopwords, noisy words, lemmatization, tokenization has been done beforehand. Both of these methods return a bunch of top-ranked sentences to create a summary. The rank of a sentence is determined by the term frequency for the first approach and the sentence similarity for the second approach. The experimental result shows a favorable outcome for both of the approaches. Further improvements of these approaches certainly will return an enchanting outcome.", 
A method has been proposed in this paper for Bengali news documents summarization which extracts significant sentences. The noticeable feature of this method is the incorporation of the sentence frequency where redundancy elimination is a consequence. Two sets of human generated summary have been utilized to train the system.,Automatic Bengali news documents summarization by introducing sentence frequency and clustering,"A method has been proposed in this paper for Bengali news documents summarization which extracts significant sentences using the four major steps (a) preprocessing, (b) sentence ranking, (c) sentence clustering, and (d) summary generation. The noticeable feature of this method is the incorporation of the sentence frequency where redundancy elimination is a consequence. Another one remarkable aspect is sentence clustering on the basis of similarity ratio among sentences. The summary sentence selection is done from all the clusters so that there will be maximum coverage of information in summary even if information is found scattered in input document. Two sets of human generated summary have been utilized where one is to train the system and another is for performance evaluation. The proposed method has been found better while turning comparison with the latest state-of-the art method of Bengali news documents summarization. The results of performance evaluation show that the average Precision, Recall and F-measure values are 0.608, 0.664 and 0.632 respectively.", 
The ability to automatically determine the political orientation of an article can be of great benefit in many areas from academia to security. This problem has been largely understudied for Arabic texts in the literature. This work includes manually labeling a corpus of articles and comments from different political orientations in the Arab world.,Automatic categorization of Arabic articles based on their political orientation,"The ability to automatically determine the political orientation of an article can be of great benefit in many areas from academia to security. However, this problem has been largely understudied for Arabic texts in the literature. The contribution of this work lies in two aspects. First, collecting and manually labeling a corpus of articles and comments from different political orientations in the Arab world and making different versions of it. Second, studying the performance of various feature reduction methods and various classifiers on these synthesized datasets. The two most popular feature extraction approaches for such a problem were compared, namely the Traditional Text Categorization (TC) approach and the Stylometric Features approach (SF). Although the experimental results show the superiority of the TC approach over the SF approach, the results also indicate that the latter approach can be significantly improved by adding new and more discriminating features. The experimental results also show that the feature selection techniques reduce the accuracies of the considered classifiers under the TC and SF approaches in general. The only exception is the Partition Membership (PM) technique which has an opposite effect. The highest accuracies are obtained when PM feature selection method is used with the Support Vector Machine (SVM) classifier.", 
"Software developers, particularly in open-source projects, rely on bug repositories to organize their work. Researchers have shown that incorrect categorization of newly received bug reports to components can cause potential delays. One drawback of an SVM-based approach is that the results of categorization can be uneven across various components.",Automatic Categorization of Bug Reports Using Latent Dirichlet Allocation,"Software developers, particularly in open-source projects, rely on bug repositories to organize their work. On a bug report, the component field is used to indicate to which team of developers a bug should be routed. Researchers have shown that incorrect categorization of newly received bug reports to components can cause potential delays in the resolution of bug reports. Approaches have been developed that consider the use of machine learning approaches, specifically Support Vector Machines (svm), to automatically categorize bug reports into the appropriate component to help streamline the process of solving a bug. One drawback of an SVM-based approach is that the results of categorization can be uneven across various components in the system if some components receive less reports than others. In this paper, we consider broadening the consistency of the recommendations produced by an automatic approach by investigating three approaches to automating bug report categorization: an approach similar to previous ones based on an SVM classifier and Term Frequency Inverse Document Frequency(svm-tf-idf), an approach using Latent Dirichlet Allocation (LDA) with SVM (svm-lda) and an approach using LDA and Kullback Leibler divergence (lda-kl). We found that lda-kl produced recalls similar to those found previously but with better consistency across all components for which bugs must be categorized.", 
"This paper aims to provide alternative approaches for automatic classification of subsurface hydrocarbon-bearing regions from 2D seismic images. The techniques are studied to identify geologic ''leads'', instead of delineating other structures of the porous medium.",Automatic classification of hydrocarbon leads in seismic images through artificial and convolutional neural networks,"This paper aims to provide alternative approaches for automatic classification of subsurface hydrocarbon-bearing regions from 2D seismic images driven by multi-layer perceptron neural networks (MLPs) (a kind of artificial neural network) and convolutional neural networks (CNNs). The first approach is based on a standard MLP whose features are controlled by Haralick’s textural descriptors; the second one is developed with a multiple-layer CNN. Both techniques are studied to identify geologic ‘‘leads’’, instead of delineating other structures of the porous medium, such as salt bodies or seismic faults. The outcomes obtained from each approach are evaluated for a dataset of seismic images corresponding to the offshore SEAL Basin in Brazil’s northeastern. Performance indicators (accuracy, recall, precision, F-measure and loss) are computed to verify training and validation of the network learning capabilities. It is shown that for both MLP and CNN configurations, good agreement is achieved in blind testing qualitatively and quantitatively.", 
"The proposed method was tested on a set of (un)impaired subjects, where it outperformed the traditional machine learning methods. The results, obtained without any human intervention, turned out not to lag much behind state-of-the-art methods.",Automatic Classification of Motor Impairment Neural Disorders from EEG Signals Using Deep Convolutional Neural Networks,"The analysis of biomedical signals, such as the EEGs for measuring brain activity, provides means for the diagnosis of various cognitive tasks and neural disorders. These signals are frequently transformed into visual representations such as spectrograms, which can reveal characteristic patterns and serve as a basis for classification, when extracting specific features from them. We designed a new method that uses spectrogram images to feed them without any feature selection/extraction procedure directly into a deep convolutional neural network architecture and train it for the classification of motor impairment neural disorder in a person. The proposed method was tested on a set of (un)impaired subjects, where it outperformed the traditional machine learning methods. The results, obtained without any human intervention and by using all the default parameter values, turned out not to lag much behind an established state-of-the-art method, that takes advantage of using domain knowledge for the analysis of EEG recordings. Based on the experimental results we believe that the proposed method can be considered as a sound basis for further optimization towards a competitive, fully automated method for classification of EEG signals.", 
New method for automatic sleep stage classification based on time-frequency image (TFI) of electroencephalogram (EEG) signals is proposed. Automatic classification of sleep stages is an important part for diagnosis and treatment of sleep disorders.,Automatic classification of sleep stages based on the time-frequency image of EEG signals,"In this paper, a new method for automatic sleep stage classification based on time-frequency image (TFI) of electroencephalogram (EEG) signals is proposed. Automatic classification of sleep stages is an important part for diagnosis and treatment of sleep disorders. The smoothed pseudo Wigner–Ville distribution (SPWVD) based time-frequency representation (TFR) of EEG signal has been used to obtain the time-frequency image (TFI). The segmentation of TFI has been performed based on the frequency-bands of the rhythms of EEG signals. The features derived from the histogram of segmented TFI have been used as an input feature set to multiclass least squares support vector machines (MC-LS-SVM) together with the radial basis function (RBF), Mexican hat wavelet, and Morlet wavelet kernel functions for automatic classification of sleep stages from EEG signals. The experimental results are presented to show the effectiveness of the proposed method for classification of sleep stages from EEG signals.", 
There is a tremendous growth of software artifacts that provide insight into how people build software. Researchers are always looking for large-scale and representative software artifacts. The manual identification of rich software artifacts is very labor-intensive. We propose an automated approach based on Machine Learning techniques to identify various types of Software artifacts.,Automatic Classification of Software Artifacts in Open-Source Applications,"With the increasing popularity of open-source software development, there is a tremendous growth of software artifacts that provide insight into how people build software. Researchers are always looking for large-scale and representative software artifacts to produce systematic and unbiased validation of novel and existing techniques. For example, in the domain of software requirements traceability, researchers often use software applications with multiple types of artifacts, such as requirements, system elements, verifications, or tasks to develop and evaluate their traceability analysis techniques. However, the manual identification of rich software artifacts is very labor-intensive. In this work, we first conduct a large-scale study to identify which types of software artifacts are produced by a wide variety of open-source projects at different levels of granularity. Then we propose an automated approach based on Machine Learning techniques to identify various types of software artifacts. Through a set of experiments, we report and compare the performance of these algorithms when applied to software artifacts.",  
"Machine learning can be used to extract events from news sources for quantitative analysis. We used WhatsApp as a news source to identify the occurrence of violent incidents in South Africa. Using machine learning, we have shown how violent incidents can be coded and recorded. This allows for a local level recording of these events over time.",Automatic classification of social media reports on violent incidents in South Africa using machine learning,"With the growing amount of data available in the digital age, it has become increasingly important to use automated methods to extract useful information from data. One such application is the extraction of events from news sources for the purpose of a quantitative analysis that does not rely on someone needing to read through thousands of news articles. Overseas, projects such as the Integrated Crisis Early Warning System (ICEWS) monitor news stories and extract events using automated coding. However, not all violent events are reported in the news, and while monitoring only news agencies is sufficient for projects such as ICEWS which have a global focus, more news sources are required when assessing a local situation. We used WhatsApp as a news source to identify the occurrence of violent incidents in South Africa. Using machine learning, we have shown how violent incidents can be coded and recorded, allowing for a local level recording of these events over time. Our experimental results show good performance on both training and testing data sets using a logistic regression classifier with unigrams and Word2vec feature models. Future work will evaluate the inclusion of pre-trained word embedding for both Afrikaans and English words to improve the performance of the machine learning classifier.", 
"Method enables a high level of detail classification from the combination of geometric and topological information. The methodology is tested in four real complex case studies acquired with a Mobile Laser Scanner Device. Results show a success rate of 97% in point classification, enough to analyse extensive urban areas.",Automatic classification of urban ground elements from mobile laser scanning data,"Accessibility diagnosis of as-built urban environments is essential for path planning, especially in case of people with reduced mobility and it requires an in-depth knowledge of ground elements. In this paper, we present a new approach for automatically detect and classify urban ground elements from 3D point clouds. The methodology enables a high level of detail classification from the combination of geometric and topological information. The method starts by a planar segmentation followed by a refinement based on split and merge operations. Next, a feature analysis and a geometric decision tree are followed to classify regions in preliminary classes. Finally, adjacency is studied to verify and correct the preliminary classification based on a comparison with a topological graph library. The methodology is tested in four real complex case studies acquired with a Mobile Laser Scanner Device. In total, five classes are considered (roads, sidewalks, treads, risers and curbs). Results show a success rate of 97% in point classification, enough to analyse extensive urban areas from an accessibility point of view. The combination of topology and geometry improves a 10% to 20% the success rate obtained with only the use of geometry.", 
"Research is based on the point-based summarization technique, where a point is a verb and its syntactic arguments. Our proposed system implement s the system in three different modules: point extraction, point curation, and summary generation. The system also includes the stance of statement to improve performance of the summarizer. The results of this research are our proposed system performs the best in terms of precision and gets the best f-score after the summaries are preprocessed.",Automatic Debate Text Summarization in Online Debate Forum,"The goal of this research is to create a system that can generate summaries from online debate forum by using abstractive technique. This research is based on the point-based summarization technique, where a point is a verb and its syntactic arguments. The point is extracted based on the dependency parse and the syntactic frame. Our proposed system implement s the system in three different modules: point extraction, point curation, and summary generation. The system also includes the stance of statement to improve performance of the summarizer. We use ROUGE metrics to evaluate this system. The results of this research are our proposed system performs the best in terms of precision and gets the best f-score after the summaries are preprocessed. The proposed system increases the ROUGE-1 score by 8.99% compared to the point-based summarization and produces 15.84% increase compared to the baseline summarization system. The goal of this research is to create a system that can generate summaries from online debate forum by using abstractive technique. This research is based on the point-based summarization technique, where a point is a verb and its syntactic arguments. The point is extracted based on the dependency parse and the syntactic frame. Our proposed system implement s the system in three different modules: point extraction, point curation, and summary generation. The system also includes the stance of statement to improve performance of the summarizer. We use ROUGE metrics to evaluate this system. The results of this research are our proposed system performs the best in terms of precision and gets the best f-score after the summaries are preprocessed. The proposed system increases the ROUGE-1 score by 8.99% compared to the point-based summarization and produces 15.84% increase compared to the baseline summarization system.", 
Modern organizations handle terabytes of data in text format alone. A lot of research has been performed for finding important sentences in a document. This research work focuses on identifying and extracting important parts of the document.,Automatic Document Summarization using Sentiment Analysis,"With the advent of information revolution, electronic documents have become the powerhouse of business and academic information. Modern organizations handle terabytes of data in text format alone. In order to fully understand and utilize these documents, it is necessary to be able to extract the essence of these documents. Having a system that would summarize text would thus be immensely useful in serving this need. For generating a summary, we have to identify the most important and relevant pieces of information from the document, omit irrelevant parts, and assemble them into a compact format. A lot of research has been performed for finding important sentences in a document. This research work focuses on identifying and extracting important parts of the document and to form a coherent summary using sentiment analysis. It uses a degrading extraction approach to create a document summarization framework where in, if one extraction strategy fails then the model gracefully degrades to another.",  
"Stress has been identified as one of the contributing factors to vehicle crashes. Motivated by the need to address the significant costs of driver stress, it is essential to build a practical system that can detect drivers' stress levels. A driver stress detection model often requires data from different modalities, including ECG signals, vehicle data and contextual data. We propose a multimodal fusion model based on convolutional neural networks and long short-term memory.",Automatic driver stress level classification using multimodal deep learning,"Stress has been identified as one of the contributing factors to vehicle crashes which create a significant cost in terms of loss of life and productivity for governments and societies. Motivated by the need to address the significant costs of driver stress, it is essential to build a practical system that can detect drivers’ stress levels in real time with high accuracy. A driver stress detection model often requires data from different modalities, including ECG signals, vehicle data (e.g., steering wheel, brake pedal) and contextual data (e.g., weather conditions and other ambient factors). Most of the current works use traditional machine learning techniques to fuse multimodal data at different levels (e.g., feature level) to classify drivers’ stress levels. Although traditional multimodal fusion models are beneficial for driver stress detection, they inherently have some critical limitations (e.g., ignore non-linear correlation across modalities) that may hinder the development of a reliable and accurate model. To overcome the limitations of traditional multimodal fusion, this paper proposes a framework based on adopting deep learning techniques for driver stress classification captured by multimodal data. Specifically, we propose a multimodal fusion model based on convolutional neural networks (CNN) and long short-term memory (LSTM) to fuse the ECG, vehicle data and contextual data to jointly learn the highly correlated representation across modalities, after learning each modality, with a single deep network. To validate the effectiveness of the proposed model, we perform experiments on our dataset collected using an advanced driving simulator. In this paper, we present a multi-modal system based on the adoption of deep learning techniques to improve the performance of driver stress classification. The results show that the proposed model outperforms model built using the traditional machine learning techniques based on handcrafted features (average accuracy: 92.8%, sensitivity: 94.13%, specificity: 97.37% and precision: 95.00%).", 
"In this paper, we investigate the potentials of applying a kernel-based learning machine, the Relevance Vector Machine (RVM), to the task of epilepsy detection through automatic electroencephalogram (EEG) signal classification. In terms of accuracy, the best-calibrated RVM models have shown very satisfactory performance levels, which are rather comparable to those of SVMs.",Automatic EEG signal classification for epilepsy diagnosis with Relevance Vector Machines,"In this paper, we investigate the potentials of applying a kernel-based learning machine, the Relevance Vector Machine (RVM), to the task of epilepsy detection through automatic electroencephalogram (EEG) signal classification. For this purpose, some experiments have been conducted over publicly available data, contrasting the performance levels exhibited by RVM models with those achieved with Support Vector Machines (SVMs), both in terms of predictive accuracy and sensitivity to the choice of the kernel function. Four settings of both types of kernel machine were considered in this study, which vary in accord with the type of input data they receive, either raw EEG signal or some statistical features extracted from the wavelet-transformed data. The empirical results indicate that: (1) in terms of accuracy, the best-calibrated RVM models have shown very satisfactory performance levels, which are rather comparable to those of SVMs; (2) an increase of accuracy is sometimes accompanied by loss of sparseness in the resulting RVM models; (3) both types of machines present similar sensitivity profiles to the kernel functions considered, having some kernel parameter values clearly associated with better accuracy rate; (4) when not making use of a feature extraction technique, the choice of the kernel function seems to be very relevant for significantly leveraging the performance of RVMs; and (5) when making use of derived features, the choice of the feature extraction technique seems to be an important factor to one take into account.", 
"Autonomous driving cars pose challenges about ethics, safety, cybersecurity, and social acceptance. The latter poses new problems since passengers are used to manually driven vehicles. To smooth the transition towards autonomous vehicles, a delicate calibration of the driving functions should be performed. Different settings of a given algorithm should be evaluated by assessing the human reaction.",Automatic Emotion Recognition for the Calibration of Autonomous Driving Functions,"The development of autonomous driving cars is a complex activity, which poses challenges about ethics, safety, cybersecurity, and social acceptance. The latter, in particular, poses new problems since passengers are used to manually driven vehicles; hence, they need to move their trust from a person to a computer. To smooth the transition towards autonomous vehicles, a delicate calibration of the driving functions should be performed, making the automation decision closest to the passengers’ expectations. The complexity of this calibration lies in the presence of a person in the loop: different settings of a given algorithm should be evaluated by assessing the human reaction to the vehicle decisions. With this work, we for an objective method to classify the people’s reaction to vehicle decisions. By adopting machine learning techniques, it is possible to analyze the passengers’ emotions while driving with alternative vehicle calibrations. Through the analysis of these emotions, it is possible to obtain an objective metric about the comfort feeling of the passengers. As a result, we developed a proof-of-concept implementation of a simple, yet effective, emotions recognition system. It can be deployed either into real vehicles or simulators, during the driving functions calibration.", 
"We train and test linguistic quality models on consecutive years of NIST evaluation data. Best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence.",Automatic Evaluation of Linguistic Quality in Multi-Document Summarization,"To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference information, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.", 
This work presents a method to perform automatic summarization of the text through sentence scoring. We propose a method which utilizes the facilities of fuzzy inference system for the purpose of scoring.,"Automatic Extractive Text Summarization Based on Fuzzy Logic, A Sentence Oriented Approach","The work presents a method to perform automatic summarization of the text through sentence scoring. We propose a method which utilizes the facilities of fuzzy inference system for the purpose of scoring. Preprocessing of the text is done since this technique has its own importance enabling us to filter high quality text. A thorough review of the concepts of summarization enabled us to make use of a group of features which are very appropriate for automatic text summarization. Experimental results obtained by the proposed system on DUC 2002 data reveal that it works to the optimality with respect to other existing methods, and hence is a concrete solution to text summarization.", 
"Summarization of the text can informally be defined as the act of condensing the document from its original size without significantly compromising the semantics. The raw text is first pre-processed which involves removing non-ASCII characters and stop-words, tokenizing and stemming. Each sentence of the document will be represented as a vector in the dimensional space of the vocabulary.",Automatic extractive text summarization using K-means clustering,"The rise in the dimension of the World Wide Web has made an explosion of the amount of accessible information. As the textual data involves several instances of redundancy, omission of part of sentences or entire sentences is possible without altering the meaning of the document. Summarization of the text can informally be defined as the act of condensing the document from its original size without significantly compromising the semantics. For the purpose of generating an appropriate summary, the raw text is first pre-processed which involves - removing non-ASCII characters and stop-words, tokenizing and stemming. Appropriate features are extracted from the data, tf-idf values for each word are computed and the entire pre-processed data is then transformed into a tf-idf matrix. Every sentence of the document will be represented as a vector in the dimensional space of the document's vocabulary. To obtain a concise summary, sentences are appropriately clustered based on the degree of separation of vectors in the Euclidean place. Association of sentences to a cluster using K-means method is totally based on cosine similarity. The count of the clusters is to be formed is predefined. As the number of clusters increase the accuracy of the summary increases. From each of the clusters the sentences which are informative are picked to form the final summary. Using recall and precision measures, the effectiveness of the summary is verified.", 
"Issue tracking systems contain different categories of issue reports such as bug, request for improvement (RFE), documentation, refactoring, task etc. Herzig et al. recently reported that more than 40% of issue. reports are given wrong categories in issue tracking systems. We propose an automated technique that reclassifies an issue report into an appropriate category.",Automatic Fine-Grained Issue Report Reclassification,"Issue tracking systems are valuable resources during software maintenance activities. These systems contain different categories of issue reports such as bug, request for improvement (RFE), documentation, refactoring, task etc. While logging issue reports into a tracking system, reporters can indicate the category of the reports. Herzig et al. recently reported that more than 40% of issue reports are given wrong categories in issue tracking systems. Among issue reports that are marked as bugs, more than 30% of them are not bug reports. The misclassification of issue reports can adversely affects developers as they then need to manually identify the categories of various issue reports. To address this problem, in this paper we propose an automated technique that reclassifies an issue report into an appropriate category. Our approach extracts various feature values from a bug report and predicts if a bug report needs to be reclassified and its reclassified category. We have evaluated our approach to reclassify more than 7,000 bug reports from HTTPClient, Jackrabbit, Lucene-Java, Rhino, and Tomcat5 into 1 out of 13 categories. Our experiments show that we can achieve a weighted precision, recall, and F1 (F-measure) score in the ranges of 0.58- 0.71, 0.61-0.72, and 0.57-0.71 respectively. In terms of F1, which is the harmonic mean of precision and recall, our approach can substantially outperform several baselines by 28.88%-416.66%.", 
"This paper proposes an approach to automatically generating a related work section by comparing the main text of the paper being written with the citations of other papers that cite the same references. The approach outperforms the three baseline methods of MEAD, LexRank, and ReWoS. The experiments show that the citations are suitable for related work generation.",Automatic generation of related work through summarizing citations,"Related work is a component of a scientific paper, which introduces other researchers' relevant works and makes comparisons with the current author's work. Automatically generating the related work section of a writing paper provides a tool for researchers to accomplish the related work section efficiently without missing related works. This paper proposes an approach to automatically generating a related work section by comparing the main text of the paper being written with the citations of other papers that cite the same references. Our approach first collects the papers that cite the reference papers of the paper being written and extracts the corresponding citation sentences to form a citation document. It then extracts keywords from the citation document and the paper being written and constructs a graph of the keywords. Once the keywords that discriminate the two documents are determined, the minimum Steiner tree that covers the discriminative keywords and the topic keywords is generated. The summary is generated by extracting the sentences covering the Steiner tree. According to ROUGE evaluations, the experiments show that the citations are suitable for related work generation and our approach outperforms the three baseline methods of MEAD, LexRank, and ReWoS. This work verifies the general summarization method based on connotation and extension through citation.", 
"In existing unsupervised methods, Latent Semantic Analysis (LSA) is used for sentence selection. The proposed method uses non-negative constraints, which are more similar to the human cognition process. The method selects more meaningful sentences for generic document summarization .",Automatic generic document summarization based on non-negative matrix factorization,"In existing unsupervised methods, Latent Semantic Analysis (LSA) is used for sentence selection. However, the obtained results are less meaningful, because singular vectors are used as the bases for sentence selection from given documents, and singular vector components can have negative values. We propose a new unsupervised method using Non-negative Matrix Factorization (NMF) to select sentences for automatic generic document summarization. The proposed method uses non-negative constraints, which are more similar to the human cognition process. As a result, the method selects more meaningful sentences for generic document summarization than those selected using LSA.", 
"This work presents a novel automatic handgun detection system appropriate for both, surveillance and control purposes. The most promising results are obtained by a Faster R-CNN based model trained on our new database. The best detector shows a high potential even in low quality youtube videos and provides satisfactory results.",Automatic Handgun Detection Alarm in Videos Using Deep Learning,"CEPTED MA Current surveillance and control systems still require human super vision and intervention. This work presents a novel automatic handgun detection system in videos appropriate for both, surveillance and control purposes. We reformulate this detection problem into the problem of minimizing false positives and solve it by i) building the key training data-set guided by the results of a deep Convolutional Neural Networks (CNN) classifier and ii) assessing the best classification model under two approaches, the sliding window approach and region proposal approach. The most promising results are obtained by Faster R-CNN based model trained on our new database. The best detector shows a high potential even in low quality youtube videos and provides satisfactory results as automatic alarm system. Among 30 scenes, it successfully activates the alarm after five successive true positives in a time interval smaller than 0.2 seconds, in 27 scenes. We also define a new metric, Alarm Activation Time per Interval (AATpI), to assess the performance of a detection model as an automatic detection system in videos.", 
"Automatic text summarization is a technique where a computer summarizes a long text into non-redundant form. Hausa, a Chadic language generally spoken in West Africa is a low resource language. This study developed a model to automatically summarize HausA Language text based on feature extraction.",Automatic Hausa language text summarization based on feature extraction using Naive Bayes Model,"Automatic text summarization, a branch of natural language processing, is a technique where a computer summarizes a long text into non-redundant form in order to reduce the problem of information overload. Although there are some language independent summarizers, there is a limited research to automatically summarize text in Hausa language. Hausa, a Chadic language generally spoken in West Africa is a low resource language. This study is conducted to develop a model to automatically summarize Hausa Language text based on feature extraction using Naive Bayes model. A dataset of 10 Hausa Language documents is used in this study. The study adopts five features such as keyword, title and cue phrases in the summarization process. Moreover, Naive Bayes model is used to weigh each sentence based on its features. The system produces a set of summary sentences at 30% compression rate. Moreover, experiments are conducted to summarize the dataset using online summarizers such as Text Compactor and Free summarizers. The overall system testing having an average F-score of 78.1% outperforms the online summarizers. The result shows that automatic text summarization tested on the Hausa Language dataset is better if morphological analysis is considered.", 
"Understanding the software system requires investigating the high-level system functionality and mapping it to its low-level implementation. This manual mapping process is expensive, time-consuming and creates a cognitive gap between the system's overall functionality and its implementation. In this paper, we present an innovative approach that can automatically construct and visualize the static call graph for a system written in Python.",Automatic Hierarchical Clustering of Static Call Graphs for Program Comprehension,"Program comprehension is an imperative and indispensable prerequisite for several software tasks, including testing, maintenance, and evolution. In practice, understanding the software system requires investigating the high-level system functionality and mapping it to its low-level implementation, i. e. source code. The implementation of a software system can be captured using a call graph. A call graph represents the system’s functions and their relationships at a single level of granularity. While call graphs can facilitate understanding the inner system functionality, developers are still required to manually map the high-level system functionality to its call graph. This manual mapping process is expensive, time-consuming and creates a cognitive gap between the system’s overall functionality and its implementation. In this paper, we present an innovative approach that can automatically (1) construct and visualize the static call graph for a system written in Python, (2) cluster the execution paths of the call graph into hierarchal abstractions, and (3) label the clusters according to their major functional behaviors. The goal is to bridge the cognitive gap between the high-level system functionality and its call graph, which ultimately facilitates the program comprehension task. To validate our approach, we conducted four case studies including a large-scale software subject, Keras. The results demonstrated that our approach is feasible to automatically construct call graphs and hierarchically cluster them into abstraction levels with proper labels.", 
"Machine learning is one of the most popular approaches for hourly solar forecasting. In this article, 68 machine learning algorithms are evaluated for 3 sky conditions, 7 locations, and 5 climate zones in the continental United States. It is found that tree-based methods consistently perform well in terms of two-year overall results.",Automatic hourly solar forecasting using machine learning models,"Owing to its recent advance, machine learning has spawned a large collection of solar forecasting works. In particular, machine learning is currently one of the most popular approaches for hourly solar forecasting. Nevertheless, there is evidently a myth on forecast accuracy—virtually all research papers claim superiority over others. Apparently, the “best” model can only be selected with hindsight, i.e., after empirical evaluation. For any new forecasting project, it is irrational for solar forecasters to bet on a single model from the start. In this article, the hourly forecasting performance of 68 machine learning algorithms is evaluated for 3 sky conditions, 7 locations, and 5 climate zones in the continental United States. To ensure a fair comparison, no hybrid model is considered, and only o?-the-shelf implementations of these algorithms are used. Moreover, all models are trained using the automatic tuning algorithm available in the R caret package. It is found that tree-based methods consistently perform well in terms of two-year overall results, however, they rarely stand out during daily evaluation. Although no universal model can be found, some preferred ones for each sky and climate condition are advised.", 
"Proposed, unsupervised approach is to summarize bug reports with complete content and diversified information. Method uses Rapid Automatic Keyword Extraction and term frequency-inverse document frequency method. fuzzy C-means clustering is used to extracts sentences having high degree of membership from each cluster above a set threshold value. The proposed approach is evaluated on newly constructed Apache project Bug Report Corpus (APBRC).",Automatic Keyword and Sentence-Based Text Summarization for Software Bug Reports,"Text Summarization is a process which efficiently retrieves the relevant information from documents. The objective of the proposed, unsupervised approach is to summarize bug reports (software artefacts) with complete content and diversified information. The proposed approach utilizes Rapid Automatic Keyword Extraction and term frequency-inverse document frequency method to extract meaningful keywords and key-phrases with a relevant score. For sentence extraction, fuzzy C-means clustering is used to extracts sentences having high degree of membership from each cluster above a set threshold value. A rule-engine is used for sentence selection. The rules are generated with the domain knowledge and based on the extracted information by the keywords and sentences selected by the clustering method. Cohesive and coherent summary is generated by the proposed method on apache bug reports. For redundancy removal and to re-rank generated summary, hierarchical clustering is presented to enrich the extracted summary. The proposed approach is evaluated on newly constructed Apache project Bug Report Corpus (APBRC) and existing Bug Report Corpus (BRC). The results are compared on the basis of performance metrics such as precision, recall, pyramid precision and F-score. The experimental results depict that our proposed approach attains significant improvement over other baseline approaches such as BRC and LRCA. It also attains significant improvement over existing state-of-art unsupervised approaches such as Hurried, centroid and others. It extracts significant keyword phrases and sentences from each cluster to achieve full coverage and coherent summary. The results evaluated on APBRC corpus attains an average value of 78.22%, 82.18%, 80.10% and 81.66% for precision, recall, f-score and pyramid precision respectively.", 
"Summarization is the process of reducing a text document to create a summary that retains the most important points. In this paper, we propose an algorithm to extract keywords automatically for text summarization in e-newspaper datasets.",Automatic Keyword Extraction for Text Summarization in e-Newspapers,"Summarization is the process of reducing a text document to create a summary that retains the most important points of the original document. Extractive summarizers work on the given text to extract sentences that best convey the message hidden in the text. Most extractive summarization techniques revolve around the concept of finding keywords and extracting sentences that have more keywords than the rest. Keyword extraction usually is done by extracting relevant words having a higher frequency than others, with stress on important ones’. Manual extraction or annotation of keywords is a tedious process brimming with errors involving lots of manual effort and time. In this paper, we proposed an algorithm to extract keyword automatically for text summarization in e-newspaper datasets. The proposed algorithm is compared with the experimental result of articles having the similar title in four different e-Newspapers to check the similarity and consistency in summarized results.", 
"Summarization is the way towards lessening the content of a text file to make it brief that holds all the critical purposes. In this paper, we proposed a hybrid approach to extract keyword automatically for multi-document text summarization in e-newspaper articles. We showed that our proposed techniques had been outperformed over other techniques for automatic keyword extraction and summarization.",Automatic Keyword Extraction for Text Summarization in Multi-Document e-Newspapers Articles,"Summarization is the way towards lessening the content of a text file to make it brief that holds all the critical purposes in the content of original text file. In the process of extractive summarization, one extracts only those sentences which are the most relevant sentences in the text document and that conveys the moral of the content. The extractive summarization techniques usually revolve around the idea of discovering most relevant and frequent keywords and then extract the sentences based on those keywords. Manual extraction or explanation of relevant keywords are a dreary procedure overflowing with errors including loads of manual exertion and time. In this paper, we proposed a hybrid approach to extract keyword automatically for multi-document text summarization in e-newspaper articles. The performance of the proposed approach is compared with three additional keyword extraction techniques namely, term frequency-inverse document frequency (TF-IDF), term frequency-adaptive inverse document frequency (TF-AIDF), and a number of false alarm (NFA) for automatic keyword extraction and summarization in e-newspapers articles for better analysis. Finally, we showed that our proposed techniques had been outperformed over other techniques for automatic keyword extraction and summarization.", 
"Text summarization is emerged as an important research area in recent past. Data is growing rapidly in every domain such as news, social media, banking, education, etc. In this paper, recent literature on automatic keyword extraction and text summarization are presented.","Automatic Keyword Extraction for Text Summarization, A Survey","In recent times, data is growing rapidly in every domain such as news, social media, banking, education, etc. Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.", 
"Video skimming is one of the recently, getting popular technique for preparing preview for long watching video sequences. In this article, we put forward an intelligent expert video skimming technique for lecture video sequences, where human intervention is not required. We propose the use of radiometric correlation technique. The fuzzy K-nearest neighborhood technique is proposed to recognize the shots in a video. The effectiveness of the proposed scheme is demonstrated in this paper using five test sequences.",Automatic lecture video skimming using shot categorization and contrast based features,"Video skimming is one of the recently, getting popular technique for preparing preview for long watching video sequences. Most of the video skimming techniques developed in the literature uses manual intervention of users to prepare the review. Mostly the literature reported video skimming for sports and movie industries. In sports the portion of video where audience claps are used and in movie important contents are manually selected for preparing the preview. However in literature rarely any work reported for skimming of lecture video sequences. Lecture videos are generally, recorded indoor, low illuminated, noisy environment condition and contents of the scene rarely changes much. Hence designing an automatic skimming scheme is quite difficult task. In this article, we put forward an intelligent expert video skimming technique for lecture video sequences, where human intervention is not required. In the proposed scheme, initially the lecture video is segmented into a number of shots. We proposed the use of radiometric correlation technique for lecture video segmentation or finding the shot transitions. After getting the shot transitions in a video, the shots are recognized. The fuzzy K-nearest neighborhood technique is proposed to recognize the shots in a video. The shots are recognized into three categories: title slides, written texts/displayed slides and talking heads/writing hands. Three contrast based features: one existing i.e., average sharpness (AS) and two newly proposed: relative height (RH) and edge potential (EP) are used to find the contents of a frame. The frames with different contrast values are categorized to prepare the video skimming or the capsule. The media recreation is achieved by selecting a set of frames around these selected content frames. The effectiveness of the proposed scheme is demonstrated in this paper using five test sequences, including three NPTEL and two non NPTEL. It is also observed that the capsule prepared by the proposed scheme, provides a better preview of the actual sequence. The performance of the proposed scheme is tested by comparing it against three state-of-the-art techniques. The evaluation of the proposed scheme is carried out by using three evaluation measures. It is also observed that the proposed scheme is found to be better than that of the existing schemes.", 
"We observed that opinions about diverse API aspects are prevalent in Stack Overflow forums. We built a suite of techniques to automatically mine and categorize opinions from forum posts. Opiner is available online as a search engine, where developers can search for APIs by their names.",Automatic Mining of Opinions Expressed About APIs in Stack Overflow,"With the proliferation of online developer forums, developers share their opinions about the APIs they use. The plethora of such information can present challenges to the developers to get quick but informed insights about the APIs. To understand the potential benefits of such API reviews, we conducted a case study of opinions in Stack Overflow using a benchmark dataset of 4522 sentences. We observed that opinions about diverse API aspects (e.g., usability) are prevalent and offer insights that can shape developers’ perception and decisions related to software development. Motivated by the finding, we built a suite of techniques to automatically mine and categorize opinions about APIs from forum posts. First, we detect opinionated sentences in the forum posts. Second, we associate the opinionated sentences to the API mentions. Third, we detect API aspects (e.g., performance, usability) in the reviews. We developed and deployed a tool called Opiner, supporting the above techniques. Opiner is available online as a search engine, where developers can search for APIs by their names to see all the aggregated opinions about the APIs that are automatically mined and summarized from developer forums.", 
Multi-document summarization is a technology that can summarize multiple documents and present them in one summary. This research method successfully generated a well-compressed and readable summary with a fast processing time.,Automatic multi-document summarization for Indonesian documents using hybrid abstractive-extractive summarization technique,"This paper discusses the development of multi-document summarization for Indonesian documents by using hybrid abstractive-extractive summarization approach. Multi-document summarization is a technology that able to summarize multiple documents and present them in one summary. The method used in this research, hybrid abstractive-extractive summarization technique, is a summarization technique that is the combination of WordNet based text summarization (abstractive technique) and title word based text summarization (extractive technique). After an experiment with LSA as the comparison method, this research method successfully generated a well-compressed and readable summary with a fast processing time.", 
"Proposed Myanmar text summarization system, automatic summarized text will produce as mail dairy notes for incoming mail system. System will identify the semantic relationships, or semantic roles, and fill by constituents of a sentence within a semantic frame using Myanmar Verb Frames.",Automatic Myanmar Text Summarization System with Semantic roles,"Automatic text summarization is to compress the larger original text into shorter text called as summary. It is the essential researches for Natural Language Processing (NLP). Automatic semantic analysis of natural language text has received much attention by NLP community. In proposed Myanmar text summarization system, automatic summarized text will produce as mail dairy notes for incoming mail system. Anaphora resolution is a key problem in natural language processing, and has correspondingly received a significant amount of attention in the literature. So, automatic pronominal anaphora resolution in Myanmar texts will be used for summary generation system in this paper. This system will identify the semantic relationships, or semantic roles, and fill by constituents of a sentence within a semantic frame using Myanmar Verb Frames. So, verb frame resource with annotated semantic roles will be developed for proposed system.", 
"Ocular artifacts (OAs) are one of the most important form of interferences in the analysis of electroencephalogram (EEG) research. For classic OAs removal methods, either an additional electrooculogram (EOG) recording or multi-channel EEG is required. The proposed method consists of offline stage and online stage. In offline stage, training samples without OAs are intercepted and used to train an DLN to reconstruct the EEG signals. In the online stage, the trained DLN is used as a filter to automatically remove OAs.",Automatic ocular artifacts removal in EEG using deep learning,"Ocular artifacts (OAs) are one the most important form of interferences in the analysis of electroencephalogram (EEG) research. OAs removal/reduction is a key analysis before the processing of EEG signals. For classic OAs removal methods, either an additional electrooculogram (EOG) recording or multi-channel EEG is required. To address these limitations of existing methods, this paper investigates the use of deep learning network (DLN) to remove OAs in EEG signals. The proposed method consists of offline stage and online stage. In the offline stage, training samples without OAs are intercepted and used to train an DLN to reconstruct the EEG signals. The high-order statistical moments information of EEG is therefore learned. In the online stage, the trained DLN is used as a filter to automatically remove OAs from the contaminated EEG signals. Compared with the exiting methods, the proposed method has the following advantages: (i) nonuse of additional EOG reference signals, (ii) any few number of EEG channels can be analyzed, (iii) time saving, and (iv) the strong generalization ability, etc. In this paper, both public database and lab individual data for EEG analysis are used, we compared the proposed method with the classic independent component analysis (ICA), kurtosis-ICA (K-ICA), Second-order blind identification (SOBI) and a shallow network method. Experimental results show that the proposed method performs better even for very noisy EEG.", 
"Academic research is crucial to the development of science and technology. When writing an academic research paper, a rhetorical structure is typically used to present the paper's ideas. Some studies have adopted text mining to assist with the writing, but the existing methods still require human intervention.",Automatic paper writing based on a RNN and the TextRank algorithm,"Academic research is crucial to the development of science and technology and is an important factor that affects national strength. When writing an academic research paper, a rhetorical structure is typically used to present the paper’s ideas, but this task is quite difficult for junior researchers. To solve this problem, some studies have adopted text mining to assist with the writing, but the existing methods still require human intervention to generate sentences. Recently, due to the increasing maturity of deep learning technology and the ability to address the problem of automatic text generation, progress has been made in this area. The highly complex deep learning operations can correctly generate sequences and find correlations between sequences. When a user provides a few keywords and key sentences, the proposed algorithm can generate an introduction section for the user. The results show that the generated introduction is more coherent, clearer, and more fluent than existing summarization methods. In addition, the method proposed in this study improves the accuracy compared with traditional text extraction methods. The manuscript produced by this study has been evaluated to show that the study can produce a comprehensive introduction compared with previous studies.", 
"Ransomware causes a major threat to the security of computer systems. Current signature-based and static detection model is often easily evadable by obfuscation, say authors. They propose a new system using data mining techniques to detect known and unknown ransomware. The accuracy and detection rate of their proposed system with Simple Logistic algorithm can achieve to 98.2% and 97.6% respectively.",Automatic Ransomware Detection and Analysis Based on Dynamic API Calls Flow Graph,"In recent cyber incidents, Ransom software (ransomware) causes a major threat to the security of computer systems. Consequently, ransomware detection has become a hot topic in computer security. Unfortunately, current signature-based and static detection model is often easily evadable by obfuscation, polymorphism, compress, and encryption. For overcoming the lack of signature-based and static ransomware detection approach, we have proposed the dynamic ransomware detection system using data mining techniques such as Random Forest (RF), Support Vector Machine (SVM), Simple Logistic (SL) and Naive Bayes (NB) algorithms for detecting known and unknown ransomware. We monitor the actual (dynamic) behaviors of software to generate API calls flow graphs (CFG) and transfer it in a feature space. Thereafter, data normalization and feature selection were applied to select informative features which are the best for discriminating between various categories of software and benign software. Finally, the data mining algorithms were used for building the detection model for judging whether the software is benign software or ransomware. Our experimental results show that our proposed system can be more effective to improve the performance for ransomware detection. Especially, the accuracy and detection rate of our proposed system with Simple Logistic (SL) algorithm can achieve to 98.2% and 97.6%, respectively. Meanwhile, the false positive rate also can be reduced to 1.2%.", 
An approach based on clustering ensembles is proposed. Experimental validation of the proposed approach on an open source project is presented. Preliminary results illustrate that the introduced approach could be successfully used to improve existing integrated development environments.,Automatic Recommendation of Move Method Refactorings using Clustering Ensembles,"In this paper, we are approaching the problem of automatic refactoring recommendation for object-oriented systems. An approach based on clustering ensembles is proposed, several heuristics to existing algorithms and to filtering and combining their results are discussed. Experimental validation of the proposed approach on an open source project is presented. The obtained preliminary results illustrate that the introduced approach could be successfully used to improve existing integrated development environments, providing developers with one more tool to reduce the complexity of their projects. The paper concludes with a discussion on the applicability of such automatic refactoring recommendation approaches to real-world software developed using common object-oriented techniques.", 
Design patterns are a reusable solution to a commonly occurring design problem in certain context. Paper proposes a methodology for automatic selection of the fit design pattern from a list of patterns. The proposed methodology is based on Text retrieval approach.,Automatic Recommendation of Software Design Patterns Text Retrieval Approach,"Design pattern is a reusable solution to a commonly occurring design problem in certain context. Using design patterns in software development improves the product’s quality, understandability and productivity. However, it is a challenging task for novice developers to select the right design pattern to solve a design problem. The paper proposes a methodology for the automatic selection of the fit design pattern from a list of patterns. The proposed methodology is based on Text retrieval approach where the design problem scenarios are described in natural language. A vector space model (VSM) was created for the catalogue of design patterns. A vector of features consists of unigrams and bigrams is generated for the given design problem scenario. The recommended design pattern is the closest to the problem scenario. The proposed mechanism was evaluated using the Gang of four design patterns and the experimental results showed the effectiveness of the proposed methodology.", 
"Proposed work uses key concepts identified from a document for creating a summary of the document. We view single-word or multi-word keyphrases of a document as the important concepts that a document elaborates on. For each important concept, we select one sentence that is the best possible elaboration of the concept. We have compared it to some state-of-the art summarization systems to prove its effectiveness.",Automatic Single Document Text Summarization Using Key Concepts in Documents,"Many previous research studies on extractive text summarization consider a subset of words in a document as keywords and use a sentence ranking function that ranks sentences based on their similarities with the list of extracted keywords. But the use of key concepts in automatic text summarization task has received less attention in literature on summarization. The proposed work uses key concepts identified from a document for creating a summary of the document. We view single-word or multi-word keyphrases of a document as the important concepts that a document elaborates on. Our work is based on the hypothesis that an extract is an elaboration of the important concepts to some permissible extent and it is controlled by the given summary length restriction. In other words, our method of text summarization chooses a subset of sentences from a document that maximizes the important concepts in the final summary. To allow diverse information in the summary, for each important concept, we select one sentence that is the best possible elaboration of the concept. Accordingly, the most important concept will contribute first to the summary, then to the second best concept, and so on. To prove the effectiveness of our proposed summarization method, we have compared it to some state-of-the art summarization systems and the results show that the proposed method outperforms the existing systems to which it is compared.", 
"A number of biomedical signals, such as EEG, EMG, ECG and EOG are used in sleep labs for diagnosis and treatment of sleep related disorders. In this work an attempt was made to classify four sleep stages consisting of Awake, Stage1 + REM, Stage 2 and Slow Wave Stage.",Automatic Sleep Stage Classification Based on EEG Signals by Using Neural Networks and Wavelet Packet Coefficients,"Currently in the world there is an alarming number of people who suffer from sleep disorders. A number of biomedical signals, such as EEG, EMG, ECG and EOG are used in sleep labs among others for diagnosis and treatment of sleep related disorders. The usual method for sleep stage classification is visual inspection by a sleep specialist. This is a very time consuming and laborious exercise. Automatic sleep stage classification can facilitate this process. The definition of sleep stages and the sleep literature show that EEG signals are similar in Stage1 of non-rapid eye movement (NREM) sleep and rapid eye movement (REM) sleep. Therefore, in this work an attempt was made to classify four sleep stages consisting of Awake, Stage1 + REM, Stage 2 and Slow Wave Stage based on the EEG signal alone. Wavelet packet coefficients and artificial neural networks were deployed for this purpose. Seven all night recordings from Physionet database were used in the study. The results demonstrated that these four sleep stages could be automatically discriminated from each other with a specificity of 94.4 ± 4.5%, a of sensitivity 84.2+3.9% and an accuracy of 93.0 ± 4.0%.", 
"Deep learning-based method that exploits the time–frequency spectrum of EEG signal. The method can achieve near state of the art accuracy even using single channel EEG signal, they say.",Automatic sleep stage classification using time_frequency images of CWT and transfer learning using convolution neural network,"For automatic sleep stage classification, the existing methods mostly rely on hand-crafted features selected from polysomnographic records. In this paper, the goal is to develop a deep learning-based method by using single channel electroencephalogram (EEG) that automatically exploits the time–frequency spectrum of EEG signal, removing the need for manual feature extraction. The time–frequency RGB color images for EEG signal are extracted using continuous wavelet transform (CWT). The transfer learning of a pre-trained convolution neural network, squeezenet is employed to classify these CWT images into its sleep stages. The proposed method is evaluated using a publicly available Physionet sleep EDFx dataset using single-channel EEG Fpz-Cz channel. Evaluation results show that this method can achieve near state of the art accuracy even using single channel EEG signal.", 
"In software engineering, automatic summarization is an emerging area that shows great potential. It poses new and exciting research challenges.","Automatic software summarization, the state of the art","Automatic text summarization has been widely studied for more than fifty years. In software engineering, automatic summarization is an emerging area that shows great potential and poses new and exciting research challenges. This technical briefing provides an introduction to the state of the art and maps future research directions in automatic software summarization.", 
We have developed an extraction based summarizer based on a word space model and PageRank. We compared the readability of the resulting summaries with the original text. The results show that the summarized texts are more readable.,"Automatic summarization as means of simplifying texts, an evaluation for swedish","We have developed an extraction based summarizer based on a word space model and PageRank and compared the readability of the resulting summaries with the original text, using various measures for Swedish and texts from different genres. The measures include among others readability index (LIX), nominal ratio (NR) and word variation index (OVIX). The measures correspond to the vocabulary load, idea density, human interest and sentence structure of the text and can be used to indicate the difficulty a reader might have in processing the text. The results show that the summarized texts are more readable, indicating that summarization can be used to reduce the effort to read a text.", 
Summarization techniques aim to extract the fundamental information in documents. This paper presents an automatic summarization application for Android devices.,Automatic Summarization of News Articles in Mobile Devices,"Smartphones and tablets provide access to the Web anywhere and anytime. Automatic Text Summarization techniques aim to extract the fundamental information in documents. Making automatic summarization work in portable devices is a challenge, in several aspects. This paper presents an automatic summarization application for Android devices. The proposed solution is a multi-feature language independent summarization application targeted at news articles. Several evaluation assessments were conducted and indicate that the proposed solution provides good results.", 
"Debate summarization is one of the novel and challenging research areas in automatic text summarization. We view that the generation of debate summaries can be achieved by clustering, cluster labeling, and visualization.",Automatic Summarization of Online Debates,"Debate summarization is one of the novel and challenging research areas in automatic text summarization which has been largely unexplored. In this paper, we develop a debate summarization pipeline to summarize key topics which are discussed or argued in the two opposing sides of online debates. We view that the generation of debate summaries can be achieved by clustering, cluster labeling, and visualization. In our work, we investigate two different clustering approaches for the generation of the summaries. In the first approach, we generate the summaries by applying purely term-based clustering and cluster labeling. The second approach makes use of X-means for clustering and Mutual Information for labeling the clusters. Both approaches are driven by ontologies. We visualize the results using bar charts. We think that our results are a smooth entry for users aiming to receive the first impression about what is discussed within a debate topic containing waste number of argumentations.", 
This paper describes the automatic summarization system developed for the Polish language. The system implements sentence-based extractive summarization technique. The presented attempt is intended to serve as the baseline for future solutions.,Automatic summarization of Polish news articles by sentence selection,"This paper describes the automatic summarization system developed for the Polish language. The system implements sentence-based extractive summarization technique, which consists in determining most important sentences in document due to their computed salience. A structure of the system is presented, as well as the evaluation method and achieved results. The presented attempt is intended to serve as the baseline for future solutions, as it is the first summarization project evaluated against the Polish Summaries Corpus, the standardized corpus of summaries for the Polish language.", 
"This paper presents the performance analysis of an automatic Turkish document summarization system. The preprocessing method called ""Consecutive Words Detection"" is an innovative approach that uses Turkish Wikipedia links.",Automatic summarization of Turkish documents using non-negative matrix factorization,"Automatic document summarization is a process, where a computer summarizes a document. This paper presents the performance analysis of an automatic Turkish document summarization system that applies Non-negative matrix factorization based summarization algorithm with different preprocessing methods. The preprocessing method called “Consecutive Words Detection” is an innovative approach that uses Turkish Wikipedia links to represent related consecutive words as a single term and the result of the evaluation process is promising for document summarization in Turkish.", 
"Software development video tutorials are emerging as a new resource for developers to support their information needs. Currently, developers have little information at their disposal to quickly decide if they found the right video or not. This can lead to missing the best tutorials or wasting time watching irrelevant ones. We propose the first set of approaches to automatically generate tags describing the contents of video tutorials.",Automatic Tag Recommendation for Sofware Development Video Tutorials,"Software development video tutorials are emerging as a new resource for developers to support their information needs. However, when trying to find the right video to watch for a task at hand, developers have little information at their disposal to quickly decide if they found the right video or not. This can lead to missing the best tutorials or wasting time watching irrelevant ones. Other external sources of information for developers, such as StackOver?ow, have benefited from the existence of informative tags, which help developers to quickly gauge the relevance of posts and find related ones. We argue that the same is valid also for videos and propose the first set of approaches to automatically generate tags describing the contents of software development video tutorials. We investigate seven tagging approaches for this purpose, some using information retrieval techniques and leveraging only the information in the videos, others relying on external sources of information, such as StackOver?ow, as well as two out-of-the-box commercial video tagging approaches. We evaluated 19 di?erent configurations of these tagging approaches and the results of a user study showed that some of the information retrieval-based approaches performed the best and were able to recommend tags that developers consider relevant for describing programming videos.", 
"This paper presents a semantic word processing technique for text categorization that utilizes semantic keywords. The Back Propagation Lion algorithm (BP Lion algorithm) is also proposed to overcome the problem in updating the neuron weight. The performance of the proposed BPLion is analysed, in terms of sensitivity, specificity and accuracy.",Automatic text classification using BPLion-neural network and semantic word processing,"Text mining has become a major research topic in which text classification is the important task for finding the relevant information from the new document. Accordingly, this paper presents a semantic word processing technique for text categorization that utilizes semantic keywords, instead of using independent features of the keywords in the documents. Hence, the dimensionality of the search space can be reduced. Here, the Back Propagation Lion algorithm (BP Lion algorithm) is also proposed to overcome the problem in updating the neuron weight. The proposed text classification methodology is experimented over two data sets, namely, 20 Newsgroup and Reuter. The performance of the proposed BPLion is analysed, in terms of sensitivity, specificity, and accuracy, and compared with the performance of the existing works. The result shows that the proposed BPLion algorithm and semantic processing methodology classifies the documents with less training time and more classification accuracy of 90.9%.", 
This article makes use of Machine Learning techniques to assess the quality of the twenty most referenced strategies used in extractive summarization. The experiments were performed on the CNN-corpus.,Automatic Text Document Summarization Based on Machine Learning,"The need for automatic generation of summaries gained importance with the unprecedented volume of information available in the Internet. Automatic systems based on extractive summarization techniques select the most significant sentences of one or more texts to generate a summary. This article makes use of Machine Learning techniques to assess the quality of the twenty most referenced strategies used in extractive summarization, integrating them in a tool. Quantitative and qualitative aspects were considered in such assessment demonstrating the validity of the proposed scheme. The experiments were performed on the CNN-corpus, possibly the largest and most suitable test corpus today for benchmarking extractive summarization.", 
"Information overloading has increased the necessity of more sophisticated summarizers. This paper introduces a taxonomy of summarization methods. A special attention is devoted to application of recent information reduction methods, based on algebraic transformations.",Automatic Text Summarization (The state of the art 2007 and new challenges),"The headline of this paper names a research area originating from the late 50’s but not loosing its popularity until the present time. Moreover, one of the most relevant today’s problems caused by the rapid growth of the Web, which is called information overloading, has increased the necessity of more sophisticated and powerful summarizers. This paper shortly introduces a taxonomy of summarization methods and an overview of their principles from classical ones, over corpus based, to knowledge rich approaches. We consider various aspects which can affect their classification. A special attention is devoted to application of recent information reduction methods, based on algebraic transformations. Further, we introduce experiences with the development of our own summarizing method. Finally, some new ideas and a conception for the future of this field are mentioned.", 
"Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet. Researchers have been trying to improve ATS techniques since the 1950s. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical.",Automatic Text Summarization A Comprehensive Survey,"Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.", 
This paper provides an overview of the most prominent algorithms for automatic text summarization that were proposed in the last years. It describes automatic and manual evaluation.,Automatic Text Summarization A State-of-the-Art Review,"Despite the progress that has been achieved in over 50 years of research, automatic text summarization systems are still far from perfect, posing many challenges to the researchers in the field. This paper provides an overview of the most prominent algorithms for automatic text summarization that were proposed in the last years, as well as describes automatic and manual evaluation methods that are currently widely adopted.", 
"In this tutorial, we consider important aspects for tagging both unstructured and structured text for downstream use. This includes summarization, in which text information is compressed.",Automatic Text Summarization and Classification,"In this tutorial, we consider important aspects (algorithms, approaches, considerations) for tagging both unstructured and structured text for downstream use. This includes summarization, in which text information is compressed for more efficient archiving, searching, and clustering. In the tutorial, we focus on the topic of automatic text summarization, covering the most important milestones of the six decades of research in this area.", 
"In this article, we develop a novel approach to extractive text summarization by modelling texts and documents as small-world networks. We model a document as a one-parameter family of graphs with its sentences or paragraphs defining the vertices. We demonstrate that for some range of parameters, the resulting graph becomes a small- world network.",Automatic text summarization and small-world networks,"Automatic text summarization is an important and challenging problem. Over the years, the amount of text available electronically has grown exponentially. This growth has created a huge demand for automatic methods and tools for text summarization. We can think of automatic summarization as a type of information compression. To achieve such compression, better modelling and understanding of document structures and internal relations is required. In this article, we develop a novel approach to extractive text summarization by modelling texts and documents as small-world networks. Based on our recent work on the detection of unusual behavior in text, we model a document as a one-parameter family of graphs with its sentences or paragraphs defining the vertex set and with edges defined by Helmholtz’s principle. We demonstrate that for some range of the parameters, the resulting graph becomes a small-world network. Such a remarkable structure opens the possibility of applying many measures and tools from social network theory to the problem of extracting the most important sentences and structures from text documents. We hope that documents will be also a new and rich source of examples of complex networks.", 
Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate.,Automatic Text Summarization Approaches to Speed up Topic Model Learning Process,"The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.", 
"Preliminary research is done to examine the potential of a new application for text summarization algorithms. A framework is built for parsing and highlighting PDF files, to which extraction strategies can be applied.",Automatic text summarization as a text extraction strategy for effective automated highlighting,"Automatic text highlighting is capable of becoming a new tool in textual information processing. Preliminary research is done to examine the potential of a new application for text summarization algorithms. A framework is built for parsing and highlighting PDF files, to which extraction strategies can be applied. Also a small dataset of highlighted documents is gathered to test the highlighting capabilities of four text summarization algorithms on. Thereby, ROUGE-N scores are obtained, the performance per task and the performance per algorithm are evaluated. In the end, the algorithms perform surprisingly well and may encourage more investment in automatic text highlighting.", 
Automatic text summary can provide a solution to the problem of information overload. This article presents an unsupervised extractive approach based on graphs. The method constructs an indirected weighted graph from the original text. It calculates a weighted edge between each pair of sentences based on a similarity/dissimilarity criterion.,Automatic Text Summarization based on Betweenness Centrality,"Automatic text summary plays an important role in information retrieval. With a large volume of information, presenting the user only a summary greatly facilitates the search work of the most relevant. Therefore, this task can provide a solution to the problem of information overload. Automatic text summary is a process of automatically creating a compressed version of a certain text that provides useful information for users. This article presents an unsupervised extractive approach based on graphs. The method constructs an indirected weighted graph from the original text by adding a vertex for each sentence, and calculates a weighted edge between each pair of sentences that is based on a similarity/dissimilarity criterion. The main contribution of the work is that we do a study of the impact of a known algorithm for the social network analysis, which allows to analyze large graphs efficiently. As a measure to select the most relevant sentences, we use betweenness centrality. The method was evaluated in an open reference data set of DUC2002 with Rouge scores.", 
Automatic summarization is considered to be an effective means of processing information resources. A method of text summarization based on latent semantic indexing (LSI) is proposed in this article. It improves the accuracy of sentence similarity calculations and subject delineation.,Automatic text summarization based on latent semantic indexing,"Automatic summarization is a topic of common concern in computational linguistics and information science, since a computer system of text summarization is considered to be an effective means of processing information resources. A method of text summarization based on latent semantic indexing (LSI), which uses semantic indexing to calculate the sentence similarity, is proposed in this article. It improves the accuracy of sentence similarity calculations and subject delineation, and helps the abstracts generated to cover the documents comprehensively as well as reducing redundancies. The effectiveness of the method is proved by the experimental results. Compared with the traditional keyword-based vector space model method of automatic text summarization, the quality of the abstracts generated was significantly improved.", 
Concept chains are a technique for identifying semantically-related terms in text. The resulting concept chains are then used to identify candidate sentences useful for extraction. The goal is to create an efficient tool that is able to summarize large documents automatically.,Automatic Text Summarization Based on Lexical Chains and Structural Features,"The rapid growth of the Internet has resulted in enormous amounts of information that has become more difficult to access efficiently. The primary goal of this research is to create an efficient tool that is able to summarize large documents automatically. We propose concept chains to link semantically-related concepts based on Hownet knowledge database to improve the performance of Text Summarization and suit Chinese text. Lexical chains is a technique for identifying semantically-related terms in text. The resulting concept chains are then used to identify candidate sentences useful for extraction. Moreover, the other method based on structural features which can makes the summary of the text have more general content and more balance is also proposed. The final experimental results proved the effectiveness of our methods.", 
"Text summarization is the objective extraction of some parts of the text. In this paper, a method based on multi-agent particle swarm optimization approach is proposed to improve the extractive text summarization. The proposed method is tested on DUC 2002 standard documents.",Automatic text summarization based on multi-agent particle swarm optimization,"Text summarization is the objective extraction of some parts of the text, such as sentence and paragraph, as the document abstract. If there are documents with a large amount of information, extractive text summarization would be arisen as an NP-complete problem. To solve these problems, metaheuristic algorithms are used. In this paper, a method based on multi-agent particle swarm optimization approach is proposed to improve the extractive text summarization. In this method, each particle will be upgraded with the status of multi-agent systems. The proposed method is tested on DUC 2002 standard documents and analyzed by ROUGE evaluation software. The experimental results show that this method has better performance than other compared methods.", 
Automatic text summarization is a technique where the text is input to the computer and it returns the clipped and concise extract of the original text. The proposed system generates the summary of the fixed format documents by analyzing all the different parts of the documents.,Automatic Text Summarization Based on Pragmatic Analysis,"The rapid growth of online information has encumbered the user with colossal amount of information. It is difficult to access large amount of data. This problem has increased the research in the field of automatic text summarization. Automatic text summarization is a technique where the text is input to the computer and it returns the clipped and concise extract of the original text and also sustains the overall meaning and main information content. In this paper, text summarization technique is designed for the documents having the fixed format. The proposed system generates the summary of the fixed format documents by analyzing all the different parts of the documents. The system consists of five stages. In first stage each sentence is partitioned into the list of tokens and stop words are removed. In second stage, frequency usage is counted for each word. In third stage, assign POS tag for each weighted term and Word sense disambiguation is done. In the fourth stage, pragmatic analysis is performed. After Pragmatic Analysis, summarized sentences will be store in a database.", 
Rhetorical Structure Theory (RST) is an analytic framework designed to account for text structure in text at the clause level. System extracts rhetorical structure of text and compound of rhetorical relations between sentences. Then it cuts out less important parts from the extracted structure.,Automatic Text Summarization Based On Rhetorical Structure Theory,"In this paper, we present an automatic text summarization method based on Rhetorical Structure Theory (RST). The first part gives a general introduction to the Rhetorical Structure Theory. Rhetorical Structure Theory (RST) is an analytic framework designed to account for text structure in text at the clause level. The system extracts the rhetorical structure of text and the compound of the rhetorical relations between sentences, and then cuts out less important parts from the extracted structure. Finally it uses the natural language generation method based on model to produce an exact and ?uent summarization. The summarization generated in this way shows its e?ectiveness and high quality.", 
This paper's goal is to define a measurement for text summarization using Semantic Analysis Approach for Documents in Indonesian language. The applied measurement requires Indonesian version of WordNet which had been implemented roughly.,Automatic text summarization based on semantic analysis approach for documents in Indonesian language,"Research about text summarization has been quite an interesting topic over the years, proven by numerous number of papers related with discussion of their studies such as approaches, challenges and trends. This paper's goal is to define a measurement for text summarization using Semantic Analysis Approach for Documents in Indonesian language. The applied measurement requires Indonesian version of WordNet which had been implemented roughly. The main idea of semantic analysis is to obtain the similarity between sentences by calculating the vector values of each sentence with the title. The need of WordNet is to define the depth of each word as being computed for word similarity. Combining all required formulas and calculations, a compact and precise summarization is produced without depriving the gist information of certain document.",  
The Maximum Marginal Relevance (MMR) method has the disadvantage of having parts that are separated from each other in summary results that are not semantically connected. Semantic-based MMR methods utilize WordNet Bahasa and corpus in processing text summaries. The MMR method is non-semantic based on the TF-IDF method.,Automatic Text Summarization Based on Semantic Networks and Corpus Statistics,"One simple automatic text summarization method that can minimize redundancy, in summary, is the Maximum Marginal Relevance (MMR) method. The MMR method has the disadvantage of having parts that are separated from each other in summary results that are not semantically connected. Therefore, this study aims to compare summary results using the MMR method based on semantic and non-semantic based MMR. Semantic-based MMR methods utilize WordNet Bahasa and corpus in processing text summaries. The MMR method is non-semantic based on the TF-IDF method. This study also carried out summary compression of 30%, 20%, and 10%. The research data used is 50 online news texts. Testing of the summary text results is done using the ROUGE toolkit. The results of the study state that the best value of the f-score in the semantic-based MMR method is 0.561, while the best f-score in the non-semantic MMR method is 0.598. This value is generated by adding a preprocessing process in the form of stemming and compression of a 30% summary result. The difference in value obtained is due to incomplete WordNet Bahasa and there are several words in the news title that are not in accordance with EYD (KBBI).", 
Text summarization is a process of reducing the size of a text while preserving its information content. This paper proposes a sentences clustering based summarization approach. The experimental result on the DUC 2003 dataset show that our proposed approach can improve the performance compared to other summarization methods.,Automatic text summarization based on sentences clustering and extraction,"Technology of automatic text summarization plays an important role in information retrieval and text classification, and may provide a solution to the information overload problem. Text summarization is a process of reducing the size of a text while preserving its information content. This paper proposes a sentences clustering based summarization approach. The proposed approach consists of three steps: first clusters the sentences based on the semantic distance among sentences in the document, and then on each cluster calculates the accumulative sentence similarity based on the multi-features combination method, at last chooses the topic sentences by some extraction rules. The purpose of present paper is to show that summarization result is not only depends the sentence features, but also depends on the sentence similarity measure. The experimental result on the DUC 2003 dataset show that our proposed approach can improve the performance compared to other summarization methods.", 
This paper considers the issues of automatic summarization of text documents. It takes into account the syntactic relations between words and word forms in sentences. It uses the Link Gramma Parser (LGP) system for the Kazakh and Turkish languages. The authors operate on the results of studies on customizing the LGP parser for agglutinative languages.,Automatic text summarization based on syntactic links,"The task of information retrieval is to find documents relevant to the query in a certain collection of documents. The document is a text selected by the author as a single fragment. A query is usually a meaningful phrase or set of words describing the information needed. Instead of searching through the whole document, organizing a search by topic or resume of the document becomes enough. By the term ""topic"" we refer to a set of small reference texts. Therefore, one of the interesting tasks in information retrieval systems is the task of classifying texts by topic. The whole classification process is carried out in four stages: preprocessing the text, weighing the terms, weighing the sentences, extracting meaningful sentences. In the process of selecting topics, fragments of the text are studied (for example, paragraphs) and compared with the chosen standard. Different fragments can be attributed to different topics. Selected fragments can be combined into a summary on this topic. This paper considers the issues of automatic summarization of text documents taking into account the syntactic relations between words and word forms in sentences that can be obtained at the output of the Link Gramma Parser (LGP) system for the Kazakh and Turkish languages. The authors operate on the results of studies on customizing the LGP parser for agglutinative languages.", 
"This paper presents two different algorithms that derive the cohesion structure in the form of lexical chains from two kinds of language resources. It also presents a novel model of automatic text summarization, based on the data provided by the data from original texts.",Automatic text summarization based on textual cohesion,"This paper presents two different algorithms that derive the cohesion structure in the form of lexical chains from two kinds of language resources HowNet and TongYiCiCiLin. The research that connects the cohesion structure of a text to the derivation of its summary is displayed. A novel model of automatic text summarization is devised, based on the data provided by lexical chains from original texts. Moreover, the construction rules of lexical chains are modified according to characteristics of the knowledge database in order to be more suitable for Chinese summarization. Evaluation results show that high quality indicative summaries are produced from Chinese texts.", 
This paper investigates a new approach for Single Document Summarization based on a Machine Learning ranking algorithm. We believe that the classification criterion for training a classifier is not adapted for SDS. We analyze the performance of our ranking algorithm on two data sets. The experiments show that the learning algorithms perform better than the non-learning systems. The difference of performance between the two learning algorithms depends on the nature of datasets.,Automatic Text Summarization Based on Word-Clusters and Ranking Algorithms,"This paper investigates a new approach for Single Document Summarization based on a Machine Learning ranking algorithm. The use of machine learning techniques for this task allows one to adapt summaries to the user needs and to the corpus characteristics. These desirable properties have motivated an increasing amount of work in this field over the last few years. Most approaches attempt to generate summaries by extracting text-spans (sentences in our case) and adopt the classification framework which consists to train a classifier in order to discriminate between relevant and irrelevant spans of a document. A set of features is first used to produce a vector of scores for each sentence in a given document and a classifier is trained in order to make a global combination of these scores. We believe that the classification criterion for training a classifier is not adapted for SDS and propose an original framework based on ranking for this task. A ranking algorithm also combines the scores of different features but its criterion tends to reduce the relative misordering of sentences within a document. Features we use here are either based on the state-of-the-art or built upon word-clusters. These clusters are groups of words which often co-occur with each other, and can serve to expand a query or to enrich the representation of the sentences of the documents. We analyze the performance of our ranking algorithm on two data sets - the Computation and Language (cmp_lg) collection of TIPSTER SUMMAC and the WIPO collection. We perform comparisons with different baseline - non learning - systems, and a reference trainable summarizer system based on the classification framework. The experiments show that the learning algorithms perform better than the non-learning systems while the ranking algorithm outperforms the classifier. The difference of performance between the two learning algorithms depends on the nature of datasets. We give an explanation of this fact by the different separability hypothesis of the data made by the two learning algorithms.", 
"Automatic Text Summarization (ATS) is ""condensing the source text into a shorter version, while preserving its information content and overall meaning"" The work of automatic text summarization started in 1950's, still it is lacking to achieve more coherent and meaningful summaries. The proposed approach provides automatic feature based extractive heading wise text summarizer.",Automatic text summarization by local scoring and ranking for improving coherence,"Existence of large amount of textual information available on the internet emerged serious research in the area of machine generated summarization. Manual summarization of these online text documents is a very difficult task for human beings. So we need an automatic text summarizer. Automatic Text Summarization (ATS) is “condensing the source text into a shorter version, while preserving its information content and overall meaning”. Even though the work of automatic text summarization started in 1950’s, still it is lacking to achieve more coherent and meaningful summaries. The proposed approach provides automatic feature based extractive heading wise text summarizer to improve the coherence thereby improving the understandability of the summary text. It summarizes the given input document using local scoring and local ranking that is it provides heading wise summary. Headings of a document give contextual information and permit visual scanning of the document to find the search contents. The proposed approach applies the same features to all document sentences. But it ranks the sentences heading wise and selects top n sentences from each heading where n depends upon compression ratio. The final heading wise summary produced by this approach is a collection of summary of individual headings. Since the heading wise summary contains the equal proportion of sentences from each heading, it reduces the coherent gap of the summary text. Also it improves the overall meaning and understanding of the summary text. The outcomes of the experiment clearly show that heading wise summarizer provides better precision, recall and f-measure over the main summarizer, Ms-word summarizer, free summarizer and Auto summarizer.", 
Proposed method is based on convex minimization and the properties of the l1 norm.,Automatic Text Summarization by Mean-absolute Constrained Convex Optimization,"In this paper we propose a new algorithm for extractive text summarization. The proposed method is based on convex minimization and the properties of the l1 norm. The algorithm has some advantages, like the extensibility and the ability to easy take into account additional information and constraints. It provides very good results, but its execution time is usually larger than that of other similar procedures.", 
We investigate a novel framework for Automatic Text Summarization. In this framework underlying language-use features are learned from a minimal sample corpus. Low complexity of features allows relying in generalization ability of a learning machine.,Automatic Text Summarization by Non-topic Relevance Estimation,"We investigate a novel framework for Automatic Text Summarization. In this framework underlying language-use features are learned from a minimal sample corpus. We argue the low complexity of this kind of features allows relying in generalization ability of a learning machine, rather than in diverse human-abstracted summaries. In this way, our method reliably estimates a relevance measure for predicting summary candidature scores, regardless topics in unseen documents. Our output summaries are comparable to the state-of-the-art. Thus we show that in order to extract meaning summaries, it is not crucial what is being said; but rather how it is being said.", 
Automatic text summarization for dialogue style is a relatively new research area. Local regions of coherence in dialogue documents often stretch across different speakers. This paper proposes an approach to automatically identify those regions of local coherence.,Automatic Text Summarization for Dialogue Style,"Automatic text summarization for dialogue style is a relatively new research area. Some key techniques are proposed in this paper, such as identifying whether or not the style of document is dialogue, identifying all of questions and their corresponding answers and linking them to form question-answer pairs. Due to the interactive nature of dialogue documents, local regions of coherence often stretch across different speakers. An approach to automatically identify those regions of local coherence is presented. Firstly, we identify all question paragraphs in a given document. Secondly, we detect the answer paragraphs of each question paragraph and exploit text segmentation to form question-answer pairs. Lastly, we extract some sentences from the generic content and the question-answer pairs to generate a complete summary. Experimental results show that our approach is high efficient and improves significantly the coherency of the summary while not compromising informativeness.", 
"Reducing text without losing the meaning not only can save time to read, but also maintain the reader's understanding. One of many algorithms to summarize text is TextTeaser. Originally, this algorithm is intended to be used for text in English.",Automatic Text Summarization for Indonesian Language Using TextTeaser,"Text summarization is one of the solution for information overload. Reducing text without losing the meaning not only can save time to read, but also maintain the reader’s understanding. One of many algorithms to summarize text is TextTeaser. Originally, this algorithm is intended to be used for text in English. However, due to TextTeaser algorithm does not consider the meaning of the text, we implement this algorithm for text in Indonesian language. This algorithm calculates four elements, such as title feature, sentence length, sentence position and keyword frequency. We utilize TextRank, an unsupervised and language independent text summarization algorithm, to evaluate the summarized text yielded by TextTeaser. The result shows that the TextTeaser algorithm needs more improvement to obtain better accuracy.", 
Thousands of electronic documents are produced and made available on the internet each day. Having a Text Summarization system would be immensely useful in serving this need. The objective of automatic text summarization is to extract essential sentences that cover almost all the concepts of a document.,Automatic Text Summarization for Oriya Language,"With the coming of the information revolution, electronic documents are becoming a principle media of business and academic information. Thousands and thousands of electronic documents are produced and made available on the internet each day. In order to fully utilizing these on-line documents effectively, it is crucial to be able to extract the gist of these documents. Having a Text Summarization system would thus be immensely useful in serving this need. The objective of automatic text summarization is to extract essential sentences that cover almost all the concepts of a document so that users are able to comprehend the ideas the documents tries to address by simply reading through the corresponding summary. In this paper we investigate some novel technique to develop an effective automatic Oriya text summarizer. These techniques can efficiently and effectively save users? time while summarizing a particular text.", 
The COVID-19 Open Research Dataset Challenge has released a corpus of scholarly articles. It is calling for machine learning approaches to help bridging the gap between researchers and the rapidly growing publications.,Automatic Text Summarization of COVID-19 Medical Research Articles using BERT and GPT-2,"With the COVID-19 pandemic, there is a growing urgency for medical community to keep up with the accelerating growth in the new coronavirus-related literature. As a result, the COVID-19 Open Research Dataset Challenge has released a corpus of scholarly articles and is calling for machine learning approaches to help bridging the gap between the researchers and the rapidly growing publications. Here, we take advantage of the recent advances in pre-trained NLP models, BERT and OpenAI GPT-2, to solve this challenge by performing text summarization on this dataset. We evaluate the results using ROUGE scores and visual inspection. Our model provides abstractive and comprehensive information based on keywords extracted from the original articles. Our work can help the the medical community, by providing succinct summaries of articles for which the abstract are not already available.", 
Lawyers spend a lot of time preparing legal briefs of their clients' case files. Automatic Text summarization is a constantly evolving field of Natural Language Processing. This paper proposes a hybrid method for automatic text summarization of legal cases.,"Automatic Text Summarization of Legal Cases, A Hybrid Approach","Manual summarization of large bodies of text involves a lot of human effort and time, especially in the legal domain. Lawyers spend a lot of time preparing legal briefs of their clients’ case files. Automatic Text summarization is a constantly evolving field of Natural Language Processing(NLP), which is a subdiscipline of the Artificial Intelligence Field. In this paper a hybrid method for automatic text summarization of legal cases using k-means clustering technique and tf-idf(term frequency-inverse document frequency) word vectorizer is proposed. The summary generated by the proposed method is compared using ROGUE evaluation parameters with the case summary as prepared by the lawyer for appeal in court. Further, suggestions for improving the proposed method are also presented.", 
"Text Summarization has always been an area of active interest in the academia. Given the increase in size and number of documents available online, an efficient automatic news summarizer is the need of the hour.",Automatic Text Summarization of News Articles,"Text Summarization has always been an area of active interest in the academia. In recent times, even though several techniques have being developed for automatic text summarization, efficiency is still a concern. Given the increase in size and number of documents available online, an efficient automatic news summarizer is the need of the hour. In this paper, we propose a technique of text summarization which focuses on the problem of identifying the most important portions of the text and producing coherent summaries. In our methodology, we do not require full semantic interpretation of the text, instead we create a summary using a model of topic progression in the text derived from lexical chains. We present an optimized and efficient algorithm to generate text summary using lexical chains and using the WordNet thesaurus. Further, we also overcome the limitations of the lexical chain approach to generate a good summary by implementing pronoun resolution and by suggesting new scoring techniques to leverage the structure of news articles.", 
"Since 2001, the Document Understanding Conferences have been the forum for researchers in automatic text summarization to compare methods and results on common test sets. This paper is an overview of the achieved results in the different types of summarization tasks. Most progress in the field has been achieved in generic multi-document summarization. The most challenging task is that of producing a focused summary in answer to a question/topic.","Automatic text summarization of newswire, Lessons learned from the document understanding conference","Since 2001, the Document Understanding Conferences have been the forum for researchers in automatic text summarization to compare methods and results on common test sets. Over the years, several types of summarization tasks have been addressed—single document summarization, multi-document summarization, summarization focused by question, and headline generation. This paper is an overview of the achieved results in the different types of summarization tasks. We compare both the broader classes of baselines, systems and humans, as well as individual pairs of summarizers (both human and automatic). An analysis of variance model is fitted, with summarizer and input set as independent variables, and the coverage score as the dependent variable, and simulation-based multiple comparisons were performed. The results document the progress in the field as a whole, rather then focusing on a single system, and thus can serve as a future reference on the work done up to date, as well as a starting point in the formulation of future tasks. Results also indicate that most progress in the field has been achieved in generic multi-document summarization and that the most challenging task is that of producing a focused summary in answer to a question/topic.", 
"Text summarization systems are used to identify the most important information from the given text. In this paper, Wikipedia articles are given as input to system. Text is pre-processed to tokenize the sentences and perform stemming operations. We then score the sentences using the different text features.",Automatic text summarization of Wikipedia articles,"The main objective of a text summarization system is to identify the most important information from the given text and present it to the end users. In this paper, Wikipedia articles are given as input to system and extractive text summarization is presented by identifying text features and scoring the sentences accordingly. The text is first pre-processed to tokenize the sentences and perform stemming operations. We then score the sentences using the different text features. Two novel approaches implemented are using the citations present in the text and identifying synonyms. These features along with the traditional methods are used to score the sentences. The scores are used to classify the sentence to be in the summary text or not with the help of a neural network. The user can provide what percentage of the original text should be in the summary. It is found that scoring the sentences based on citations gives the best results.", 
This paper concentrates on single document multi news Punjabi extractive summarizer. Selection of sentences is on the basis of statistical and linguistic features of sentences. Various linguistic resources for Punjabi were also developed first time as part of this project. It is first time that this system has been developed for Punjabi language and is available online at: http://pts.learnpunjab.org/. Punjab is one of Indian states and Punjabi is its official language.,Automatic text summarization system for Punjabi language,"This paper concentrates on single document multi news Punjabi extractive summarizer. Although lot of research is going on in field of multi document news summarization systems but not even a single paper was found in literature for single document multi news summarization for any language. It is first time that this system has been developed for Punjabi language and is available online at: http://pts.learnpunjabi.org/. Punjab is one of Indian states and Punjabi is its official language. Punjabi is under resourced language. Various linguistic resources for Punjabi were also developed first time as part of this project like Punjabi noun morph, Punjabi stemmer and Punjabi named entity recognition, Punjabi keywords identification, normalization of Punjabi nouns etc. A Punjabi document (like single page of Punjabi E-news paper) can have hundreds of multi news of varying length. Based on compression ratio selected by user, this system starts by extracting headlines of each news, lines just next to headlines and other important lines depending upon their importance. Selection of sentences is on the basis of statistical and linguistic features of sentences. This system comprises of two main steps: Pre Processing and Processing phase. Pre Processing phase represents the Punjabi text in structured way. In processing phase, different features deciding the importance of sentences are determined and calculated. Some of the statistical features are Punjabi keywords identification, relative sentence length feature and numbered data feature. Various linguistic features for selecting important sentences in summary are: Punjabi headlines identification, identification of lines just next to headlines, identification of Punjabi-nouns, identification of Punjabi-proper-nouns, identification of common-English Punjabi-nouns, identification of Punjabi-cue-phrases and identification of title-keywords in sentences. Scores of sentences are determined from sentence-feature-weight equation. Weights of features are determined using mathematical regression. Using regression, feature values of some Punjabi documents which are manually summarized are treated as independent input values and their corresponding dependent output values are provided. In the training phase, manually summaries of fifty news documents are made by giving fuzzy scores to the sentences of those documents and then regression is applied for finding values of feature-weights and then average values of feature-weights are calculated. High scored sentences in proper order are selected for final summary. In final summary, sentences coherence is maintained by properly ordering the sentences in the same order as they appear in the input text at the selective compression ratios. This extractive Punjabi summarizer is available online.", 
"This paper focuses on the automatic text summarization by sentence extraction with important features based on fuzzy logic. The results show that the highest average precision, recall, and F-mean for the summaries are conducted from fuzzy method.",Automatic text summarization using feature-based fuzzy extraction,"Automatic text summarization is to compress the original text into a shorter version and help the user to quickly understand large volumes of information. This paper focuses on the automatic text summarization by sentence extraction with important features based on fuzzy logic. In our experiment, we used 6 test documents in DUC2002 data set. Each document is prepared by preprocessing process: sentence segmentation, tokenization, remuving Stop Word and stemming Word. Then, we use 8 important features and calculate their score for each sentence. We propose a method using fuzzy logic for sentence extraction and compare our result with the baseline summarizer and Microsoft Word 2007 summarizers. The results show that the highest average precision, recall, and F-mean for the summaries are conducted from fuzzy method.", 
"Text summarization process is commonly divided into two areas-Extractive and Abstractive. In this paper, a sentence-based model has been proposed where the sentence ranking procedure adopts fuzzy C-Means (FCM) clustering, an unsupervised classification method for sentence extraction purpose.",Automatic Text Summarization using Fuzzy C-Means Clustering,"Automatic text summarization process has been significantly explored throughout the years to cope with the staggering increase of virtual data. Text summarization process is commonly divided into two areas-Extractive and Abstractive. Extractive summarization processes largely depend on sentence extraction techniques- implementing graph models or sentence-based models. In this paper, a sentence-based model has been proposed where the sentence ranking procedure adopts fuzzy C-Means (FCM) clustering, an unsupervised classification method, for sentence extraction purpose. The sentence scoring task relies on five key features, including Topic Sentence which is the first novelty of the proposed model. Furthermore, C-Means clustering is a soft-computing technique that is usually used for pattern recognition tasks but can be improved significantly by hard clustering the membership of the elements which has not been regarded in similar processes in any of the previous works, adding to the novelty of the presented model. Standard summary evaluation techniques have been used to gauge the precision, recall and f-measure of the proposed FCM model and have been compared with different summarizers from different perspectives. The outcome shows that the FCM model surpasses the previous approaches significantly.", 
Summarizing the main idea and the major concept of the context enables the humans to read the summary of a large volume of text quickly. Most of the existing summarization approaches have applied probability and statistics based techniques. We simulate human like methods by integrating fuzzy logic with traditional statistical approaches.,Automatic text summarization using fuzzy inference,"Due to the high volume of information and electronic documents on the Web, it is almost impossible for a human to study, research and analyze this volume of text. Summarizing the main idea and the major concept of the context enables the humans to read the summary of a large volume of text quickly and decide whether to further dig into details. Most of the existing summarization approaches have applied probability and statistics based techniques. But these approaches cannot achieve high accuracy. We observe that attention to the concept and the meaning of the context could greatly improve summarization accuracy, and due to the uncertainty that exists in the summarization methods, we simulate human like methods by integrating fuzzy logic with traditional statistical approaches in this study. The results of this study indicate that our approach can deal with uncertainty and achieve better results when compared with existing methods.", 
A novel technique is proposed for summarizing text using a combination of Genetic Algorithms and Genetic Programming. The goal is to develop an optimal intelligent system to extract important sentences. Simulations demonstrate several significant improvements with the proposed approach.,Automatic Text Summarization Using Hybrid Fuzzy GA-GP,A novel technique is proposed for summarizing text using a combination of Genetic Algorithms (GA) and Genetic Programming (GP) to optimize rule sets and membership functions of fuzzy systems. The novelty of the proposed algorithm is that fuzzy system is optimized for extractive based text summarizing. In this method GP is used for structural part and GA for the string part (Membership functions). The goal is to develop an optimal intelligent system to extract important sentences in the texts by reducing the redundancy of data. The method is applied in 3 test documents and compared with the standard fuzzy systems as well as two other commercial summarizers: Microsoft word and Copernic Summarizer. Simulations demonstrate several significant improvements with the proposed approach., 
"Sentence ranking is an important step of an extractive text summarization process. Traditionally, sentences are ranked using the information internal to the specified document. We propose a hybrid summarization method that combines the internal methods and the external methods.",Automatic Text Summarization Using Intenal and Extemal Information,"Sentence ranking is an important step of an extractive text summarization process. Traditionally, sentences are ranked using the information internal to the specified document, that is, they only use the data that is self-contained in the document. External methods use the information that is not necessarily contained in the document, such as manual annotation, labeling, etc. In this paper, we propose a hybrid summarization method that combines the internal methods and the external methods for improving text summarization performance. Comparisons of our proposed summarization system with some state-of-the art summarization systems reveal that our proposed approach is effective.", 
Proposed method shows better summarization quality and performance than state-of-the-art methods on the DUC 2001 and DUC 2002 standard data sets. Proposed sentence relevance estimation is based on normalization of topic space and weighting of each topic using sentences representation in topic space.,Automatic text summarization using latent semantic analysis,"In the paper, the most state-of-the-art methods of automatic text summarization, which build summaries in the form of generic extracts, are considered. The original text is represented in the form of a numerical matrix. Matrix columns correspond to text sentences, and each sentence is represented in the form of a vector in the term space. Further, latent semantic analysis is applied to the matrix obtained to construct sentences representation in the topic space. The dimensionality of the topic space is much less than the dimensionality of the initial term space. The choice of the most important sentences is carried out on the basis of sentences representation in the topic space. The number of important sentences is defined by the length of the demanded summary. This paper also presents a new generic text summarization method that uses nonnegative matrix factorization to estimate sentence relevance. Proposed sentence relevance estimation is based on normalization of topic space and further weighting of each topic using sentences representation in topic space. The proposed method shows better summarization quality and performance than state-of-the-art methods on the DUC 2001 and DUC 2002 standard data sets.", 
"Text Summarization provide large text data into a shorter version without changing its information content and meaning. It is very difficult for human beings to read whole document to understand and manually summarize the documents. In this Research paper, Automatic Text Summarized using Extractive techniques with the help of Genetic Algorithm has been presented.",Automatic Text Summarization Using Regression Model (GA),"Text Summarization provide large text data into a shorter version without changing its information content and meaning. It is very difficult for human beings to read whole document to understand and manually summarize the documents. Text Summarization methods are divide into two types: extractive and abstractive summarization. An abstractive summarization is understanding of document, finding the new concepts and providing summary in few words (sentences different form texts sentences).it is very hard or impossible to design (now a days). In Extractive summarization method select important sentences, paragraphs etc. from the original text document and concatenating them into shorter form. The importance of sentences is decided based on some features of sentences. In this Research paper, Automatic Text Summarization using Extractive techniques with the help of Genetic Algorithm has been presented.", 
"This work explores the advantages of simple embedding features in Reinforcement leaning approach to automatic text summarization tasks. In addition, we propose a novel deep learning network for estimating Q-values.",Automatic text summarization using reinforcement learning with embedding features,"An automatic text summarization system can automatically generate a short and brief summary that contains a main concept of an original document. In this work, we explore the advantages of simple embedding features in Reinforcement leaning approach to automatic text summarization tasks. In addition, we propose a novel deep learning network for estimating Q-values used in Reinforcement learning. We evaluate our model by using ROUGE scores with DUC 2001, 2002, Wikipedia, ACL-ARC data. Evaluation results show that our model is competitive with the previous models.", 
"This work explores the advantages of simple embedding features in Reinforcement leaning approach to automatic text summarization tasks. In addition, we propose a novel deep learning network for estimating Q-values.",Automatic text summarization using supervised machine learning technique for Hindi langauge,"Automatic text summarization is a technique which compresses large text into a shorter text which includes the important information. Hindi is the top-most language used in India and also in a few neighboring countries there is a lack of proper summarization system for Hindi text. Hence, in this paper, we present an approach to the design an automatic text summarizer for Hindi text that generates a summary by extracting sentences. It deals with a single document summarization based on machine learning approach. Each sentence in the document is represented by a set of various features namely- sentence paragraph position, sentence overall position, numeric data, presence of inverted commas, sentence length and keywords in sentences. The sentences are classified into one of four classes namely- most important, important, less important and not important. The classes are in turn having ranks from 4 to 1 respectively with “4”indicating most important sentence and “1” being least relevant sentence . Next a supervised machine learning tool SVMrank is used to train the summarizer to extract important sentences, based on the feature vector. The sentences are ordered according to the ranking of classes. Then based on the required compression ratio, sentences are included in the final summary. The experiment was performed on news articles of different category such as bollywood, politics and sports. The performance of the technique is compared with the human generated summaries. The average result of experiments indicates 72% accuracy at 50% compression ratio and 60% accuracy at 25% compression ratio.", 
Text summarization is nothing but summarizing the content of given text document. Due to massive amount of information getting increased on internet; it is difficult for the user to go through all the information available. Summarization techniques need to be used to reduce the user's time in reading the whole information available on web.,Automatic text summarization with statistical and linguistic features using successive thresholds,"Text summarization is an emerging technique for finding out the summary of the text document. Text summarization is nothing but summarizing the content of given text document. Text summarization has got so uses such as Due to the massive amount of information getting increased on internet; it is difficult for the user to go through all the information available on web. Summarization techniques need to be used to reduce the user’s time in reading the whole information available on web. In this paper, we propose an automatic text summarization technique using both linguistic and statistical features using successive threshold for finding the summary i.e., important sentences from the given input text document. Here the sentences are selected for summary based on the weight of the sentence. The weight of the sentences is calculated based on the statistical and linguistic features. Our approach assigns scores to the sentences by weighting the features like term frequency, word occurrences, and noun weight, phrases etc. In our approach, the number of sentences present in our summary would be equal to the number of paragraphs present in a text document, which can be achieved by using our successive threshold approach.", 
"Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet. Researchers have been trying to improve ATS techniques since the 1950s. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical.","Automatic text summarization, A comprehensive survey","Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.", 
This paper provides an overview of the most prominent algorithms for automatic text summarization that were proposed in the last years. It describes automatic and manual evaluation,"Automatic Text Summarization, A State-of-the-Art Review","Despite the progress that has been achieved in over 50 years of research, automatic text summarization systems are still far from perfect, posing many challenges to the researchers in the field. This paper provides an overview of the most prominent algorithms for automatic text summarization that were proposed in the last years, as well as describes automatic and manual evaluation methods that are currently widely adopted.", 
Research in text summarization is over 50 years old. Some efforts are still needed given the insufficient quality of automatic summaries. This paper gives a short overview of summarization methods and evaluation.,"Automatic Text Summarization, Past, Present and Future","Automatic text summarization, the computer-based production of condensed versions of documents, is an important technology for the information society. Without summaries it would be practically impossible for human beings to get access to the ever growing mass of information available online. Although research in text summarization is over 50 years old, some efforts are still needed given the insufficient quality of automatic summaries and the number of interesting summarization topics being proposed in different contexts by end users (“domain-specific summaries”, “opinion-oriented summaries”, “update summaries”, etc.). This paper gives a short overview of summarization methods and evaluation.", 
"Summaries are important when it comes to process huge amounts of information. Their most important benefit is saving time, which we do not have much nowadays. Generating summaries automatically can be beneficial for humans, since it can save time. Automatic summarization is not a new research field; It was known since the 50s.","Automatic text summarization, What has been done and what has to be done","Summaries are important when it comes to process huge amounts of information. Their most important benefit is saving time, which we do not have much nowadays. Therefore, a summary must be short, representative and readable. Generating summaries automatically can be beneficial for humans, since it can save time and help selecting relevant documents. Automatic summarization and, in particular, Automatic text summarization (ATS) is not a new research field; It was known since the 50s. Since then, researchers have been active to find the perfect summarization method. In this article, we will discuss different works in automatic summarization, especially the recent ones. We will present some problems and limits which prevent works to move forward. Most of these challenges are much more related to the nature of processed languages. These challenges are interesting for academics and developers, as a path to follow in this field.", 
Text Summarization is the creation of a shortened version of a text by a computer program. The product of this procedure still contains the most important points of the original text.,Automatic Text Summarization,"Automatic Text Summarization is a Natural Language Processing task which has experienced great development in recent years, mostly due to the rapid growth of the Internet. Therefore, we need methods and tools that help users to manage large amounts of information. Text Summarization aims to condense the information contained in one or more documents and present it in a more concise way, can be very useful for this purpose. It is the creation of a shortened version of a text by a computer program. The product of this procedure still contains the most important points of the original text.", 
Text summarization is the process of summarizing a source text to a shorter version containing all its information. There are two methods of text summarization- Extractive and Abstractive summarization. This paper describes the extractive features of text summation.,Automatic Text Summarization1,"In today’s fast growing information world, text summarization has become an important matter for interpreting text information. It is the process of summarizing a source text to a shorter version containing all its information and overall meaning. There are two methods of text summarization- Extractive and Abstractive summarization. This paper describes the extractive features of text summarization. Extractive summarization is the method of selecting sentences or paragraphs from the source document and concatenating them into shorter forms while abstractive summarization is the method of understanding the source document and generate its meaning into a shorter form. It also presents the evaluation measures of a text summarizer.", 
"Automatic text summarization system allows, from an input text to produce another smaller and more condensed without losing relevant data and meaning conveyed by the original text. Research works carried out on this area have experienced lately strong progress especially in English language.","AUTOMATIC TEXTS SUMMARIZATION, CURRENT STATE OF THE ART","To facilitate the task of reading and searching information, it became necessary to find a way to reduce the size of documents without affecting the content. The solution is in Automatic text summarization system, it allows, from an input text to produce another smaller and more condensed without losing relevant data and meaning conveyed by the original text. The research works carried out on this area have experienced lately strong progress especially in English language. However, researches in Arabic text summarization are very few and are still in their beginning. In this paper we expose a literature review of recent techniques and works on automatic text summarization field research, and then we focus our discussion on some works concerning automatic text summarization in some languages. We will discuss also some of the main problems that affect the quality of automatic text summarization systems.", 
Informal language and the absence of a standard taxonomy for software technologies make it difficult to reliably analyze technology trends. We propose an automated approach called Witt for the categorization of software technology. Witt takes as input a phrase describing a software technology and returns a general category that describes it.,Automatically Categorizing Software Technologies,"Informal language and the absence of a standard taxonomy for software technologies make it difficult to reliably analyze technology trends on discussion forums and other on-line venues. We propose an automated approach called Witt for the categorization of software technology (an expanded version of the hypernym discovery problem). Witt takes as input a phrase describing a software technology or concept and returns a general category that describes it (e.g., integrated development environment), along with attributes that further qualify it (commercial, php, etc.). By extension, the approach enables the dynamic creation of lists of all technologies of a given type (e.g., web application frameworks). Our approach relies on Stack Overflow and Wikipedia, and involves numerous original domain adaptations and a new solution to the problem of normalizing automatically-detected hypernyms. We compared Witt with six independent taxonomy tools and found that, when applied to software terms, Witt demonstrated better coverage than all evaluated alternate solutions, without a corresponding degradation in false positive rate.", 
"Comments which are an integral part of software development improve program comprehension and software maintainability. Many text retrieval methods for software engineering tasks take comments as an important source for code semantic analysis. In this paper, we use machine learning to detect the scopes of source code comments automatically in Java programs.",Automatically Detecting the Scopes of Source Code Comments,"Comments which are an integral part of software development improve program comprehension and software maintainability. They convey useful information about the system functionalities and many text retrieval methods for software engineering tasks take comments as an important source for code semantic analysis. However, it is challenging to identify the relationship between the functional semantics of the code and its corresponding textual descriptions and apply it to automatic mining approaches in software engineering efficiently. In this paper, we use machine learning which utilizes features of code snippets and comments to detect the scopes of source code comments automatically in Java programs. Based on the dataset of comment-statement pairs from 4 popular open source projects, our method achieved a high accuracy of 81.15% in detecting the scopes of comments. Furthermore, the experimental results demonstrated the feasibility and effectiveness of our comment scope detection method.", 
"Automatic method does not require creation of human model summaries. Results on a large scale evaluation from the Text Analysis Conference show it is effective. The best feature, Jensen-Shannon divergence, leads to a correlation as high as 0.88 with manual pyramid.",Automatically evaluating content selection in summarization without human models,"We present a fully automatic method for content selection evaluation in summarization that does not require the creation of human model summaries. Our work capitalizes on the assumption that the distribution of words in the input and an informative summary of that input should be similar to each other. Results on a large scale evaluation from the Text Analysis Conference show that input-summary comparisons are very effective for the evaluation of content selection. Our automatic methods rank participating systems similarly to manual model-based pyramid evaluation and to manual human judgments of responsiveness. The best feature, Jensen-Shannon divergence, leads to a correlation as high as 0.88 with manual pyramid and 0.73 with responsiveness evaluations.", 
Software systems are increasingly being used in business or mission critical scenarios. Orthogonal Defect Classification (ODC) has emerged as a popular method for classifying software defects. ODC requires one or more experts to categorize each defect in a complex and time-consuming process.,Automating orthogonal defect classification using machine learning algorithms,"Software systems are increasingly being used in business or mission critical scenarios, where the presence of certain types of software defects, i.e., bugs, may result in catastrophic consequences (e.g., financial losses or even the loss of human lives). To deploy systems in which we can rely on, it is vital to understand the types of defects that tend to affect such systems. This allows developers to take proper action, such as adapting the development process or redirecting testing efforts (e.g., using a certain set of testing techniques, or focusing on certain parts of the system). Orthogonal Defect Classification (ODC) has emerged as a popular method for classifying software defects, but it requires one or more experts to categorize each defect in a quite complex and time-consuming process. In this paper, we evaluate the use of machine learning algorithms (k-Nearest Neighbors, Support Vector Machines, Naïve Bayes, Nearest Centroid, Random Forest and Recurrent Neural Networks) for automatic classification of software defects using ODC, based on unstructured textual bug reports. Experimental results reveal the difficulties in automatically classifying certain ODC attributes solely using reports, but also suggest that the overall classification accuracy may be improved in most of the cases, if larger datasets are used.", 
"Average EEG signal covarianance matrices is a key step in designing brain-computer interfaces. Riemannian geometry improves performances for small dimensional problems, but also the limits of this approach when the dimensionality increases. We review several approaches from the literature and compare them on three publicly available datasets.",AVERAGING COVARIANCE MATRICES FOR EEG SIGNAL CLASSIFICATION BASED ON THE CSP AN EMPIRICAL STUDY,"This paper presents an empirical comparison of covariance matrix averaging methods for EEG signal classification. Indeed, averaging EEG signal covariance matrices is a key step in designing brain-computer interfaces (BCI) based on the popular common spatial pattern (CSP) algorithm. BCI paradigms are typically structured into trials and we argue that this structure should be taken into account. Moreover, the non-Euclidean structure of covariance matrices should be taken into consideration as well. We review several approaches from the literature for averaging covariance matrices in CSP and compare them empirically on three publicly available datasets. Our results show that using Riemannian geometry for averaging covariance matrices improves performances for small dimensional problems, but also the limits of this approach when the dimensionality increases.", 
AZOM combines statistical and conceptual property of text and document structure to extract the summary. Proposed approach is localized for Persian language but easily can apply to other languages.,"AZOM, A Persian Structured Text Summarizer","In this paper we propose a summarization approach, nicknamed AZOM, that combines statistical and conceptual property of text and in regards of document structure, extracts the summary of text. AZOM is also capable of summarizing unstructured documents. Proposed approach is localized for Persian language but easily can apply to other languages. The empirical results show comparatively superior results than common structured text summarizers, also than existing Persian text summarizers.", 
"Training procedure refinements can help improve CNN models. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. Improvement on image classification accuracy leads to better transfer learning performance in other application domains.",Bag of Tricks for Image Classification with Convolutional Neural Networks,"Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.", 
"VANETs play an important role in intelligent transportation. Traditional authentication and key distribution schemes suffer from network failure and high response latency. This article proposes a BAGKD protocol to achieve robust and efficient networking for the security and efficiency of VANets. In our protocol, bilinear mapping is used to realize batch authentication.",BAGKD A Batch Authentication and Group Key Distribution Protocol for VANETs,"As an important application of mobile ad hoc networks, VANETs play an important role in intelligent transportation. However, with the development of mobile communication technology, as well as the needs of intelligent transportation, both security and efficiency are required for real-time authentication and communication. Traditional authentication and key distribution schemes suffer from network failure and high response latency. To solve the above problems, this article proposes a BAGKD protocol to achieve robust and efficient networking for the security and efficiency of VANETs. In our protocol, bilinear mapping is used to realize batch authentication, which can improve authentication efficiency and reduce message errors caused by factors such as high speed of vehicles. The group key distribution mechanism can update the group key dynamically, which reduces the risk of group key leakage e?ectively. For the sake of privacy protection, vehicles utilize pseudonyms issued by a trusted authority to communicate with RSUs. The security of BAGKD protocol is verified by simulation in our experiments on AVISPA. In addition, when compared to three existing protocols based on bilinear mapping, our proposed BAGKD outperforms them in terms of efficiency and communication overhead while maintaining security. The simulation results further confirm that BAGKD is suitable for short-range communication scenarios such as VANETs.", 
"In the guided text summarization task, it is required that a generated summary cover all aspects. We formalize the coverage of the aspects as a max-min problem, which enables a summary to cover aspects in a well-balanced manner. We show that our model outperforms other approaches in terms of ROUGE-2.",Balanced coverage of aspects for text summarization,"We propose a new model for the guided text summarization task. In this task, it is required that a generated summary covers all the aspects, which are predefined for the topic of the given document cluster; for example, aspects for the topic “Accidents and Natural Disasters” include WHAT, WHEN, WHERE, WHY, WHO AFFECTED, DAMAGES and COUNTERMEASURES. We use as a scorer for an aspect, the maximum entropy classifier that predicts whether each sentence reflects the aspect or not. We formalize the coverage of the aspects as a max-min problem, which enables a summary to cover aspects in a well-balanced manner. In the max-min problem, the minimum of the aspect scores is going to be maximized so that the summary contains all the aspects as much as possible. Furthermore, we integrate the model based on the max-min problem with the maximum coverage summarization model, which generates a summary containing as many conceptual units as possible. Through the experiments on benchmark datasets for the guided summarization, we show that our model outperforms other approaches in terms of ROUGE-2.", 
Ballistocardiogram (BCG) is a large-amplitude artifact caused by cardiac induced movement. It contaminates the EEG during EEG-fMRI recordings. We present a novel method for BCG artifact suppression using recurrent neural networks (RNNs) EEG signals were recovered by training RNNs on the nonlinear mappings between ECG and the BCG corrupted EEG.,Ballistocardiogram artifact reduction insimultaneous EEG-fMRI using deep learning,"The concurrent recording of electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) is a technique that has received much attention due to its potential for combined high temporal and spatial resolution. However, the ballistocardiogram (BCG), a large-amplitude artifact caused by cardiac induced movement contaminates the EEG during EEG-fMRI recordings. Removal of BCG in software has generally made use of linear decompositions of the corrupted EEG. This is not ideal as the BCG signal propagates in a manner which is non-linearly dependent on the electrocardiogram (ECG). In this paper, we present a novel method for BCG artifact suppression using recurrent neural networks (RNNs). EEG signals were recovered by training RNNs on the nonlinear mappings between ECG and the BCG corrupted EEG. We evaluated our model’s performance against the commonly used Optimal Basis Set (OBS) method at the level of individual subjects, and investigated generalization across subjects. We show that our algorithm can generate larger average power reduction of the BCG at critical frequencies, while simultaneously improving task relevant EEG based classification. The presented deep learning architecture can be used to reduce BCG related artifacts in EEG-fMRI recordings. Significance: We present a deep learning approach that can be used to suppress the BCG artifact in EEG-fMRI without the use of additional hardware. This method may have scope to be combined with current hardware methods, operate in real-time and be used for direct modeling of the BCG.", 
Automatic malware categorization plays an important role in combating the current large volume of malware. How to combine the merits of the multiple categories of features and algorithms to further improve the analysis result is very critical. This paper proposes a novel scalable malware analysis framework to exploit the complementary nature of different features. It combines partitions from individual category of feature and algorithm to obtain better quality.,Based on Multi-Features and Clustering Ensemble Method for Automatic Malware Categorization,"Automatic malware categorization plays an important role in combating the current large volume of malware and aiding the corresponding forensics. Generally, there are lot of sample information could be extracted with the static tools and dynamic sandbox for malware analysis. Combine these obtained features effectively for further analysis would provides us a better understanding. On the other hand, most current works on malware analysis are based on single category of machine learning algorithm to categorize samples. However, different clustering algorithms have their own strengths and weaknesses. And then, how to combine the merits of the multiple categories of features and algorithms to further improve the analysis result is very critical. In this paper, we propose a novel scalable malware analysis framework to exploit the complementary nature of different features and algorithms to optimally integrate their results. By using the concept of clustering ensemble, our system combines partitions from individual category of feature and algorithm to obtain better quality and robustness. Our system composed of the following three parts: (1) extract multiple categories of static and dynamic features; (2) use the k-means and hierarchical clustering algorithms to construct the base clustering; (3) proposed an efficient method based on mixture model clustering ensemble to conduct an effective clustering analysis. We have evaluated our method on two malware datasets, namely the Microsoft malware dataset and our own malware dataset which contained 10868 and 53760 samples respectively. Our experiment results show that our method could categorize malware with better quality and robustness. Also, our method has certain advantages in the system run time and memory consumption compared with the state-of-the art malware analysis works.", 
"Batch Normalization is a new way to train deep neural networks. It has 14 times fewer training steps and beats the original model by a significant margin. It reaches 4.82% top-5 test error, exceeding the accuracy of human raters.",Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", 
"In this paper, we revisit the classical Bayesian face recognition method by Baback Moghaddam et al. and propose a new joint formulation. Our method achieved 92.4% test accuracy on the challenging Labeled Face in Wild (LFW) dataset.",Bayesian Face Revisited A Joint Formulation,"In this paper, we revisit the classical Bayesian face recognition method by Baback Moghaddam et al. and propose a new joint formulation. The classical Bayesian method models the appearance difference between two faces. We observe that this “difference” formulation may reduce the separability between classes. Instead, we model two faces jointly with an appropriate prior on the face representation. Our joint formulation leads to an EM-like model learning at the training time and an efficient, closed-formed computation at the test time. On extensive experimental evaluations, our method is superior to the classical Bayesian face and many other supervised approaches. Our method achieved 92.4% test accuracy on the challenging Labeled Face in Wild (LFW) dataset. Comparing with current best commercial system, we reduced the error rate by 10%.", 
A trust model called Implicit Web of Trust in VANET (IWOT-V) is used to reason out the trustworthiness of vehicles. The idea of IWot-V is mainly inspired by web page ranking algorithms such as PageRank. The simulation results show that IWOT -V can accurately identify trusted and untrusted nodes.,BayesTrust and VehicleRank Constructing an Implicit Web of Trust in VANET,"As Vehicular Ad hoc Network (VANET) features random topology and accommodates freely connected nodes, it is important that the cooperation among the nodes exists. This paper proposes a trust model called Implicit Web of Trust in VANET (IWOT-V) to reason out the trustworthiness of vehicles. Such that untrusted nodes can be identified and avoided when we make a decision regarding whom to follow or cooperate with. Furthermore, the performance of Cooperative Intelligent Transport System (C-ITS) applications improves. The idea of IWOT-V is mainly inspired by web page ranking algorithms such as PageRank. Although there does not exist explicit link structure in VANET because of random topology and dynamic connections, social trust relationship among vehicles exists and an implicit web of trust can be derived. To accomplish the derivation, two algorithms are presented, i.e., BayesTrust and VehicleRank. They are responsible for deriving the local and global trust relationships, respectively. The simulation results show that IWOT-V can accurately identify trusted and untrusted nodes if enough local trust information is collected. The performance of IWOT-V affected by five threat models is demonstrated, and the related discussions are also given.", 
"Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. Few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems.",Benchmarking Deep Learning Models on Large Healthcare Datasets,"Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets. In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks. Our results show that deep learning models consistently outperform all the other approaches especially when the ‘raw’ clinical time series data is used as input features to the models.", 
Text summarization is one of the leading problem of natural language processing and deep learning in recent years. Our purpose is to create an efficient and effective abstractive Bengali text summarizer. Our model works with bi-directional RNNs with LSTM in encoding layer and attention model at decoding layer.,Bengali abstractive text summarization using sequence to sequence RNNs,"Text summarization is one of the leading problem of natural language processing and deep learning in recent years. Text summarization contains a condensed short note on a large text document. Our purpose is to create an efficient and effective abstractive Bengali text summarizer what can generate an understandable and meaningful summary from a given Bengali text document. To do this we have collected various texts such as newspaper articles, Facebook posts etc. and to generate summary from those text we will be using our model. Our model works with bi-directional RNNs with LSTM in encoding layer and attention model at decoding layer. Our model works as sequence to sequence model to generate summary. There are some challenges we have faced while building this model such as text pre-processing, vocabulary counting, missing words counting, word embedding, unknown words find out and so on. In this model, our main goal was to make an abstractive summarizer and reduce the train loss of that. During our research experiment, we have successfully reduced the train loss to 0.008 and able to generate a fluent short summary note from a given text.", 
This paper presents a method for Bengali text summarization. It extracts important sentences from a Bengali document to produce a summary.,Bengali text summarization by sentence extraction,"Text summarization is a process to produce an abstract or a summary by selecting significant portion of the information from one or more texts. In an automatic text summarization process, a text is given to the computer and the computer returns a shorter less redundant extract or abstract of the original text(s). Many techniques have been developed for summarizing English text(s). But, a very few attempts have been made for Bengali text summarization. This paper presents a method for Bengali text summarization which extracts important sentences from a Bengali document to produce a summary.", 
"ROUGE is a widely adopted, automatic evaluation measure for text summarization. It has been shown to correlate well with human judgements, but is biased towards surface lexical similarities. We study the effectiveness of word embeddings to overcome this disadvantage.",Better Summarization Evaluation with Word Embeddings for ROUGE,"ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients.", 
CPSum is a summarizer that learns the importance of information objects from a background source. CPSum proves to be able to perform well in this challenging scenario whereas reference systems fail.,"Beyond centrality and structural features, Learning information importance for text summarization","Most automatic text summarization systems proposed to date rely on centrality and structural features as indicators for information importance. In this paper, we argue that these features cannot reliably detect important information in heterogeneous document collections. Instead, we propose CPSum, a summarizer that learns the importance of information objects from a background source. Our hypothesis is tested on a multi-document corpus where we remove centrality and structural features. CPSum proves to be able to perform well in this challenging scenario whereas reference systems fail.", 
"Cognitive workload is an important indicator of mental activity that has implications for human-computer interaction, biomedical and task analysis applications. We used a cognitive task inducing seven different levels of workload to investigate workload discrimination using electroencephalography (EEG) signals. The findings bring the use of passive brain-computer interfaces (BCI) for continuous memory load measurement closer to reality.",Beyond Subjective Self-Rating EEG Signal Classification of Cognitive Workload,"Cognitive workload is an important indicator of mental activity that has implications for human-computer interaction, biomedical and task analysis applications. Previously, subjective rating (self-assessment) has often been a preferred measure, due to its ease of use and relative sensitivity to the cognitive load variations. However, it can only be feasibly measured in a post-hoc manner with the user’s cooperation, and is not available as an on-line, continuous measurement during the progress of the cognitive task. In this study, we used a cognitive task inducing seven different levels of workload to investigate workload discrimination using electroencephalography (EEG) signals. The entropy, energy, and standard deviation of the wavelet coefficients extracted from the segmented EEGs were found to change very consistently in accordance with the induced load, yielding strong significance in statistical tests of ranking accuracy. High accuracy for subject-independent multi-channel classification among seven load levels was achieved, across the twelve subjects studied. We compare these results with alternative measures such as performance, subjective ratings, and reaction time (response time) of the subjects and compare their reliability with the EEG-based method introduced. We also investigate test/re-test reliability of the recorded EEG signals to evaluate their stability over time. These findings bring the use of passive brain-computer interfaces (BCI) for continuous memory load measurement closer to reality, and suggest EEG as the preferred measure of working memory load.", 
"BiCoS is a scalable, alternation-based algorithm for co-segmentation. It has superior performance on standard benchmark image datasets. The algorithm is simpler than many of its predecessors.",BiCoS A Bi-level Co-Segmentation Method for Image Classification,"The objective of this paper is the unsupervised segmentation of image training sets into foreground and background in order to improve image classification performance. To this end we introduce a new scalable, alternation-based algorithm for co-segmentation, BiCoS, which is simpler than many of its predecessors, and yet has superior performance on standard benchmark image datasets. We argue that the reason for this success is that the co-segmentation task is represented at the appropriate levels – pixels and color distributions for individual images, and super-pixels with learnable features at the level of sharing across the image set – together with powerful and efficient inference algorithms (GrabCut and SVM) for each level. We assess both the segmentation and classification performance of the algorithm and compare to previous results on Oxford Flowers 17 & 102, Caltech-UCSD Birds-200, the Weizmann Horses, Caltech-4 benchmark datasets.", 
"Big Data has significant impact in developing functional smart cities and supporting modern societies. Different computational intelligence techniques have been considered as tools for Big Data analytics. We identify a number of areas, where novel applications in real world smart city problems can be developed.",Big data analytics Computational intelligence techniques and application areas,"Big Data has significant impact in developing functional smart cities and supporting modern societies. In this paper, we investigate the importance of Big Data in modern life and economy, and discuss challenges arising from Big Data utilization. Different computational intelligence techniques have been considered as tools for Big Data analytics. We also explore the powerful combination of Big Data and Computational Intelligence (CI) and identify a number of areas, where novel applications in real world smart city problems can be developed by utilizing these powerful tools and techniques. We present a case study for intelligent transportation in the context of a smart city, and a novel data modelling methodology based on a biologically inspired universal generative modelling approach called Hierarchical Spatial-Temporal State Machine (HSTSM). We further discuss various implications of policy, protection, valuation and commercialization related to Big Data, its applications and deployment.", 
"Big data analytics in massive manufacturing data can extract huge business values. It can also result in research challenges due to heterogeneous data types, enormous volume and real-time velocity of manufacturing data.","Big data analytics for manufacturing internet of things opportunities, challenges and enabling technologies","Data analytics in massive manufacturing data can extract huge business values while can also result in research challenges due to the heterogeneous data types, enormous volume and real-time velocity of manufacturing data. This paper provides an overview on big data analytics in manufacturing Internet of Things (MIoT). This paper first starts with a discussion on necessities and challenges of big data analytics in manufacturing data of MIoT. Then, the enabling technologies of big data analytics of manufacturing data are surveyed and discussed. Moreover, this paper also outlines the future directions in this promising area.", 
"Big data analytics can process large amounts of raw data and extract useful, smaller-sized information. Integrating big data analytics with the networks' control/traffic layers might be the best way to build robust data communication networks. This is the first survey that addresses the use of big data Analytics techniques for the design of a broad range of networks.",Big data analytics for wireless and wired network design A survey,"Currently, the world is witnessing a mounting avalanche of data due to the increasing number of mobile network subscribers, Internet websites, and online services. This trend is continuing to develop in a quick and diverse manner in the form of big data. Big data analytics can process large amounts of raw data and extract useful, smaller-sized information, which can be used by different parties to make reliable decisions. In this paper, we conduct a survey on the role that big data analytics can play in the design of data communication networks. Integrating the latest advances that employ big data analytics with the networks’ control/traffic layers might be the best way to build robust data communication networks with refined performance and intelligent features. First, the survey starts with the introduction of the big data basic concepts, framework, and characteristics. Second, we illustrate the main network design cycle employing big data analytics. This cycle represents the umbrella concept that unifies the surveyed topics. Third, there is a detailed review of the current academic and industrial efforts toward network design using big data analytics. Forth, we identify the challenges confronting the utilization of big data analytics in network design. Finally, we highlight several future research directions. To the best of our knowledge, this is the first survey that addresses the use of big data analytics techniques for the design of a broad range of networks.", 
"The Industrial Internet of Things (IIoT) incorporates machine learning and massively parallel distributed systems. In IIoT, end devices continuously generate and transmit data streams, resulting in increased network traffic. This article introduces a novel concentric computing model (CCM) for the deployment of big data analytics applications.",Big Data Analytics in Industrial IoT Using a Concentric Computing Model,"The unprecedented proliferation of miniaturized sensors and intelligent communication, computing, and control technologies have paved the way for the development of the Industrial Internet of Things. The IIoT incorporates machine learning and massively parallel distributed systems such as clouds, clusters, and grids for big data storage, processing, and analytics. In IIoT, end devices continuously generate and transmit data streams, resulting in increased network traffic between device-cloud communication. Moreover, it increases in-network data transmissions. requiring additional e?orts for big data processing, management, and analytics. To cope with these engendered issues, this article first introduces a novel concentric computing model (CCM) paradigm composed of sensing systems, outer and inner gateway processors, and central processors (outer and inner) for the deployment of big data analytics applications in IIoT. Second, we investigate, highlight, and report recent research efforts directed at the IIoT paradigm with respect to big data analytics. Third, we identify and discuss indispensable challenges that remain to be addressed for employing CCM in the IIoT paradigm. Lastly, we provide several future research directions (e.g., real-time data analytics, data integration, transmission of meaningful data, edge analytics, real-time fusion of streaming data, and security and privacy).", 
Intelligent transportation systems will produce a large amount of data. Big data will have profound impacts on the design and application of intelligent transportation systems. Studying big data analytics in ITS is a flourishing field.,Big Data Analytics in Intelligent in Transportation system A Survey,"Big data is becoming a research focus in intelligent transportation systems (ITS), which can be seen in many projects around the world. Intelligent transportation systems will produce a large amount of data. The produced big data will have profound impacts on the design and application of intelligent transportation systems, which makes ITS safer, more efficient, and profitable. Studying big data analytics in ITS is a flourishing field. This paper first reviews the history and characteristics of big data and intelligent transportation systems. The framework of conducting big data analytics in ITS is discussed next, where the data source and collection methods, data analytics methods and platforms, and big data analytics application categories are summarized. Several case studies of big data analytics applications in intelligent transportation systems, including road traffic accidents analysis, road traffic flow prediction, public transportation service plan, personal travel route plan, rail transportation management and control, and assets maintenance are introduced. Finally, this paper discusses some open challenges of using big data analytics in ITS.", 
"Big data analytics in medicine and healthcare covers integration and analysis of large amount of complex heterogeneous data. Big data characteristics: value, volume, velocity, variety, veracity and variability are described. We underline the challenging issues about big data privacy and security.",Big Data Analytics in Medicine and Healthcare,"This paper surveys big data with highlighting the big data analytics in medicine and healthcare. Big data characteristics: value, volume, velocity, variety, veracity and variability are described. Big data analytics in medicine and healthcare covers integration and analysis of large amount of complex heterogeneous data such as various – omics data (genomics, epigenomics, transcriptomics, proteomics, metabolomics, interactomics, pharmacogenomics, diseasomics), biomedical data and electronic health records data. We underline the challenging issues about big data privacy and security. Regarding big data characteristics, some directions of using suitable and promising open-source distributed data processing software platform are given.", 
"There is growing interest in the application of big data analytics (BDA) in supply chain management (SCM) This review proposes a novel classification framework to provide a full picture of current literature. The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.",Big data analytics in supply chain management A state-of-the-art literature review,"The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.", 
This article examines how to use big data analytics services to enhance business intelligence (BI) It presents a big data Analytics service-oriented architecture (BASOA) The proposed approach in this article might facilitate the research and development of business analytics.,Big Data Analytics Services for Enhancing Business Intelligence,"This article examines how to use big data analytics services to enhance business intelligence (BI). More specifically, this article proposes an ontology of big data analytics and presents a big data analytics service-oriented architecture (BASOA), and then applies BASOA to BI, where our surveyed data analysis shows that the proposed BASOA is viable for enhancing BI and enterprise information systems. This article also explores temporality, expectability, and relativity as the characteristics of intelligence in BI. These characteristics are what customers and decision makers expect from BI in terms of systems, products, and services of organizations. The proposed approach in this article might facilitate the research and development of business analytics, big data analytics, and BI as well as big data science and big data computing.", 
"Health care industry has not fully grasped the potential benefits to be gained from big data analytics. A study examines the historical development, architectural design and component functionalities of big data. It also maps the benefits driven by big data in terms of IT infrastructure, operational, organizational and managerial areas.",Big data analytics Understanding its capabilities and potential benefits for healthcare organizations,"To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.", 
"Digitalization process and its outcomes in the 21st century accelerate transformation and the creation of sustainable societies. Our decisions, actions and even existence in the digital world generate data, which offer tremendous opportunities for revising current business methods and practices. There is a critical need for novel theories embracing big data analytics ecosystems.",Big data and business analytics ecosystems paving the way towards digital transformation and sustainable societies,"The digitalization process and its outcomes in the 21st century accelerate transformation and the creation of sustainable societies. Our decisions, actions and even existence in the digital world generate data, which offer tremendous opportunities for revising current business methods and practices, thus there is a critical need for novel theories embracing big data analytics ecosystems. Building upon the rapidly developing research on digital technologies and the strengths that information systems discipline brings in the area, we conceptualize big data and business analytics ecosystems and propose a model that portraits how big data and business analytics ecosystems can pave the way towards digital transformation and sustainable societies, that is the Digital Transformation and Sustainability (DTS) model. This editorial discusses that in order to reach digital transformation and the creation of sustainable societies, first, none of the actors in the society can be seen in isolation, instead we need to improve our understanding of their interactions and interrelations that lead to knowledge, innovation, and value creation. Second, we gain deeper insight on which capabilities need to be developed to harness the potential of big data analytics. Our suggestions in this paper, coupled with the five research contributions included in the special issue, seek to offer a broader foundation for paving the way towards digital transformation and sustainable societies.", 
"This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data, device data and transaction data.",Big data in tourism research A literature review,"Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.", 
"Big data problem means that data is growing at a much faster rate than computational speeds. Social activities, scientific experiments, biological explorations along with the sensor devices are great big data contributors. Traditional tools, machine learning algorithms, and techniques are not capable of handling, managing, and analyzing big data.","Big Data Issues, Challenges, and Techniques in Business Intelligence","During the last decade, the most challenging problem the world envisaged was big data problem. The big data problem means that data is growing at a much faster rate than computational speeds. And it is the result of the fact that storage cost is getting cheaper day by day, so people as well as almost all business or scientific organizations are storing more and more data. Social activities, scientific experiments, biological explorations along with the sensor devices are great big data contributors. Big data is beneficial to the society and business but at the same time, it brings challenges to the scientific communities. The existing traditional tools, machine learning algorithms, and techniques are not capable of handling, managing, and analyzing big data, although various scalable machine learning algorithms, techniques, and tools (e.g., Hadoop and Apache Spark open source platforms) are prevalent. In this paper, we have identified the most pertinent issues and challenges related to big data and point out a comprehensive comparison of various techniques for handling big data problem.", 
"Problem/project Based Learning (PBL) is a highly effective student-centered teaching method, where student teams learn by solving problems. Students are engaging in PBL with the semester long challenge of generating good English summaries of an event.","Big Data Text Summarization for Events, A Problem Based Learning Course","Problem/project Based Learning (PBL) is a highly effective student-centered teaching method, where student teams learn by solving problems. This paper describes an instance of PBL applied to digital library education. We show the design, implementation, results, and partial evaluation of a Computational Linguistics course that provides students an opportunity to engage in active learning about adding value to digital libraries with large collections of text, i.e., one aspect of “big data.” Students are engaging in PBL with the semester long challenge of generating good English summaries of an event, given a large collection from our webpage archives. Six teams, each working with a different type of event, and applying three different summarization methods, learned how to generate good summaries; these have fair precision relative to the Wikipedia page that describes their event.", 
Huge data in English and Hindi is available on internet and social media which need to be extracted or summarized. In this paper we are presenting Bilingual (Hindi and English) unsupervised automatic text summarization using deep learning.,Bilingual automatic text summarization using unsupervised deep learning,"In the world of digitization, the growth of big data is raising at large scale with usage of high performance computing. The huge data in English and Hindi is available on internet and social media which need to be extracted or summarized in user required form. In this paper we are presenting Bilingual (Hindi and English) unsupervised automatic text summarization using deep learning. which is an important research area with in Natural Language Processing, Machine Learning and data mining, to improve result accuracy, we are using restricted Boltzmann machine to generate a shorter version of original document without losing its important information. In this algorithm we are exploring the features to improve the relevance of sentences in the dataset.", 
"Text summarization has been appeared as one of the solution to text search problem. The main objective is to retrieve a condensed document that pertain the original information. In this paper, a comparative analysis of few meta-heuristic approaches such as Cuckoo Search is presented.","Bio-inspired approaches for extractive document summarization, A comparative study","With the exponential growth of information in World Wide Web, extracting relevant information from huge amount of data has become a critical task. Text summarization has been appeared as one of the solution to such problem. As the main objective is to retrieve a condensed document that pertain the original information, so it can be considered as an optimization problem. In this paper, a comparative analysis of few meta-heuristic approaches such as Cuckoo Search (CS), Cat Swarm Optimization (CSO), Particle Swarm Optimization (PSO), Harmony Search (HS), and Differential Evolution (DE) algorithm is presented for single document summarization problem. The performance of all these algorithms are compared in terms of different evaluation metrics such as F score, true positive rate and positive predicate value to validate summary relevancy and non-redundancy over traditional and standard Document Understanding Conference (DUC) datasets.", 
Concept chaining is a technique for identifying semantically-related terms in text. It is proposed that concept chaining could be used in biomedical text to link concepts together. The resulting concept chains are then used to identify candidate sentences useful for extraction. The extracted sentences are used to produce a summary of the biomedical text.,"BioChain, lexical chaining methods for biomedical text summarization","Lexical chaining is a technique for identifying semantically-related terms in text. We propose concept chaining to link semantically-related concepts within biomedical text together. The resulting concept chains are then used to identify candidate sentences useful for extraction. The extracted sentences are used to produce a summary of the biomedical text. The concept chaining process is adapted from existing lexical chaining approaches, which focus on chaining semantically-related terms, rather than semantically-related concepts. The Unified Medical Language System (UMLS) Metathesaurus and Semantic Network are used as semantic resources. The UMLS MetaMap Transfer tool is used to perform text-to-concept mapping. The goal is to propose concept chaining and develop a novel concept chaining system for the biomedical domain using UMLS lexicon and the ideas of lexical chaining. The resulting concept chains from the full-text are evaluated against the concepts of a human summary (the paper’s abstract). Precision is measured at 0.90 and recall at 0.92. The resulting concept chains are used to summarize the text. We also evaluate generated summaries using existing summarization systems using sentence matching, and confirm the generated summaries are useful to a domain expert. Our results show that the proposed concept chaining is a promising methodology for biomedical text summarization.", 
"Extractive method involves summarizing text through objective extraction of some parts of a text like word, sentence, and paragraph. A summarization issue would be unsolvable by exact methods in a reasonable time with considering documents with high amount of information. A biogeography - based optimization algorithm (BBO) is used in this article.",Biogeography-based optimization algorithm for automatic extractive text summarization,"Given the increasing number of documents, sites, online sources, and the users’ desire to quickly access information, automatic textual summarization has caught the attention of many researchers in this field. Researchers have presented different methods for text summarization as well as a useful summary of those texts including relevant document sentences. This study selects extractive method out of different summarizing methods (e.g. abstract method). Extractive method involves summarizing text through objective extraction of some parts of a text like word, sentence, and paragraph. A summarization issue would be unsolvable by exact methods in a reasonable time with considering documents with high amount of information (NP complete). These kinds of issues are usually solved using metaheuristic methods. A biogeography - based optimization algorithm (BBO), which is a new metaheuristic method in the domain of extractive text summarization is used in this article. This method is tested on a set of Doc’s standard documents in 2002 and is analyzed, using ROUGE software. The obtained results of these tests show that this kind of method can be used as an effective method for text summarization.", 
"High dimensionality is critical to high performance, say the authors. A 100K-dim feature can achieve significant improvements over both its low-dimensional version and the state-of-the-art. With our proposed sparse projection method, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.",Blessing of Dimensionality High-dimensional Feature and Its Efficient Compression for Face Verification,"Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a high-dimensional feature. In this paper, we study the performance of a high-dimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.", 
Clustering of plant disease from digital images is an arduous task due to its dynamic nature and change of appearance. This paper presents an effective deep clustering-based plant disease categorization algorithm. The proposed Boosted-DEPICT attains promising results with an accuracy of 97.73% and 91.25% on PV and PDD datasets.,Boosted-DEPICT an effective maize disease categorization framework using deep clustering,"Clustering of plant disease from digital images is an arduous task due to its dynamic nature and change of appearance under different environmental conditions. In most cases, the image captured in the real-time scenario is subjected to added noise, distortion, poor lighting conditions, and other potential factors that results in poor model performance during the process of discriminating between normal and disease-affected samples. It eventually maximizes the margin of the error rate, thereby leading to misclassification of disease of different varieties of plants in the database with other categories. This paper presents an effective deep clustering-based plant disease categorization algorithm, Boosted-Deep Embedded Regularized Clustering (DEPICT). This model integrates the convolutional autoencoder model with locality-preserving constraints and group sparsity into the network, which improves the embedded learning representation of the images. The PlantVillage and PDD image databases are accessed to develop this model for maize crop. The images are segmented by eliminating the background, cropped, augmented before model training. The performance of the system is evaluated by clustering accuracy and normalized mutual information. The proposed Boosted-DEPICT exhibits better performance, attains promising results with an accuracy of 97.73% and 91.25% on PV and PDD datasets, and outperforms state-of-the-art deep clustering algorithms. This system could be further enhanced by automating the entire process and transforming it into a mobile application for real-time analysis to gain instant results from any region.", 
"Neural network-based methods for abstractive summarization produce fluent output, but poor content selection. We use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. This two-step process is both simpler and higher performing than other end-to-end content selection models.",Bottom-Up Abstractive Summarization,"Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but perform poorly at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain.", 
Firms report increased competitive value gains from the use of business intelligence and analytics (BI&A) Little is known about how insights from BI&A are transformed to added value to date.,Business intelligence and analytics for value creation The role of absorptive capacity,"Firms continuously report increased competitive value gains from the use of business intelligence and analytics (BI&A), however, little is known about how insights from BI&A are transformed to added value to date. We have conducted fourteen in-depth, semi-structured interviews with a sample of informants in CEO positions, IT managers, CIO, Heads of R&D, as well as Market Managers from nine medium or large-sized European firms. Applying the absorptive capacity’s theoretical lens, we have provided evidence that absorptive capacity’s capabilities are an underlying foundation in the process of transforming BI&A triggered insights into valuable knowledge. Moreover, this process is supported by technological, human, and relationship assets.", 
Caffe is a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. It is a BSD-licensed C ++ library with Python and MATLAB bindings. Caffe fits industry and internet-scale media needs by CUDA GPU computation.,Caffe Convolutional Architecture for Fast Feature Embedding,"Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (? 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", 
Several methods have been developed in Automatic Extractive Text Summarization (AETS) that have allowed the continuous improvement of this task. This paper presents a new method based on a Genetic Algorithm to determine the best sentence combination of DUC01 and DUC02 datasets.,Calculating the significance of automatic extractive text summarization using a genetic algorithm,"In the last 16 years with the existence of Document Understanding Conference (DUC), several methods have been developed in Automatic Extractive Text Summarization (AETS) that have allowed the continuous improvement of this task. However, no significant analysis has been performed to determine the significance of the AETS methods. In this paper, we present a new method based on a Genetic Algorithm to determine the best sentence combination of DUC01 and DUC02 datasets to rank the newest methods of AETS. Using three heuristics presented in the state-of-the-art, we rank the most recent AETS methods, obtaining upper bounds and recovering lower bounds of the state-of-the-art.", 
"GPT-3 can internalize the rules of language without explicit programming or rules. It learns language through repeated exposure, albeit on a much larger scale. It can sometimes fail at the simplest of linguistic tasks, but it can excel at more difficult ones.",Can GPT-3 Pass a Writer's Turing Test,"Until recently the field of natural language generation relied upon formalized grammar systems, small-scale statistical models, and lengthy sets of heuristic rules. This older technology was fairly limited and brittle: it could remix language into word salad poems or chat with humans within narrowly defined topics. Recently, very large-scale statistical language models have dramatically advanced the field, and GPT-3 is just one example. It can internalize the rules of language without explicit programming or rules. Instead, much like a human child, GPT-3 learns language through repeated exposure, albeit on a much larger scale. Without explicit rules, it can sometimes fail at the simplest of linguistic tasks, but it can also excel at more difficult ones like imitating an author or waxing philosophical.", 
This paper presents a potential solution based on the Internet of Medical Things for the COVID-19 epidemic. It determines the symptoms of the disease without required any user action and warns the user. A wrist-worn device is proposed to track the health conditions of the person.,Can IoMT Help to Prevent the Spreading of New Coronavirus,"This paper presents a potential solution based on the Internet of Medical Things for the COVID-19 epidemic which determines the symptoms of the disease without required any user action and warns the user. In medical studies on COVID-19 in the literature, the most common symptoms encountered since the onset of COVID-19 disease have been reported as fever, dry cough, fatigue, anorexia, anosmia, dyspnea. The average incubation period is 5 days; therefore, it is very important to detect the symptoms even before the infected people notice the signs, and warn them. Thus, it can be ensured that they isolate themselves from society against the possible disease condition and prevent the spread of the disease. A wrist-worn device that does not physically disturb the person is proposed to track the health conditions of the person and detect the symptoms of COVID-19 at the early stages.", 
"Text summarization is a process of generating a brief version of documents by preserving the fundamental information of documents. Most of the existing summarization datasets do not have human-generated goal summaries which are vital for both summary generation and evaluation. In this study, a new dataset was presented for abstractive and extractive summarization tasks. The use of ensembled feature space, which corresponds to the joint use of syntactic and semantic features was proposed on a long short-term memory-based neural network model.",Candidate sentence selection for extractive text summarization,"Text summarization is a process of generating a brief version of documents by preserving the fundamental information of documents as much as possible. Although most of the text summarization research has been focused on supervised learning solutions, there are a few datasets indeed generated for summarization tasks, and most of the existing summarization datasets do not have human-generated goal summaries which are vital for both summary generation and evaluation. Therefore, a new dataset was presented for abstractive and extractive summarization tasks in this study. This dataset contains academic publications, the abstracts written by the authors, and extracts in two sizes, which were generated by human readers in this research. Then, the resulting extracts were evaluated to ensure the validity of the human extract production process. Moreover, the extractive summarization problem was reinvestigated on the proposed summarization dataset. Here the main point taken into account was to analyze the feature vector to generate more informative summaries. To that end, a comprehensive syntactic feature space was generated for the proposed dataset, and the impact of these features on the informativeness of the resulting summary was investigated. Besides, the summarization capability of semantic features was experienced by using GloVe and word2vec embeddings. Finally, the use of ensembled feature space, which corresponds to the joint use of syntactic and semantic features, was proposed on a long short-term memory-based neural network model. ROUGE metrics evaluated the model summaries, and the results of these evaluations showed that the use of the proposed ensemble feature space remarkably improved the single-use of syntactic or semantic features. Additionally, the resulting summaries of the proposed approach on ensembled features prominently outperformed or provided comparable performance than summaries obtained by state-ofthe-art models for extractive summarization.", 
"This paper deals with the prediction of Cardiovascular Disease (CVD) by performing an analysis of six supervised machine learning algorithms. By introducing two health risk factors (feature extraction) into the dataset, we have observed an increase in the accuracy.",Cardiovascular Disease (CVD) Prediction using Machine Learning Algorithms,"This paper deals with the prediction of Cardiovascular Disease (CVD) by performing an analysis of six supervised machine learning algorithms such as K-Nearest Neighbors Classifier, Naive Bayes Classifier, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine Classifier and Linear Discriminant Analysis. Further, by introducing two health risk factors (feature extraction) – Blood Pressure and Body Mass Index – into the dataset, we have observed an increase in the accuracy. Feature selection was performed to find out the important risk factors. Our main goal is to find the most optimal results in terms of CVD prediction from the available dataset.", 
"Cardiovascular Disease or coronary illness is one of the significant dangerous infections in India as well as in the entire world. It is estimated that 28.1% of deaths occur due to heart diseases. Proper and timely diagnosis, treatment of such diseases require a system that can predict with precise accuracy and reliability.",Cardiovascular disease prediction using deep learning techniques,"Cardiovascular Disease or coronary illness is one of the significant dangerous infections in India as well as in the entire world. It is estimated that 28.1 % of deaths occur due to heart diseases. It is also the major cause for significant number of deaths which as more than 17.6 million in the year 2016.So proper and timely diagnosis, treatment of such diseases require a system that can predict with precise accuracy and reliability. Intensive research is carried out by various researchers using diverse machine learning algorithms to forecast the heart disease taking different datasets which consists of different attributes that result in heart attack. In this paper we analyzed the dataset collected from kaggle which consists of attributes related to heart disease such as age, gender, blood pressure, cholesterol and so on. We have also investigated the accuracy levels of various machine learning techniques such as Support Vector Machines (SVM), K-Nearest Neighbor (KNN), Decision Trees (DT).The performance and accuracy of above algorithms is not so well when executed using large dataset, so here we tried to improving the prediction accuracy using Artificial Neural Network(ANN),Tensor Flow Keras.", 
"The alarmingly high mortality rate and increasing global prevalence of cardiovascular diseases (CVDs) signify the crucial need for early detection schemes. Phonocardiogram (PCG) signals have been historically applied in this domain owing to its simplicity and cost-effectiveness. In this article, we propose CardioXNet, a novel lightweight end-to-end CRNN architecture for automatic detection of five classes of cardiac auscultation.",CardioXNet A Novel Lightweight Deep Learning Framework for Cardiovascular Disease Classification Using Heart Sound Recordings,"The alarmingly high mortality rate and increasing global prevalence of cardiovascular diseases (CVDs) signify the crucial need for early detection schemes. Phonocardiogram (PCG) signals have been historically applied in this domain owing to its simplicity and cost-effectiveness. In this article, we propose CardioXNet, a novel lightweight end-to-end CRNN architecture for automatic detection of five classes of cardiac auscultation namely normal, aortic stenosis, mitral stenosis, mitral regurgitation and mitral valve prolapse using raw PCG signal. The process has been automated by the involvement of two learning phases namely, representation learning and sequence residual learning. Three parallel CNN pathways have been implemented in the representation learning phase to learn the coarse and fine-grained features from the PCG and to explore the salient features from variable receptive fields involving 2D-CNN based squeeze-expansion. Thus, in the representation learning phase, the network extracts efficient time-invariant features and converges with great rapidity. In the sequential residual learning phase, because of the bidirectional-LSTMs and the skip connection, the network can proficiently extract temporal features without performing any feature extraction on the signal. The obtained results demonstrate that the proposed end-to-end architecture yields outstanding performance in all the evaluation metrics compared to the previous state-of-the-art methods with up to 99.60% accuracy, 99.56% precision, 99.52% recall and 99.68% F1- score on an average while being computationally comparable. This model outperforms any previous works using the same database by a considerable margin. Moreover, the proposed model was tested on PhysioNet/CinC 2016 challenge dataset achieving an accuracy of 86.57%. Finally the model was evaluated on a merged dataset of Github PCG dataset and PhysioNet dataset achieving excellent accuracy of 88.09%. The high accuracy metrics on both primary and secondary dataset combined with a significantly low number of parameters and end-to-end prediction approach makes the proposed network especially suitable for point of care CVD screening in low resource setups using memory constraint mobile devices.", 
"A simple algorithm based on partial hypothesis pruning can speed up object detection. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. We outline a cascade detection algorithm for a general class of models defined by a grammar formalism.",Cascade Object Detection with Deformable Part Models,"We describe a general method for building cascade classifiers from part-based deformable models such as pictorial structures. We focus primarily on the case of star-structured models and show how a simple algorithm based on partial hypothesis pruning can speed up object detection by more than one order of magnitude without sacrificing detection accuracy. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. In analogy to probably approximately correct (PAC) learning, we introduce the notion of probably approximately admissible (PAA) thresholds. Such thresholds provide theoretical guarantees on the performance of the cascade method and can be computed from a small sample of positive examples. Finally, we outline a cascade detection algorithm for a general class of models defined by a grammar formalism. This class includes not only tree-structured pictorial structures but also richer models that can represent each part recursively as a mixture of other parts.", 
"NCP represents the mental/cognitive capacity in performing a specific task. Sleep deprivation may cause prominent cognitive risks in many common activities such as driving or controlling a generic device. In this study, a novel cascaded recurrent neural network (RNN) architecture based on long short-term memory (LSTM) blocks, is proposed for the automated scoring of sleep stages. The overall percentage of correct classification for five sleep stages is found to be 86.7%.",Cascaded LSTM recurrent neural network for automated sleep stage classifcation using single-channel EEG signals,"Automated evaluation of a subject's neurocognitive performance (NCP) is a relevant topic in neurological and clinical studies. NCP represents the mental/cognitive human capacity in performing a specific task. It is difficult to develop the study protocols as the subject's NCP changes in a known predictable way. Sleep is time-varying NCP and can be used to develop novel NCP techniques. Accurate analysis and interpretation of human sleep electroencephalographic (EEG) signals is needed for proper NCP assessment. In addition, sleep deprivation may cause prominent cognitive risks in performing many common activities such as driving or controlling a generic device; therefore, sleep scoring is a crucial part of the process. In the sleep cycle, the first stage of non-rapid eye movement (NREM) sleep or stage N1 is the transition between wakefulness and drowsiness and becomes relevant for the study of NCP. In this study, a novel cascaded recurrent neural network (RNN) architecture based on long short-term memory (LSTM) blocks, is proposed for the automated scoring of sleep stages using EEG signals derived from a single-channel. Fifty-five time and frequency-domain features were extracted from the EEG signals and fed to feature reduction algorithms to select the most relevant ones. The selected features constituted as the inputs to the LSTM networks. The cascaded architecture is composed of two LSTM RNNs: the first network performed 4- class classification (i.e. the five sleep stages with the merging of stages N1 and REM into a single stage) with a classification rate of 90.8%, and the second one obtained a recognition performance of 83.6% for 2-class classification (i.e. N1 vs REM). The overall percentage of correct classification for five sleep stages is found to be 86.7%. The objective of this work is to improve classification performance in sleep stage N1, as a first step of NCP assessment, and at the same time obtain satisfactory classification results in the other sleep stages.", 
CaseSummarizer is a tool for automated text summarization of legal documents. Uses standard summary methods based on word frequency augmented with additional domain-specific knowledge.,"CaseSummarizer, A System for Automated Summarization of Legal Texts","Attorneys, judges, and others in the justice system are constantly surrounded by large amounts of legal text, which can be difficult to manage across many cases. We present CaseSummarizer, a tool for automated text summarization of legal documents which uses standard summary methods based on word frequency augmented with additional domain-specific knowledge. Summaries are then provided through an informative interface with abbreviations, significance heat maps, and other flexible controls. It is evaluated using ROUGE and human scoring against several other summarization systems, including summary text and feedback provided by domain experts.", 
A novel Cat Swarm Optimization (CSO) based multi document summarizer is proposed to address the problem of multi- document summarization. The proposed CSO based model is also compared with two other nature inspired based summarizers. The experimental analysis clearly reveals that the proposed approach outperforms the other summarizers included in the study.,Cat swarm optimization based evolutionary framework for multi document summarization,"Today, World Wide Web has brought us enormous quantity of on-line information. As a result, extracting relevant information from massive data has become a challenging issue. In recent past text summarization is recognized as one of the solution to extract useful information from vast amount documents. Based on number of documents considered for summarization, it is categorized as single document or multi document summarization. Rather than single document, multi document summarization is more challenging for the researchers to find accurate summary from multiple documents. Hence in this study, a novel Cat Swarm Optimization (CSO) based multi document summarizer is proposed to address the problem of multi document summarization. The proposed CSO based model is also compared with two other nature inspired based summarizer such as Harmonic Search (HS) based summarizer and Particle Swarm Optimization (PSO) based summarizer. With respect to the benchmark Document Understanding Conference (DUC) datasets, the performance of all algorithms are compared in terms of different evaluation metrics such as ROUGE score, F score, sensitivity, positive predicate value, summary accuracy, inter sentence similarity and readability metric to validate non-redundancy, cohesiveness and readability of the summary respectively. The experimental analysis clearly reveals that the proposed approach outperforms the other summarizers included in the study.", 
We examine the role of category representations for decision-making in real-life tasks. We examine how people categorize kitchen objects and make use of categories,Categorization in Real-World Tasks,"We examine the role of category representations for decision-making in real-life tasks. To this end, we empirically examine how people categorize kitchen objects and make use of categories when storing objects in a kitchen. We then compare two computational models and their ability to represent the participants’ mental models. We discuss the advantages and disadvantages of the models and point the way to further research.", 
"Categorization of processes according to their functionality is a key point to understand, how the user spent his or her time working with a computer. We evaluated the proposed approach on our collected data and got 81% correctly classified processes.",Categorization of process names,"The categorization of processes according to their functionality is a key point to understand, how the user spent his or her time working with a computer. This task is not trivial due to the limited textual information available with applications. The existing approaches of this area commonly use predefined categories or clusterization techniques for applications, not processes. These techniques also do not describe the choice of these particular categories. In this paper, we describe our approach for the categorization of process names and explain, why we choose the set of categories we used. We evaluated the proposed approach on our collected data and got 81% correctly classified processes.", 
Text Summarization aims at producing a quick and concise summary of the Text. The algorithm we have developed extracts key words from Kannada text documents. A document from a given category is selected from our database custom built for this purpose. For removing stop words we have presented a novel technique which finds structurally similar words in a document.,Categorized Text Document Summarization in the Kannada Language by sentence ranking,"The growth of internet has given rise to the need for better Information Retrieval (IR) techniques which help in obtaining relevant information at a faster rate. Text Summarization is one such technique which aims at producing a quick and concise summary of the Text. Of late, Key word based summary has drawn wide attention of researchers in Natural Language Processing community. The algorithm we have developed extracts key words from Kannada text documents, for which we combine GSS ( Galavotti, Sebastiani, Simi )[13] coefficients and IDF(Inverse Document Frequency) methods along with TF(Term Frequency) for extracting key words and later uses these for summarization. The important objective our work is to assign a weight to each word in a sentence, the weight of a sentence is the sum of weights of all words, based on the scoring of sentences; we choose top 'm' sentences. A document from a given category is selected from our database custom built for this purpose. The files are obtained from Kannada Webdunia. Kannada Webdunia is a Kannada Portal which o?ers Political News, Cinema News, Sports news, Shopping and Jokes. Depending on the number of sentences given by the user, a summary is generated. Finally we make comparison of machine generated summary with that of human summary. Yet another objective of this work is to perform feature extraction through removal of stop words. For removing stop words we have presented a novel technique which finds structurally similar words in a document.", 
Text Summarization aims at producing a quick and concise summary of the Text. The algorithm we have developed extracts key words from Kannada text documents. A document from a given category is selected from our database custom built for this purpose. For removing stop words we have presented a novel technique which finds structurally similar words in a document.,Categorizing Software Applications for Maintenance,"Software repositories hold applications that are often categorized to improve the effectiveness of various maintenance tasks. Properly categorized applications allow stakeholders to identify requirements related to their applications and predict maintenance problems in software projects. Unfortunately, for different legal and organizational reasons the source code is often not available, thus making it difficult to automatically categorize binary executables of software applications. In this paper, we propose a novel approach in which we use Application Programming Interface (API) calls from third-party libraries as attributes for automatic categorization of software applications that use these API calls. API calls can be extracted from source code and more importantly, from the byte-code of applications, thus making automatic categorization approaches applicable to closed source repositories. We evaluate our approach along with other machine learning algorithms for software categorization on two large Java repositories: an opensource repository containing 3,286 projects and a closed-source one with 745 applications. Our contribution is twofold: not only do we propose a new approach that makes it possible to categorize software projects without any source code using a small number of API calls as attributes, but also we carried out the first comprehensive empirical evaluation of automatic categorization approaches.", 
"Centroid-based method exploits the compositional capabilities of word embeddings. Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models. Our method is unsupervised and it can be adopted in other summarization tasks.",Centroid-based text summarization through compositionality of word embeddings,"The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for text summarization that exploits the compositional capabilities of word embeddings. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the bag-of-words model. Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models. Our method is unsupervised and it can be adopted in other summarization tasks.", 
Life cycle and chain of digital evidence are very important parts of digital investigation process. Investigators and expert witness must know all details on how the evidence was handled every step of the way.,Chain of Custody and Life Cycle of Digital Evidence,"Life cycle and chain of digital evidence are very important parts of digital investigation process. It is very difficult to maintain and prove chain of custody. Investigators and expert witness must know all details on how the evidence was handled every step of the way. At each stage in life cycle of digital evidence, there is more impact (human, technical and natural) that can violate digital evidence. This paper presents a basic concept of ""chain of custody of digital evidence"" and ""life cycle of digital evidence"". It will address a phase in life cycle in digital archiving. The authors also warn of certain shortcomings in terms of answering specific questions, and gives same basic definition.", 
"In this study, the latest research articles which are involved in the Internet of Things (IoT) based healthcare system are analyzed. Numerous research has been carried out in the IoT-based healthcare system to improve monitoring efficiency. The performance of the IoT based healthcare systems along with its advantages and limitations are reviewed.",Challenges and opportunities in IoT healthcare systems a systematic review,"In this study, the latest research articles which are involved in the Internet of Things (IoT) based healthcare system are analyzed as the IoT is growing enormously in the healthcare systems such as health monitoring, fitness programs, etc. Numerous research has been carried out in the IoT based healthcare system to improve monitoring efficiency. The architecture used in the IoT especially the cloud integrated systems are investigated in this work. The factors such as accuracy and power consumption are the important concern in the IoT, hence the research works which are involved in improving the performance of the IoT based healthcare systems are discussed. Data management methods in the IoT based healthcare system with cloud facilities are also systematically analyzed in this study. The performance of the IoT based healthcare system along with its advantages and limitations are reviewed. Most research works are efficient in detecting several symptoms and can accurately predict the diseases. The IoT based healthcare system designed especially for elders is an efficient solution in monitoring their healthcare issues. Major limitations in the existing systems are high power consumption, availability of fewer resources and security issues due to the utilization of many devices.", 
ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges. We describe the datasets created for these challenges and summarize the results of the competitions.,Challenges in Representation Learning A Report on Three Machine Learning Contests,"The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.", 
"High-temperature effects and water effects explain interdependently the rutting decay of rock asphalt mixture, which is neglected in existing studies. The temperature and water play an important role in theRutting resistance of rock Asphalt mixture. A Bayesian approach for modelling the dynamic stability is also established.",Characterization and prediction of rutting resistance of rock asphalt mixture under the coupling effect of water and high-temperature,"In recent years, the use of rock asphalt as modifier has attracted research interests because of the advantage in improving the thermal stability of asphalt mixture, which is conducive to the rutting resistance of asphalt pavement in high-temperature regions. However, due to a large rainfall in most of high-temperature regions (e.g., China southeastern coastal areas and south China areas), the high-temperature effects and water effects explain interdependently the rutting decay of rock asphalt mixture, which is neglected in existing studies. It is necessary to investigate the coupling effect of high-temperature and water on the rutting resistance of rock asphalt mixture. In response to the above questions, water saturated characteristics of rock asphalt mixture at high-temperature are investigated to determine the water absorption conditions via a new index, namely the water saturated ratio. Moreover, a total of sixty groups of modified wheel tracking tests for rock asphalt mixtures are implemented to reveal the coupling effect of water and high-temperature on the rutting resistance with the change of water conditions, temperatures and rock asphalt dosages. The temperature and water play an important role in the rutting resistance of rock asphalt mixture. Finally, a Bayesian approach for modelling the dynamic stability is also established to predict the rutting resistance of rock asphalt mixture and evaluate the effect degree of different factors.", 
Method is an extension of the conventional two-tone test for power amplifiers to concurrent dual-band transmitters. Output signal is affected by intermodulation and cross-modulation products. Memory effects are more dominant in the third-order IM products than in the CM products.,Characterization of Concurrent Dual-Band Power Amplifiers Using a Dual Two-Tone Excitation Signal,"A method to characterize the memory effects in a nonlinear concurrent dual-band transmitter is presented. It is an extension of the conventional two-tone test for power amplifiers to concurrent dual-band transmitters. The output signal of a concurrent dual-band transmitter is affected not only by intermodulation (IM) products but also by cross-modulation (CM) products. In one frequency band, the transmitter is excited by a two-tone signal which frequency separation is swept. In the second band, the transmitter is concurrently excited by another two-tone signal with slightly wider frequency separation. The frequency difference of the two signals is fixed during the frequency sweep. The two-tone test is made at different power levels. The upper and lower third-order IM and CM products are measured. The asymmetry between the upper and lower third-order IM and CM products are measures of the transmitter’s memory effects. The measurement results show that the memory effects are more dominant in the third-order IM products than in the CM products. An error analysis and system calibration was performed and measurement results for two different devices are presented.", 
Algorithm for constructing lexical chains based on Hownet knowledge database is given in the method. Lexical chains are firstly constructing by calculating the semantic similarity between terms. Keywords are extracted and the importance of each sentence is calculated.,Chinese Automatic Text Summarization Based on Keyword Extraction,"In order to over the shortcoming of the incomprehensive of summarization, a new lexical-chain-based keywords extraction and automatic summarization algorithm from Chinese texts based on the unknown word recognition using co-occurrence of neighbor words is proposed in this paper, and an algorithm for constructing lexical chains based on Hownet knowledge database is given in the method, lexical chains are firstly constructing by calculating the semantic similarity between terms, then keywords are extracted and the importance of each sentence is calculated according to the lexical chain’s intensity, the terms’ entropy and position. The experimental results show that the summarization generated by the improved algorithm gets better performance than other methods both in recall and precision.",  
"Two approaches to text classification based on summarization technique are proposed. In the first approach, the heuristic rules of auto-summarization are used to select and weight features for every category.",Chinese Text Classification Based on Summarization Technique,"Two approaches to text classification based on summarization technique are proposed: in the first approach, the heuristic rules of auto-summarization are used to select and weight features for every category, and texts are classified by these features; in the second approach, the text summarization is directly used for classification instead of the original text. Experimental results show that the combination of summarization technology and classification technology can not only reduce the time of feature selection and classification but also improve the performance of text classification.", 
A Chinese text summarization algorithm based on Word2vec is used to extract some sentences that can cover a topic. Words in an article are represented as vectors. The weight of each word is calculated by combining word-sentence relationship with graph-based model.,Chinese Text Summarization Algorithm Based on Word2vec,"In order to extract some sentences that can cover the topic of a Chinese article, a Chinese text summarization algorithm based on Word2vec is used in this paper. Words in an article are represented as vectors trained by Word2vec, the weight of each word, the sentence vector and the weight of each sentence are calculated by combining word-sentence relationship with graph-based ranking model. Finally the summary is generated on the basis of the final sentence vector and the final weight of the sentence. The experimental results on real datasets show that the proposed algorithm has a better summarization quality compared with TF-IDF and TextRank.", 
Methodology is based on extracting significant keyphrases from the set of citation sentences. It is then used to build the summary. Comparisons show how this methodology excel.,Citation Summarization Through Keyphrase Extraction,"This paper presents an approach to summarize single scientific papers, by extracting its contributions from the set of citation sentences written in other papers. Our methodology is based on extracting significant keyphrases from the set of citation sentences and using these keyphrases to build the summary. Comparisons show how this methodology excels at the task of single paper summarization, and how it out-performs other multi-document summarization methods.", 
"The prediction of cardiac disease helps practitioners make more accurate decisions regarding patients' health. Chi-square and principal component analysis (CHI-PCA) with random forests (RF) had the highest accuracy, with 98.7% for Cleveland, 99.0% for Hungarian, and 99.4% for CH datasets.",Classifcation models for heart disease prediction using feature selection and PCA,"The prediction of cardiac disease helps practitioners make more accurate decisions regarding patients’ health. Therefore, the use of machine learning (ML) is a solution to reduce and understand the symptoms related to heart disease. The aim of this work is the proposal of a dimensionality reduction method and finding features of heart disease by applying a feature selection technique. The information used for this analysis was obtained from the UCI Machine Learning Repository called Heart Disease. The dataset contains 74 features and a label that we validated by six ML classifiers. Chi-square and principal component analysis (CHI-PCA) with random forests (RF) had the highest accuracy, with 98.7% for Cleveland, 99.0% for Hungarian, and 99.4% for Cleveland-Hungarian (CH) datasets. From the analysis, ChiSqSelector derived features of anatomical and physiological relevance, such as cholesterol, highest heart rate, chest pain, features related to ST depression, and heart vessels. The experimental results proved that the combination of chi-square with PCA obtains greater performance in most classifiers. The usage of PCA directly from the raw data computed lower results and would require greater dimensionality to improve the results.",  
"Ransomware is a special type of malware that can lock victims' screen and/or encrypt their files to obtain ransoms. Mapping ransomware into families is useful for identifying variants and reducing analysts' workload, authors say. Authors: We are the first to propose an approach based on static analysis to classifying ransomware.",Classifcation of ransomware families with machine learning based on N-gram of opcodes,"Ransomware is a special type of malware that can lock victims’ screen and/or encrypt their files to obtain ransoms, resulting in great damage to users. Mapping ransomware into families is useful for identifying the variants of a known ransomware sample and for reducing analysts’ workload. However, ransomware that can fingerprint the environment can evade the precious work of dynamic analysis. To the best of our knowledge, to overcome this shortcoming, we are the first to propose an approach based on static analysis to classifying ransomware. First, opcode sequences from ransomware samples are transformed into N-gram sequences. Then, Term frequency-Inverse document frequency (TF-IDF) is calculated for each N-gram to select feature N-grams so that these N-grams exhibit better discrimination between families. Finally, we treat the vectors composed of the TF values of the feature N-grams as the feature vectors and subsequently feed them to five machine-learning methods to perform ransomware classification. Six evaluation criteria are employed to validate the model. Thorough experiments performed using real datasets demonstrate that our approach can achieve the best Accuracy of 91.43%. Furthermore, the average F1-measure of the “wannacry” ransomware family is up to 99%, and the Accuracy of binary classification is up to 99.3%. The proposed method can detect and classify ransomware that can fingerprint the environment. In addition, we discover that different feature dimensions are required for achieving similar classifier performance with feature N-grams of diverse lengths.", 
"The ability to correctly identify likes and dislikes as well as excitement and boredom would facilitate novel applications in neuromarketing, affective entertainment, virtual rehabilitation and forensic neuroscience that leverage on sub-conscious human affective states. In this neuroinformatics investigation, we seek to recognize human preferences and excitement passively through the use of electroencephalography. In the second part of the study, users are exposed to a roller-coaster experience as the emotional stimuli which are expected to evoke the emotion of excitement.",Classification of Affective States via EEG and Deep Learning,"Human emotions play a key role in numerous decision-making processes. The ability to correctly identify likes and dislikes as well as excitement and boredom would facilitate novel applications in neuromarketing, affective entertainment, virtual rehabilitation and forensic neuroscience that leverage on sub-conscious human affective states. In this neuroinformatics investigation, we seek to recognize human preferences and excitement passively through the use of electroencephalography (EEG) when a subject is presented with some 3D visual stimuli. Our approach employs the use of machine learning in the form of deep neural networks to classify brain signals acquired using a brain-computer interface (BCI). In the first part of our study, we attempt to improve upon our previous work, which has shown that EEG preference classification is possible although accuracy rates remain relatively low at 61%-67% using conventional deep learning neural architectures, where the challenge mainly lies in the accurate classification of unseen data from a cohort-wide sample that introduces inter-subject variability on top of the existing intra-subject variability. Such an approach is significantly more challenging and is known as subject-independent EEG classification as opposed to the more commonly adopted but more time-consuming and less general approach of subject-dependent EEG classification. In this new study, we employ deep networks that allow dropouts to occur in the architecture of the neural network. The results obtained through this simple feature modification achieved a classification accuracy of up to 79%. Therefore, this study has shown that the use of a deep learning classifier was able to achieve an increase in emotion classification accuracy of between 13% and 18% through the simple adoption of the use of dropouts compared to a conventional deep learner for EEG preference classification. In the second part of our study, users are exposed to a roller-coaster experience as the emotional stimuli which are expected to evoke the emotion of excitement, while simultaneously wearing virtual reality goggles, which delivers the virtual reality experience of excitement, and an EEG headset, acquires the raw brain signals detected when exposed to this excitement stimuli. Here, a deep learning approach is used to improve the excitement detection rate to well above the 90% accuracy level. In a prior similar study, the use of conventional machine learning approaches involving k-Nearest Neighbour (kNN) classifiers and Support Vector Machines (SVM) only achieved prediction accuracy rates of between 65% and 89%. Using a deep learning approach here, rates of 78%-96% were achieved. This demonstrates the superiority of adopting a deep learning approach over other machine learning approaches for detecting human excitement when immersed in an immersive virtual reality environment.", 
Malware targeting mobile devices is a pervasive problem in modern life. The ability of DNNs to learn complex and flexible features may lead to timely and effective detection of malware. Both CNN and LSTM significantly outperformed n-gram based methods.,Classification of Android Apps and Malware Using Deep Neural Networks,"Malware targeting mobile devices is a pervasive problem in modern life. The detection of malware is essentially a software classification problem based on information gathered from program analysis. We focus on classification of Android applications using system API-call sequences and investigate the effectiveness of Deep Neural Networks (DNNs) for such purpose. The ability of DNNs to learn complex and flexible features may lead to timely and effective detection of malware. We design a Convolutional Neural Network (CNN) for sequence classification and conduct a set of experiments on malware detection and categorization of software into functionality groups to test and compare our CNN with classifications by recurrent neural network (LSTM) and other n-gram based methods. Both CNN and LSTM significantly outperformed n-gram based methods. Surprisingly, the performance of our CNN is also much better than that of the LSTM, which is considered a natural choice for sequential data.", 
Feature extraction and classification of electroencephalogram (EEGs) signals for (normal and epileptic) is a challenge for engineers and scientists. SVM (support vector machine) based classifier was employed to detect epileptic seizure activity from background electro encephalographs. The results show the promising classification accuracy of nearly 91.2% in detection of abnormal from normal EEG signals.,Classification of EEG Signal Using Wavelet Transform and Support Vector Machine for Epileptic Seizure Diction,"Feature extraction and classification of electroencephalogram (EEGs) signals for (normal and epileptic) is a challenge for engineers and scientists. Various signal processing techniques have already been proposed for classification of non-linear and nonstationary signals like EEG. In this work, SVM (support vector machine) based classifier was employed to detect epileptic seizure activity from background electro encephalographs (EEGs). Five types of EEG signals (healthy subject with eye open condition, eye close condition, epileptic, seizure signal from hippocampal region) were selected for the analysis. Signals were preprocessed, decomposed by using discrete wavelet transform DWT till 5th level of decomposition tree. Various features like energy, entropy and standard deviation were computed and consequently used for classification of signals. The results show the promising classification accuracy of nearly 91.2% in detection of abnormal from normal EEG signals. This proposed classifier can be used to design expert system for epilepsy diagnosis purpose in various hospitals.", 
High accuracy for the classification of electroencephalogram (EEG) signal is an important basis for a brain-computer interface (BCI) system. The proposed method is able to achieve a classification accuracy of 91.13%. Using this method might enhance the performance of a BCI system.,Classification of EEG Signals by Multi-Scale Filtering and PCA,"High accuracy for the classification of electroencephalogram (EEG) signal is an important basis for a brain-computer interface (BCI) system. In this paper, we proposed a novel approach to enhance the classification performance in identifying EEG signals, which classify EEG by combining multi-scale filters and Principal Component Analysis (PCA). First, a multi-scale filter with different size of filter window was used to extract major frequency-band components from EEG signals. This might not only enhance the adaptability of filter to the EEG signals, but also satisfy the diversity of frequency resolution. Then PCA was utilized for feature extraction to reduce data dimension and improve the classification accuracy. The experimental results on EEG signals of motor imagery indicate that the proposed method is able to achieve a classification accuracy of 91.13%. Using this method might enhance the performance of a BCI system in signal recognition.", 
"Measuring cognitive load is crucial for many applications such as information personalization, adaptive intelligent tutoring systems, etc. Cognitive load estimation using Electroencephalogram (EEG) signals is widespread as it produces clear indications of cognitive activities. Deep learning has not been extensively studied for the classification of cognitive load data captured by an EEG.",Classification of EEG Signals for Cognitive Load Estimation Using Deep Learning Architectures,"Measuring cognitive load is crucial for many applications such as information personalization, adaptive intelligent tutoring systems, etc. Cognitive load estimation using Electroencephalogram (EEG) signals is widespread as it produces clear indications of cognitive activities by measuring changes of neural activation in the brain. However, the existing cognitive load estimation techniques are based on machine learning algorithms, which follow signal denoising and hand-crafted feature extraction to classify different loads. There is a need to find a better alternative to the machine learning approach. Of late, deep learning approach has been successfully applied to many applications namely, computer vision, pattern recognition, speech processing, etc. However, deep learning has not been extensively studied for the classification of cognitive load data captured by an EEG. In this work, two deep learning models are studied, namely stacked denoising autoencoder (SDAE) followed by a multilayer perceptron (MLP) and long short term memory (LSTM) followed by an MLP to classify cognitive load data. SDAE and LSTM are used for feature extraction and MLP for classification. It is observed that deep learning models perform significantly better than the conventional machine learning classifiers such as support vector machine (SVM), k-nearest neighbors (KNN), and linear discriminant analysis (LDA).", 
"Electroencephalogram (EEG) signals are often used to diagnose diseases such as seizure, alzheimer, and schizophrenia. One main problem with the recorded EEG samples is that they are not equally reliable due to artifacts at the time of recording. In this paper, a general adaptive method named weighted distance nearest neighbor (WDNN) is applied for EEG signal classification.",Classification of EEG Signals using adaptive weighted distance nearest neighbor algorithm,"Electroencephalogram (EEG) signals are often used to diagnose diseases such as seizure, alzheimer, and schizophrenia. One main problem with the recorded EEG samples is that they are not equally reliable due to the artifacts at the time of recording. EEG signal classification algorithms should have a mechanism to handle this issue. It seems that using adaptive classifiers can be useful for the biological signals such as EEG. In this paper, a general adaptive method named weighted distance nearest neighbor (WDNN) is applied for EEG signal classification to tackle this problem. This classification algorithm assigns a weight to each training sample to control its influence in classifying test samples. The weights of training samples are used to find the nearest neighbor of an input query pattern. To assess the performance of this scheme, EEG signals of thirteen schizophrenic patients and eighteen normal subjects are analyzed for the classification of these two groups. Several features including, fractal dimension, band power and autoregressive (AR) model are extracted from EEG signals. The classification results are evaluated using Leave one (subject) out cross validation for reliable estimation. The results indicate that combination of WDNN and selected features can significantly outperform the basic nearest-neighbor and the other methods proposed in the past for the classification of these two groups. Therefore, this method can be a complementary tool for specialists to distinguish schizophrenia disorder.", 
"The electroencephalogram (EEG) is widely used clinically to investigate brain disorders. abnormalities in the EEG in serious psychiatric disorders are at times too subtle to be detected using conventional techniques. This paper describes the application of an artificial neural network (ANN) technique together with a feature extraction technique, viz., the wavelet transform.",Classification of EEG signals using the wavelet transform,"The electroencephalogram (EEG) is widely used clinically to investigate brain disorders. However, abnormalities in the EEG in serious psychiatric disorders are at times too subtle to be detected using conventional techniques. This paper describes the application of an artificial neural network (ANN) technique together with a feature extraction technique, viz., the wavelet transform, for the classification of EEG signals. The data reduction and preprocessing operations of signals are performed using the wavelet transform. Three classes of EEG signals were used: Normal, Schizophrenia (SCH), and Obsessive Compulsive Disorder (OCD). The architecture of the artificial neural network used in the classification is a three-layered feedforward network which implements the backpropagation of error learning algorithm. After training, the network with wavelet coefficients was able to correctly classify over 66% of the normal class and 71% of the schizophrenia class of EEGs, respectively. The wavelet transform thus provides a potentially powerful technique for preprocessing EEG signals prior to classification.",  
This paper describes the analysis of a deep neural network for the classification of epileptic EEG signals. The deep learning architecture is made up of two convolutional layers for feature extraction and three fully-connected layers for classification. The accuracy increased from 99.0% to 99.5% for classifying non-seizure vs. seizure recordings.,Classification of epileptic EEG recordings using signal transforms and convolutional neural networks,"This paper describes the analysis of a deep neural network for the classification of epileptic EEG signals. The deep learning architecture is made up of two convolutional layers for feature extraction and three fully-connected layers for classification. We evaluated several EEG signal transforms for generating the inputs to the deep neural network: Fourier, wavelet and empirical mode decomposition. This analysis was carried out using two public datasets (Bern-Barcelona EEG and Epileptic Seizure Recognition datasets) obtaining significant improvements in accuracy. For the Bern-Barcelona EEG, we obtained an increase in accuracy from 92.3% to 98.9% when classifying between focal and non-focal signals using the empirical mode decomposition. For the Epileptic Seizure Recognition dataset, we evaluated several scenarios for seizure detection obtaining the best results when using the Fourier transform. The accuracy increased from 99.0% to 99.5% for classifying non-seizure vs. seizure recordings, from 91.7% to 96.5% when di?erentiating between healthy, non-focal and seizure recordings, and from 89.0% to 95.7% when considering healthy, focal and seizure recordings.", 
This paper describes the analysis of a deep neural network for the classification of epileptic EEG signals. The deep learning architecture is made up of two convolutional layers for feature extraction and three fully-connected layers for classification. The accuracy increased from 99.0% to 99.5% for classifying non-seizure vs. seizure recordings.,Classification of Epileptic EEG Signals with Stacked Sparse Autoencoder Based on Deep Learning,"Automatic detection of epileptic seizure plays an important role in the diagnosis of epilepsy for it can obtain invisible information of epileptic electroencephalogram (EEG) signals exactly and reduce the heavy burdens of doctors efficiently. Current automatic detection technologies are almost shallow learning models that are insufficient to learn the complex and non-stationary epileptic EEG signals. Moreover, most of their feature extraction or feature selection methods are supervised and depend on domain-specific expertise. To solve these problems, we proposed a novel framework for the automatic detection of epileptic EEG by using stacked sparse autoencoder (SSAE) with a softmax classifier. The proposed framework firstly learns the sparse and high level representations from the preprocessed data via SSAE, and then send these representations into softmax classifier for training and classification. To verify the performance of this framework, we adopted the epileptic EEG datasets to conduct experiments. The simulation results with an average accuracy of 96 % illustrated the effectiveness of the proposed framework.", 
"The electroencephalogram (EEG) is the frequently used signal to detect epileptic seizures in the brain. For a successful epilepsy surgery, it is very essential to localize epileptogenic area. In this article, we present a computer aided automatic detection and classification method for focal and nonfocal EEG signal.",Classification of Focal and Nonfocal EEG Signals Using ANFIS Classifier for Epilepsy Detection,"The electroencephalogram (EEG) is the frequently used signal to detect epileptic seizures in the brain. For a successful epilepsy surgery, it is very essential to localize epileptogenic area in the brain. The signals from the epileptogenic area are focal signals and signals from other area of the brain region nonfocal signals. Hence, the classification of focal and nonfocal signals is important for locating the epileptogenic area for epilepsy surgery. In this article, we present a computer aided automatic detection and classification method for focal and nonfocal EEG signal. The EEG signal is decomposed by Dual Tree Complex Wavelet Transform (DT-CWT) and the features are computed from the decomposed coefficients. These features are trained and classified using Adaptive Neuro Fuzzy Inference System (ANFIS) classifier. The proposed system achieves 98% sensitivity, 100% specificity, and 99% accuracy for EEG signal classification. The experimental results are presented to show the effectiveness of the proposed classification method to classify the focal and nonfocal EEG signals.", 
"New method for electroencephalogram (EEG) signal classification based on fractional-order calculus. The method, termed fractional linear prediction (FLP), is used to model ictal and seizure-free EEG signals. It is found that the proposed method gives a classification accuracy of 95.33%",Classification of ictal and seizure-free EEG signals using fractional linear prediction,"In this paper, we present a new method for electroencephalogram (EEG) signal classification based on fractional-order calculus. The method, termed fractional linear prediction (FLP), is used to model ictal and seizure-free EEG signals. It is found that the modeling error energy is substantially higher for ictal EEG signals compared to seizure-free EEG signals. Moreover, it is known that ictal EEG signals have higher energy than seizure-free EEG signals. These two parameters are then given as inputs to train a support vector machine (SVM). The trained SVM is then used to classify a set of EEG signals into ictal and seizure-free categories. It is found that the proposed method gives a classification accuracy of 95.33% when the SVM is trained with the radial basis function (RBF) kernel.", 
"A machine learning algorithm called Extreme Learning Machine (ELM) is used to classify five mental tasks from different subjects. ELM needs an order of magnitude less training time compared with SVMs, the study says. smoothing of the classifiers' outputs can significantly improve their classification accuracy.",CLASSIFICATION OF MENTAL TASKS FROM EEG SIGNALS USING EXTREME LEARNING MACHINE,"In this paper, a recently developed machine learning algorithm referred to as Extreme Learning Machine (ELM) is used to classify five mental tasks from different subjects using electroencephalogram (EEG) signals available from a well-known database. Performance of ELM is compared in terms of training time and classification accuracy with a Backpropagation Neural Network (BPNN) classifier and also Support Vector Machines (SVMs). For SVMs, the comparisons have been made for both 1-against-1 and 1-against-all methods. Results show that ELM needs an order of magnitude less training time compared with SVMs and two orders of magnitude less compared with BPNN. The classification accuracy of ELM is similar to that of SVMs and BPNN. The study showed that smoothing of the classifiers’ outputs can significantly improve their classification accuracies.", 
Motor imagery (MI) based brain–computer interface systems are highly required in many real-time applications. The existing classification techniques are either computationally expensive or not so accurate or both. The proposed method computes only wavelet energy directly from the segmented MI data and constructs a dictionary.,Classification of multiclass motor imagery EEG signal using sparsity approach,"Motor imagery (MI) based brain–computer interface systems involving multiple tasks are highly required in many real-time applications such as hands and touch-free text entry, prosthetic arms, virtual reality systems, movement of a wheel chair, cursor movement, etc. The classification of MI data is the core computing in all these systems. However, the existing classification techniques are either computationally expensive or not so accurate or both. To address this limitation, in this work, a sparse representation based classification technique has been proposed to classify multi-tasks MI electroencephalogram data. The proposed method computes only wavelet energy directly from the segmented MI data and constructs a dictionary. The sparse representation from the dictionary is then used to classify given a test data. The proposed approach is faster as it works with only a single feature and without the need for any preprocessing. Further, with a reduced length of an imaging period, the proposed method provides accurate classification in a lesser computation time. The performance of the proposed approach has been evaluated and also compared with other classifiers reported in the literature. The results substantiate that the proposed sparsity approach performs significantly better than the existing classifiers.", 
"More than 21 million people worldwide suffer from schizophrenia. This serious mental disorder exposes people to stigmatization, discrimination, and violation of their human rights. Different works on classification and diagnosis of mental illnesses use electroencephalogram signals. This work proposes a model for the classification of schizophrenic and healthy people through EEG signals using Deep Learning methods.",Classification of People who Suffer Schizophrenia and Healthy People by EEG Signals using Deep Learning,"More than 21 million people worldwide suffer from schizophrenia. This serious mental disorder exposes people to stigmatization, discrimination, and violation of their human rights. Different works on classification and diagnosis of mental illnesses use electroencephalogram signals (EEG) because it reflects brain functioning, and how these diseases affect it. Due to the information provided by the EEG signals and the performance demonstrated by Deep Learning algorithms, the present work proposes a model for the classification of schizophrenic and healthy people through EEG signals using Deep Learning methods. Considering the properties of an EEG, high-dimensional and multichannel, we applied the Pearson Correlation Coefficient (PCC) to represent the relations between the channels, this way instead of using the large amount of data that an EEG provides, we used a shorter matrix as an input of a Convolutional Neural Network (CNN). Finally, results demonstrated that the proposed EEG-based classification model achieved Accuracy, Specificity, and Sensitivity of 90%, 90%, and 90%, respectively.", 
"More than 21 million people worldwide suffer from schizophrenia. This serious mental disorder exposes people to stigmatization, discrimination, and violation of their human rights. Different works on classification and diagnosis of mental illnesses use electroencephalogram signals. This work proposes a model for the classification of schizophrenic and healthy people through EEG signals using Deep Learning methods.",Classification of Seizure and Nonseizure EEG Signals Using Empirical Mode Decomposition,"In this paper, we present a new method for classification of electroencephalogram (EEG) signals using empirical mode decomposition (EMD) method. The intrinsic mode functions (IMFs) generated by EMD method can be considered as a set of amplitude and frequency modulated (AM–FM) signals. The Hilbert transformation of IMFs provides an analytic signal representation of the IMFs. The two bandwidths, namely amplitude modulation bandwidth (BAM ) and frequency modulation bandwidth (BF M ), computed from the analytic IMFs, have been used as an input to least squares support vector machine (LS-SVM) for classifying seizure and nonseizure EEG signals. The proposed method for classification of EEG signals based on the bandwidth features (BAM and BF M ) and the LS-SVM has provided better classification accuracy than the method adopted by Liang and coworkers in their study published in 2010. The experimental results with the recorded EEG signals from a published dataset are included to show the effectiveness of the proposed method for EEG signal classification.", 
Deep learning has been rarely used for MI EEG signal classification. The proposed framework outperforms all other competing methods in terms of reducing the maximum error. The framework can be used for developing BCI systems using wearable devices as it is computationally less expensive and more reliable.,A Deep Learning Approach for Motor Imagery EEG Signal Classification,"Over the last few decades, the use of electroencephalography (EEG) signals for motor imagery based brain-computer interface (MI-BCI) has gained widespread attention. Deep learning have also gained widespread attention and used in various application such as natural language processing, computer vision and speech processing. However, deep learning has been rarely used for MI EEG signal classification. In this paper, we present a deep learning approach for classification of MI-BCI that uses adaptive method to determine the threshold. The widely used common spatial pattern (CSP) method is used to extract the variance based CSP features, which is then fed to the deep neural network for classification. Use of deep neural network (DNN) has been extensively explored for MI-BCI classification and the best framework obtained is presented. The effectiveness of the proposed framework has been evaluated using dataset IVa of the BCI Competition III. It is found that the proposed framework outperforms all other competing methods in terms of reducing the maximum error. The framework can be used for developing BCI systems using wearable devices as it is computationally less expensive and more reliable compared to the best competing methods.", 
One-dimensional local binary pattern (1D-LBP) based features are used for classification of seizure and seizure-free electroencephalogram (EEG) signals. The proposed method employs a bank of Gabor filters for processing the EEG signals. Experimental results suggest that the proposed features effectively characterize local variations.,Classification of seizure and seizure-free EEG signals using local binary patterns,Local binary pattern (LBP) is a texture descriptor that has been proven to be quite effective for various image analysis tasks in image processing. In this paper one-dimensional local binary pattern (1D-LBP) based features are used for classification of seizure and seizure-free electroencephalogram (EEG) signals. The proposed method employs a bank of Gabor filters for processing the EEG signals. The processed EEG signal is divided into smaller segments and histograms of 1D-LBPs of these segments are computed. Nearest neighbor classifier utilizes the histogram matching scores to determine whether the acquired EEG signal belongs to seizure or seizure-free category. Experimental results on publicly available database suggest that the proposed features effectively characterize local variations and are useful for classification of seizure and seizure-free EEG signals with a classification accuracy of 98.33%. This result demonstrates the superiority of our approach for classification of seizure and seizure-free EEG signals over recently proposed approaches in the literature., 
"Code comments are a key software component containing information about the underlying implementation. Not all code comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments.",Classifying code comments in Java open-source software systems,"Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments; subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning; initial results are promising and suggest that an accurate classification is within reach.", 
"Deep learning has gained a central position in machine learning circles for its great advantages in feature representation and pattern recognition. Although there are challenges involved in applying deep learning techniques to clinical data, it is still worthwhile to look forward to a promising future for deep learning applications in clinical big data.","Clinical Big Data and Deep Learning Applications, Challenges, and Future Outlooks","The explosion of digital healthcare data has led to a surge of data-driven medical research based on machine learning. In recent years, as a powerful technique for big data, deep learning has gained a central position in machine learning circles for its great advantages in feature representation and pattern recognition. This article presents a comprehensive overview of studies that employ deep learning methods to deal with clinical data. Firstly, based on the analysis of the characteristics of clinical data, various types of clinical data (e.g., medical images, clinical notes, lab results, vital signs, and demographic informatics) are discussed and details provided of some public clinical datasets. Secondly, a brief review of common deep learning models and their characteristics is conducted. Then, considering the wide range of clinical research and the diversity of data types, several deep learning applications for clinical data are illustrated: auxiliary diagnosis, prognosis, early warning, and other tasks. Although there are challenges involved in applying deep learning techniques to clinical data, it is still worthwhile to look forward to a promising future for deep learning applications in clinical big data in the direction of precision medicine.", 
"Machine learning (ML) and large-scale big data are key factors in developing an accurate prediction model for cardiovascular disease (CVD) In this work, we analyzed the Korean National Health Insurance Service–National Health Sample Cohort data. We developed various ML-based prediction models using logistic regression, deep neural networks, random forests, and LightGBM. All models outperformed the baseline method derived from the ACC/AHA guidelines for estimating the 10-year CVD risk.",Clinical Implication of Machine Learning in Predicting the Occurrence of Cardiovascular Disease Using Big Data (Nationwide Cohort Data in Korea),"Machine learning (ML) and large-scale big data are key factors in developing an accurate prediction model for cardiovascular disease (CVD). Although the CVD risk often depends on the race and ethnicity, most previous studies considered only US or European populations for the CVD risk prediction. In this work, to complement previous researches, we analyzed the Korean National Health Insurance Service–National Health Sample Cohort (KNHSC) data and studied the characteristics of ML and big data for predicting the CVD risk. More specifically, we assessed the effectiveness of various ML methods in predicting the 2-year and 10-year risk of CVD such as atrial fibrillation, coronary artery disease, heart failure, and strokes. To develop prediction models, we considered the usual medical examination data, questionnaire survey results, comorbidities, and past medication information available in the KNHSC data. We developed various ML-based prediction models using logistic regression, deep neural networks, random forests, and LightGBM, and validated them using various metrics such as receiver operating characteristic curves, precision-recall curves, sensitivity, specificity, and F1 score. Experimental results showed that all ML models outperformed the baseline method derived from the ACC/AHA guidelines for estimating the 10-year CVD risk, demonstrating the usefulness of ML methods. In addition, in our analysis, whether we included the past medication information as a feature or not, the prediction accuracy of all ML models was comparable to each other. Since the use of medications by the physicians provided important information on the occurrence of diseases, when we included it as a feature, all prediction models achieved a slightly higher prediction accuracy.", 
"Challenge and issues introduced, require new solutions in cloud forensics. This paper focuses on the identification of the available technical solutions addressed in the respective literature that have an applicability on cloud computing.",Cloud Forensics Solutions A Review,"Cloud computing technology attracted many Internet users and organizations the past few years and has become one of the hottest topics in IT. However, due to the newly appeared threats and challenges arisen in cloud computing, current methodologies and techniques are not designed for assisting the respective forensic processes in cloud environments. Challenges and issues introduced, require new solutions in cloud forensics. To date, the research conducted in this area concerns mostly the identification of the major challenges in cloud forensics. This paper focuses on the identification of the available technical solutions addressed in the respective literature that have an applicability on cloud computing. Furthermore it matches the identified solutions with the respective challenges already mentioned in the respective literature. Specifically, it summarizes the methods and the proposed solutions used to conduct an investigation, in comparison to the respective cloud challenges and finally it highlights the open problems in the area of cloud forensics.", 
It is expected that half of the total data will be on the cloud by 2016. Cloud computing provides an apt platform for big data analytics in view of the storage and computing requirements. This makes cloud-based analytics a viable research field.,Cloud-Based Big Data Analytic A Survey of Current and future direction,"The advent of the digital age has led to a rise in different types of data with every passing day. In fact, it is expected that half of the total data will be on the cloud by 2016. This data is complex and needs to be stored, processed, and analyzed for information that can be used by organizations. Cloud computing provides an apt platform for big data analytics in view of the storage and computing requirements of the latter. This makes cloud-based analytics a viable research field. However, several issues need to be addressed and risks need to be mitigated before practical applications of this synergistic model can be popularly used. This paper explores the existing research, challenges, open issues, and future research direction for this field of study.", 
"Vehicular Ad hoc Networks (VANETs) have drawn incredible interest in both academic and industrial sectors. Vehicles' position plays a significant role in many location-based applications and services such as public emergency, vehicles tracking, resource discovery, traffic monitoring and position-based routing. Cluster-based location service schemes have gained a growing interest due to their advantages over non-cluster-based schemes.","Cluster-based location service schemes in VANETs current state, challenges and future directions","Vehicular Ad hoc Networks (VANETs) have drawn incredible interest in both academic and industrial sectors due to their potential applications and services. Vehicles’ position plays a significant role in many location-based applications and services such as public emergency, vehicles tracking, resource discovery, traffic monitoring and position-based routing. A location service is used to keep up-to-date records of current positions of vehicles. However, locating vehicles’ positions and maintaining an accurate view of the entire network are quite challenging tasks due to the high number of nodes, and high and fast nodes mobility which results in rapid topological changes and sudden network disconnections. In the past literature, various location based services have been proposed to solve the above mentioned issues. Moreover, the cluster-based location service schemes have gained a growing interest due to their advantages over non-cluster-based schemes. The cluster-based schemes improve the network scalability, reduce the communications overhead and resolve the mobility issues within the clusters preventing them from propagating in the whole network. Therefore, this paper presents the taxonomy of the existing location service schemes, inspects the cluster-based location service by highlighting their strengths and limitations, and provides a comparison between location-based clustering and application specific clustering such as the one used in routing, information dissemination, channel access management and security. In addition, the existing clustering schemes, challenges and future directions for efficient cluster-based location service are also discussed.", 
"Multi-document summarization aims to produce a compressed version of numerous online text documents. This paper introduces a clustered genetic semantic graph approach for multi-document abstractive summarization. Experimental results indicate that the proposed approach outperforms other summarization systems. The study is performed using DUC-2002, a standard corpus for text summarizing.",Clustered genetic semantic graph approach for multi-document abstractive summarization,"Multi-document summarization aims to produce a compressed version of numerous online text documents and preserves the salient information. A particular challenge for multi-document summarization is that there is an inevitable overlap in the information stored in different documents. Thus, effective summarization methods that merge similar information across the documents are desirable. This paper introduces a clustered genetic semantic graph approach for multi-document abstractive summarization. The semantic graph from the document set is constructed in such a way that the graph vertices represent the predicate argument structures (PASs), extracted automatically by employing semantic role labeling (SRL); and the edges of graph correspond to semantic similarity weight determined from PAS-to-PAS semantic similarity, and PAS-to-document relationship. The PAS-to document relationship is expressed by different features, weighted and optimized by genetic algorithm. The salient graph nodes (PASs) are ranked based on modified weighted graph based ranking algorithm. The clustering algorithm is performed to eliminate redundancy in such a way that representative PAS with the highest salience score from each cluster is chosen, and fed to language generation to generate summary sentences. Experiment of this study is performed using DUC-2002, a standard corpus for text summarization. Experimental results indicate that the proposed approach outperforms other summarization systems.", 
"Vehicular ad hoc networks (VANETs) have attracted great interest in the research community. With the help of wireless communication, many of the safety related and non-safety related applications can be realised in VANets. A position-based clustering multi-hop routing method is proposed to disseminate information among the vehicles and infrastructures efficiently.",Clustering multi-hop information dissemination method in vehicular ad hoc networks,"As a special case of mobile ad hoc networks, vehicular ad hoc networks (VANETs) have attracted great interest in the research community. With the help of wireless communication, many of the safety related and non-safety related applications can be realised in VANETs. In the multi-hop and dynamic topology networks, building and maintaining a route is very challenging. To disseminate information among the vehicles and the infrastructures efficiently, a position-based clustering multi-hop routing method is proposed. The method hierarchically organises VANETs based on the competitive learning Hebb neural network, which partitions the vehicles into clusters, and these clusters are represented by virtual cluster-heads. The method is evaluated using NS2 and compared with typical ad hoc routing protocol Ad hoc On-Demand Distance Vector, Distance Routing Effect Algorithm for Mobility. The simulation results prove that the method is efficient.",  
"This paper presents a new approach called clustering technique-based least square support vector machine (CT-LS-SVM) for the classification of EEG signals. Decision making is performed in two stages. In the first stage, clustering techniques are used to extract representative features of EEG data. In second stage, least square Support Vector Machine is applied to the extracted features to classify signals.",Clustering technique-based least square support vector machine for EEG signal classification,"This paper presents a new approach called clustering technique-based least square support vector machine (CT-LS-SVM) for the classification of EEG signals. Decision making is performed in two stages. In the first stage, clustering technique (CT) has been used to extract representative features of EEG data. In the second stage, least square support vector machine (LS-SVM) is applied to the extracted features to classify two-class EEG signals. To demonstrate the effectiveness of the proposed method, several experiments have been conducted on three publicly available benchmark databases, one for epileptic EEG data, one for mental imagery tasks EEG data and another one for motor imagery EEG data. Our proposed approach achieves an average sensitivity, specificity and classification accuracy of 94.92%, 93.44% and 94.18%, respectively, for the epileptic EEG data; 83.98%, 84.37% and 84.17% respectively, for the motor imagery EEG data; and 64.61%, 58.77% and 61.69%, respectively, for the mental imagery tasks EEG data. The performance of the CT-LS-SVM algorithm is compared in terms of classification accuracy and execution (running) time with our previous study where simple random sampling with a least square support vector machine (SRS-LS-SVM) was employed for EEG signal classification. We also compare the proposed method with other existing methods in the literature for the three databases. The experimental results show that the proposed algorithm can produce a better classification rate than the previous reported methods and takes much less execution time compared to the SRS-LS-SVM technique. The research findings in this paper indicate that the proposed approach is very efficient for classification of two-class EEG signals.", 
"Spontaneous speech in meetings leads to incomplete, ill-formed sentences with high redundancy. We propose an extension of the TextRank algorithm that clusters the meeting utterances and uses these clusters to construct the graph.","ClusterRank, A Graph Based Method for Meeting Summarization","This paper presents an unsupervised, graph based approach for extractive summarization of meetings. Graph based methods such as TextRank have been used for sentence extraction from news articles. These methods model text as a graph with sentences as nodes and edges based on word overlap. A sentence node is then ranked according to its similarity with other nodes. The spontaneous speech in meetings leads to incomplete, ill-formed sentences with high redundancy and calls for additional measures to extract relevant sentences. We propose an extension of the TextRank algorithm that clusters the meeting utterances and uses these clusters to construct the graph. We evaluate this method on the AMI meeting corpus and show a significant improvement over TextRank and other baseline methods.", 
Co-expressed gene clusters can provide evidence for genetic or physical interactions. Clustering is a routine step in large-scale analyses of gene expression data. We show that commonly used clustering methods produce results that do not match the biological,Clust_automatic extraction of optimal coexpressed gene clusters from gene expression data,"Identifying co-expressed gene clusters can provide evidence for genetic or physical interactions. Thus, co-expression clustering is a routine step in large-scale analyses of gene expression data. We show that commonly used clustering methods produce results that substantially disagree and that do not match the biological expectations of co-expressed gene clusters. We present clust, a method that solves these problems by extracting clusters matching the biological expectations of co-expressed genes and outperforms widely used methods. Additionally, clust can simultaneously cluster multiple datasets, enabling users to leverage the large quantity of public expression data for novel comparative analysis.", 
Convolutional neural networks can be used to extract generic descriptors from images. This paper reports on experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. They report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks.,CNN Features off-the-shelf an Astounding Baseline for Recognition,"Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.", 
"Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. The severity of code smells is an important factor to take into consideration when reporting results, say the authors. The accuracy of the classification of severity is not high, but the ranking correlation of the actual and predicted severity for the best models reaches 0.88–0.96.",Code Smell Severity Classification using Machine Learning Techniques,"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.88–0.96, measured through Spearman’s ?.", 
"A static call graph is an imperative prerequisite used in most interprocedural analyses and software comprehension tools. Currently, there is a lack of software tools that can automatically analyze the Python source-code. Code2graph is a prototype tool that automates the tasks of analyzing Python source code and constructing static call graphs.",Code2graph Automatic Generation of Static Call Graphs for Python Source Code,"A static call graph is an imperative prerequisite used in most interprocedural analyses and software comprehension tools. However, there is a lack of software tools that can automatically analyze the Python source-code and construct its static call graph. In this paper, we introduce a prototype Python tool, named code2graph, which automates the tasks of (1) analyzing the Python source-code and extracting its structure, (2) constructing static call graphs from the source code, and (3) generating a similarity matrix of all possible execution paths in the system. Our goal is twofold: First, assist the developers in understanding the overall structure of the system. Second, provide a stepping stone for further research that can utilize the tool in software searching and similarity detection applications. For example, clustering the execution paths into a logical workflow of the system would be applied to automate specific software tasks. Code2graph has been successfully used to generate static call graphs and similarity matrices of the paths for three popular open-source Deep Learning projects (TensorFlow, Keras, PyTorch).", 
CodeShovel is a tool for uncovering method histories. It produces complete and accurate change histories for 90% methods. It outperforms leading tools from both research (e.g. FinerGit) and practice. A field study on industrial code bases with 16 industrial developers confirmed its correctness.,CodeShovel Constructing Method-Level Source Code Histories,"Source code histories are commonly used by developers and researchers to reason about how software evolves. Through a survey with 42 professional software developers, we learned that developers face significant mismatches between the output provided by developers’ existing tools for examining source code histories and what they need to successfully complete their historical analysis tasks. To address these shortcomings, we propose CodeShovel, a tool for uncovering method histories that quickly produces complete and accurate change histories for 90% methods (including 97% of all method changes) outperforming leading tools from both research (e.g, FinerGit) and practice (e.g., IntelliJ / git log). CodeShovel helps developers to navigate the entire history of source code methods so they can better understand how the method evolved. A field study on industrial code bases with 16 industrial developers confirmed our empirical findings of CodeShovel’s correctness, low runtime overheads, and additionally showed that the approach can be useful for a wide range of industrial development tasks.", 
The 5.9 GHz band is being actively explored for possible spectrum sharing opportunities between Dedicated Short Range Communications (DSRC) and IEEE 802.11ac networks. This is to address the increasing demand for bandwidth-intensive Wi-Fi applications. The authors propose a Real-time Channelization Algorithm (RCA) for Wi-fi Access Points operating in the shared spectrum.,Coexistence of Dedicated Short Range Communications (DSRC) and Wi-Fi Implications to Wi-Fi Performance,"The 5.9 GHz band is being actively explored for possible spectrum sharing opportunities between Dedicated Short Range Communications (DSRC) and IEEE 802.11ac networks in order to address the increasing demand for bandwidth-intensive Wi-Fi applications. In this paper, we study the implications of this spectrum sharing to the performance of Wi-Fi systems. Through experiments performed on our testbed, we first investigate band sharing options available for Wi-Fi devices. Using experimental results, we show the need for using conservative Wi-Fi transmission parameters to enable harmonious coexistence between DSRC and Wi-Fi. Moreover, we show that under the current 802.11ac standard, certain channelization options, particularly the high bandwidth ones, cannot be used by Wi-Fi devices without causing interference to the DSRC nodes. Under these constraints, we propose a Real-time Channelization Algorithm (RCA) for Wi-Fi Access Points (APs) operating in the shared spectrum. Evaluation of the proposed algorithm using a prototype implementation on commodity hardware as well as via simulations show that informed channelization decisions can significantly increase Wi-Fi throughput compared to static channelization schemes.", 
"There is growing interest from Wi-Fi proponents, cellular operators, and other stakeholders to use the spectrum in the 5GHz bands. The 5 GHz bands have emerged as the most coveted bands for launching new wireless applications and services. To boost cellular network capacity, wireless service providers are considering the deployment of unlicensed Long Term Evaluation (LTE).",Coexistence of Wireless Technologies in the 5 GHz Bands A Survey of Existing Solutions and a Roadmap for Future Research,"As the 2:4 GHz spectrum band has become significantly congested, there is growing interest from the Wi-Fi proponents, cellular operators, and other stakeholders to use the spectrum in the 5 GHz bands. The 5 GHz bands have emerged as the most coveted bands for launching new wireless applications and services, because of their relatively favorable propagation characteristics and the relative abundance of spectrum therein. To meet the exploding demand for more unlicensed spectrum, regulators across the world such as the United States (US) Federal Communications Commission (FCC) and the European Electronic Communications Committee (ECC) have recently started considerations for opening up additional spectrum in the 5 GHz bands for use by unlicensed devices. Moreover, to boost cellular network capacity, wireless service providers are considering the deployment of unlicensed Long Term Evaluation (LTE) in the 5 GHz bands. This and other emerging wireless technologies and applications have resulted in likely deployment scenarios where multiple licensed and unlicensed networks operate in overlapping spectrum. This paper provides a comprehensive overview of the various coexistence scenarios in the 5 GHz bands. In this paper, we discuss coexistence issues between a number of important wireless technologies—viz., LTE and Wi-Fi, radar and Wi-Fi, Dedicated Short Range Communication (DSRC) and Wi-Fi, and coexistence among various 802.11 protocols operating in the 5 GHz bands. Additionally, we identify and provide brief discussions on an impending coexistence issue – one between Cellular V2X and DSRC/Wi-Fi. We summarize relevant standardization initiatives, explain existing coexistence solutions, and discuss open research problems.", 
"Traditional power grids are currently being transformed into Smart Grids (SGs) Cognitive Radio Networks (CRNs) in particular are highly promising for providing timely SG wireless communications. Despite several challenges, such as trade-offs between wireless coverage and capacity, wireless communication is a promising SG communications technology.","Cognitive Radio for Smart Grids Survey of Architectures, Spectrum Sensing Mechanisms, and Networking Protocols","Traditional power grids are currently being transformed into Smart Grids (SGs). SGs feature multi-way communication among energy generation, transmission, distribution, and usage facilities. The reliable, efficient, and intelligent management of complex power systems requires integration of highspeed, reliable, and secure data information and communication technology into the SGs to monitor and regulate power generation and usage. Despite several challenges, such as trade-offs between wireless coverage and capacity as well as limited spectral resources in SGs, wireless communication is a promising SG communications technology. Cognitive Radio Networks (CRNs) in particular are highly promising for providing timely SG wireless communications by utilizing all available spectrum resources. We provide in this paper a comprehensive survey on the CRN communication paradigm in SGs, including the system architecture, communication network compositions, applications, and CR-based communication technologies. We highlight potential applications of CR-based SG systems. We survey CR-based spectrum sensing approaches with their major classifications. We also provide a survey on CR-based routing and MAC protocols, and describe interference mitigation schemes. We furthermore present open issues and research challenges faced by CR-based SG networks along with future directions.", 
"In citation-based summarization, text is leveraged to identify important aspects of a target paper. Previous work has focused on extraction aspect of summaries, but not on fluency or readability. In this work, we present an approach for producing readable and cohesive summaries.",Coherent Citation-Based Summarization of Scientific Papers,"In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency.", 
A novel approach for improvement in automatic text summarization has been proposed. The approach exploits the strengths of different techniques like metaheuristic approaches and collaborative ranking. The proposed approach has been done in Python 3.5 in Anaconda.,Collaborative Ranking-Based Text Summarization Using a Metaheuristic Approach,"In the present work, a novel approach for improvement in automatic text summarization has been proposed. We introduce a different model for summarization problem by exploiting the strengths of different techniques like metaheuristic approaches and collaborative ranking. First, the sentences of document are scored via two methods. Method one assigns the weight to each text feature using a new metaheuristic approach ‘Jaya’ and scores the sentences by linearly combining these feature scores with their optimal weights. Method two scores the sentences by simple averaging the scores of each text feature. Moreover, the sentences are then ranked according to these scores which generates two sets of ranks for the documents. To calculate the final ranking of sentences, the concept of collaborative ranking has been adopted. The implementation of the proposed approach has been done in Python 3.5 in Anaconda environment. The experiments are performed on the DUC 2002 dataset using the co-selection-based performance parameter. We show empirically that the proposed method is viable and effective for extractive text summarization.", 
"In cooperative systems, path prediction is a promising method for re?ecting a driver's intention and estimating the future position of vehicles. The emergence of wireless communication technologies such as Dedicated Short-Range Communication has promoted the evolution of collision warning systems.",COLLISION AVOIDANCE ON WINDING ROADS USING DEDICATED SHORT-RANGE COMMUNICATION,"The emergence of wireless communication technologies such as Dedicated Short-Range Communication (DSRC) has promoted the evolution of collision warning from simple ranging-sensor-based systems to cooperative systems. In cooperative systems, path prediction is a promising method for re?ecting a driver’s intention and estimating the future position of vehicles. In this study, a short-term trajectory-modelling method is proposed to predict vehicle motion behaviour in the cooperative vehicular environment. In addition, a collision detection algorithm for winding roads is presented based on a model for determining the minimum distance of vehicles’ future trajectories. The cooperative collision avoidance system’s performance is analysed through simulation, providing useful theoretical insights into the e?ects of DSRC technology on vehicle collision avoidance in a curved road environment.", 
"Text summarization has become an important research area, especially in the biomedical domain, where information overload is a major problem. We propose a novel biomedical text summarization system that combines two popular data mining techniques: clustering and frequent itemset mining. We evaluated the performances of our system by comparing the resulting summaries with the abstracts of these papers using the ROUGE metrics.",Combine clustering and frequent itemsets mining to enhance biomedical text summarization,"Text summarization has become an important research area, especially in the biomedical domain, where information overload is a major problem. In this paper, we propose a novel biomedical text summarization system that combines two popular data mining techniques: clustering and frequent itemset mining. Biomedical paper is expressed as a set of biomedical concepts using the UMLS metathesaurus. The K-means algorithm is used to cluster similar sentences. Then, the Apriori algorithm is applied to discover the frequent itemsets among the clustered sentences. Finally, the salient sentences from each cluster are selected to build the summary using the discovered frequent itemsets. For the evaluation step, we selected randomly 100 biomedical papers from the BioMed Central database full-text, and we evaluated the performances of our system by comparing the resulting summaries with the abstracts of these papers using the ROUGE metrics in term of recall, precision, and F-measure. We also compared the obtained summaries with those achieved by five well-known summarizers: TextRank, TextTeaser, SweSum, ItemSet Based Summarizer, Microsoft AutoSummarize, and two baselines: summarization using only the frequent itemsets mining (FRQ-CL), and summarization using only the clustering (CL-FRQ). The results demonstrate that this combination can successfully enhance the summarization performances, and the proposed system outperforms other tested summarizers.", 
"Summarization, like other natural language processing tasks, is tackled with a range of different techniques. We show how a preliminary knowledge base, composed of only 23 rules, already outperforms competitive baselines. We apply this approach to automatic summarization of legal case reports.",Combining Different Summarization Techniques for Legal Text,"Summarization, like other natural language processing tasks, is tackled with a range of different techniques - particularly machine learning approaches, where human intuition goes into attribute selection and the choice and tuning of the learning algorithm. Such techniques tend to apply differently in different contexts, so in this paper we describe a hybrid approach in which a number of different summarization techniques are combined in a rule-based system using manual knowledge acquisition, where human intuition, supported by data, specifies not only attributes and algorithms, but the contexts where these are best used. We apply this approach to automatic summarization of legal case reports. We show how a preliminary knowledge base, composed of only 23 rules, already outperforms competitive baselines.", 
"Unsupervised, extractive text summarization system that leverages a submodularity framework. The framework allows summaries to be generated in a greedy way while preserving near-optimal performance guarantees. Results indicate that our method is particularly well-suited to the meeting domain.",Combining graph degeneracy and submodularity for unsupervised extractive summarization,"We present a fully unsupervised, extractive text summarization system that leverages a submodularity framework introduced by past research. The framework allows summaries to be generated in a greedy way while preserving near-optimal performance guarantees. Our main contribution is the novel coverage reward term of the objective function optimized by the greedy algorithm. This component builds on the graph-of-words representation of text and the k-core decomposition algorithm to assign meaningful scores to words. We evaluate our approach on the AMI and ICSI meeting speech corpora, and on the DUC2001 news corpus. We reach state-of-the-art performance on all datasets. Results indicate that our method is particularly well-suited to the meeting domain.", 
In this paper we explore multiple features for extractive automatic summarization using machine learning. Four different classifiers and automatic feature selection are explored.,Combining Multiple Features for Automatic Text Summarization through Machine Learning,"In this paper we explore multiple features for extractive automatic summarization using machine learning. They account for SuPor-2 features, a supervised summarizer for Brazilian Portuguese, and graph-based features mirroring complex networks measures. Four different classifiers and automatic feature selection are explored. ROUGE is used for assessment of single-document summarization of news texts.", 
"In this paper we present a methodology to justify recommendations that relies on the information extracted from users' reviews discussing the available items. The approach is able to make the recommendation process more transparent, engaging and trustful for the users.",Combining text summarization and aspect-based sentiment analysis of users' reviews to justify recommendations,"In this paper we present a methodology to justify recommendations that relies on the information extracted from users’ reviews discussing the available items. The intuition behind the approach is to conceive the justification as a summary of the most relevant and distinguishing aspects of the item, automatically obtained by analyzing its reviews. To this end, we designed a pipeline of natural language processing techniques including aspect extraction, sentiment analysis and text summarization to gather the reviews, process the relevant excerpts, and generate a unique synthesis presenting the main characteristics of the item. Such a summary is finally presented to the target user as a justification of the received recommendation. In the experimental evaluation we carried out a user study in the movie domain (N=141) and the results showed that our approach is able to make the recommendation process more transparent, engaging and trustful for the users.", 
"Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. Various data mining techniques are available for predicting diseases. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease.",Comparative Analysis of Classification Function Techniques for Heart Disease Prediction,"The data mining can be referred as discovery of relationships in large databases automatically and in some cases it is used for predicting relationships based on the results discovered. Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. In the health care industry, the data mining is mainly used for Disease Prediction. Various data mining techniques are available for predicting diseases namely clustering, classification, association rules, regression and etc. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease from the heart disease data set. The classification function algorithms used and tested in this work are Logistics, Multi Layer Perception and Sequential Minimal Optimization algorithms. Comparative analysis is done by using Waikato Environment for Knowledge Analysis or in short, WEKA. It is open source software which consists of a collection of machine learning algorithms for data mining tasks. The performance factors used for analysing the efficiency of algorithms are clustering accuracy and error rate. The result shows that logistics classification function efficiency is better than multi layer perception and sequential minimal optimization.", 
Intelligent-based techniques have developed in literature for document summarization. Study of two population-based stochastic optimization techniques has been proposed. It specifies the relationship among sentences based on similarity.,Comparative Study of DE and PSO over Document Summarization,"With the exponential growth in the quantity and complexity of information sources, a number of computational intelligent-based techniques have developed in literature for document summarization. In this paper, a comparative study of two population-based stochastic optimization techniques has been proposed for document summarization. It specifies the relationship among sentences based on similarity and minimizes the weight of each sentence to extract summary sentences at different compression level. Comparison of both the optimization techniques based on fallout value of extracted sentences shows the good performance of PSO compared to DE on five different English corpus data.", 
Summarization has been an area of interest since many years. This paper presents a comparison of various text summarization methods seen in Indian languages. Sample text consisting of three sentences is taken as an example.,Comparative study of text summarization in Indian Languages,"Text Summarization has been an area of interest since many years .There are a lot of summarization methods in foreign languages like English, Chinese, Arabic, Korean, Persian etc. Recently some methods have been developed for Indian languages also. This paper presents a comparison of various text summarization methods seen in Indian languages. Summarization techniques in Tamil, Kannada, Odia, Bengali, Punjabi and Gujarathi are taken for the purpose of comparison. Sample text consisting of three sentences is taken as an example and we try to find out the summary sentences using all the eight methods.", 
Text summarization is one of application of natural language processing and is becoming more popular for information condensation. This paper gives comparative study of various text summarization methods based on different types of application.,Comparative Study of Text Summarization Methods,Text summarization is one of application of natural language processing and is becoming more popular for information condensation. Text summarization is a process of reducing the size of original document and producing a summary by retaining important information of original document. This paper gives comparative study of various text summarization methods based on different types of application. The paper discusses in detail two main categories of text summarization methods these are extractive and abstractive summarization methods. The paper also presents taxonomy of summarization systems and statistical and linguistic approaches for summarization., 
"Main area of research is extractive summarization, more specifically, contrastive opinion summarization. Most commonly used evaluation technique is ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Measures do not take into account the semantics of words and thus, for example, synonyms are not treated as equal.",Comparing Semantic Models for Evaluating Automatic Document Summarization,"The main focus of this paper is the examination of semantic modelling in the context of automatic document summarization and its evaluation. The main area of our research is extractive summarization, more specifically, contrastive opinion summarization. And as it is with all summarization tasks, the evaluation of their performance is a challenging problem on its own. Nowadays, the most commonly used evaluation technique is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). It includes measures (such as the count of overlapping n-grams or word sequences) for automatically determining the quality of summaries by comparing them to ideal human-made summaries. However, these measures do not take into account the semantics of words and thus, for example, synonyms are not treated as equal. We explore this issue by experimenting with various language models, examining their performance in the task of computing document similarity. In particular, we chose four semantic models (LSA, LDA, Word2Vec and Doc2Vec) and one frequency-based model (TfIdf), for extracting document features. The experiments were then performed on our custom dataset and the results of each model are then compared to the similarity values assessed by human annotators. We also compare these values with the ROUGE scores and observe the correlations between them. The aim of our experiments is to find a model, which can best imitate a human estimate of document similarity.", 
Choosing a proper color space is a very important issue for color image segmentation process. L*A*B* and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces .,Comparing the Performance of LAB and HSV Color Spaces with Respect to Color Image Segmentation,"Color image segmentation is a very emerging topic for image processing research. Since it has the ability to present the result in a way that is much more close to the human eyes perceive, so today’s more research is going on this area. Choosing a proper color space is a very important issue for color image segmentation process. Generally L*A*B* and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces with respect to color image segmentation. For measuring their performance, we consider the parameters: mse and psnr . It is found that HSV color space is performing better than L*A*B*.", 
This paper compares algorithms for extractive summarization of microblog posts. We evaluate the generated summaries by comparing them to both manually produced summaries and summaries produced by several leading traditional summarization systems.,Comparing Twitter Summarization Algorithms for Multiple Post Summaries,"Due to the sheer volume of text generated by a microblog site like Twitter, it is often difficult to fully understand what is being said about various topics. In an attempt to understand microblogs better, this paper compares algorithms for extractive summarization of microblog posts. We present two algorithms that produce summaries by selecting several posts from a given set. We evaluate the generated summaries by comparing them to both manually produced summaries and summaries produced by several leading traditional summarization systems. In order to shed light on the special nature of Twitter posts, we include extensive analysis of our results, some of which are unexpected.", 
Locating captured cells in the device by manual counting bottlenecks data processing. Some recent work has been done to automate the cell location and classification process. The trained ML algorithms locate and classify thousands of possible cells in a few minutes rather than a few hours. Optimal algorithm selection depends on the peculiarities of the individual dataset.,Comparison and Optimization of Machine Learning Methods for Automated Classification of Circulating Tumor Cells,"Advances in rare cell capture technology have made possible the interrogation of circulating tumor cells (CTCs) captured from whole patient blood. However, locating captured cells in the device by manual counting bottlenecks data processing by being tedious (hours per sample) and compromises the results by being inconsistent and prone to user bias. Some recent work has been done to automate the cell location and classification process to address these problems, employing image processing and machine learning (ML) algorithms to locate and classify cells in fluorescent microscope images. However, the type of machine learning method used is a part of the design space that has not been thoroughly explored. Thus, we have trained four ML algorithms on three different datasets. The trained ML algorithms locate and classify thousands of possible cells in a few minutes rather than a few hours, representing an order of magnitude increase in processing speed. Furthermore, some algorithms have a significantly (P < 0.05) higher area under the receiver operating characteristic curve than do other algorithms. Additionally, significant (P < 0.05) losses to performance occur when training on cell lines and testing on CTCs (and vice versa), indicating the need to train on a system that is representative of future unlabeled data. Optimal algorithm selection depends on the peculiarities of the individual dataset, indicating the need of a careful comparison and optimization of algorithms for individual image classification tasks.", 
"Recently, instantaneous wave-free ratio (iFR) or diastolic pressure-ratio (dPR) have been used in practice. Study sought to compare the current to a new ECG-independent algorithm for calculating resting physiologic indices. Both algorithms provided nearly identical values of iFR or dPR without systemic bias.",Comparison of Current and Novel ECG-Independent Algorithms for Resting Pressure Derived Physiologic Indices,"Recently, instantaneous wave-free ratio (iFR) or diastolic pressure-ratio (dPR) have been used in practice. For these indices, the reliability of electrocardiography (ECG)-independent algorithm for pressure-only data is essential. The current study sought to compare the current to a new ECG-independent algorithm for calculating resting physiologic indices. The main purpose of developing a new ECG-independent algorithm was to raise the detection rates over the entire heart cycle despite irregular heartbeats. Both iFR and dPR were calculated from resting pressure tracings using current and new algorithms by a core laboratory in 975 vessels (393 patients). The diagnostic performance of resting physiologic indices with a new algorithm to predict fractional flow reserve (FFR) was compared with the current algorithm. Both algorithms provided nearly identical values of iFR or dPR without systemic bias. iFR and dPR, which were calculated using current and new ECG-independent algorithms, provided comparable discrimination ability and diagnostic performance to predict functionally significant stenosis defined by FFR?0.80. However, detection rates of the new algorithm were significantly higher than current algorithm in the patients with irregular heartbeats (for per patient [59.5% vs. 83.8%] and per unit-heartbeats analysis [84.3% vs. 90.3%]), such as arterial fibrillation or multiple premature ventricular contractions.", 
"Source code classification (SCC) is a task to assign codes into different categories according to a criterion such as according to their functionalities, programming languages or vulnerabilities. Many source code archives are organized according to the programming languages. This study proposes new convolutional neural network (CNN) architectures to build source code classifiers.",Comparison of Image?Based and Text?Based Source Code Classifcation Using Deep Learning,"Source code classification (SCC) is a task to assign codes into different categories according to a criterion such as according to their functionalities, programming languages or vulnerabilities. Many source code archives are organized according to the programming languages, and thereby, the desired code fragments can be easily accessed by searching within the archive. However, manually organizing source code archives by field experts is labor intensive and impractical because of the fast-growing available source codes. Therefore, this study proposes new convolutional neural network (CNN) architectures to build source code classifiers that automatically identify programming languages from source codes. This is the first study in which the performances of deep learning algorithms on programming language identification are compared on both image and text files. In this study, the experiments are performed on three source code datasets to identify eight programming languages, including C, C++, C# , Go, Python, Ruby, Rust, and Java. The comparative results indicate that although text-based SCC and image-based SCC approaches achieve very high (> 93.5%) and similar accuracies, text-based classification has significantly better performance in terms of execution time.", 
The operation of brain–computer interfaces (BCIs) based on spontaneous electroencephalogram (EEG) signals requires accurate classification of multichannel EEG. The high-dimensional and noisy nature of EEG may limit the advantage of nonlinear classification methods over linear ones.,"Comparison of Linear, Nonlinear, and Feature Selection Methods for EEG Signal Classification","The reliable operation of brain–computer interfaces (BCIs) based on spontaneous electroencephalogram (EEG) signals requires accurate classification of multichannel EEG. The design of EEG representations and classifiers for BCI are open research questions whose difficulty stems from the need to extract complex spatial and temporal patterns from noisy multidimensional time series obtained from EEG measurements. The high-dimensional and noisy nature of EEG may limit the advantage of nonlinear classification methods over linear ones. This paper reports the results of a linear (linear discriminant analysis) and two nonlinear classifiers (neural networks and support vector machines) applied to the classification of spontaneous EEG during five mental tasks, showing that nonlinear classifiers produce only slightly better classification results. An approach to feature selection based on genetic algorithms is also presented with preliminary results of application to EEG during finger movement.", 
"Coronavirus disease (COVID-19) is spreading across the world. There are no accurate resources to predict and find the disease. By knowing the past patients' records, it could guide the clinicians to fight against the pandemic. Machine learning techniques can be implemented to predict healthiness from symptoms.",Comparison of Machine Learning algorithm for COVID-19 Death Risk Prediction,"Coronavirus disease (COVID-19) is spreading across the world. Since at first it has appeared in Wuhan, China in December 2019, it has become a serious issue across the globe. There are no accurate resources to predict and find the disease. So, by knowing the past patients’ records, it could guide the clinicians to fight against the pandemic. Therefore, for the prediction of healthiness from symptoms Machine learning techniques can be implemented. From this we are going to analyse only the symptoms which occurs in every patient. These predictions can help clinicians in the easier manner to cure the patients. Already for prediction of many of the diseases, techniques like SVM (Support vector Machine), Fuzzy k-Means Clustering, Decision Tree algorithm, Random Forest Method, ANN (Artificial Neural Network), KNN (k-Nearest Neighbour), Naïve Bayes, Linear Regression model are used. As we haven’t faced this disease before, we can’t say which technique will give the maximum accuracy. So, we are going to provide an efficient result by comparing all the such algorithms in RStudio.", 
Cardiotocography provides vital information on fetal status during antepartum and intrapartum periods. The role of machine learning approaches in diagnosing diseases is becoming increasingly essential and intertwined. The study results show that the artificial neural network was superior to other algorithms.,Comparison of Machine Learning Techniques for Fetal Heart Rate Classification,"Cardiotocography is a monitoring technique providing important and vital information on fetal status during antepartum and intrapartum periods. The advances in modern obstetric practice allowed many robust and reliable machine learning techniques to be utilized in classifying fetal heart rate signals. The role of machine learning approaches in diagnosing diseases is becoming increasingly essential and intertwined. The main aim of the present study is to determine the most efficient machine learning technique to classify fetal heart rate signals. Therefore, the research has been focused on the widely used and practical machine learning techniques, such as artificial neural network, support vector machine, extreme learning machine, radial basis function network, and random forest. In a comparative way, fetal heart rate signals were classified as normal or hypoxic using the aforementioned machine learning techniques. The performance metrics derived from confusion matrix were used to measure classifiers’ success. According to experimental results, although all machine learning techniques produced satisfactory results, artificial neural network yielded the rather well results with the sensitivity of 99.73% and specificity of 97.94%. The study results show that the artificial neural network was superior to other algorithms.", 
"Many systems, such as autonomous vehicles, rely on components using machine learning for recognizing objects. This paper compares different visual datasets and frameworks for machine learning. It investigates object detection labels with respect to size, location, and contextual information.",Comparison of Visual Datasets for Machine Learning,"One of the greatest technological improvements in recent years is the rapid progress using machine learning for processing visual data. Among all factors that contribute to this development, datasets with labels play crucial roles. Several datasets are widely reused for investigating and analyzing different solutions in machine learning. Many systems, such as autonomous vehicles, rely on components using machine learning for recognizing objects. This paper compares different visual datasets and frameworks for machine learning. The comparison is both qualitative and quantitative and investigates object detection labels with respect to size, location, and contextual information. This paper also presents a new approach creating datasets using real-time, geo-tagged visual data, greatly improving the contextual information of the data. The data could be automatically labeled by cross-referencing information from other sources (such as weather).", 
"This paper presents compendium, a text summarization system, which has achieved good results in extractive summarization. The main goal in this research is to extend it, suggesting a new approach for generating abstractive-oriented summaries of research papers.","COMPENDIUM, A Text Summarization System for Generating Abstracts of Research Papers","This paper presents compendium, a text summarization system, which has achieved good results in extractive summarization. Therefore, our main goal in this research is to extend it, suggesting a new approach for generating abstractive-oriented summaries of research papers. We conduct a preliminary analysis where we compare the extractive version of compendium (compendiumE) with the new abstractive-oriented approach (compendiumE?A). The final summaries are evaluated according to three criteria (content, topic, and user satisfaction) and, from the results obtained, we can conclude that the use of compendium is appropriate for producing summaries of research papers automatically, going beyond the simple selection of sentences.", 
VANETs are scale-free ones characterized by strong connectivity and survivability. Some works find that they are not; this study analyzes VANET topologies in more realistic settings. We find that the evolved VANet exhibits a scale-invariant feature under certain conditions. We also find that such phenomenon has no relation to communication range if the range is large.,Complex Network Analysis of VANET Topology With Realistic Vehicular Traces,"Recent years have witnessed growing interests in vehicular ad hoc network (VANET) topology. Existing works assume that the networks generated at each time are independent. In these works, the VANET topologies are formed based on simple traffic and communication models such as unit disk graph model. Interestingly, some works find that VANETs are scale-free ones characterized by strong connectivity and survivability, while some argue that they are not. This study analyzes VANETs in more realistic settings, trying to find the answer to the paradox. Specifically, in this paper we propose a Dynamically Evolving Networking (DEN) model and take as input realistic vehicular traces to make the research and results more practical. We consider the effect of node addition, node deletion and link loss due to node mobility and keep the network evolving by including preferential attachment and link compensation mechanisms. We find that the evolved VANET exhibits a scale-invariant feature under certain conditions. We also find that the emergence of such phenomenon has no relation to communication range if the range is large. Furthermore, we apply complex network theory to capture the dynamics of the VANET. Theoretically for the first time we show that VANET would evolve into the scale-free topology when the probability of node addition is relatively large or the probability of link compensation is properly set, the presence of which would help establish a strongly connected VANET topology.", 
"Electroencephalogram (EEG) signals acquired from brain can provide an effective representation of the human's physiological and pathological states. Much work has been conducted to study and analyze the EEG signals, aiming at spying the current states or the evolution characteristics of the complex brain system.",Complex networks and deep learning for EEG signal analysis,"Electroencephalogram (EEG) signals acquired from brain can provide an effective representation of the human’s physiological and pathological states. Up to now, much work has been conducted to study and analyze the EEG signals, aiming at spying the current states or the evolution characteristics of the complex brain system. Considering the complex interactions between different structural and functional brain regions, brain network has received a lot of attention and has made great progress in brain mechanism research. In addition, characterized by autonomous, multi-layer and diversified feature extraction, deep learning has provided an effective and feasible solution for solving complex classification problems in many fields, including brain state research. Both of them show strong ability in EEG signal analysis, but the combination of these two theories to solve the difficult classification problems based on EEG signals is still in its infancy. We here review the application of these two theories in EEG signal research, mainly involving brain–computer interface, neurological disorders and cognitive analysis. Furthermore, we also develop a framework combining recurrence plots and convolutional neural network to achieve fatigue driving recognition. The results demonstrate that complex networks and deep learning can effectively implement functional complementarity for better feature extraction and classification, especially in EEG signal analysis.", 
"Sleep stage scoring is a challenging task. Most of existing sleep stage classification approaches rely on analysing electroencephalography (EEG) signals in time or frequency domain. A novel technique for EEG sleep stages classification is proposed in this paper. The proposed method attains better classification results and a reasonable execution time compared with the SVM, k-means and other existing methods.",Complex Networks Approach for EEG Signal Sleep Stages Classification,"Sleep stage scoring is a challenging task. Most of existing sleep stage classification approaches rely on analysing electroencephalography (EEG) signals in time or frequency domain. A novel technique for EEG sleep stages classification is proposed in this paper. The statistical features and the similarities of complex networks are used to classify single channel EEG signals into six sleep stages. Firstly, each EEG segment of 30 second is divided into 75 sub-segments, and then different statistical features are extracted from each sub-segment. In this paper, feature extraction is important to reduce dimensionality of EEG data and the processing time in classification stage. Secondly, each vector of the extracted features, which represents one EEG segment, is transferred into a complex network. Thirdly, the similarity properties of the complex networks are extracted and classified into one of the six sleep stages using a k-means classifier. For further investigation, in the statistical features extraction phase two statistical features sets are tested and ranked based on the performance of the complex networks. To investigate the classification ability of complex networks combined with k-means, the extracted statistical features were also forwarded to a k-means and a support vector machine (SVM) for comparison. We also compare the proposed method with other existing methods in the literature. The experimental results show that the proposed method attains better classification results and a reasonable execution time compared with the SVM, k-means and the other existing methods. The research results in this paper indicate that the proposed method can assist neurologists and sleep specialists in diagnosing and monitoring sleep disorders.", 
"VANETs are formed by intelligent vehicles equipped with On Board Units and wireless communication devices. This study provides a summary about the VANET, characteristics and security challenges. It also gives a summary of some major security attacks on security services.",Comprehensive survey on security services in vehicular ad-hoc networks,"Vehicular ad-hoc networks (VANETs) are the most hopeful approach to provide safety information and other infotainment applications to both drivers and passengers. VANETs are formed by intelligent vehicles equipped with On Board Units and wireless communication devices. Hence, VANETs become a key component of the intelligent transport system. Even though VANETs are used in enormous number of applications, there are many security challenges and issues that need to be overcome to make VANETs usable in practice. A great deal of study has been done towards it, but security mechanisms in VANETs are not effective. This study provides a summary about the VANET, characteristics and security challenges. This study also provides a summary of some major security attacks on security services such as availability, confidentiality, authentication, integrity and non-repudiation and the corresponding countermeasures to make VANET communications more secure.", 
This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. Clusters of similar sentences are compressed using a multi-sentence compression (MSC) method and single sentences are compression using a Neural Network model.,Compressive approaches for cross-language multi-document summarization,"The popularization of social networks and digital documents has quickly increased the multilingual information available on the Internet. However, this huge amount of data cannot be analyzed manually. This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. We describe three compressive CLTS approaches that analyze the text in the source and target languages to compute the relevance of sentences. Our systems compress sentences at two levels: clusters of similar sentences are compressed using a multi-sentence compression (MSC) method and single sentences are compressed using a Neural Network model. The version of our approach using multi-sentence compression generated more informative French-to-English cross-lingual summaries than extractive state-of-the-art systems. Moreover, these cross-lingual summaries have a grammatical quality similar to extractive approaches.", 
"In the near future, vehicular Ad hoc networks (VANETs) will be making use of long-distance communication techniques, such as cellular networks and Wi-Fi. We propose a computationally efficient anonymous mutual authentication scheme to validate the message source and verify the integrity of messages. We also introduce an efficient anonymous batch authentication protocol to be used in IoT for Road Side Units (RSUs) to authenticate multiple vehicles simultaneously rather than one after the other.",Computationally effcient privacy preserving anonymous mutual and batch authentication schemes for vehicular ad hoc networks,"In the near future, it is envisioned that vehicular Ad hoc networks (VANETs) will be making use of long-distance communication techniques, such as cellular networks and Worldwide Interoperability for Microwave Access (WiMAX), to get instant Internet access for making the communication between vehicles and fixed road side infrastructure. Moreover, VANETs will also make use of short-distance communication methods, such as Dedicated Short-Range Communications (DSRC) and Wireless Fidelity (Wi-Fi) to perform short range communication between vehicles in an ad hoc manner. This Internet connection can provide facility to other vehicles to send traffic related messages, collisions, infotainment messages other useful safety alerts. In such a scenario, providing authentication between vehicle to infrastructure and vehicle to vehicle is a challenging task. In order to provide this facility, in this paper, we propose a computationally efficient privacy preserving anonymous authentication scheme based on the use of anonymous certificates and signatures for VANETs in making them an important component of Internet of Things (IoT) and the development of smart cities. Even though there are several existing schemes available to provide such anonymous authentication based on anonymous certificates and signatures in VANETs, the existing schemes suffer from high computational cost in the certificate revocation list (CRL) checking process and in the certificate and the signature verification process. Therefore, it is not possible to verify a large number of messages in a particular period in VANETs which would lead to increased message loss. Hence, we use a computationally efficient anonymous mutual authentication scheme to validate the message source as well as to verify the integrity of messages along with a conditional tracking2 mechanism to trace the real identity of misbehaving vehicles and revoke them from VANET in the case of dispute. In this paper, we also introduce an efficient anonymous batch authentication protocol to be used in IoT for Road Side Units (RSUs) to authenticate multiple vehicles simultaneously rather than one after the other such that the total authentication time can be dramatically reduced. This proposed scheme is implemented and the performance analysis shows that our scheme is more efficient in terms of certificate and signature verification cost, while preserving conditional privacy in VANETs.", 
The use of deep learning algorithms for automatic detection of ADHD is still limited. This paper proposes a novel computer aided diagnosis system based on deep learning approach. It uses a deep convolutional neural network that is capable of extracting both spatial and frequency band features from the raw electroencephalograph (EEG) signal.,Computer aided diagnosis system using deep convolutional neural networks for ADHD subtypes,"Attention deficit hyperactivity disorder (ADHD) is a ubiquitous neurodevelopmental disorder affecting many children. Therefore, automated diagnosis of ADHD can be of tremendous value. Unfortunately, unlike many other applications, the use of deep learning algorithms for automatic detection of ADHD is still limited. Method: In this paper, we proposed a novel computer aided diagnosis system based on deep learning approach to classify the EEG signal of Healthy children (Control) from ADHD children with two subtypes of Combined ADHD (ADHD-C) and Inattentive ADHD (ADHD-I). Inspired by the classical approaches, we proposed a deep convolutional neural network that is capable of extracting both spatial and frequency band features from the raw electroencephalograph (EEG) signal and then performing the classification. Result: We achieved the highest classification accuracy with the combination of ?1, ?2, and ? bands. Accuracy Recall, Precision, and Kappa values were %99.46, %99.45, %99.48, and 0.99, respectively. After investigating the spatial channels, we observed that electrodes in the Posterior side had the most contribution. Conclusions: To the best of our knowledge, all previous multiclass studies were based on fMRI and MRI imaging. Therefore, the presented research is novel in terms of using a deep neural network architecture and EEG signal for multiclass classification of ADHD and healthy children with high accuracy.", 
Computerized features corresponding to ultrasound BI-RADs categories were designed and tested using a database of 283 pathology-proven benign and malignant lesions. Lesion margin and orientation were optimum features common to all of the different machine learning methods. These features can be used in CAD systems to help distinguish benign from worrisome lesions.,COMPUTER-AIDED DIAGNOSIS FOR BREAST ULTRASOUND USING COMPUTERIZED BI-RADS FEATURES AND MACHINE LEARNING METHODS,"This work identifies effective computable features from the Breast Imaging Reporting and Data System (BI-RADS), to develop a computer-aided diagnosis (CAD) system for breast ultrasound. Computerized features corresponding to ultrasound BI-RADs categories were designed and tested using a database of 283 pathology-proven benign and malignant lesions. Features were selected based on classification performance using a ‘‘bottom-up’’ approach for different machine learning methods, including decision tree, artificial neural network, random forest and support vector machine. Using 10-fold cross-validation on the database of 283 cases, the highest area under the receiver operating characteristic (ROC) curve (AUC) was 0.84 from a support vector machine with 77.7% overall accuracy; the highest overall accuracy, 78.5%, was from a random forest with the AUC 0.83. Lesion margin and orientation were optimum features common to all of the different machine learning methods. These features can be used in CAD systems to help distinguish benign from worrisome lesions.", 
Image fusion is studied for detecting weapons or other objects hidden underneath a person's clothing. The focus of this paper is to develop a new algorithm to fuse a color visual image and a corresponding IR image.,Concealed Weapon Detection Using Color Image Fusion,"Image fusion is studied for detecting weapons or other objects hidden underneath a person’s clothing. The focus of this paper is to develop a new algorithm to fuse a color visual image and a corresponding IR image for such a concealed weapon detection application. The fused image obtained by the proposed algorithm will maintain the high resolution of the visual image, incorporate any concealed weapons detected by the IR sensor, and keep the natural color of the visual image. The feasibility of the proposed fusion technique is demonstrated by some experimental results.", 
"Text summarization is a data reduction process. It is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. Authors propose a novel frequency distribution model and algorithm for identifying important sentences.",Concept frequency distribution in biomedical text summarization,"Text summarization is a data reduction process. The use of text summarization enables users to reduce the amount of text that must be read while still assimilating the core information. The data reduction offered by text summarization is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. Such efforts are often hampered by the high-volume of publications. Our contribution is two-fold: 1) to propose the frequency of domain concepts as a method to identify important sentences within a full-text; and 2) propose a novel frequency distribution model and algorithm for identifying important sentences based on term or concept frequency distribution. An evaluation of several existing summarization systems using biomedical texts is presented in order to determine a performance baseline. For domain concept comparison, a recent high-performing frequency-based algorithm using terms is adapted to use concepts and evaluated using both terms and concepts. It is shown that the use of concepts performs closely with the use of terms for sentence selection. Our proposed frequency distribution model and algorithm outperforms a state-of-the-art approach.", 
"Using text summarization tools, clinicians and researchers in the biomedical domain can save their time and effort. Various summarization methods have been developed so far using different approaches. We propose a method that uses concept-level analysis of text in combination with itemset mining to identify the main subtopics of input text. We perform extensive experiments to evaluate the performance of these two methods for single-document and multi-document summarization.",Concept-based single- and multi-document biomedical text summarization,"In recent decades with the rapid growth in the volume of available textual information resources, automatic text summarization has become a useful tool to acquire and manage intended information. Using text summarization tools, clinicians and researchers in the biomedical domain can save their time and effort to manage numerous textual information resources. Various summarization methods have been developed so far using different approaches, and we divide the available challenges into two parts. The first part of challenges is related to summarizers which utilize term-based methods and generic criteria to measure the informativeness of sentences. Regarding the characteristics of biomedical text, it seems that there is a requirement to employ more efficient measures by biomedical summarizers. To address this issue, we propose a method that uses concept-level analysis of text in combination with itemset mining to identify the main subtopics of input text. In this method, the informativeness of each sentence is measured according to its meaning and the occurrence of main subtopics in the sentence. The results reported by the evaluation of this method show that using concept extraction and itemset mining, we can quantify the informativeness of sentences more efficiently, leading to an increase in the performance of biomedical summarization. The second part of challenges concerns biomedical summarizers which use the frequency of concepts extracted from input text to select related sentences. To address challenges related to such methods, we propose another summarization method that utilizes concept-level analysis and a probabilistic classification method. This summarizer estimates the probability of consistency of sentences with the distribution of concepts within the input document by following the distribution of important concepts. To identify important concepts in this summarization method, we introduce and evaluate different feature selection approaches. The results reported by the evaluation of this method show that using an appropriate feature selection method and following the distribution of important concepts, we can improve the performance of biomedical summarization. We perform extensive experiments to evaluate the performance of these two methods for single-document and multi-document summarization. The results of evaluations show that compared to the competitor methods, the two summarizers proposed in this thesis improve the performance of biomedical text summarization.", 
"There is a growing need for tools to help users find, filter and manage online resources. One of the possible solutions is abstractive text summarization. The idea is to propose a system that will accept single document.",Conceptual framework for abstractive text summarization,"As the volume of information available on the Internet increases, there is a growing need for tools helping users to find, filter and manage these resources. While more and more textual information is available online, effective retrieval is difficult without proper indexing and summarization of the content. One of the possible solutions to this problem is abstractive text summarization. The idea is to propose a system that will accept single document as input in English and processes the input by building a rich semantic graph and then reducing this graph for generating the final summary.", 
"An enormous amount of on-line text grows on the World-Wide Web. The development of methods for automatically summarizing this text becomes more important. We propose an Evolving connectionist System that is adaptive, incremental learning.",Connectionist approach to generic text summarization,"As the enormous amount of on-line text grows on the World-Wide Web, the development of methods for automatically summarizing this text becomes more important. The primary goal of this research is to create an efficient tool that is able to summarize large documents automatically. We propose an Evolving connectionist System that is adaptive, incremental learning and knowledge representation system that evolves its structure and functionality. In this paper, we propose a novel approach for Part of Speech disambiguation using a recurrent neural network, a paradigm capable of dealing with sequential data. We observed that connectionist approach to text summarization has a natural way of learning grammatical structures through experience. Experimental results show that our approach achieves acceptable performance.", 
Connectivity probability analysis for vehicular ad hoc network (VANET) freeway traffic is of practical significance. It can provide theoretical guidance to reduce the occurrence of traffic accidents. The cell transmission model (CTM) can describe highly dynamic characteristics of VANET freeway traffic.,Connectivity Probability Analysis for VANET Freeway Traffic Using a Cell Transmission Model,"The connectivity probability analysis for vehicular ad hoc network (VANET) freeway traffic is of practical significance and can provide theoretical guidance to reduce the occurrence of traffic accidents and also enhance the traffic efficiency. The cell transmission model (CTM) can describe highly dynamic characteristics of VANET freeway traffic and has been recognized as a traffic flow model. However, few previous studies have used such a model to investigate the connectivity probability for VANET freeway traffic. In this article, a new approach for cooperative communication is proposed for a bidirectional two-lane freeway that utilizes a CTM to dynamically describe the variety of vehicle flows with ON-ramp/OFF-ramp and effectively repair broken chains. The mathematical formulas of the connectivity probability for VANET freeway traffic are derived. Some parameters of traffic flow, such as the vehicle flow, path loss exponent, the communication radius, the message size, and the data rate, are dynamically set and studied. The numerical simulation results are consistent with the actual situation that confirms the accuracy of our theoretical results.", 
"In this paper, we describe the purpose and the way in the construction of a text summarization corpus. The constructed corpus contains six query sentences, 24 manually-constructed summaries, and 24 collections of source Web documents. We also investigated how the descriptions of interpretation, which help a user judge the credibility of other descriptions, appear in the corpus.",Construction of Text Summarization Corpus for the Credibility of Information on the Web,"Recently, the credibility of information on the Web has become an important issue. In addition to telling about content of source documents, indicating how to interpret the content, especially showing interpretation of the relation between statements appeared to contradict each other, is important for helping a user judge the credibility of information. In this paper, we will describe the purpose and the way in the construction of a text summarization corpus. Our purpose in the construction of the corpus includes the following three points; to collect Web documents relevant to several query sentences, to prepare gold standard data to evaluate smaller sub-processes in the extraction process and the summary generation process, to investigate the summaries made by human summarizers. The constructed corpus contains six query sentences, 24 manually-constructed summaries, and 24 collections of source Web documents. We also investigated how the descriptions of interpretation, which help a user judge the credibility of other descriptions in the summary, appear in the corpus. As a result, we confirmed that showing interpretation on conflicts is important for helping a user judge the credibility of information.", 
"We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models.",Content Selection in Deep Learning Models of Summarization,"We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the summarization task.", 
In this paper we introduce the concept of OpenNLP tool for natural language processing of text for word matching. Data mining document clustering algorithm is adopted to extract meaningful and query dependent information from offline documents.,Context sensitive text summarization using K means clustering algorithm,"The field of Information retrieval plays an important role in searching on the Internet. Most of the information retrieval systems are limited to the query processing based on keywords. In the information retrieval system matching of words with huge data is core task. Retrieval of the relevant natural language text document is of more challenging. In this paper we introduce the concept of OpenNLP tool for natural language processing of text for word matching. And in order to extract meaningful and query dependent information from large set of offline documents, data mining document clustering algorithm are adopted. Furthermore performance of the summary using OpenNLP tool and clustering techniques will be analysed and the optimal approach will be suggested.", 
"Entities are the major proportion and build up the topic of text summaries. We start with a selected entity, generate the left part first, then the right part of a complete summary. Our model outperforms the state-of-the-art methods in both automatic evaluation scores and informativeness metrics.",Controllable Abstractive Sentence Summarization with Guiding Entities,"Entities are the major proportion and build up the topic of text summaries. Although existing text summarization models can produce promising results of automatic metrics, for example, ROUGE, it is difficult to guarantee that an entity is contained in generated summaries. In this paper, we propose a controllable abstractive sentence summarization model which generates summaries with guiding entities. Instead of generating summaries from left to right, we start with a selected entity, generate the left part first, then the right part of a complete summary. Compared to previous entity-based text summarization models, our method can ensure that entities appear in final output summaries rather than generating the complete sentence with implicit entity and article representations. Our model can also generate more novel entities with them incorporated into outputs directly. To evaluate the informativeness of the proposed model, we develop a fine-grained informativeness metrics in the relevance, extraness and omission perspectives. We conduct experiments in two widely-used sentence summarization datasets and experimental results show that our model outperforms the state-of-the-art methods in both automatic evaluation scores and informativeness metrics.", 
"Current models disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. With user input, our system can produce high quality summaries that follow user preferences. ",Controllable Abstractive Summarization,"Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically – on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation).", 
"Current models disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. With user input, our system can produce high quality summaries that follow user preferences. ",ConvAI at SemEval-2019 Task 6 Offensive Language Identification and Categorization with Perspective and BERT,"This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media. Perspective is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet. BERT is a recently popular language representation model, fine tuned per task and achieving state of the art performance in multiple NLP tasks. Perspective performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type. Both baselines were ranked surprisingly high in the SEMEVAL-2019 OFFENSEVAL competition, Perspective in detecting an offensive post (12th) and BERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (Perspective) and the categorization (BERT) of offensive language with little or no additional training data.", 
"Conversational user interfaces for online shops via messaging services and voice assistants influence customers' satisfaction. A Segmented Kano perspective is used to derive use case groups and related customer segments. Two representative surveys with 2,165 customers of a major German online fashion retailer were conducted and analyzed.",Conversational User Interfaces for Online Shops A Categorization of Use Cases,"How do conversational user interfaces for online shops via messaging services and voice assistants influence customers’ satisfaction? Which use cases are attractive from a customer’s view point? Which use cases are must-be and for which customer segments? The answer to these questions is looked for in this paper. A Segmented Kano perspective is used to derive use case groups and related customer segments simultaneously. The paper starts with an overview on conversational commerce and on chatbots for this purpose. Then, the research method and the use case development is described. Two representative surveys with 2,165 customers of a major German online fashion retailer evaluating 13 messaging service and 2,025 customers evaluating 13 voice assistant use cases were conducted and analyzed. The focus was on the intention to use conversational user interfaces for online shops and the influence on customer satisfaction.", 
"We trained a large, deep convolution neural network to classify the 1.2 million high-resolution images in the Image Net LSVRC-2010 contest. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art.",Convolution Neural Networks by Image Net,"We trained a large, deep convolution neural network to classify the 1.2 million high-resolution images in the Image Net LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolution layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way soft ax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ?dropout? that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry", 
Deep learning has not been extensively studied for electroencephalographic (EEG) data. We applied convolutional deep belief networks to the feature learning of EEG data. Compared with other state-of-the-art feature extraction methods.,Convolutional Deep Belief Networks for Feature Extraction of EEG Signal,"In recent years, deep learning approaches have been successfully used to learn hierarchical representations of image data, audio data etc. However, to our knowledge, these deep learning approaches have not been extensively studied for electroencephalographic (EEG) data. Considering the properties of EEG data, high-dimensional and multichannel, we applied convolutional deep belief networks to the feature learning of EEG data and evaluated it on the datasets from previous BCI competitions. Compared with other state-of-the-art feature extraction methods, the learned features using convolutional deep belief network have better performance.", 
"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. We present the convolutional deep belief network which scales to realistic image sizes. We demonstrate excellent performance on several visual recognition tasks.",Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations,"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.", 
"The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs) In this paper, we propose to exploit shape information via masking convolutional features. We further propose a joint method to handle objects and ""stuff"" (e.g., grass, sky, water) in the same framework.",Convolutional Feature Masking for Joint Object and Stuff Segmentation,"The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs). The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and “stuff” (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed.", 
Convolutional Networks (ConvNets) are a biologicallyinspired trainable architecture that can learn categorical features. They have been successfully deployed in many commercial applications from OCR to video surveillance. We describe new unsupervised learning algorithms that allow ConvNets to be trained with very few labeled samples.,Convolutional Networks and Applications in Vision,"Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ”features”), which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologicallyinspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some non-linearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.", 
"Programming language processing (similar to natural language processing) is a hot research topic. Traditional NLP models may be inappropriate for programs, say authors. They propose a novel tree-based convolutional neural network (TBCNN) for programming language processing.",Convolutional Neural Networks over Tree Structures for Programming Language Processing,"Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs’ abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.", 
"The semantic gap between source code and natural language poses a major challenge for automatic code generation. In this paper, we propose a novel Contextualized code representation learning strategy for commit message Generation. CoreGen learns contextualized code representations which exploit the contextual information behind code commit sequences.",CoreGen Contextualized Code Representation Learning for Commit Message Generation,"Automatic generation of high-quality commit messages for code commits can substantially facilitate software developers’ works and coordination. However, the semantic gap between source code and natural language poses a major challenge for the task. Several studies have been proposed to alleviate the challenge but none explicitly involves code contextual information during commit message generation. Specifically, existing research adopts static embedding for code tokens, which maps a token to the same vector regardless of its context. In this paper, we propose a novel Contextualized code representation learning strategy for commit message Generation (CoreGen). CoreGen first learns contextualized code representations which exploit the contextual information behind code commit sequences. The learned representations of code commits built upon Transformer are then fine-tuned for downstream commit message generation. Experiments on the benchmark dataset demonstrate the superior effectiveness of our model over the baseline models with at least 28.18% improvement in terms of BLEU-4 score. Furthermore, we also highlight the future opportunities in training contextualized code representations on larger code corpus as a solution to low-resource tasks and adapting the contextualized code representation framework to other code-to-text generation tasks.", 
"Heart disease is the top cause of death worldwide, killing over 17 million people a year. In the U.S., 25% of deaths are attributed to heart disease. An enhanced deep neural network (DNN) learning was developed to aid patients and healthcare professionals. The models can be used to aid healthcare professionals and patients throughout the world to advance both public health and global health, especially in developing countries and resource-limited areas.",Coronary Heart Disease Diagnosis using Deep Neural Networks,"According to the World Health Organization, cardiovascular disease (CVD) is the top cause of death worldwide. In 2015, over 30% of global deaths were due to CVD, leading to over 17 million deaths, a global health burden. Of those deaths, over 7 million were caused by heart disease, and greater than 75% of deaths due to CVD were in developing countries. In the United States alone, 25% of deaths were attributed to heart disease, killing over 630,000 Americans annually. Among heart disease conditions, coronary heart disease is the most common, causing over 360,000 American deaths due to heart attacks in 2015. Thus, coronary heart disease is a public health issue. In this research paper, an enhanced deep neural network (DNN) learning was developed to aid patients and healthcare professionals and to increase the accuracy and reliability of heart disease diagnosis and prognosis in patients. The developed DNN learning model is based on a deeper multilayer perceptron architecture with regularization and dropout using deep learning. The developed DNN learning model includes a classification model based on training data and a prediction model for diagnosing new patient cases using a data set of 303 clinical instances from patients diagnosed with coronary heart disease at the Cleveland Clinic Foundation. The testing results showed that the DNN classification and prediction model achieved the following results: diagnostic accuracy of 83.67%, sensitivity of 93.51%, specificity of 72.86%, precision of 79.12%, F-Score of 0.8571, area under the ROC curve of 0.8922, Kolmogorov-Smirnov (K-S) test of 66.62%, diagnostic odds ratio (DOR) of 38.65, and 95% confidence interval for the DOR test of [38.65, 110.28]. Therefore, clinical diagnoses of coronary heart disease were reliably and accurately derived from the developed DNN classification and prediction models. Thus, the models can be used to aid healthcare professionals and patients throughout the world to advance both public health and global health, especially in developing countries and resource-limited areas with fewer cardiac specialists available.", 
"The mortality from coronary heart disease gets the highest prevalence in Indonesia at 1.5 percent. Some researchers have developed a system using conventional neural network or other machine learning algorithm. The overall predictive accuracy, sensitivity, and specificity acquired was 96%, 99%, 92%, respectively.",Coronary Heart Disease Interpretation Based on Deep Neural Network,"Coronary heart disease (CHD) population increases every year with a significant number of deaths. Moreover, the mortality from coronary heart disease gets the highest prevalence in Indonesia at 1.5 percent. The misdiagnosis of coronary heart disease is a crucial fundamental that is the major factor that caused death. To prevent misdiagnosis of CHD, an intelligent system has been designed. This paper proposed a simulation which can be used to diagnose the coronary heart disease in better performance than the traditional diagnostic methods. Some researchers have developed a system using conventional neural network or other machine learning algorithm, but the results are not a good performance. Based on a conventional neural network, deeper neural network (DNN) is proposed to our model in this work. As known as, the neural network is a supervised learning algorithm that good in the classification task. In DNN model, the implementation of binary classification was implemented to diagnose CHD present (representative “1”) or CHD absent (representative “0”). To help performance analysis using the UCI machine learning repository heart disease dataset, ROC Curve and its confusion matrix were implemented in this work. The overall predictive accuracy, sensitivity, and specificity acquired was 96%, 99%, 92%, respectively.", 
"Text summarization is a process of extracting salient information from a source text. Research based on clustering, optimization, and evolutionary algorithm for text summarization has recently shown good results, making this a promising area. In this paper, a two-stage sentences selection model based on clusters and optimization techniques, called COSUM, is proposed. Experimental results demonstrated that COS UM outperforms the state-of-the-art methods in terms of ROUGE?1 and ROUge?2 measures.","COSUM, Text summarization based on clustering and optimization","Text summarization is a process of extracting salient information from a source text and presenting that information to the user in a condensed form while preserving its main content. In the text summarization, most of the difficult problems are providing wide topic coverage and diversity in a summary. Research based on clustering, optimization, and evolutionary algorithm for text summarization has recently shown good results, making this a promising area. In this paper, for a text summarization, a two?stage sentences selection model based on clustering and optimization techniques, called COSUM, is proposed. At the first stage, to discover all topics in a text, the sentences set is clustered by using k?means method. At the second stage, for selection of salient sentences from clusters, an optimization model is proposed. This model optimizes an objective function that expressed as a harmonic mean of the objective functions enforcing the coverage and diversity of the selected sentences in the summary. To provide readability of a summary, this model also controls length of sentences selected in the candidate summary. For solving the optimization problem, an adaptive differential evolution algorithm with novel mutation strategy is developed. The method COSUM was compared with the 14 state?of?the?art methods: DPSO?EDASum; LexRank; CollabSum; UnifiedRank; 0–1 non?linear; query, cluster, summarize; support vector machine; fuzzy evolutionary optimization model; conditional random fields; MA?SingleDocSum; NetSum; manifold ranking; ESDS?GHS? GLO; and differential evolution, using ROUGE tool kit on the DUC2001 and DUC2002 data sets. Experimental results demonstrated that COSUM outperforms the state?of?the?art methods in terms of ROUGE?1 and ROUGE?2 measures.", 
Extremists create new propaganda videos from fragments of old terrorist attack videos. pHash-based algorithm identifies the original content of a video. Automatic novelty verification is now possible.,Counter-Terrorism Video Analysis Using Hash-Based Algorithms,"The Internet is becoming a major source of radicalization. The propaganda efforts of new extremist groups include creating new propaganda videos from fragments of old terrorist attack videos. This article presents a web-scraping method for retrieving relevant videos and a pHash-based algorithm which identifies the original content of a video. Automatic novelty verification is now possible, which can potentially reduce and improve journalist research work, as well as reduce the spreading of fake news. The obtained results have been satisfactory as all original sources of new videos have been identified correctly.", 
Intelligent technologies have begun to play a vital role in improving the quality of patients' lives and helping reduce the costs and workload involved in their daily healthcare. A new system that uses a fast Fourier transformation-coupled machine learning ensemble model is proposed for short-term disease risk prediction. The proposed system yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations.,Coupling a Fast Fourier Transformation With a Machine Learning Ensemble Model to Support Recommendations for Heart Disease Patients in a Telehealth Environment,"Recently, the use of intelligent technologies in clinical decision making in the telehealth environment has begun to play a vital role in improving the quality of patients’ lives and helping reduce the costs and workload involved in their daily healthcare. In this paper, an effective medical recommendation system that uses a fast Fourier transformation-coupled machine learning ensemble model is proposed for short-term disease risk prediction to provide chronic heart disease patients with appropriate recommendations about the need to take a medical test or not on the coming day based on analysing their medical data. The input sequence of sliding windows based on the patient’s time series data are decomposed by using the fast Fourier transformation in order to extract the frequency information. A bagging-based ensemble model is utilized to predict the patient’s condition one day in advance for producing the final recommendation. A combination of three classifiers–artificial neural network, least squares-support vector machine, and naive bayes–are used to construct an ensemble framework. A real-life time series telehealth data collected from chronic heart disease patients are utilized for experimental evaluation. The experimental results show that the proposed system yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations as well as reduce the workload for heart disease patients in conducting body tests every day. The results conclusively ascertain that the proposed system is a promising tool for analyzing time series medical data and providing accurate and reliable recommendations to patients suffering from chronic heart diseases.", 
"Code comments provide abundant information that have been leveraged to help perform various software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, refine, and propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. 41573 new comments can be derived by propagation from other code locations with 88% accuracy.",CPC Automatically Classifying and Propagating Natural Language Comments via Program Analysis,"Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, specification inference, and code synthesis. However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, refine, and propagate comments. For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments. Developers usually comment on different aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties. To more effectively utilize comments, a fine-grained and elaborated taxonomy of comments and a reliable classifier to automatically categorize a comment are needed. In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy. Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code. Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been confirmed and fixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments. This demonstrates the effectiveness of our approach. Our user study confirms propagated comments align well with existing comments in terms of quality.", 
"In concrete structures, surface cracks are important indicators of durability and serviceability. This article presents a methodology for identifying concrete cracks using machine learning. The method helps in determining the existence and location of cracks from surface images. In the training stage of the proposed approach, image binarization is used to extract crack candidate regions.",Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning,"In concrete structures, surface cracks are important indicators of structural durability and serviceability. Generally, concrete cracks are visually monitored by inspectors who record crack information such as the existence, location, and width. Manual visual inspection is often considered ineffective in terms of cost, safety, assessment accuracy, and reliability. Digital image processing has been introduced to more accurately obtain crack information from images. A critical challenge is to automatically identify cracks from an image containing actual cracks and crack-like noise patterns (e.g. dark shadows, stains, lumps, and holes), which are often seen in concrete structures. This article presents a methodology for identifying concrete cracks using machine learning. The method helps in determining the existence and location of cracks from surface images. The proposed approach is particularly designed for classifying cracks and noncrack noise patterns that are otherwise difficult to distinguish using existing image processing algorithms. In the training stage of the proposed approach, image binarization is used to extract crack candidate regions; subsequently, classification models are constructed based on speeded-up robust features and convolutional neural network. The obtained crack identification methods are quantitatively and qualitatively compared using new concrete surface images containing cracks and noncracks.", 
"Most literature on BDA focuses on how it can be used to enhance tactical organizational capabilities. Success of any BDA project lies in realizing strategic business value, which gives firms a competitive advantage. framing in this study could help develop a significant research agenda for BDA.",Creating Strategic Business Value from Big Data Analytics A Research Framework,"Despite the publicity regarding big data and analytics (BDA), the success rate of these projects and strategic value created from them are unclear. Most literature on BDA focuses on how it can be used to enhance tactical organizational capabilities, but very few studies examine its impact on organizational value. Further, we see limited framing of how BDA can create strategic value for the organization. After all, the ultimate success of any BDA project lies in realizing strategic business value, which gives firms a competitive advantage. In this study, we describe the value proposition of BDA by delineating its components. We offer a framing of BDA value by extending existing frameworks of information technology value, then illustrate the framework through BDA applications in practice. The framework is then discussed in terms of its ability to study constructs and relationships that focus on BDA value creation and realization. We also present a problem-oriented view of the framework—where problems in BDA components can give rise to targeted research questions and areas for future study. The framing in this study could help develop a significant research agenda for BDA that can better target research and practice based on effective use of data resources.", 
SUMMA is a text summarization toolkit for the development of adaptive summarization applications. It includes algorithms for computation of various sentence relevance features. It also offers methods for content-based evaluation of summaries.,Creating Summarization Systems with SUMMA,"Automatic text summarization, the reduction of a text to its essential content is fundamental for an on-line information society. Although many summarization algorithms exist, there are few tools or infrastructures providing capabilities for developing summarization applications. This paper presents a new version of SUMMA, a text summarization toolkit for the development of adaptive summarization applications. SUMMA includes algorithms for computation of various sentence relevance features and functionality for single and multi-document summarization in various languages. It also offers methods for content-based evaluation of summaries.", 
Feature extraction is the promising issue to be addressed in algebraic based Automatic Text Summarization (ATS) methods. This work proposes a trainable supervised method.,CRF based Feature Extraction Applied for Supervised Automatic Text Summarization,Feature extraction is the promising issue to be addressed in algebraic based Automatic Text Summarization (ATS) methods. The most vital role of any ATS is the identification of most important sentences from the given text. This is possible only when the correct features of the sentences are identified properly. Hence this paper proposes a Conditional Random Field (CRF) based ATS which can identify and extract the correct features which is the main issue that exists with the Non-negative Matrix Factorization (NMF) based ATS. This work proposes a trainable supervised method. Result clearly indicates that the newly proposed approach can identify and segment the sentences based on features more accurately than the existing method addressed., 
Conceptualized-Representation Hierarchical-Attention Summarization (CRHASum) uses the contextual information and relations among sentences to improve the sentence regression performance for extractive text summarization. The model can achieve comparable performance to the state-of-the-art approach.,"CRHASum, extractive text summarization with contextualized-representation hierarchical-attention summarization network","The requirements for automatic document summarization that can be applied to practical applications are increasing rapidly. As a general sentence regression architecture, extractive text summarization captures sentences from a document by leveraging externally related information. However, existing sentence regression approaches have not employed features that mine the contextual information and relations among sentences. To alleviate this problem, we present a neural network model, namely the Contextualized-Representation Hierarchical-Attention Summarization (CRHASum), that uses the contextual information and relations among sentences to improve the sentence regression performance for extractive text summarization. This framework makes the most of their advantages. One advantage is that the contextual representation is allowed to vary across linguistic context information, and the other advantage is that the hierarchical attention mechanism is able to capture the contextual relations from the word-level and sentence-level by using the Bi-GRU. With this design, the CRHASum model is capable of paying attention to the important context in the surrounding context of a given sentence for extractive text summarization. We carry out extensive experiments on three benchmark datasets. CRHASum alone can achieve comparable performance to the state-of-the-art approach. Meanwhile, our method significantly outperforms the state-of-the-art baselines in terms of multiple ROUNG metrics and includes a few basic useful features.", 
This paper takes a first step towards a critical thinking curriculum for neural autoregressive language models. We introduce a synthetic text corpus of deductively valid arguments. We use this artificial argument corpus to train and evaluate GPT-2.,Critical Thinking for Language Models,"This paper takes a first step towards a critical thinking curriculum for neural autoregressive language models. We introduce a synthetic text corpus of deductively valid arguments, and use this artificial argument corpus to train and evaluate GPT-2. Significant transfer learning effects can be observed: Training a model on a few simple core schemes allows it to accurately complete conclusions of different, and more complex types of arguments, too. The language models seem to connect and generalize the core argument schemes in a correct way. Moreover, we obtain consistent and promising results for the GLUE and SNLI benchmarks. The findings suggest that there might exist a representative sample of paradigmatic instances of good reasoning that will suffice to acquire general reasoning skills and that might form the core of a critical thinking curriculum for language models.", 
This paper takes a first step towards a critical thinking curriculum for neural autoregressive language models. We introduce a synthetic text corpus of deductively valid arguments. We use this artificial argument corpus to train and evaluate GPT-2.,"Cross-App Interference Threats in Smart Homes Categorization, Detection and Handling","A number of Internet of Things (IoTs) platforms have emerged to enable various IoT apps developed by third-party developers to automate smart homes. Prior research mostly concerns the overprivilege problem in the permission model. Our work, however, reveals that even IoT apps that follow the principle of least privilege, when they interplay, can cause unique types of threats, named Cross-App Interference (CAI) threats. We describe and categorize the new threats, showing that unexpected automation, security and privacy issues may be caused by such threats, which cannot be handled by existing IoT security mechanisms. To address this problem, we present HOMEGUARD, a system for appified IoT platforms to detect and cope with CAI threats. A symbolic executor module is built to precisely extract the automation semantics from IoT apps. The semantics of different IoT apps are then considered collectively to evaluate their interplay and discover CAI threats systematically. A user interface is presented to users during IoT app installation, interpreting the discovered threats to help them make decisions. We evaluate HOMEGUARD via a proof-of-concept implementation on Samsung’s SmartThings and discover many threat instances among apps in the SmartThings public repository. The evaluation shows that it is precise, effective and efficient.", 
"Cross-document structure theory is a paradigm for multi-document analysis. It takes into account the rhetorical structure of clusters of related textual documents. It can be the basis for summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts, says the paper. It argues that a binary classifier for determining existence of structural relationships significantly outperforms the baseline.",Cross-document relationship classification for text summarization,"Multiple documents describing the same event present some interesting challenges for natural language processing. They contain similar information and yet they also exhibit a number of interesting properties: paraphrases, partial agreement, difference in judgment and emphasis, and contradictions. When the sources track an event that evolves over time, more phenomena can be observed: additions, updates, corrections, comments, clarifications, etc. In the current work, we introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. The goals of the paper are fourfold: first, we present a taxonomy of 18 CST relationships and give our motivation for proposing the theory. Second, we present an experiment that demonstrates that CST is essential for extractive, multi-document summarization. We then argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts. In particular, we propose a CST-enhancement procedure for extractive summarization, and demonstrate that CST-enhanced summaries outperform their unmodified counterparts as measured by the relative utility evaluation metric. Next we discuss the building of the CSTBank, a corpus of document clusters in which sentence pairs have been annotated for CST relationships. Finally, we discuss our efforts in the automatic detection of CST relationships between sentence pairs extracted from related documents. We show that a binary classifier for determining existence of structural relationships significantly outperforms the baseline. We also achieve promising results on the multi-class case in which the full taxonomy of relationships are considered.",  
Cross-Language Automatic Text Summarization produces a summary in a language different from the language of the source documents. We propose a French-to-English cross-lingual summarization framework that analyzes the information in both languages.,Cross-Language Text Summarization Using Sentence and Multi-Sentence Compression,"Cross-Language Automatic Text Summarization produces a summary in a language different from the language of the source documents. In this paper, we propose a French-to-English cross-lingual summarization framework that analyzes the information in both languages to identify the most relevant sentences. In order to generate more informative cross-lingual summaries, we introduce the use of chunks and two compression methods at the sentence and multi-sentence levels. Experimental results on the MultiLing 2011 dataset show that our framework improves the results obtained by state-of-the art approaches according to ROUGE metrics.", 
Evaluation of Mental Workload (MW) levels via ongoing electroencephalogram (EEG) is quite promising in Human-Machine (HM) collaborative task environment. Recognition ofMW states via a static pattern classifier with training and testing EEG signals recoded on separate days is particularly challenging. An adaptive Stacked Denoising AutoEncoder (SDAE) is developed to tackling such cross-session MW classification task.,Cross-session classification of mental workload levels using EEG and an adaptive deep learning model,"Evaluation of operator Mental Workload (MW) levels via ongoing electroencephalogram (EEG) is quite promising in Human-Machine (HM) collaborative task environment to alarm the temporal operator performance degradation. However, accurate recognition of MW states via a static pattern classifier with training and testing EEG signals recoded on separate days is particularly challenging as EEG features are differently distributed across different sessions. Motivated by the superiority of the deep learning approaches for stable feature abstractions in higher levels, an adaptive Stacked Denoising AutoEncoder (SDAE) is developed to tackling such cross-session MW classification task in which the weights of the shallow hidden neurons could be adaptively updated during the testing procedure. The generalization capability of the adaptive SDAE is first evaluated under within/cross-session conditions. Then, we compare it with the state of the art MW classifiers under different feature selection and the noise corruption paradigms. The results indicate a higher performance of the adaptive SDAE in dealing with the cross-session EEG features. By analyzing the optimal step length, the data augmentation scheme and the computational cost for iterative tuning, the adaptive SDAE is also demonstrated to be acceptable for online implementation.", 
Brain-Computer Interface (BCI) is a system able to serve as a mean of communication between machine and human. Brainwaves are the control signals acquired by electroencephalography (EEG) One of the most used brainwaves is the sensorimotor rhythm (SMR) which appears for real or imagined motor movement.,Cross-Subject EEG Signal Classification with Deep Neural Networks Applied to Motor Imagery,"The Brain-Computer Interface (BCI) is a system able to serve as a mean of communication between machine and human where the brainwaves are the control signals acquired by electroencephalography (EEG). One of the most used brainwaves is the sensorimotor rhythm (SMR) which appears for real or imagined motor movement. In general, EEG signals need feature extraction methods and classification algorithms to interpret the raw signals. Deep learning approaches; however, permit the processing of the raw data without any transformation. In this paper, we present a deep learning neural network architecture to classify SMR signals due to its success for some previous works and to visualize the learned features. The architecture is composed of three parts. The first part contains a temporal convolution operation followed by a spatial convolution one. The second part contains recurrent layers. Finally, we use a dense layer to assign the signal to its class. The model is trained with Adam optimizer algorithm. Also, we use various regularization techniques such as dropout to prevent learning problem like overfitting. To evaluate the performance of the proposed architecture, the well known Dataset IIa of the BCI Competition IV is used. As a result, we get equivalent results to those ones of EEGNet.", 
Neuroscientists develop a novel deep domain adaptation network (DDAN) for cross-subject EEG signal recognition. DDAN uses a special end-to-end convolutional neural network to automatically extract deep features from the raw EEG data. This study may promote the practical use of EEG signal processing technology and expand its application range.,Cross-Subject EEG Signal Recognition Using Deep Domain Adaptation Network,"Collecting sufficient labeled electroencephalography (EEG) data to build an individual classifier for each subject is extremely time-consuming and labor-intensive, especially for the disabled patients. A feasible way is to use labeled EEG data from other subjects (source domains) to train a model for classifying EEG data from the new subjects (target domains). However, the model trained using other subjects EEG data may degrade the classification performance of the target subject, when there exists the substantial inter-subject variability of EEG data. In this paper, to account for the domain shift between different subjects, we propose a novel deep domain adaptation network (DDAN) for cross-subject EEG signal recognition. Specifically, a special end-to-end convolutional neural network (CNN) is firstly adopted to automatically extract deep features from the raw EEG data. Then, maximum mean discrepancy (MMD) is used to minimize the distribution discrepancy of deep features between source and target subjects. Finally, a center-based discriminative feature learning (CDFL) method is used to force the deep features closer to their corresponding class centers and make the inter-class centers more separable, so that it is possible to further improve the recognition performance of target domain EEG data. Experiments on public EEG datasets prove the effectiveness of the proposed method. This study may promote the practical use of EEG signal processing technology and expand its application range.", 
This letter presents a novel technique for classification of motor imagery (MI) electroencephalogram (EEG) signals. The technique uses a multiplex weighted visibility graph (MWVG) algorithm. The proposed model can be implemented to develop a robust and effective brain computer interface system.,Cross-Subject Motor Imagery Tasks EEG Signal Classification Employing Multiplex Weighted Visibility Graph and Deep Feature Extraction,"This letter presents a novel technique for classification of motor imagery (MI) electroencephalogram (EEG) signals employing a multiplex weighted visibility graph (MWVG) algorithm. A weighted visibility graph (WVG) is an effective tool to map a univariate time series into a graphical representation while preserving its temporal characteristics. In this contribution, the concept of WVG of univariate time series is extended to analyze multivariate EEG time series known as a MWVG algorithm. From the graphical representation of the transformed EEG time series, a new method for construction of complex functional brain connectivity network using clustering co-efficient was proposed based on mutual correlation between different electrodes. An auto encoder based deep feature extraction technique was employed to extract meaningful features from the images of brain connectivity matrix and classification of different MI tasks was performed using different benchmark classifiers. In this contribution, a cross-subject classification is performed to address the problem of lack of generalized features from EEG signals across different subjects. It was observed that an average classification accuracy of 99.92% and 99.96% is obtained using the Random Forest classifier. Experimental investigations on two publicly available databases revealed that the proposed model can be implemented to develop a robust and effective brain computer interface system.", 
Machine reading comprehension (MRC) and query-based text summarization are two related tasks. We build a system that transfers knowledge between the two tasks. Our models achieve state-of-the-art results on the publicly available CNN/Daily Mail and Debatepedia datasets.,Cross-Task Knowledge Transfer for Query-Based Text Summarization,"We demonstrate the viability of knowledge transfer between two related tasks: machine reading comprehension (MRC) and query-based text summarization. Using an MRC model trained on the SQuAD1.1 dataset as a core system component, we first build an extractive query-based summarizer. For better precision, this summarizer also compresses the output of the MRC model using a novel sentence compression technique. We further leverage pre-trained machine translation systems to abstract our extracted summaries. Our models achieve state-of-the-art results on the publicly available CNN/Daily Mail and Debatepedia datasets, and can serve as simple yet powerful baselines for future systems. We also hope that these results will encourage research on transfer learning from large MRC corpora to query-based summarization.", 
CrossRec is a recommender system to assist open source software developers in selecting suitable third-party libraries. It exploits a collaborative filtering technique to recommend libraries to developers by relying on the set of dependencies. It outperforms LibRec and LibCUP with respect to various quality metrics.,CrossRec Supporting software developers by recommending third-party libraries,"When creating a new software system, or when evolving an existing one, developers do not reinvent the wheel but, rather, seek available libraries that suit their purpose. In such a context, open source software repositories contain rich resources that can provide developers with helpful advice to support their tasks. However, the heterogeneity of resources and the dependencies among them are the main obstacles to the effective mining and exploitation of the available data. In this sense, advanced techniques and tools are needed to mine the metadata to bring in meaningful recommendations. In this paper, we present CrossRec, a recommender system to assist open source software developers in selecting suitable third-party libraries. CrossRec exploits a collaborative filtering technique to recommend libraries to developers by relying on the set of dependencies, which are currently included in the project being developed. We perform an empirical evaluation to compare the proposed approach with three state-of-the-art baselines, i.e., LibRec, LibFinder, and LibCUP on three considerably large datasets. The experimental results show that CrossRec overcomes the limitation of the baselines by recommending also libraries with a specific version. More importantly, it outperforms LibRec and LibCUP with respect to various quality metrics.", 
CTLsum is a novel framework for controllable summarization. It enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input. It achieves state-of-the-art results on the CNN/DailyMail dataset.,"CTRLsum, Towards Generic Controllable Text Summarization","Current summarization systems yield generic summaries that are disconnected from users’ preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", 
We classify and re-examine some of the current approaches to improve the performance-computes trade-off of language models. We identify some limitations (1) - (4) suffer from. We argue (5) would resolve many of these limitations and it can reduce the amount of supervision.,"Current Limitations of Language Models, What You Need is Retrieval","We classify and re-examine some of the current approaches to improve the performance-computes trade-off of language models, including (1) non-causal models (such as masked language models), (2) extension of batch length with efficient attention, (3) recurrence, (4) conditional computation and (5) retrieval. We identify some limitations (1) - (4) suffer from. For example, (1) currently struggles with open-ended text generation with the output loosely constrained by the input as well as performing general textual tasks like GPT-2/3 due to its need for a specific fine-tuning dataset. (2) and (3) do not improve the prediction of the first ? 103 tokens. Scaling up a model size (e.g. efficiently with (4)) still results in poor performance scaling for some tasks. We argue (5) would resolve many of these limitations, and it can (a) reduce the amount of supervision and (b) efficiently extend the context over the entire training dataset and the entire past of the current sample. We speculate how to modify MARGE to perform unsupervised causal modeling that achieves (b) with the retriever jointly trained.", 
"Cybersickness is a symptom of dizziness that occurs while experiencing Virtual Reality (VR) technology. It is presumed to occur mainly by crosstalk between the sensory and cognitiave systems. It can't be measured objectively, making it difficult to measure.",Cybersickness Analysis with EEG Using Deep Learning Algorithms,"Cybersickness is a symptom of dizziness that occurs while experiencing Virtual Reality (VR) technology and it is presumed to occur mainly by crosstalk between the sensory and cognitive systems. However, since the sensory and cognitive systems cannot be measured objectively, it is difficult to measure cybersickness. Therefore, methodologies for measuring cybersickness have been studied in various ways. Traditional studies have collected answers to questionnaires or analyzed EEG data using machine learning algorithms. However, the system relying on the questionnaires lacks objectivity, and it is difficult to obtain highly accurate measurements with the machine learning algorithms in previous studies. In this work, we apply and compare Deep Neural Network (DNN) and Convolutional Neural Network (CNN) deep learning algorithms for objective cybersickness measurement from EEG data. We also propose a data preprocessing for learning and signal quality weights allowing us to achieve high performance while learning EEG data with the deep learning algorithms. Besides, we analyze video characteristics where cybersickness occurs by examining the 360 video stream segments causing cybersickness in the experiments. Finally, we draw common patterns that cause cybersickness.", 
"An optimum allocation based principal component analysis method named as OA PCA is developed for the feature extraction from epileptic EEG data. LS-SVM 1v1 approach yields 100% of the overall classification accuracy (OCA), improving up to 7.10% over the existing algorithms for the epileptic brain data.",Designing a robust feature extraction method based on optimum allocation and principal component analysis for epileptic EEG signal classification,"The aim of this study is to design a robust feature extraction method for the classification of multiclass EEG signals to determine valuable features from original epileptic EEG data and to discover an efficient classifier for the features. An optimum allocation based principal component analysis method named as OA PCA is developed for the feature extraction from epileptic EEG data. As EEG data from different channels are correlated and huge in number, the optimum allocation (OA) scheme is used to discover the most favorable representatives with minimal variability from a large number of EEG data. The principal component analysis (PCA) is applied to construct uncorrelated components and also to reduce the dimensionality of the OA samples for an enhanced recognition. In order to choose a suitable classifier for the OA PCA feature set, four popular classifiers: least square support vector machine (LS-SVM), naive bayes classifier (NB), k-nearest neighbor algorithm (KNN), and linear discriminant analysis (LDA) are applied and tested. Furthermore, our approaches are also compared with some recent research work. The experimental results show that the LS-SVM 1v1 approach yields 100% of the overall classification accuracy (OCA), improving up to 7.10% over the existing algorithms for the epileptic EEG data. The major finding of this research is that the LS-SVM with the 1v1 system is the best technique for the OA PCA features in the epileptic EEG signal classification that outperforms all the recent reported existing methods in the literature.", 
"Existing data dissemination schemes are generally built upon some random-access protocol. This results in the unavoidable collision problem. We design a novel data dissemination strategy from the scheduling perspective. Compared with the random access dissemination such as CodeOn-Basic and the noncooperative transmission, our proposed strategy performs better in terms of the dissemination delay.",Data Dissemination in VANETs A Scheduling Approach,"Data dissemination is a promising application for the vehicular network. Existing data dissemination schemes are generally built upon some random-access protocol, which results in the unavoidable collision problem. To address this problem, in this paper we design a novel data dissemination strategy from the scheduling perspective. A data dissemination scheduling framework is then proposed. In the proposed framework, the main challenge is how best to assign the transmission opportunity to nodes with maximum dissemination utility and to avoid the collision problem. We then propose a novel and practical relay selection strategy and adopt the space–time network coding (STNC) with low detection complexity and space–time diversity gain to improve the dissemination efficiency. Compared with the random access dissemination such as CodeOn-Basic and the noncooperative transmission, our proposed data dissemination strategy performs better in terms of the dissemination delay. In addition, the proposed strategy works even better in the dense network than the sparse scenario, benefitting from the space–time diversity gain of STNC and no-collision transmissions. This is in sharp contrary to the CodeOn-Basic method.", 
"According to the characteristics of vehicle sequential pattern, the topology of vehicular ad hoc network (VANET) is linked to the real road vehicle movement trajectory. Some new definitions related to sequential patterns in VANET environment are proposed.",Data Mining Method of Sequential Patterns for Vehicle Trajectory Prediction in VANET,"In order to provide the future direction of vehicle flow for travelers, it is necessary to predict the driving trajectory of vehicles running on the road in a certain segments. According to the characteristics of vehicle sequential pattern, the topology of vehicular ad hoc network (VANET) is linked to the real road vehicle movement trajectory. Some new definitions related to sequential patterns in VANET environment are proposed. Based on RSU and V2V schemes, a vehicle movement database is established, and sequential pattern data mining is carried out. Afterward, their communication overhead is evaluated. The support and confidence of the movement rules generated by vehicle routing patterns are calculated to extract the probability of frequent driving trajectories.", 
"Blockchain-based data sharing and access control system is proposed, for communication between Internet of Things (IoT) devices. System intended to overcome issues related to trust and authentication for access control in IoT networks. Objectives of the system are to achieve trustfulness, authorization, and authentication.",Data sharing system integrating access control mechanism using blockchain-based smart contracts for IoT devices,"In this paper, a blockchain-based data sharing and access control system is proposed, for communication between the Internet of Things (IoT) devices. The proposed system is intended to overcome the issues related to trust and authentication for access control in IoT networks. Moreover, the objectives of the system are to achieve trustfulness, authorization, and authentication for data sharing in IoT networks. Multiple smart contracts such as Access Control Contract (ACC), Register Contract (RC), and Judge Contract (JC) are used to provide efficient access control management. Where ACC manages overall access control of the system, and RC is used to authenticate users in the system, JC implements the behavior judging method for detecting misbehavior of a subject (i.e., user). After the misbehavior detection, a penalty is defined for that subject. Several permission levels are set for IoT devices’ users to share services with others. In the end, performance of the proposed system is analyzed by calculating cost consumption rate of smart contracts and their functions. A comparison is made between existing and proposed systems. Results show that the proposed system is efficient in terms of cost. The overall execution cost of the system is 6,900,000 gas units and the transaction cost is 5,200,000 gas units.", 
We evaluate whether features extracted from a deep convolutional network can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks. We report novel results that significantly outperform the state-of-the-art on several important vision challenges.,DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition,"We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.", 
"Scientists are using various computational techniques to predict and prevent heart diseases. In this paper, hybridization technique is proposed in which decision tree and artificial neural network classifiers are hybridized for better performance.",Decision Tree Algorithms for Prediction of Heart Disease,"In the present scenario, maximum causes of death are heart disease. Many researches are taking place to detect all types of heart diseases at very early stage. Scientists are using various computational techniques to predict and prevent heart diseases. Using data mining techniques, the number of tests that are required for the detection of heart disease reduces. In this paper, hybridization technique is proposed in which decision tree and artificial neural network classifiers are hybridized for better performance of prediction of heart disease. This is done using WEKA. To validate the performance of the proposed algorithm, tenfold validation test is performed on the dataset of heart disease patients which is taken from UCI repository. The accuracy, sensitivity, and specificity of the individual classifier and hybrid technique are analyzed.", 
"A new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Extensive experiments show that our approach outperforms state-of-the-art methods.",Deep Convolutional Network Cascade for Facial Point Detection,"We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-of-the-art methods in both detection accuracy and reliability.", 
"This paper provides a new single-channel approach to automatic classification of sleep stages from EEG signal. The main idea is to directly apply the raw EEG signal to deep convolutional neural network without involving feature extraction/selection, which is a challenging process in the previous literature. The simulation results of the proposed method show an accuracy of 98.10% and Cohen's Kappa coefficient of 0.89%.",Deep convolutional neural network for classification of sleep stages from single-channel EEG signals,"Using a smart method for automatic diagnosis in medical applications, such as sleep stage classification is considered as one of the important challenges of the last few years which can replace the time-consuming process of visual inspection done by specialists. One of the problems regarding the automatic diagnosis of sleep patterns is extraction and selection of discriminative features generally demanding high computational burden. This paper provides a new single-channel approach to automatic classification of sleep stages from EEG signal. The main idea is to directly apply the raw EEG signal to deep convolutional neural network, without involving feature extraction/selection, which is a challenging process in the previous literature. The proposed network architecture includes 9 convolutional layers followed by 2 fully connected layers. In order to make the samples of different classes balanced, we used a preprocessing method called data augmentation. The simulation results of the proposed method for classification of 2 to 6 classes of sleep stages show the accuracy of 98.10%, 96.86%, 93.11%, 92.95%, 93.55% and Cohen’s Kappa coefficient of 0.98%, 0.94%, 0.90%, 0.86% and 0.89%, respectively. Furthermore, comparing the obtained results with the state-of-the-art methods reveals the performance improvement of the proposed sleep stage classification in terms of accuracy and Cohen’s Kappa coefficient.", 
"An encephalogram (EEG) is a commonly used ancillary test to aide in the diagnosis of epilepsy. Traditionally, neurologists employ direct visual inspection to identify epileptiform abnormalities. This is the first study to employ the convolutional neural network (CNN) for analysis of EEG signals.",Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals,"An encephalogram (EEG) is a commonly used ancillary test to aide in the diagnosis of epilepsy. The EEG signal contains information about the electrical activity of the brain. Traditionally, neurologists employ direct visual inspection to identify epileptiform abnormalities. This technique can be time-consuming, limited by technical artifact, provides variable results secondary to reader expertise level, and is limited in identifying abnormalities. Therefore, it is essential to develop a computer-aided diagnosis (CAD) system to automatically distinguish the class of these EEG signals using machine learning techniques. This is the first study to employ the convolutional neural network (CNN) for analysis of EEG signals. In this work, a 13-layer deep convolutional neural network (CNN) algorithm is implemented to detect normal, preictal, and seizure classes. The proposed technique achieved an accuracy, specificity, and sensitivity of 88.67%, 90.00% and 95.00%, respectively.", 
Deep learning and computer vision algorithms can deliver highly accurate interpretation of medical imaging. Medical imaging presents uniquely pertinent obstacles such as a lack of accessible data or a high-cost of annotating. We develop data-efficient deep learning classifiers for prediction tasks in cardiology.,Deep echocardiography data-efficient supervised and semisupervised deep learning towards automated diagnosis of cardiac disease,"Deep learning and computer vision algorithms can deliver highly accurate and automated interpretation of medical imaging to augment and assist clinicians. However, medical imaging presents uniquely pertinent obstacles such as a lack of accessible data or a high-cost of annotation. To address this, we developed data-efficient deep learning classifiers for prediction tasks in cardiology. Using pipeline supervised models to focus relevant structures, we achieve an accuracy of 94.4% for 15-view still-image echocardiographic view classification and 91.2% accuracy for binary left ventricular hypertrophy classification. We then develop semi-supervised generative adversarial network models that can learn from both labeled and unlabeled data in a generalizable fashion. We achieve greater than 80% accuracy in view classification with only 4% of labeled data used in solely supervised techniques and achieve 92.3% accuracy for left ventricular hypertrophy classification. In exploring trade-offs between model type, resolution, data resources, and performance, we present a comprehensive analysis and improvements of efficient deep learning solutions for medical imaging assessment especially in cardiology.", 
Machine learning approaches have been developed to utilize information in Electronic Health Record (EHR) for this task. Majority of previous attempts focus on structured fields and lose the vast amount of information in unstructured notes. In this work we propose a general multi-task framework for disease onset prediction that combines free-text medical notes and structured information.,Deep EHR Chronic Disease Prediction Using Medical Notes,"Early detection of preventable diseases is important for better disease management, improved interventions, and more efficient health-care resource allocation. Various machine learning approaches have been developed to utilize information in Electronic Health Record (EHR) for this task. Majority of previous attempts, however, focus on structured fields and lose the vast amount of information in the unstructured notes. In this work we propose a general multi-task framework for disease onset prediction that combines both free-text medical notes and structured information. We compare performance of different deep learning architectures including CNN, LSTM and hierarchical models. In contrast to traditional text-based prediction models, our approach does not require disease specific feature engineering, and can handle negations and numerical values that exist in the text. Our results on a cohort of about 1 million patients show that models using text outperform models using just structured data, and that models capable of using numerical values and negations in the text, in addition to the raw text, further improve performance. Additionally, we compare different visualization methods for medical professionals to interpret model predictions.", 
Advances in automatic text summarization using deep learning technique is prime focus of research. Data-driven approach has been used to generate extractive summaries. Approach proposed uses paraphrasing techniques to classify sentences.,Deep Extractive Text Summarization,"With introduction of deep learning techniques their has been an increase in intelligent classification of text in many applications. Advances in automatic text summarization using deep learning technique is prime focus of research now a days. Earlier traditional approaches for extractive text summarization have been heavily dependent on human engineered features. However, it is a laborious and tedious task. In this paper, a data-driven approach has been used to generate extractive summaries using deep learning. Approach proposed uses paraphrasing techniques to classify sentences as a candidate sentence for inclusion in summary or not.", 
"Deep learning has aroused wide interest in machine learning fields. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network. We apply MLELM to EEG classification in this paper. By simulating and analyzing the results of the experiments, the effectiveness of DELM in EEG classification is confirmed.",Deep Extreme Learning Machine and Its Application in EEG Classification,"Recently, deep learning has aroused wide interest in machine learning fields. Deep learning is a multilayer perceptron artificial neural network algorithm. Deep learning has the advantage of approximating the complicated function and alleviating the optimization difficulty associated with deep models. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network which takes advantages of deep learning and extreme learning machine. Not only does MLELM approximate the complicated function but it also does not need to iterate during the training process. We combining with MLELM and extreme learning machine with kernel (KELM) put forward deep extreme learning machine (DELM) and apply it to EEG classification in this paper. Tis paper focuses on the application of DELM in the classification of the visual feedback experiment, using MATLAB and the second brain-computer interface (BCI) competition datasets. By simulating and analyzing the results of the experiments, e?ectiveness of the application of DELM in EEG classification is confirmed.", 
"Texture recognition often concentrates on the problem of material recognition in uncluttered conditions. FV-CNN substantially improves the state-of-the-art in texture, material and scene recognition. It achieves 79.8% accuracy on Flickr material dataset and 81% accuracy in MIT indoor scenes.",Deep Filter Banks for Texture Recognition and Segmentation,"Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture attributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor, FV-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. FV-CNN substantially improves the state-of-the-art in texture, material and scene recognition. Our approach achieves 79.8% accuracy on Flickr material dataset and 81% accuracy on MIT indoor scenes, providing absolute gains of more than 10% over existing approaches. FV-CNN easily transfers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore, FV-CNN can seamlessly incorporate multiscale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at localizing “stuff” categories and obtains state-of-the-art results on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset.", 
Brain–computer interfaces (BCIs) provide direct communication between the human brain and external devices. Motor imagery EEG (MI-EEG) has become an active research field where a subject's active intent can be detected. Traditional deep learning scheme failed to generate spatio-temporal representation simultaneously and capture dynamic correlation for an MI- EEG sequence.,Deep Fusion Feature Learning Network for MI-EEG Classification,"Brain–computer interfaces (BCIs) are used to provide a direct communication between the human brain and the external devices, such as wheelchairs and intelligent apparatus, by interpreting the electroencephalograph (EEG) signals. Recently, motor imagery EEG (MI-EEG) has become an active research field where a subject’s active intent can be detected. The accurate decoding of MI-EEG signals is essential for effective BCI systems but also very challenging due to the lack of informative correlation between the signals and the brain activities. To improve the precision performance of a BCI system, accurate feature discrimination from input signals and proper classification are necessary. However, the traditional deep learning scheme is failed to generate spatio-temporal representation simultaneously and capture the dynamic correlation for an MI-EEG sequence. To address this problem, we propose a long short-term memory network combined with a spatial convolutional network that concurrently learns spatial information and temporal correlations from raw MI-EEG signals. In addition, spectral representations of EEG signals are obtained via a discrete wavelet transformation decomposition. In order to achieve even higher learning rates and less demanding initialization, we employ a batch normalization method before training and recognition. Various experiments have been performed to evaluate the performance of the proposed deep learning architectures. Results indicate a high level of accuracy over both the public data set and the local data set. Our method can also serve as a useful and robust model for multi-task classification and subject-independent movement class decoder across many different methods.", 
Emotive EPOC headset will help the blind and paralyzed people who are unable to control parts of their body. It uses deep learning to recognize four different movements from the recorded EEG signal.,Deep Learning AI Application to an EEG driven BCI Smart Wheelchair,"This paper describes designing, 3-D printing, building and testing of a prototype smart wheelchair using Emotive EPOC headset that will help the blind and paralyzed people who are unable to control parts of their body. It uses deep learning in order to recognize four different movements from the recorded EEG signal, to move left, right, forward, and stop. Data from 10 volunteers shows a success rate of 70% from the row EEG and 96% from the spectrum (Fourier Transform) of the data from the frequency bins corresponding to the delta, theta, alpha, beta, and gamma waves.", 
Paper reviews recent approaches for abstractive text summarisation using deep learning models. Gigaword dataset is commonly employed for single-sentence summary approaches. Cable News Network (CNN/Daily Mail) is commonly used for multi-sentenced approaches. Most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time.,"Deep Learning Based Abstractive Text Summarization Approaches, Datasets, Evaluation Measures, and Challenges","In recent years, the volume of textual data has rapidly increased, which has generated a valuable resource for extracting and analysing information. To retrieve useful knowledge within a reasonable time period, this information must be summarised. *is paper reviews recent approaches for abstractive text summarisation using deep learning models. In addition, existing datasets for training and validating these approaches are reviewed, and their features and limitations are presented. *e Gigaword dataset is commonly employed for single-sentence summary approaches, while the Cable News Network (CNN)/Daily Mail dataset is commonly employed for multi-sentence summary approaches. Furthermore, the measures that are utilised to evaluate the quality of summarisation are investigated, and Recall-Oriented Understudy for Gisting Evaluation 1 (ROUGE1), ROUGE2, and ROUGEL are determined to be the most commonly applied metrics. *e challenges that are encountered during the summarisation process and the solutions proposed in each approach are analysed. *e analysis of the several approaches shows that recurrent neural networks with an attention mechanism and long short-term memory (LSTM) are the most prevalent techniques for abstractive text summarisation. *e experimental results show that text summarisation with a pretrained encoder model achieved the highest values for ROUGE1, ROUGE2, and ROUGE-L (43.85, 20.34, and 39.9, respectively). Furthermore, it was determined that most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time, out-of-vocabulary (OOV) words, summary sentence repetition, inaccurate sentences, and fake facts.", 
This paper reviews the most recent extractive text summarization approaches that are based on deep learning techniques. The mostly used datasets for extractive summarizations are Daily Mail and DUC2002. SummaRuNNer approach which is based on Gated Recurrent Unit Recurrent Neural Network achieved the highest values over Daily Mail datasets.,"Deep Learning Based Extractive Text Summarization, Approaches, Datasets and Evaluation Measures","Recently, the number of online documents witness huge increase in volume. Thus, these documents need to be summarized in order to be effective. This paper reviews the most recent extractive text summarization approaches that are based on deep learning techniques. These approaches are classified into three categories based on deep learning techniques which are Restricted Boltzmann Machine, Variation Auto-Encoder and Recurrent Neural Network. The mostly used datasets for extractive summarizations are Daily Mail and DUC2002. Moreover, ROUGE is the mainly used evaluation measure to assess the quality of the extractive summarization process. The results show that SummaRuNNer approach which is based on Gated Recurrent Unit Recurrent Neural Network achieved the highest values for ROUGE1, ROUGE2 and ROUGE-L over Daily Mail datasets. On the other hand, the approach that is based on Recurrent Neural Network achieved the best results over DUC2002 datasets in term of ROUGE1 and ROUGE2.", 
"This technique is used for finding heart disease. Based on risk factor the heart diseases can be defined very easily. Compared to KNN, Convolution Neural Network provides better performance.",Deep learning based heart disease prediction,"Data mining is the process of data analyzing from various perspectives and combining it into useful information. This technique is used for finding heart disease. Based on risk factor the heart diseases can be defined very easily. The main aim of this work is to evaluate different classification techniques in heart diagnosis. First, the ECG numeric dataset is extracted and preprocess them. After that using extract the features that is condition to be find to be classified by Convolution Neural Network (NN).Compared to existing; Convolution Neural Network provides better performance. After classification, performance criteria including accuracy, precision, F-measure is to be calculated. Compared to KNN, Convolution Neural Network provides better performance. The comparison measure expose that Convolution Neural Network is the best classifier for the diagnosis of heart disease on the existing dataset.", 
Sentiment analysis aims to reveal semantic knowledge of written texts. Data shared by users on social networks consist of short texts. A text classification problem has arisen for data reaching large dimensions. Study: Two categories of emotion analysis were studied with data obtained from many social networks.,Deep Learning Based Sentiment Analysis and Text Summarization in Social Networks,"Sentiment analysis aims to reveal semantic knowledge of written texts where users share their feelings and thoughts on sharing platforms such as personal blogs and social networks. The data shared by users on social networks consist of short texts. A text classification problem has arisen for data reaching large dimensions shared on social networks. Although language libraries have been developed in other languages to solve the problem of sentiment analysis, studies for the Turkish language are limited. In this study, two categories of emotion analysis were studied with data obtained from many social networks. Also, there is a feeling class on a topic on Twitter, and text is summarized in the class. Sentiment analysis considered a classification problem. To increase the success rate, the study was carried out by focusing on the words with semantic context word embedding methods. LSA is used for text summarization. In this study, where both emotion analysis and text summation are carried out, the main goal is to analyze the sentiments and thoughts about a subject and present brief information to the user. The primary model was created with data collected from many social networks. Analysis and summary of the text were made with data from a hashtag on Twitter. The methods used in the analysis of emotions were compared to the methods of word embedding and a success rate of 93% was obtained.", 
Deep learning approaches for text summarization has gained momentum recently. Convolutional Neural Network (CNN) based approach achieve better accuracy for extractive summarization. Deep Recurrent Generative Networks (DRGN) provides state-of-the-art results.,"Deep learning based text summarization, approaches, databases and evaluation measures","The ever-growing volume of documents available online have compelled the need for smart text summarization systems and due to their state-of-the-art performances, deep learning approaches for text summarization has gained momentum recently. The objectives of this paper are threefold; first, we present a comprehensive review of the state-of-the-art approaches in automatic summarization research. Next, we provide an overview of the databases available for training and benchmarking the text summarization algorithms. Furthermore, we discuss the available evaluation measures for quality assessment of system generated summary. A performance comparison of the methods demonstrates that Convolutional Neural Network (CNN) based approach achieve better accuracy for extractive summarization whereas, for abstractive summarization, Deep Recurrent Generative Networks (DRGN) provides state-of-the-art results.", 
Multi-scale deep convolutional neural networks are introduced to deal with the representation for imagined motor Electroencephalography (EEG) signals. We propose to learn a set of high-level feature representations through deep learning algorithm. Our approach achieves 100% accuracy for 4 classes imagined motor EEG signals classification.,Deep Learning EEG Response Representation for Brain Computer Interface,"In this paper, the multi-scale deep convolutional neural networks are introduced to deal with the representation for imagined motor Electroencephalography (EEG) signals. We propose to learn a set of high-level feature representations through deep learning algorithm, referred to as Deep Motor Features (DeepMF), for brain computer interface (BCI) with imagined motor tasks. As the extracted DeepMF are dissimilar for different tasks and alike for the same tasks, it is convenient to separate the diverse EEG signals for imagined motor tasks apart. Our approach achieves 100% accuracy for 4 classes imagined motor EEG signals classification on Project BCI - EEG motor activity dataset. Moreover, thanks to the highly abstract features DeepMF learned, only 4.125 seconds trials of training data are needed, compared with the conventional BLDA algorithm for 8.75 seconds trials demand to achieve the same accuracy, accordingly the BCI response time and the required trials for training are almost declined by half. Experiments are provided to illustrate the effectiveness of the proposed design approach.", 
This paper proposes to learn a set of high-level feature representations through deep learning. The proposed features are extracted from various face regions to form complementary and over-complete representations. 97:45% verification accuracy on LFW is achieved with only weakly aligned faces.,"Deep Learning Face Representation from Predicting 10,000 Classes","This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. We argue that DeepID can be effectively learned through challenging multi-class face identification tasks, whilst they can be generalized to other tasks (such as verification) and new identities unseen in the training set. Moreover, the generalization capability of DeepID increases as more face classes are to be predicted at training. DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets). When learned as classifiers to recognize about 10; 000 face identities in the training set and configured to keep reducing the neuron numbers along the feature extraction hierarchy, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. The proposed features are extracted from various face regions to form complementary and over-complete representations. Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. 97:45% verification accuracy on LFW is achieved with only weakly aligned faces.", 
"This work reviews literature review about deep learning for analyzing EEG data for decoding the activity of human's brain and diagnosing disease. It has analyzed a word, which presented a model based on CNN and LSTM methods. It studied how semi-supervised learning on EEG data analytics can be applied.",Deep learning for EEG data analytics A survey,"In this work, we conducted a literature review about deep learning (DNN, RNN, CNN, and so on) for analyzing EEG data for decoding the activity of human's brain and diagnosing disease and explained details about various architectures for understanding the details of CNN and RNN. It has analyzed a word, which presented a model based on CNN and LSTM methods, and how these methods can be used to both optimize and set up the hyper parameters of deep learning architecture. Later, it is studied how semi-supervised learning on EEG data analytics can be applied. We review some studies about different methods of semi-supervised learning on EEG data analytics and discussing the importance of semi-supervised learning for analyzing EEG data. In this paper, we also discuss the most common applications for human EEG research and review some papers about the application of EEG data analytics such as Neuromarketing, human factors, social interaction, and BCI. Finally, some future trends of development and research in this area, according to the theoretical background on deep learning, are given.", 
Electroencephalography (EEG) motor imagery (MI) signals have recently gained a lot of attention. MI signals encode a person's intent of performing an action. decoding these signals accurately is important for a Brain–Computer interface (BCI) system. Convolution neural network (CNN) has shown that it can extract spatial and temporal features from EEG.,Deep Learning for EEG motor imagery classification based on multi-layer CNNs feature fusion,"Electroencephalography (EEG) motor imagery (MI) signals have recently gained a lot of attention as these signals encode a person’s intent of performing an action. Researchers have used MI signals to help disabled persons, control devices such as wheelchairs and even for autonomous driving. Hence decoding these signals accurately is important for a Brain–Computer interface (BCI) system. But EEG decoding is a challenging task because of its complexity, dynamic nature and low signal to noise ratio. Convolution neural network (CNN) has shown that it can extract spatial and temporal features from EEG, but in order to learn the dynamic correlations present in MI signals, we need improved CNN models. CNN can extract good features with both shallow and deep models pointing to the fact that, at different levels relevant features can be extracted. Fusion of multiple CNN models has not been experimented for EEG data. In this work, we propose a multi-layer CNNs method for fusing CNNs with different characteristics and architectures to improve EEG MI classification accuracy. Our method utilizes different convolutional features to capture spatial and temporal features from raw EEG data. We demonstrate that our novel MCNN and CCNN fusion methods outperforms all the state-of-the-art machine learning and deep learning techniques for EEG classification. We have performed various experiments to evaluate the performance of the proposed CNN fusion method on public datasets. The proposed MCNN method achieves 75.7% and 95.4% on the BCI Competition IV-2a dataset and the High Gamma Dataset respectively. The proposed CCNN method based on autoencoder cross-encoding achieves more than 10% improvement for cross-subject EEG classification.", 
"Traditional marketing methodologies may be unsuccessful at selling products because they do not robustly stimulate the consumers to purchase a particular product. This research study was aimed at bridging the gap between traditional market research and neuromarketing research, which reflects the implicit consumer responses. In this work, a deep-learning approach is adopted to detect the consumer preferences.",Deep Learning for EEG-Based Preference Classification in Neuromarketing,"The traditional marketing methodologies (e.g., television commercials and newspaper advertisements) may be unsuccessful at selling products because they do not robustly stimulate the consumers to purchase a particular product. Such conventional marketing methods attempt to determine the attitude of the consumers toward a product, which may not represent the real behavior at the point of purchase. It is likely that the marketers misunderstand the consumer behavior because the predicted attitude does not always reflect the real purchasing behaviors of the consumers. This research study was aimed at bridging the gap between traditional market research, which relies on explicit consumer responses, and neuromarketing research, which reflects the implicit consumer responses. The EEG-based preference recognition in neuromarketing was extensively reviewed. Another gap in neuromarketing research is the lack of extensive data-mining approaches for the prediction and classification of the consumer preferences. Therefore, in this work, a deep-learning approach is adopted to detect the consumer preferences by using EEG signals from the DEAP dataset by considering the power spectral density and valence features. The results demonstrated that, although the proposed deep-learning exhibits a higher accuracy, recall, and precision compared with the k-nearest neighbor and support vector machine algorithms, random forest reaches similar results to deep learning on the same dataset.", 
Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI) The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions.,Deep learning for EEG-Based preference classification,"Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI). The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions such as in forensic neuroscience, rehabilitative medicine, affective entertainment and neuro-marketing. One particularly useful yet rarely explored areas of EEG-based emotion classification is preference recognition, which is simply the detection of like versus dislike. Within the limited investigations into preference classification, all reported studies were based on musically-induced stimuli except for a single study which used 2D images. The main objective of this study is to apply deep learning, which has been shown to produce state-of-the-art results in diverse hard problems such as in computer vision, natural language processing and audio recognition, to 3D object preference classification over a larger group of test subjects. A cohort of 16 users was shown 60 bracelet-like objects as rotating visual stimuli on a computer display while their preferences and EEGs were recorded. After training a variety of machine learning approaches which included deep neural networks, we then attempted to classify the users' preferences for the 3D visual stimuli based on their EEGs. Here, we show that that deep learning outperforms a variety of other machine learning classifiers for this EEG-based preference classification task particularly in a highly challenging dataset with large inter- and intra-subject variability.", 
"This paper proposes applying transfer learning on aggregated data from multiple users, while leveraging the capacity of deep learning algorithms to learn discriminant features from large datasets. The proposed transfer learning scheme is shown to systematically and significantly enhance the performance for all three networks on the two datasets. Real-time feedback allows users to adapt their muscle activation strategy which reduces the degradation in accuracy normally experienced over time.",Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning,"In recent years, deep learning algorithms have become increasingly more prominent for their unparalleled ability to automatically learn discriminant features from large amounts of data. However, within the field of electromyography-based gesture recognition, deep learning algorithms are seldom employed as they require an unreasonable amount of effort from a single person, to generate tens of thousands of examples. This work’s hypothesis is that general, informative features can be learned from the large amounts of data generated by aggregating the signals of multiple users, thus reducing the recording burden while enhancing gesture recognition. Consequently, this paper proposes applying transfer learning on aggregated data from multiple users, while leveraging the capacity of deep learning algorithms to learn discriminant features from large datasets. Two datasets comprised of 19 and 17 able-bodied participants respectively (the first one is employed for pre-training) were recorded for this work, using the Myo Armband. A third Myo Armband dataset was taken from the NinaPro database and is comprised of 10 able-bodied participants. Three different deep learning networks employing three different modalities as input (raw EMG, Spectrograms and Continuous Wavelet Transform (CWT)) are tested on the second and third dataset. The proposed transfer learning scheme is shown to systematically and significantly enhance the performance for all three networks on the two datasets, achieving an offline accuracy of 98.31% for 7 gestures over 17 participants for the CWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw EMG-based ConvNet. Finally, a use-case study employing eight able-bodied participants suggests that real-time feedback allows users to adapt their muscle activation strategy which reduces the degradation in accuracy normally experienced over time.", 
Detection algorithms for electroencephalography (EEG) data typically employ handcrafted features that take advantage of the signal's specific properties. This study suggests that automatic feature generation via deep learning is suitable for IEDs and EEG in general.,DEEP LEARNING FOR EPILEPTIC INTRACRANIAL EEG DATA,"Detection algorithms for electroencephalography (EEG) data typically employ handcrafted features that take advantage of the signal’s specific properties. In the field of interictal epileptic discharge (IED) detection, the feature representation that provides optimal classification performance is still an unresolved issue. In this paper, we consider deep learning for automatic feature generation from epileptic intracranial EEG data in the time domain. Specifically, we consider convolutional neural networks (CNNs) in a subject independent fashion and demonstrate that meaningful features, representing IEDs are automatically learned. The resulting model achieves state of the art classification performance, provides insights for the different types of IEDs within the group, and is invariant to time differences between the IEDs. This study suggests that automatic feature generation via deep learning is suitable for IEDs and EEG in general.", 
Deep learning performs better for big and varied datasets than classic analysis and machine classification methods. In future it will be used for more healthcare areas to improve the quality of diagnosis. Deep learning algorithms try to develop the model by using all the available input.,Deep learning for healthcare applications based on physiological signals-A review,"We have cast the net into the ocean of knowledge to retrieve the latest scientific research on deep learning methods for physiological signals. We found 53 research papers on this topic, published from 01.01.2008 to 31.12.2017. An initial bibliometric analysis shows that the reviewed papers focused on Electromyogram(EMG), Electroencephalogram(EEG), Electrocardiogram(ECG), and Electrooculogram(EOG). These four categories were used to structure the subsequent content review. During the content review, we understood that deep learning performs better for big and varied datasets than classic analysis and machine classification methods. Deep learning algorithms try to develop the model by using all the available input. This review paper depicts the application of various deep learning algorithms used till recently, but in future it will be used for more healthcare areas to improve the quality of diagnosis.", 
"Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data.","Deep learning for healthcare review, opportunities and challenges","Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.", 
"Most of death causes are related to cardiovascular disease. There are several anomalies afflicting the heart beat, for instance heart murmur or artefact. We propose a method for heart disease detection.",Deep learning for heart disease detection through cardiac sounds,"Most of death causes are related to cardiovascular disease. In fact, there are several anomalies afflicting the heart beat, for instance heart murmur or artefact. We propose a method for heart disease detection. By gathering a set of feature obtainable directly from cardiac sounds, we consider this feature vector as input for a deep neural network to discriminate whether a cardiac sound is belonging to an healthy or to a patient with a cardiac disease. The experiment we performed demonstrated the effectiveness of the proposed approach in real-world environment.", 
"Brain-Computer Interface (BCI) refers to procedures that link the central nervous system to a device. BCI was historically performed using Electroencephalography (EEG) In the last years, encouraging results were obtained by combining EEG with other neuroimaging technologies, such as functional Near Infrared Spectroscopy (fNIRS) A crucial step of BCI is brain state classification from recorded signal features. We investigated the capabilities of combining EEG and fNIRS recordings with state-of-the-art Deep Learning procedures.",Deep Learning for hybrid EEG-fNIRS Brain-Computer Interface application to Motor Imagery Classification,"Brain-Computer Interface (BCI) refers to procedures that link the central nervous system to a device. BCI was historically performed using Electroencephalography (EEG). In the last years, encouraging results were obtained by combining EEG with other neuroimaging technologies, such as functional Near Infrared Spectroscopy (fNIRS). A crucial step of BCI is brain state classification from recorded signal features. Deep Artificial Neural Networks (DNNs) recently reached unprecedented complex classification outcomes. These performances were achieved through increased computational power, efficient learning algorithms, valuable activation functions, and restricted or back-fed neurons connections. By expecting significant overall BCI performances, we investigated the capabilities of combining EEG and fNIRS recordings with state-of-the-art Deep Learning procedures. Approach. We performed a guided Left and Right Hand Motor Imagery task on 15 subjects with a fixed classification response time of 1 second and overall experiment length of 10 minutes. Left vs. Right classification accuracy of a DNN in the multi-modal recording modality was estimated and it was compared to standalone EEG and fNIRS and other classifiers. Main Results. At a group level we obtained significant increase in performance when considering multi-modal recordings and DNN classifier with synergistic effect. Significance. BCI performances can be significantly improved by employing multi-modal recordings that provide electrical and hemodynamic brain activity information, in combination with advanced non-linear Deep Learning classification procedures.", 
"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process. This paper provides an overview on using a class of advanced machine learning techniques, namely Deep Learning (DL), to facilitate the analytics and learning in the IoT domain.",Deep Learning for IoT Big Data and Streaming Analytics A survey,"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely Deep Learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.", 
"Healthcare sector is on high priority sector and people expect highest level of care and services regardless of cost. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, complexity of the image. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification.","Deep Learning for Medical Image Processing Overview, Challenges and Future","Healthcare sector is totally different from other industry. It is on high priority sector and people expect highest level of care and services regardless of cost. It did not achieve social expectation even though it consume huge percentage of budget. Mostly the interpretations of medical data is being done by medical expert. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, complexity of the image, extensive variations exist across different interpreters, and fatigue. After the success of deep learning in other real world application, it is also providing exciting solutions with good accuracy for medical imaging and is seen as a key method for future applications in health sector. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification. In the last section, we have discussed the challenges deep learning based methods for medical imaging and open research issue.",  
Motor Imagery (MI) EEG has attracted us due to its significant applications in daily life. Deep neural networks build robust and automated systems for the classification of MI EEG recordings by exploiting the whole input data. Convolutional neural networks (CNN) and hybrid-CNN (h-CNN) are the dominant architectures with high performance.,Deep learning for motor imagery EEG-based classification A review,"The availability of large and varied Electroencephalogram (EEG) datasets, rapidly advances and inventions in deep learning techniques, and highly powerful and diversified computing systems have all permitted to easily analyzing those datasets and discovering vital information within. However, the classification process of EEG signals and discovering vital information should be robust, automatic, and with high accuracy. Motor Imagery (MI) EEG has attracted us due to its significant applications in daily life. Methods: This paper attempts to achieve those goals throughout a systematic review of the state-of-the-art studies within this field of research. The process began by intensely surfing the well-known specialized digital libraries and, as a result, 40 related papers were gathered. The papers were scrutinized upon multiple noteworthy technical issues, among them deep neural network architecture, input formulation, number of MI EEG tasks, and frequency range of interest. Outcomes: Deep neural networks build robust and automated systems for the classification of MI EEG recordings by exploiting the whole input data throughout learning salient features. Specifically, convolutional neural networks (CNN) and hybrid-CNN (h-CNN) are the dominant architectures with high performance in comparison to public datasets with other types of architectures. The MI related datasets, input formulation, frequency ranges, and preprocessing and regularization methods were also reviewed. Inferences: This review gives the required preliminaries in developing MI EEG-based BCI systems. The review process of the published articles in the last five years aims to help in choosing the appropriate deep neural network architecture and other hyperparameters for developing those systems.", 
"Deep Learning (DL) techniques for Natural Language Processing have been evolving remarkably fast. The potential of DL in Software Engineering cannot be overlooked, especially in the field of program learning. We present the state-of-the-art practices and discuss their challenges.","Deep Learning for Source Code Modeling and Generation Models, Applications, and Challenges","Deep Learning (DL) techniques for Natural Language Processing have been evolving remarkably fast. Recently, the DL advances in language modeling, machine translation, and paragraph understanding are so prominent that the potential of DL in Software Engineering cannot be overlooked, especially in the field of program learning. To facilitate further research and applications of DL in this field, we provide a comprehensive review to categorize and investigate existing DL methods for source code modeling and generation. To address the limitations of the traditional source code models, we formulate common program learning tasks under an encoder-decoder framework. After that, we introduce recent DL mechanisms suitable to solve such problems. Then, we present the state-of-the-art practices and discuss their challenges with some recommendations for practitioners and researchers as well.", 
Researchers develop the first visual object classifier driven by human brain signals. They use a 128-channel EEG with active electrodes to record brain activity of several subjects. The proposed RNN-based approach for discriminating object classes using brain signals reaches an average accuracy of about 83%. It obtains competitive performance comparable to those achieved by powerful CNN models.,Deep Learning Human Mind for Automated Visual Classification,"What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories in a reading the mind effort. Afterward, we transfer the learned capabilities to machines by training a Convolutional Neural Network (CNN)–based regressor to project images onto the learned manifold, thus allowing machines to employ human brain–based features for automated visual classification. We use a 128-channel EEG with active electrodes to record brain activity of several subjects while looking at images of 40 ImageNet object classes. The proposed RNN-based approach for discriminating object classes using brain signals reaches an average accuracy of about 83%, which greatly outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain–driven approach obtains competitive performance, comparable to those achieved by powerful CNN models and it is also able to generalize over different visual datasets.", 
"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. This overview provides technical and historical pointers to the field. It surveys current applications of deep learning to biomedical data organized around five subareas.",Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.", 
"This Perspective provides examples of current and future applications of deep learning in pharmacogenomics. Deep learning encapsulates a family of machine learning algorithms that has transformed many important subfields of artificial intelligence over the last decade. In the future, deep learning will be widely used to predict personalized drug response.",Deep learning in pharmacogenomics from gene regulation to patient stratification,"This Perspective provides examples of current and future applications of deep learning in pharmacogenomics, including: identification of novel regulatory variants located in noncoding domains of the genome and their function as applied to pharmacoepigenomics; patient stratification from medical records; and the mechanistic prediction of drug response, targets and their interactions. Deep learning encapsulates a family of machine learning algorithms that has transformed many important subfields of artificial intelligence over the last decade, and has demonstrated breakthrough performance improvements on a wide range of tasks in biomedicine. We anticipate that in the future, deep learning will be widely used to predict personalized drug response and optimize medication selection and dosing, using knowledge extracted from large and complex molecular, epidemiological, clinical and demographic datasets.", 
Deep Learning (DL) has proved its high potential in 2D medical imaging analysis. physiological data in the form of 1D signals have yet to be exploited from this novel approach to fulfil the desired medical tasks. In this paper we survey the latest scientific research on deep learning in physiological signal data such as electromyogram (EMG) and electrocardiogram (ECG).,Deep Learning in Physiological Signal Data-A Survey,"Deep Learning (DL), a successful promising approach for discriminative and generative tasks, has recently proved its high potential in 2D medical imaging analysis; however, physiological data in the form of 1D signals have yet to be beneficially exploited from this novel approach to fulfil the desired medical tasks. Therefore, in this paper we survey the latest scientific research on deep learning in physiological signal data such as electromyogram (EMG), electrocardiogram (ECG), electroencephalogram (EEG), and electrooculogram (EOG). We found 147 papers published between January 2018 and October 2019 inclusive from various journals and publishers. The objective of this paper is to conduct a detailed study to comprehend, categorize, and compare the key parameters of the deep?learning approaches that have been used in physiological signal analysis for various medical applications. The key parameters of deep?learning approach that we review are the input data type, deep?learning task, deep?learning model, training architecture, and dataset sources. Those are the main key parameters that affect system performance. We taxonomize the research works using deep?learning method in physiological signal analysis based on: (1) physiological signal data perspective, such as data modality and medical application; and (2) deep? learning concept perspective such as training architecture and dataset sources.", 
"Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks. They have often matched or exceeded human performance. Since radiology relies on extracting useful information from images, it is a natural application area for deep learning. In this article, we review the clinical reality of radiology and discuss the opportunities for application of deep learning algorithms.",Deep learning in radiology an overview of the concepts and a survey of the state of the art,"Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mostly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we review the clinical reality of radiology and discuss the opportunities for application of deep learning algorithms. We also introduce basic concepts of deep learning including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review the broad range of utilized deep learning algorithms. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future.", 
This paper addresses and tries to solve the problem of extractive text summarization. It works by selecting a subset of phrases or sentences from the original document(s) to form a summary. Selections are done based on certain criteria which formulates a feature set. Multilayer ELM (Extreme Learning Machine) is trained over this feature set.,Deep Learning in the Domain of Multi-Document Text Summarization,Text summarization is the process of generating a shorter version of the input text which captures its most important information. This paper addresses and tries to solve the problem of extractive text summarization which works by selecting a subset of phrases or sentences from the original document(s) to form a summary. Selections of such sentences are done based on certain criteria which formulates a feature set. Multilayer ELM (Extreme Learning Machine) which is based on the underlying deep network architecture is trained over this feature set to classify the sentences as important or unimportant. The used approach is unique and highlights the effectiveness of Multilayer ELM and its stability for usage in the domain of text summarization. Effectiveness of Multilayer ELM is justified by the experimental results on DUC and TAC datasets wherein it significantly outperforms the other well known classifiers., 
"Cardiovascular disease is one of the most important diseases that endanger human health at present. It is very meaningful to diagnose and treat cardiovascular disease by means of in-depth learning. This paper outlines the development and causes of cardiovascular diseases, then describes several theoretical models of deep learning.",Deep Learning Methods for Cardiovascular Diseases,"In the medical field, the analysis and processing of medical images plays an important auxiliary role in the diagnosis of diseases. In recent years, more and more researchers have begun to pay attention to such processing technologies as pattern recognition, classification and segmentation in medical image processing. Cardiovascular disease is one of the most important diseases that endanger human health at present. It is very meaningful to diagnose and treat cardiovascular disease by means of in-depth learning. In order to make deep learning better applied to cardiovascular diseases, this paper first outlines the development and causes of cardiovascular diseases, then describes several theoretical models of deep learning, and then summarizes the application of deep learning in heart image segmentation, classification and other aspects combined with existing technologies. Finally, the future direction of development is prospected.", 
Alternative splicing (AS) is a fundamental step in mRNA maturation and gene expression. Advances in RNA sequencing technologies have shed light on the role of AS in increasing protein isoform diversity. AS is recognized to be involved in the regulation of both physiological and pathological functions.,Deep Learning Models Based on Distributed Feature Representations for Alternative Splicing Prediction,"Alternative splicing (AS) is a fundamental step in mRNA maturation and gene expression. The advancement in RNA sequencing technologies has shed light on the role of AS in increasing protein isoform diversity. AS is recognized to be involved in the regulation of both physiological and pathological functions, hence it is an essential part of the study of gene regulation development and diseases. With the recent advances in machine learning, there is an interest in developing accurate deep learning based computational models for AS prediction. In this paper, we propose a convolutional neural network and multilayer perceptron models to tackle the AS prediction task as classification and regression. These models use feature representations learned from genomic data and cellular context. Unlike previous works which use hand-crafted feature extraction, we propose an automatic feature learning approach to avoid explicit and predefined feature extraction. The proposed approach is based on the adaptation of two extensively used natural language processing techniques, namely word2vec and doc2vec. In order to understand the effects of different representation learning techniques, many experiments have been conducted to predict AS based on the cassette exons and cell type. Overall, experimental results on five tissues data set prove that learning features from genome sequence add a significant improvement to AS outcome prediction in both classification and regression tasks.", 
"Software Engineering (SE) tasks such as clone detection, impact analysis, refactoring, etc. rely on manually defined or hand-crafted features to assess code similarity. Deep Learning (DL) can e?ectively replace manual feature engineering for the task of clone detection. DL-based approach can automatically learn code similarities from different representations.",Deep Learning Similarities from Different Representations of Source Code,"Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can e?ectively replace manual feature engineering for the task of clone detection. However, source code can be represented at di?erent levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a di?erent, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from di?erent representations.", 
"Electroencephalogram (EEG) can track the brain waves which contain the neural activity of the brain. In the era of Artificial Intelligence (AI), machine learning algorithms were useful in brain disorder detection and classification. Different deep learning methods, using varied architecture in EEG signal analysis, offer an understanding to develop the next level of AI-based systems.",Deep Learning Techniques for EEG Signal Applications – a  Review,"Electroencephalogram (EEG) can track the brain waves which contain the neural activity of the brain. EEG signals help to understand the physiological and functional details and activities of the brain. In the era of Artificial Intelligence (AI), machine learning algorithms were useful in brain disorder detection and classification. Recently, a rapid increase in using Deep Learning (DL) methods in various applications in EEG signals not only helps in the detection of brain disorders but also facilitates the recognition of human emotions and various psycho-neuro disorders. In order to offer a beneficial and broad perspective, a detailed survey on the application of deep learning architecture in EEG signals has been carried out in this paper. Different deep learning methods, using varied architecture in EEG signal analysis, offer an understanding to develop the next level of AI-based systems. This review will provide information about how deep learning methods are used in EEG signals and the challenges and limitations of each method in classification; moreover making it helpful for those who are exploring EEG signals using DL algorithms.", 
This paper presents a first approximation using a single channel and information from the current epoch to perform the classification. Deep learning has been applied to the time and frequency domains from the EEG signal obtaining a good performance and promising further work.,Deep Learning Using EEG Data in Time and Frequency Domains for Sleep Stage Classification,"Polysomnography analysis for sleeping disorders is a discipline that is showing interest in the development of reliable classifiers to determine the sleep stage. The most common methods shown in the literature bet for classical learning techniques and statistics that are applied to a reduced number of features in order to tackle the computational load. Nowadays, the application of deep learning to the sleep stage classification problem seems very interesting and novel, therefore, this paper presents a first approximation using a single channel and information from the current epoch to perform the classification. The complete Physionet database has been used in the experiments. Deep learning has been applied to the time and frequency domains from the EEG signal obtaining a good performance and promising further work.", 
"Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning. There is increasing interest in using deep ConvNet for end-To-end EEG analysis, but it's hard to design and train them. Study shows how to train ConvN nets to decode task-related information from the raw EEG without handcrafted features.",Deep Learning With Convolutional Neural Networks for EEG Decoding and Visualization,"Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, that is, learning from the raw data. There is increasing interest in using deep ConvNets for end-to-end EEG analysis, but a better understanding of how to design and train ConvNets for end-to-end EEG decoding and how to visualize the informative EEG features the ConvNets learn is still needed. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed tasks from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching at least as good performance as the widely used filter bank common spatial patterns (FBCSP) algorithm (mean decoding accuracies 82.1% FBCSP, 84.0% deep ConvNets). While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta, and high gamma frequencies, and proved useful for spatially mapping the learned features by revealing the topography of the causal contributions of features in different frequency bands to the decoding decision. Our study thus shows how to design and train ConvNets to decode task-related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG-based brain mapping.", 
"Face verification is the task of determining by analyzing face images, whether a person is who he/she claims to be. Until now, many metric learning algorithms have been proposed, but they are usually limited to learning a linear transformation. In this brief, we propose a nonlinear metric learning method, which learns an explicit mapping from the original space to an optimal subspace.",Deep Nonlinear Metric Learning with Independent Subspace Analysis for Face Verification,"Face verification is the task of determining by analyzing face images, whether a person is who he/she claims to be. It is a very challenge problem, due to large variations in lighting, background, expression, hairstyle and occlusion. The crucial problem is to compute the similarity of two face vectors. Metric learning has provides a viable solution to this problem. Until now, many metric learning algorithms have been proposed, but they are usually limited to learning a linear transformation (i.e. finding a global Mahalanobis metric). In this brief, we propose a nonlinear metric learning method, which learns an explicit mapping from the original space to an optimal subspace, using deep Independent Subspace Analysis network. Compared to kernel methods, which can also learn nonlinear transformations, our method is a deep and local learning architecture, and therefore exhibits more powerful ability to learn the nature of highly variable dataset. We evaluate our method on the LFW benchmark, and results show very comparable performance to the state-of-art methods (achieving 92.28% accuracy), while maintaining simplicity and good generalization ability.", 
We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN) Latent structure information.,Deep Recurrent Generative Decoder for Abstractive Text Summarization,We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods., 
"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set.",Deep Residual Learning for Image Recognition,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", 
Deep Sliding Shapes is a ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input. The algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200× faster than the original.,Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images,"We focus on the task of amodal 3D object detection in RGB-D images, which aims to produce a 3D bounding box of an object in metric form at its full extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input and outputs 3D object bounding boxes. In our approach, we propose the first 3D Region Proposal Network (RPN) to learn objectness from geometric shapes and the first joint Object Recognition Network (ORN) to extract geometric features in 3D and color features in 2D. In particular, we handle objects of various sizes by training an amodal RPN at two different scales and an ORN to regress 3D bounding boxes. Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200× faster than the original Sliding Shapes.", 
"Deep learning techniques are language agnostic and hence can overcome these shortcomings. In this paper, Generative Adversarial Networks(GAN(s) are assimilated to create gist for longer piece of text in conjunction to paraphrase detection.",Deep Text Summarization using Generative Adversarial Networks in Indian Languages,"Abstractive Text Summarization (ATS) is a task of capturing information from different sources and condense it such that, content is represented well and there is no loss of information. It has been an active area of research for quiet sometime now. ATS is more closer to human generated summaries and have the capability of representing and combining multiple information. With advent of deep learning architectures, many tasks relating to natural language processing have achieved persistent and comparable high performances. It has proven advantageous and showed promising results in machine translation, speech recognition, image captioning and many others using sequence to sequence models. Language tools such as Part of Speech taggers, Named Entity Recognizer for Indian languages are not very competitive and hence, language specific techniques do not perform very well for Indian languages. Deep learning techniques are language agnostic and hence can overcome these shortcomings. In this paper, Generative Adversarial Networks(GAN(s)) are assimilated to create gist for longer piece of text in conjunction to paraphrase detection.", 
Deep neural networks face difficulties when attempting to train on small text datasets. Transfer learning is a potential solution but their effectiveness in the text domain is not as explored as in areas such as image analysis. We propose a reinforcement learning framework based on a self-critic policy gradient approach.,Deep Transfer Reinforcement Learning for Text Summarization,"Deep neural networks are data hungry models and thus face difficulties when attempting to train on small text datasets. Transfer learning is a potential solution but their effectiveness in the text domain is not as explored as in areas such as image analysis. In this paper, we study the problem of transfer learning for text summarization and discuss why existing state-of-the-art models fail to generalize well on other (unseen) datasets. We propose a reinforcement learning framework based on a self-critic policy gradient approach which achieves good generalization and state-of-the-art results on a variety of datasets. Through an extensive set of experiments, we also show the ability of our proposed framework to fine-tune the text summarization model using only a few training samples. To the best of our knowledge, this is the first work that studies transfer learning in text summarization and provides a generic solution that works well on unseen data.", 
"Deep learning is a cutting-edge machine learning technique for finding patterns in images. It has not yet been applied to prenatal diagnosis of congenital heart disease. We used 685 retrospectively collected echocardiograms from fetuses 18-24 weeks of gestational age. We trained models to distinguish by view between normal hearts, TOF, and HLHS. Using guideline-recommended imaging, deep learning models can significantly improve detection of fetal congenital Heart disease.",Deep-learning models improve on community-level diagnosis for common congenital heart disease lesions,"Prenatal diagnosis of tetralogy of Fallot (TOF) and hypoplastic left heart syndrome (HLHS), two serious congenital heart defects, improves outcomes and can in some cases facilitate in utero interventions1,2. In practice, however, the fetal diagnosis rate for these lesions is only 30-50 percent in community settings3-6. Improving fetal diagnosis of congenital heart disease is therefore critical. Deep learning is a cutting-edge machine learning technique for finding patterns in images but has not yet been applied to prenatal diagnosis of congenital heart disease. Using 685 retrospectively collected echocardiograms from fetuses 18-24 weeks of gestational age from 2000-2018, we trained convolutional and fully-convolutional deep learning models in a supervised manner to (i) identify the five canonical screening views of the fetal heart and (ii) segment cardiac structures to calculate fetal cardiac biometrics. We then trained models to distinguish by view between normal hearts, TOF, and HLHS. In a holdout test set of images, F-score for identification of the five most important fetal cardiac views was 0.95. Binary classification of unannotated cardiac views of normal heart vs. TOF reached an overall sensitivity of 75% and a specificity of 76%, while normal vs. HLHS reached a sensitivity of 100% and specificity of 90%, both well above average diagnostic rates for these lesions3,6. Furthermore, segmentation-based measurements for cardiothoracic ratio (CTR), cardiac axis (CA), and ventricular fractional area change (FAC) were compatible with clinically measured metrics for normal, TOF, and HLHS hearts. Thus, using guideline-recommended imaging, deep learning models can significantly improve detection of fetal congenital heart disease compared to the common standard of care.", 
DeePM is a latent graphical model based on the state-of-the-art R-CNN framework. It learns an explicit representation of the object-part configuration with flexible type sharing. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012.,DEEPM A DEEP PART-BASED MODEL FOR OBJECT DETECTION AND SEMANTIC PART LOCALIZATION,"In this paper, we propose a deep part-based model (DeePM) for symbiotic object detection and semantic part localization. For this purpose, we annotate semantic parts for all 20 object categories on the PASCAL VOC 2012 dataset, which provides information on object pose, occlusion, viewpoint and functionality. DeePM is a latent graphical model based on the state-of-the-art R-CNN framework, which learns an explicit representation of the object-part configuration with flexible type sharing (e.g., a sideview horse head can be shared by a fully-visible sideview horse and a highly truncated sideview horse with head and neck only). For comparison, we also present an end-to-end Object-Part (OP) R-CNN which learns an implicit feature representation for jointly mapping an image ROI to the object and part bounding boxes. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in detecting objects and parts. In addition, it obtains superior performance to Fast and Faster R-CNNs in object detection.", 
"A vehicle-to-infrastructure (V2I) connected vehicle system was installed along Redwood Road in Salt Lake City, Utah, in November 2017. Dedicated short-range communication (DSRC) radios were used to connect transit buses to traffic signals. The goal of the system was to improve the schedule reliability of the bus.",Demonstrating Transit Schedule Benefits with a Dedicated Short-Range Communication-Based Connected Vehicle System,"A vehicle-to-infrastructure (V2I) connected vehicle system was installed along Redwood Road in Salt Lake City, Utah, United States, in November 2017 using dedicated short-range communication (DSRC) radios to connect transit buses to traffic signals. One of the goals of this system was to improve the schedule reliability of the bus by providing signal priority at traffic signals when the bus is behind its published schedule by a certain threshold. Data for the analysis were obtained from the DSRC communications, the Automated Traffic Signal Performance Measures (ATSPM) system, and the transit operations system. The robust data available from these three systems allow for detailed analysis of priority requests made, requests served, and bus on-time performance in a way that is not possible without these data sets. By comparing actual schedules of the four DSRC-equipped buses over a 4-month period from April to July 2018 with buses which do not have the ability to request signal priority, it has been determined that the equipped buses meet their published schedule about 2% to 6% more frequently, depending on direction and time of day, with the most significant improvement of 6% in the southbound PM peak.", 
"The dense captioning task generalizes object detection and Image Captioning. We propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images.",DenseCap Fully Convolutional Localization Networks for Dense Captioning,"We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.", 
"We propose a method for recognizing attributes, such as the gender, hair style and types of clothes of people. We use a part-based approach based on poselets. We train attribute classifiers for each such aspect and combine them together in a discriminative model.",Describing People A Poselet-Based Approach to Attribute Classification,"We propose a method for recognizing attributes, such as the gender, hair style and types of clothes of people under large variation in viewpoint, pose, articulation and occlusion typical of personal photo album images. Robust attribute classifiers under such conditions must be invariant to pose, but inferring the pose in itself is a challenging problem. We use a part-based approach based on poselets. Our parts implicitly decompose the aspect (the pose and viewpoint). We train attribute classifiers for each such aspect and we combine them together in a discriminative model. We propose a new dataset of 8000 people with annotated attributes. Our method performs very well on this dataset, significantly outperforming a baseline built on the spatial pyramid match kernel method. On gender recognition we outperform a commercial face recognition system.", 
Convolutional neural networks outperform previous descriptors on classification tasks by a large margin. These networks still work well when they are applied to datasets or recognition tasks different from those they were trained on.,Descriptor Matching with Convolutional Neural Networks a Comparison to SIFT,"Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching.", 
"Code clones detection is becoming a hot issue as the number of web and desktop based applications are increasing. Cloning increases the risk of software maintenance and increases the complexity as well. In this paper, a novel method has been proposed using Levenshtein Distance method.",Design and Analysis of a Levenshtein Distance Based Code Clones Detection Algorithm,"Code clones detection is becoming a hot issue as the number of web and desktop based applications are increasing day to day. Reusing the existing modules with or without little change can result in code clones while developing software. The clones can be categorized as type-1, type-2, type-3 and type-4.The significance of cloning is that it increases the risk of software maintenance and increases the complexity as well. By clone detection and re-factorization, the maintenance process can be made easy. Various techniques have been developed in recent years and can be based on string matching, token-based, semantic-based, tree-based etc. In this paper, a novel method has been proposed using Levenshtein Distance method and type-1 clones have been detected. A tool named JB Clone Scanner is developed in order to implement and validate the proposed technique.", 
This paper introduces an application research on taxi calling and dispatching system (TCnS) It is a prototype of LBS application based on GPS mobile phone.,Design and Implementation of Taxi Calling and Dispatching System based on GPS Mobile Phone,"This paper introduces an application research on taxi calling and dispatching system (TCnS in short), which is a prototype of LBS application based on GPS mobile phone. The paper gives the basic design, modules division, GIS display algorithm design, test results and feature analysis of TCnS prototype. Finally, the paper points out the further improvement direction of TCnS, and prompts its usage in future.", 
"An optimum allocation based principal component analysis method named as OA PCA is developed for the feature extraction from epileptic EEG data. LS-SVM 1v1 approach yields 100% of the overall classification accuracy (OCA), improving up to 7.10% over the existing algorithms for the epileptic brain data.",Designing a robust feature extraction method based on optimum allocation and principal component analysis for epileptic EEG signal classification,"The aim of this study is to design a robust feature extraction method for the classification of multiclass EEG signals to determine valuable features from original epileptic EEG data and to discover an efficient classifier for the features. An optimum allocation based principal component analysis method named as OA PCA is developed for the feature extraction from epileptic EEG data. As EEG data from different channels are correlated and huge in number, the optimum allocation (OA) scheme is used to discover the most favorable representatives with minimal variability from a large number of EEG data. The principal component analysis (PCA) is applied to construct uncorrelated components and also to reduce the dimensionality of the OA samples for an enhanced recognition. In order to choose a suitable classifier for the OA PCA feature set, four popular classifiers: least square support vector machine (LS-SVM), naive bayes classifier (NB), k-nearest neighbor algorithm (KNN), and linear discriminant analysis (LDA) are applied and tested. Furthermore, our approaches are also compared with some recent research work. The experimental results show that the LS-SVM 1v1 approach yields 100% of the overall classification accuracy (OCA), improving up to 7.10% over the existing algorithms for the epileptic EEG data. The major finding of this research is that the LS-SVM with the 1v1 system is the best technique for the OA PCA features in the epileptic EEG signal classification that outperforms all the recent reported existing methods in the literature.", 
"Natural language processing is an integration of artificial intelligence, computer science and computer linguistics. The research towards natural Language Processing is focused on creating innovations towards creating the devices or machines which operates basing on the single command of a human.",Design of optimal search engine using text summarization through artificial intelligence techniques,"Natural language processing is the trending topic in the latest research areas, which allows the developers to create the human-computer interactions to come into existence. The natural language processing is an integration of artificial intelligence, computer science and computer linguistics. The research towards natural Language Processing is focused on creating innovations towards creating the devices or machines which operates basing on the single command of a human. It allows various Bot creations to innovate the instructions from the mobile devices to control the physical devices by allowing the speech-tagging. In our paper, we design a search engine which not only displays the data according to user query but also performs the detailed display of the content or topic user is interested for using the summarization concept. We find the designed search engine is having optimal response time for the user queries by analyzing with number of transactions as inputs. Also, the result findings in the performance analysis show that the text summarization method has been an efficient way for improving the response time in the search engine optimizations.", 
Code smells are symptoms of poor design and implementation choices weighing heavily on the quality of produced source code. During the last decades several code smell detection tools have been proposed. The literature shows that the results of these tools can be subjective and are intrinsically tied to the nature and approach of the detection.,Detecting Code Smells using Machine Learning Techniques Are We There Yet,"Code smells are symptoms of poor design and implementation choices weighing heavily on the quality of produced source code. During the last decades several code smell detection tools have been proposed. However, the literature shows that the results of these tools can be subjective and are intrinsically tied to the nature and approach of the detection. In a recent work Arcelli Fontana et al. proposed the use of Machine-Learning (ML) techniques for code smell detection, possibly solving the issue of tool subjectivity giving to a learner the ability to discern between smelly and non-smelly source code elements. While this work opened a new perspective for code smell detection, in the context of our research we found a number of possible limitations that might threaten the results of this study. The most important issue is related to the metric distribution of smelly instances in the used dataset, which is strongly different than the one of non-smelly instances. In this work, we investigate this issue and our findings show that the high performance achieved in the study by Arcelli Fontana et al. was in fact due to the specific dataset employed rather than the actual capabilities of machine-learning techniques for code smell detection.", 
"Software Defined Networking (SDN) involves specific security problems, especially if its controller is defenseless against DDoS attacks. In this study, attacks in SDN were detected using machine learning-based models. Feature selection methods were preferred to simplify the models and provide a shorter training time. The test results showed that the use of the wrapper feature selection with a KNN classifier achieved the highest accuracy rate (98.3%) in DDoS attack detection.",Detecting DDoS Attacks in Software-Defined Networks Through Feature Selection Methods and Machine Learning Models,"Software Defined Networking (SDN) offers several advantages such as manageability, scaling, and improved performance. However, SDN involves specific security problems, especially if its controller is defenseless against Distributed Denial of Service (DDoS) attacks. The process and communication capacity of the controller is overloaded when DDoS attacks occur against the SDN controller. Consequently, as a result of the unnecessary flow produced by the controller for the attack packets, the capacity of the switch flow table becomes full, leading the network performance to decline to a critical threshold. In this study, DDoS attacks in SDN were detected using machine learning-based models. First, specific features were obtained from SDN for the dataset in normal conditions and under DDoS attack traffic. Then, a new dataset was created using feature selection methods on the existing dataset. Feature selection methods were preferred to simplify the models, facilitate their interpretation, and provide a shorter training time. Both datasets, created with and without feature selection methods, were trained and tested with Support Vector Machine (SVM), Naive Bayes (NB), Artificial Neural Network (ANN), and K-Nearest Neighbors (KNN) classification models. The test results showed that the use of the wrapper feature selection with a KNN classifier achieved the highest accuracy rate (98.3%) in DDoS attack detection. The results suggest that machine learning and feature selection algorithms can achieve better results in the detection of DDoS attacks in SDN with promising reductions in processing loads and times.", 
This work presents a solution for fatigue recognition through a new deep learning model. It uses a characteristic input of the power spectrum of an electroencephalogram (EEG) signal.,Detecting Fatigue Status of Pilots Based on Deep Learning Network Using EEG Signals,"This work presents a solution for fatigue recognition through a new deep learning model that has a characteristic input of the power spectrum of an electroencephalogram (EEG) signal. Firstly, four rhythms are obtained through the designed FIR filters, and the curve areas of their power spectrum density are coupled into four fatigue indicators. Secondly, a deep sparse contractive autoencoder network is proposed to learn more local fatigue characteristics, and the recognition results of pilots mental fatigue status are given. Compared with the state-of-the-art models, the results show that our model has good learning performance in extracting local features and fatigue status detection.", 
Chamfer Matching builds upon the basic Chamfer method to address issues with occlusions. It combines results from sub-polygon templates using voting and machine learning.,Detecting Guns Using Parametric Edge Matching,"Weapon detection is a difficult problem with numerous applications, particularly in the world of airport security. We present an attempt to identify pistols in x-ray images using Chamfer Matching. Our approach builds upon the basic Chamfer method to address issues with occlusions by combining results from sub-polygon templates using voting and machine learning. Experiments show that the underlying Chamfer method does not produce results with significant accuracy to replace airport security personnel.", 
"In Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization, sometimes to the point that human vision often fails to recognize them. In this paper, we evaluate existing object detection methods on these abstract renditions of objects, comparing human annotators to four state-of-the-art object detectors on a corpus of Picasso paintings.",Detecting People in Cubist Art,"Although the human visual system is surprisingly robust to extreme distortion when recognizing objects, most evaluations of computer object detection methods focus only on robustness to natural form deformations such as people’s pose changes. To determine whether algorithms truly mirror the flexibility of human vision, they must be compared against human vision at its limits. For example, in Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization, sometimes to the point that human vision often fails to recognize them. In this paper, we evaluate existing object detection methods on these abstract renditions of objects, comparing human annotators to four state-of-the-art object detectors on a corpus of Picasso paintings. Our results demonstrate that while human perception significantly outperforms current methods, human perception and part-based models exhibit a similarly graceful degradation in object detection performance as the objects become increasingly abstract and fragmented, corroborating the theory of part-based object representation in the brain.", 
Intelligent transport systems need to use all possible information coming from vehicles and infrastructure. Data are sometimes unreliable due to source and communication network quality. Each vehicle computes how confident it is about a potential dangerous event using both local and remote data. The method mixes measurements from vehicle onboard sensors and wireless sensors placed close to the road.,Detecting Road Events Using Distributed Data Fusion Experimental Evaluation for the Icy Roads Case,"One of the main ideas in the area of intelligent transport systems is to use all possible information coming from vehicles and infrastructure, in order to make the system “smarter” and avoid potentially dangerous situations—collisions, accidents, bottleneck, etc. However, data are sometimes unreliable due to source and communication network quality, leading vehicles or even the whole system to wrong decisions. We present a generic method for detecting dangerous events on the road. To support unreliable data sources, it uses distributed data fusion. Moreover, to deal with network failures, it relies on a self-stabilizing generic distributed algorithm. Our method mixes measurements obtained from vehicle onboard sensors, as well as wireless sensors placed close to the road and connected to road side units. Each vehicle computes how confident it is about a potential dangerous event using both local and remote data. To evaluate our approach, we implemented it using a specific hardware and software platform. Moreover, we instantiated a simple, yet efficient application to detect icy roads, based on temperature measurements. Thanks to both in-lab and actual on-the-road experiments, we demonstrate the possibility to deduce proper results from unreliable data and, consequently, the correctness and usefulness of our approach.", 
The electroencephalogram (EEG) signal plays a key role in the diagnosis of epilepsy. This study describes an automated classification of EEG signal for the detection of Epilepsy disease using soft computing methods. The average classification rate of the proposed EEG signal classification system is 99.4%.,Detection and classification of electroencephalogram signals for epilepsy disease using machine learning methods,"The electroencephalogram (EEG) signal plays a key role in the diagnosis of epilepsy. This study describes an automated classification of EEG signal for the detection of Epilepsy disease using soft computing methods. The proposed method is comprised of three modules: (a) transformation, (b) feature computation, and (c) feature classifications. In the first module, the nonsubsampled contourlet transform is applied on the EEG signal which decomposes the signal into approximate and directional subbands. The decomposition is done using nonsubsampled pyramid filter bank and nonsubsampled directional filter bank respectively. Secondly, the statistical features are extracted from the decomposed directional subbands using wavelet packet decomposition method. Finally, these features are classified by adaptive neuro-Fuzzy inference system classification method, which classifies the EEG signal into either focal or nonfocal signal. The proposed method is tested on a set of EEG signals for validation. The average classification rate of the proposed EEG signal classification system is 99.4%. The proposed EEG signal classification methodology achieves a sensitivity of 99.7%, a specificity of 99.7%, and an accuracy of 99.4%. The results confirmed that the proposed method has a potential in the classification of EEG signals and thereby could further improve the diagnosis of epilepsy.", 
"VANET is a subset of Mobile Ad-Hoc Networks (MANET) It enables communication between the vehicles (V2V) and vehicles to infrastructure. VANET can be used to coordinate the traffic, improve safety measures, support the drivers for hassle-free driving.",Detection of Denial of Service (DoS) Attacks in VANET using Filters,"Vehicular Ad-Hoc Networks (VANET) are considered as a subset of Mobile Ad-Hoc Networks (MANET). VANET is mainly used for the construction of an intelligent transport system. VANET enables communication between the vehicles (V2V) and vehicles to infrastructure (V2I). VANET can be used to coordinate the traffic, improve safety measures, support the drivers for hassle-free driving. It plays a major role in building smart cities in the near future. VANET is vulnerable to a number of security issues among which the DoS attack is a major part. DoS attack in VANET involves a malicious node flooding a huge amount of traffic using spoofed identities. This, in turn, may disrupt the services of vehicles in the network. The detection of the attack becomes very difficult due to fake identities. The detection scheme uses a cuckoo filter and IP detection technique to detect the attack in the network. Once the attack is detected it generates a broadcast message to all the other vehicles that are present in the network.", 
This paper uses deep learning framework to detect epilepsy in the Electroencephalography (EEG) signal. The proposed Long Short-Term Memory (LSTM) classifier classifies these three kinds of signals with up to 95% accuracy. Double-layered LSTM approach gives the highest accuracy in comparison to previously used Support Vector Machine.,Detection of Epilepsy Seizures in Neo-Natal EEG Using LSTM Architecture,"Epilepsy is the most unpredictable and recurrent disease among neurological diseases. Early detection of epileptic seizures can play a critical role in providing timely treatment to patients especially when a patient is in a remote area. This paper uses deep learning framework to detect epilepsy in the Electroencephalography (EEG) signal. The dataset used is publicly available and has a recording of three kinds of EEG signals: pre-ictal, inter-ictal (seizure-free epileptic) and ictal (epileptic with seizure). The proposed Long Short-Term Memory (LSTM) classifier classifies these three kinds of signals with up to 95% accuracy. For binary classification such as detection of inter-ictal or ictal only, its accuracy increases to 98%. The EEG signal is modelled as wide sense non-stationary random signal. Hurst Exponent and Autoregressive Moving Average (ARMA) features are extracted from each signal. In this work, two different configurations of LSTM architecture: single-layered memory units and double-layered memory units are also modelled. After standardising the features, double-layered LSTM approach gives the highest accuracy in comparison to previously used Support Vector Machine (SVM) classifier and proved to be computationally efficient at Graphics Processing Unit (GPU).", 
"Security experts have demonstrated numerous risks imposed by Internet of Things (IoT) devices on organizations. Organizations require an intelligent mechanism capable of automatically detecting suspicious devices. In this research, Random Forest, a supervised machine learning algorithm, was applied to features extracted from network traffic data. Some IoT device types were identified quicker than others.",Detection of Unauthorized IoT Devices Using Machine Learning Techniques,"Security experts have demonstrated numerous risks imposed by Internet of Things (IoT) devices on organizations. Due to the widespread adoption of such devices, their diversity, standardization obstacles, and inherent mobility, organizations require an intelligent mechanism capable of automatically detecting suspicious IoT devices connected to their networks. In particular, devices not included in a white list of trustworthy IoT device types (allowed to be used within the organizational premises) should be detected. In this research, Random Forest, a supervised machine learning algorithm, was applied to features extracted from network traffic data with the aim of accurately identifying IoT device types from the white list. To train and evaluate multi-class classifiers, we collected and manually labeled network traffic data from 17 distinct IoT devices, representing nine types of IoT devices. Based on the classification of 20 consecutive sessions and the use of majority rule, IoT device types that are not on the white list were correctly detected as unknown in 96% of test cases (on average), and white listed device types were correctly classified by their actual types in 99% of cases. Some IoT device types were identified quicker than others (e.g., sockets and thermostats were successfully detected within five TCP sessions of connecting to the network). Perfect detection of unauthorized IoT device types was achieved upon analyzing 110 consecutive sessions; perfect classification of white listed types required 346 consecutive sessions, 110 of which resulted in 99.49% accuracy. Further experiments demonstrated the successful applicability of classifiers trained in one location and tested on another. In addition, a discussion is provided regarding the resilience of our machine learning-based IoT white listing method to adversarial attacks.", 
Method of Automatic Extractive Summarization (AES) uses features of the sentences to extract the most important information. Position of the sentence is used to determine its relevance. First sentences of the text are more relevant.,Determining the importance of sentence position for automatic text summarization,"The methods of Automatic Extractive Summarization (AES) uses the features of the sentences of the original text to extract the most important information that will be considered in summary. It is known that the first sentences of the text are more relevant than the rest of the text (this heuristic is called baseline), so the position of the sentence (in reverse order) is used to determine its relevance, which means that the last sentences have practically no possibility of being selected. In this paper, we present a way to soften the importance of sentences according to the position. The comprehensive tests were done on one of the best AES methods using the bag of words and n-grams models with the with DUC02 and DUC01 data sets to determine the importance of sentences.", 
"As bugs become prevalent in software development, bug triaging has become one of the most important activities in software maintenance. To decrease the time cost in manual work, text classification techniques have been applied in automatic bug triaged. The approach is based on convolution neural network (CNN) and developer activities.",Developer Activity Motivated Bug Triaging Via Convolutional Neural Network,"As bugs become prevalent in software development, bug triaging has become one of the most important activities in software maintenance. To decrease the time cost in manual work, text classification techniques have been applied in automatic bug triaging. In this paper, we present a new automatic bug triaging approach which is based on convolution neural network (CNN) and developer activities. Firstly, we implement the word vector representation of the text features in bug report by using Word2vec. Then, we combine CNN with batch normalization, pooling and full connection approach to learn from the word vector representation of bug report with known fixers. In addition, we also study the recent activities of the developers which can effectively distinguish similar bug reports and get a more suitable developer recommendation list. We empirically investigate the accuracy of automatic bug triaging on three large open source projects, namely Eclipse, Mozilla and NetBeans. The experimental results show that our approach can effectively improve the performance of automatic bug triaging.", 
Disease prediction can help individuals avoid dangerous health situations before it is too late. Four datasets were utilized to build the model and extract the most significant risks factors. Results showed that the proposed DPM achieved highest accuracy when compared to other models and previous studies.,Development of Disease Prediction Model Based on Ensemble Learning Approach for Diabetes and Hypertension,"Early diseases prediction plays an important role for improving healthcare quality and can help individuals avoid dangerous health situations before it is too late. This paper proposes a disease prediction model (DPM) to provide an early prediction for type 2 diabetes and hypertension based on individual’s risk factors data. The proposed DPM consists of isolation forest (iForest) based outlier detection method to remove outlier data, synthetic minority oversampling technique tomek link (SMOTETomek) to balance data distribution, and ensemble approach to predict the diseases. Four datasets were utilized to build the model and extract the most significant risks factors. The results showed that the proposed DPM achieved highest accuracy when compared to other models and previous studies. We also developed a mobile application to provide the practical application of the proposed DPM. The developed mobile application gathers risk factor data and send it to a remote server, so that an individual’s current condition can be diagnosed with the proposed DPM. The prediction result is then sent back to the mobile application; thus, immediate and appropriate action can be taken to reduce and prevent individual’s risks once unexpected health situations occur (i.e., type 2 diabetes and/or hypertension) at early stages.", 
"Four groups tackle Task 1 - Acoustic Scene Classification (ASC) in the DCASE 2020 Challenge. GT, USTC, Tencent, and UKE present a joint effort of four groups to tackle the task. Task 1 comprises two different sub-tasks: Task 1a focuses on ASC of audio signals recorded with multiple (real and simulated) devices into ten different fine-grained classes.",DEVICE-ROBUST ACOUSTIC SCENE CLASSIFICATION BASED ON TWO-STAGE CATEGORIZATION AND DATA AUGMENTATION,"In this technical report, we present a joint effort of four groups, namely GT, USTC, Tencent, and UKE, to tackle Task 1 - Acoustic Scene Classification (ASC) in the DCASE 2020 Challenge. Task 1 comprises two different sub-tasks: (i) Task 1a focuses on ASC of audio signals recorded with multiple (real and simulated) devices into ten different fine-grained classes, and (ii) Task 1b concerns with classification of data into three higher-level classes using low complexity solutions. For Task 1a, we propose a novel two-stage ASC system leveraging upon ad-hoc score combination of two convolutional neural networks (CNNs), classifying the acoustic input according to three classes, and then ten classes, respectively. Four different CNN-based architectures are explored to implement the two-stage classifiers, and several data augmentation techniques are also investigated. For Task 1b, we leverage upon a quantization method to reduce the complexity of two of our top-accuracy three classes CNN-based architectures. On Task 1a development data set, an ASC accuracy of 76.9% is attained using our best single classifier and data augmentation. An accuracy of 81.9% is then attained by a final model fusion of our two-stage ASC classifiers. On Task 1b development data set, we achieve an accuracy of 96.7% with a model size smaller than 500KB.", 
"This paper shows how to analyze the influences of object characteristics on detection performance. We examine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization error, and confusion with semantically similar objects. We make our software and annotations available, making it easy for future researchers to perform similar analysis.",Diagnosing Error in Object Detectors,"This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives. In particular, we examine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization error, and confusion with semantically similar objects, other labeled objects, and background. We analyze two classes of detectors: the Vedaldi et al. multiple kernel learning detector and different versions of the Felzenszwalb et al. detector. Our study shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error. Our analysis also reveals that many different kinds of improvement are necessary to achieve large gains, making more detailed analysis essential for the progress of recognition research. By making our software and annotations available, we make it effortless for future researchers to perform similar analysis.", 
"Twitter datasets have a significant fraction of posts written in different English dialects. Most summarization algorithms return summaries that under-represent the minority dialect. We propose a framework that takes an existing summarization algorithm as a blackbox and uses a small set of dialect-diverse sentences. In all cases, our approach leads to improved dialect diversity compared to the standard summarization approaches.",Dialect Diversity in Text Summarization on Twitter,"Extractive summarization algorithms can be used on Twitter data to return a set of posts that succinctly capture a topic. However, Twitter datasets have a significant fraction of posts written in different English dialects. We study the dialect bias in the summaries of such datasets generated by common summarization algorithms and observe that, for datasets that have sentences from more than one dialect, most summarization algorithms return summaries that under-represent the minority dialect. To correct for this bias, we propose a framework that takes an existing summarization algorithm as a blackbox and, using a small set of dialect-diverse sentences, returns a summary that is relatively more dialect-diverse. Crucially, our approach does not need the sentences in the dataset to have dialect labels, ensuring that the diversification process is independent of dialect classification and language identification models. We show the efficacy of our approach on Twitter datasets containing posts written in dialects used by different social groups defined by race, region or gender; in all cases, our approach leads to improved dialect diversity compared to the standard summarization approaches.", 
"Some biomedical text summarization systems put the basis of their sentence selection approach on the frequency of concepts extracted from the input text. We show that with the use of an appropriate feature selection approach, the Bayesian summarizer can improve the performance of biomedical summarization. We use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) toolkit to perform evaluations on a corpus of scientific papers.",Different approaches for identifying important concepts in probabilistic biomedical text summarization,"Automatic text summarization tools help users in the biomedical domain to acquire their intended information from various textual resources more efficiently. Some of biomedical text summarization systems put the basis of their sentence selection approach on the frequency of concepts extracted from the input text. However, it seems that exploring other measures rather than the raw frequency for identifying valuable contents within an input document, or considering correlations existing between concepts, may be more useful for this type of summarization. In this paper, we describe a Bayesian summarization method for biomedical text documents. The Bayesian summarizer initially maps the input text to the Unified Medical Language System (UMLS) concepts; then it selects the important ones to be used as classification features. We introduce six different feature selection approaches to identify the most important concepts of the text and select the most informative contents according to the distribution of these concepts. We show that with the use of an appropriate feature selection approach, the Bayesian summarizer can improve the performance of biomedical summarization. Using the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) toolkit, we perform extensive evaluations on a corpus of scientific papers in the biomedical domain. The results show that when the Bayesian summarizer utilizes the feature selection methods that do not use the raw frequency, it can outperform the biomedical summarizers that rely on the frequency of concepts, domain-independent and baseline methods.", 
"In this paper, three similarity measures; Normalized Google Distance (NGD), Jaccard and Cosine Similarity measures were employed and tested for textual based clustering problem. Experimental results showed that all of our proposed methods outperformed the benchmark methods.",Differential evolution cluster-based text summarization methods,"In this paper, three similarity measures; Normalized Google Distance (NGD), Jaccard and Cosine Similarity measures were employed and tested for textual based clustering problem. A robust evolutionary algorithm called Differential Evolution algorithm was also used to optimize the data clustering process and increase the quality of the generated text summaries. The Recall Oriented Under Gisting Evaluation (ROUGE) was used as an evaluation measure toolkit to assess the quality of the summaries. Experimental results showed that all of our proposed methods outperformed the benchmark methods. More importantly, the Jaccard-similarity based method surpassed all the other proposed methods in this study.", 
"Blockchain works over the principle of distributed, secured, and shared ledger. It is used to record, and track data within a decentralized network. This technology has successfully replaced certain systems of .economic transactions in organizations. Data perturbation strategy such as differential privacy could be a novel approach to overcome privacy issues.",Differential privacy in blockchain technology A futuristic approach,"Blockchain has received a widespread attention because of its decentralized, tamper-proof, and transparent nature. Blockchain works over the principle of distributed, secured, and shared ledger, which is used to record, and track data within a decentralized network. This technology has successfully replaced certain systems of .economic transactions in organizations and has the potential to overtake various industrial business models in future. Blockchain works over peer-to-peer (P2P) phenomenon for its operation and does not require any trusted-third party authorization for data tracking and storage. The information stored in blockchain is distributed throughout the decentralized network and is usually protected using cryptographic hash functions. Since the beginning of blockchain technology, its use in different applications is increasing exponentially, but this increased use has also raised some questions regarding privacy and security of data being stored in it. Protecting privacy of blockchain data using data perturbation strategy such as differential privacy could be a novel approach to overcome privacy issues in blockchain. In this article, we cover the topic of integration of differential privacy in each layer of blockchain and in certain blockchain based scenarios. Moreover, we highlight some future challenges and application scenarios in which integration of differential privacy in blockchain can produce fruitful results.", 
Chain of custody is an attempt to preserve the integrity of digital evidence as well as a procedure for performing documentation chronologically toward evidence. Handling chain of custody has become more complicated and complex. A number of researchers have contributed to provide solutions for the digital chain custody.,Digital Chain of Custody State of the Art,"Digital forensics starts to show its role and contribution in the society as a solution in disclosure of cybercrime. The essential in digital forensics is chain of custody, which is an attempt to preserve the integrity of digital evidence as well as a procedure for performing documentation chronologically toward evidence. The characteristics of digital evidence have caused the handling chain of custody is becoming more complicated and complex. A number of researchers have contributed to provide solutions for the digital chain custody through a different point of views. This paper gives an overview of the extent to which the problem and challenges are faced in the digital chain of custody issue as well as the scope of researches that can be done to contribute in the issue of the digital chain of custody.", 
"Chain of custody is the procedure to do a chronological documentation of evidence. Handling the chain of custody for digital evidence is more difficult than the handling of physical evidence. The handling of digital evidence should still have the same procedure with the handling for physical evidence, says the study.",Digital evidence cabinets A proposed framework for handling digital chain of custody,"Chain of custody is the procedure to do a chronological documentation of evidence, and it is an important procedure in the investigation process. Both physical and digital evidence is an important part in the process of investigation and courtroom. However, handling the chain of custody for digital evidence is more difficult than the handling of physical evidence. Nevertheless, the handling of digital evidence should still have the same procedure with the handling of physical evidence. Until now handling the chain of custody for digital evidence is still an open problem with a number of challenges, including the business model of the interaction of the parties that deal with digital evidence, recording of metadata information as well as issues of access control and security for all the handling digital chain of custody. The solution offered in this research is to build a model of Digital Evidence Cabinets as a new approach in implementing the digital evidence handling and chain of custody. The model is constructed through three approaches: Digital Evidence Management Frameworks, Digital Evidence Bags with Tag Cabinets as well as access control and secure communication. The proposed framework is expected to be a solution for the availability of an environment handling of digital evidence and to improve the integrity and credibility of digital evidence.", 
"Forensic software is not tailored to separate accurate results from inaccurate ones, authors argue. Authors: Some percentage of forensic software errors will necessarily have negative effects on parties. They say this dynamic constitutes an inefficiency to be corrected through the proper application of rules.",Digital Evidence Challenging the Presumption of Reliability,"There is a general tendency among courts to presume that forensic software reliably yields accurate digital evidence. As a judicial construct, this presumption is unjustified in that it is not tailored to separate accurate results from inaccurate ones. The authors illustrate this unfortunate truth by the presentation of two currently uncorrected weaknesses in popular computer forensic tools, methods, and assumptions. Some percentage of these forensic software errors (and ones like them) will necessarily have negative effects on parties, whether in terms of faulty criminal convictions or improper civil judgments. The authors argue that the collective value of these negative effects among parties is far larger than the costs of research and development required to prevent such negative effects. Under a purely rational economic approach to the law, this dynamic constitutes an inefficiency to be corrected through the proper application of rules. The authors advance two approaches to cure current defects. One is through the proper application of scientific jurisprudence to questions of digital evidence and the other is through some combination of certain broad market and social corrections.", 
"Law enforcement and digital forensic units must establish and maintain an effective quality assurance system. An acceptable and thorough Digital Forensics process depends on the sequential DF phases. Each case is unique and needs special examination, it is not possible to cover every aspect of crime scene digital forensics, but the proposed procedure model is supposed to be a general guideline for practitioners. The proposed analytical procedure model for digital investigations at a crime scene is developed and defined for crime scene practitioners.",Digital forensics An Analytical Crime Scene Procedure Model (ACSPM),"In order to ensure that digital evidence is collected, preserved, examined, or transferred in a manner safeguarding the accuracy and reliability of the evidence, law enforcement and digital forensic units must establish and maintain an effective quality assurance system. The very first part of this system is standard operating procedures (SOP’s) and/or models, conforming chain of custody requirements, those rely on digital forensics ‘‘process-phase-procedure-task-subtask’’ sequence. An acceptable and thorough Digital Forensics (DF) process depends on the sequential DF phases, and each phase depends on sequential DF procedures, respectively each procedure depends on tasks and subtasks. There are numerous amounts of DF Process Models that define DF phases in the literature, but no DF model that defines the phase-based sequential procedures for crime scene identified. An analytical crime scene procedure model (ACSPM) that we suggest in this paper is supposed to fill in this gap. The proposed analytical procedure model for digital investigations at a crime scene is developed and defined for crime scene practitioners; with main focus on crime scene digital forensic procedures, other than that of whole digital investigation process and phases that ends up in a court. When reviewing the relevant literature and interrogating with the law enforcement agencies, only device based charts specific to a particular device and/or more general perspective approaches to digital evidence management models from crime scene to courts are found. After analyzing the needs of law enforcement organizations and realizing the absence of crime scene digital investigation procedure model for crime scene activities we decided to inspect the relevant literature in an analytical way. The outcome of this inspection is our suggested model explained here, which is supposed to provide guidance for thorough and secure implementation of digital forensic procedures at a crime scene. In digital forensic investigations each case is unique and needs special examination, it is not possible to cover every aspect of crime scene digital forensics, but the proposed procedure model is supposed to be a general guideline for practitioners.", 
"Digital forensics has a number of branches and different parts, and image forensics is one of them. The budget for the images branch goes up every day in response to the need. We offer general information about digital forensics, focusing on images.",Digital Forensics Focusing on Image Forensics,"The world is continuously developing, and people’s needs are increasing as well; so too are the number of thieves increasing, especially electronic thieves. For that reason, companies and individuals are always searching for experts who will protect them from thieves, and these experts are called digital investigators. Digital forensics has a number of branches and different parts, and image forensics is one of them. The budget for the images branch goes up every day in response to the need. In this paper we offer some information about images and image forensics, image components and how they are stored in digital devices and how they can be deleted and recovered. We offer general information about digital forensics, focusing on image forensics.", 
DIGITS is a deep learning training system with a web interface.,DIGITS the Deep learning GPU Training System,DIGITS is a deep learning training system with a web interface. Tools are provided for designing custom network architectures and for rapidly evaluating their effectiveness through various visualizations of training outputs and learned network parameters allowing for rapid prototyping and collaboration., 
"Summarization is one of the key features of human intelligence. With rapid expansion of texts, pictures and videos in cyberspace, automatic summarization becomes more and more desirable. Traditional methods process texts empirically and neglect the fundamental characteristics and principles of language use and understanding.",Dimensionality on Summarization,"Summarization is one of the key features of human intelligence. It plays an important role in understanding and representation. With rapid and continual expansion of texts, pictures and videos in cyberspace, automatic summarization becomes more and more desirable. Text summarization has been studied for over half century, but it is still hard to automatically generate a satisfied summary. Traditional methods process texts empirically and neglect the fundamental characteristics and principles of language use and understanding. This paper summarizes previous text summarization approaches in a multi-dimensional classification space, introduces a multi-dimensional methodology for research and development, unveils the basic characteristics and principles of language use and understanding, investigates some fundamental mechanisms of summarization, studies the dimensions and forms of representations, and proposes a multi-dimensional evaluation mechanisms. Investigation extends to the incorporation of pictures into summary and to the summarization of videos, graphs and pictures, and then reaches a general summarization framework. Further, some basic behaviors of summarization are studied in the complex space consisting of cyberspace, physical space and social space. The basic viewpoints include: (1) a representation suitable for summarization should have a core, indicated by its intention and extension; (2) summarization is an open process of various interactions, involved in various explicit and implicit citations; and, (3) the form of summary is diverse and summarization carries out through multiple dimensions.", 
"Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies. In contrast, convolutional neural network is good at extracting local and position-invariant features.",Disconnected Recurrent Neural Networks for Text Categorization,"Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.", 
"Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. DISCOBERT extracts sub-sentential discourse units as candidates for extractive selection. Structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks.",Discourse-Aware Neural Extractive Text Summarization,"Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DISCOBERT1. DISCOBERT extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.", 
"Topoi are summaries of the main capabilities of a program, given under the form of collections of code functions along with an index. FEAT acts in two steps: Clustering. By mining the available source code, possibly augmented with code-level comments, hierarchical agglomerative clustering groups similar code functions.",Discovering Program Topoi via Hierarchical Agglomerative Clustering,"In long lifespan software-systems, specification documents can be outdated or even missing. Developing new software releases or checking whether some user requirements are still valid becomes challenging in this context. This challenge can be addressed by extracting high-level observable capabilities of a system by mining its source code and the available source-level documentation. This paper presents FEAT, an approach that automatically extracts topoi, which are summaries of the main capabilities of a program, given under the form of collections of code functions along with an index. FEAT acts in two steps: (1) Clustering. By mining the available source code, possibly augmented with code-level comments, hierarchical agglomerative clustering groups similar code functions. In addition, this process gathers an index for each function; (2) Entry-Point Selection. Functions within a cluster are then ranked and presented to validation engineers as topoi candidates. We implemented FEAT on top of a general-purpose test management and optimization platform and performed an experimental study over 15 open-source software projects amounting to more than 1 MLOC proving that automatically discovering topoi is feasible and meaningful on realistic projects.", 
The paper proposes a modified version of Differential Evolution (DE) algorithm and optimization criterion function for extractive text summarization applications. Experiments showed a 95.5% improvement in time in the Discrete DE approach over the conventional DE approach.,Discrete Differential Evolution for Text Summarization,"The paper proposes a modified version of Differential Evolution (DE) algorithm and optimization criterion function for extractive text summarization applications. Cosine Similarity measure has been used to cluster similar sentences based on a proposed criterion function designed for the text summarization problem, and important sentences from each cluster are selected to generate a summary of the document. The modified Differential Evolution model ensures integer state values and hence expedites the optimization as compared to conventional DE approach. Experiments showed a 95.5% improvement in time in the Discrete DE approach over the conventional DE approach, while the precision and recall of extracted summaries remained comparable in all cases.", 
Achieving a reliable classification of motor imagery (MI) tasks is a major challenge in brain– computer interface (BCI) implementation. This paper presents a supervised approach to select discriminative features for the enhancement of MI classification using multichannel electroencephalography (EEG) signal. It is a nearest-neighbor-based approach to learn the feature weights with regularization. The selected features are used to train the support vector machine for classification.,Discriminative Feature Selection-Based Motor Imagery Classification Using EEG Signal,"Achieving a reliable classification of motor imagery (MI) tasks is a major challenge in brain– computer interface (BCI) implementation. The set of relevant and discriminative features plays an important role in the classification scheme. This paper presents a supervised approach to select discriminative features for the enhancement of MI classification using multichannel electroencephalography (EEG) signal. The dimension of multiband feature space is reduced using the feature selection method. Each trial of the multichannel EEG signal representing MI tasks is decomposed into a finite set of narrowband signals. The common spatial pattern-based features are extracted from each subband. The features obtained from the multiple subbands are combined to derive a high-dimensional feature vector. The neighborhood component analysis-based feature selection method is implemented to select the features that are relevant in performing an accurate classification. It is a nearest-neighbor-based approach to learn the feature weights with regularization by maximizing the average leave-one-out classification accuracy over the labeled training data. The selected features are used to train the support vector machine for classification. The features relatively irrelevant to the classification task are discarded, yielding a reduction of feature dimension. The evaluation of the proposed method is performed using BCI Competition III dataset 4a and IV dataset 2b. Both are publicly available datasets and are used as types of benchmark data to evaluate the MI classification algorithm to implement BCI. The obtained simulation results confirm the superiority of the proposed method compared to the recently developed algorithms.", 
"Extreme learning machine (ELM) uses a non-iterative method to train single-hidden-layer feed-forward networks. In DMELM, the input weights and hidden layer biases can be randomly generated, which contributes to the analytical solution of output weights. The results show that DMELM consistently achieves better performance than original ELM and yields promising results.",Discriminative manifold extreme learning machine and applications to image and EEG signal classification,"Extreme learning machine (ELM) uses a non-iterative method to train single-hidden-layer feed-forward networks (SLFNs), which has been proven to be an efficient and effective learning model for both classification and regression. The main advantage of ELM lies in that the input weights as well as the hidden layer biases can be randomly generated, which contributes to the analytical solution of output weights. In this paper, we propose a discriminative manifold ELM (DMELM) by simultaneously considering the discriminative information and geometric structure of data; specifically, we exploit the discriminative information in the local neighborhood around each data point. To this end, a graph regularizer based on a newly designed graph Laplacian to characterize both properties is formulated and incorporated into the ELM objective. In DMELM, the output weights can also be obtained in analytical form. Extensive experiments are conducted on image and EEG signal classification to evaluate the effectiveness of DMELM. The results show that DMELM consistently achieves better performance than original ELM and yields promising results in comparison with several state-of-the-art algorithms, which suggests that both the discriminative as well as manifold information are beneficial to classification.", 
"Optimization-based design is an effective and promising approach to realizing collective behaviours for robot swarms. However, the literature often remains vague about the exact role played by the human designer, if any. In this Perspective, we briefly review the relevant literature and illustrate the hypotheses, characteristics and core challenges.",Disentangling automatic and semi-automatic approaches to the optimization-based design of control software for robot swarms,"Optimization-based design is an effective and promising approach to realizing collective behaviours for robot swarms. Unfortunately, the domain literature often remains vague about the exact role played by the human designer, if any. It is our contention that two cases should be disentangled: semi-automatic design, in which a human designer operates and steers an optimization process (for example, by fine-tuning the parameters of the optimization algorithm); and (fully) automatic design, in which the optimization process does not involve, need or allow any human intervention. In this Perspective, we briefly review the relevant literature; illustrate the hypotheses, characteristics and core challenges of semi-automatic and automatic design; and sketch out the context in which they could be ideally applied.", 
"Distracted driver behaviors include texting, talking on the phone, operating the radio, drinking, reaching behind and talking to the passenger. The developed system involves a dashboard camera capable of detecting distracted drivers through 2D camera images.",Distracted driver classification using deep learning,"One of the most challenging topics in the field of intelligent transportation systems is the automatic interpretation of the driver’s behavior. This research investigates distracted driver posture recognition as a part of the human action recognition framework. Numerous car accidents have been reported that were caused by distracted drivers. Our aim was to improve the performance of detecting drivers’ distracted actions. The developed system involves a dashboard camera capable of detecting distracted drivers through 2D camera images. We use a combination of three of the most advanced techniques in deep learning, namely the inception module with a residual block and a hierarchical recurrent neural network to enhance the performance of detecting the distracted behaviors of drivers. The proposed method yields very good results. The distracted driver behaviors include texting, talking on the phone, operating the radio, drinking, reaching behind, fixing hair and makeup, and talking to the passenger.",  
"A typical problem of document-level modeling is automatic summarization. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents. We train the models on two large datasets without engineering any features. The models achieve the state-of-the-art performance and they significantly benefit from the distraction modeling, particularly when input documents are long.",Distraction-Based Neural Networks for Document Summarization,"Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long.", 
"Query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases.",Diversity driven Attention Model for Query-based Abstractive Summarization,"Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.", 
Document Categorization Engine (DCE) aims to develop a system that is capable of classifying documents based on user defined criteria. DCE uses concepts of machine learning techniques.,Document Categorization Engine Based on Machine Learning Techniques,"Analysts, researchers, and other users from different fields may spend a lot of time recognizing their work-related documents and organizing them in a way that allows them to make the best use of their content. In this regard, we propose a Document Categorization Engine (DCE) that utilizes concepts of machine learning techniques and data mining. The project aims to develop a system that is capable of classifying documents based on user defined criteria.", 
We argue that the quality of a summary can be evaluated based on how many concepts in the original document can be preserved after summarization. We propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics.,Document concept lattice for text understanding and summarization,"We argue that the quality of a summary can be evaluated based on how many concepts in the original document(s) that can be preserved after summarization. Here, a concept refers to an abstract or concrete entity or its action often expressed by diverse terms in text. Summary generation can thus be considered as an optimization problem of selecting a set of sentences with minimal answer loss. In this paper, we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. The local topics will specify the promising sub-spaces related to the selected concepts and sentences. Based on this lattice, the summary is an optimized selection of a set of distinct and salient local topics that lead to maximal coverage of concepts with the given number of sentences. Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow-on tests.", 
"The technique discussed in this paper is considered to be a pioneering attempt in the field of NLP(Natural Language Processing) The technique involves an information extractor and a slide generator. The process of the technique comprises of extracting text, creating an ontology, identifying important phrases for bullets and generating slides.",Document Summarization and Information Extraction for Generation of Presentation Slides,"In this paper, a semi automated technique to generate slide presentations from English text documents is proposed. The technique discussed in this paper is considered to be a pioneering attempt in the field of NLP(Natural Language Processing). The technique involves an information extractor and a slide generator, which combines certain NLP methods such as segmentation, chunking, summarization etc., with certain special linguistic features of the text such as the ontology of the words, noun phrases found, semantic links, sentence centrality etc., In order to aid the language processing task, two tools can be utilized namely, MontyLingua which helps in chunking and Doddle helps in creating an ontology for the input text represented as an OWL (Ontology Web Language) file. The process of the technique comprises of extracting text, creating an ontology, identifying important phrases for bullets and generating slides.", 
"In the age of big data, automatic methods for creating summaries of documents become increasingly important. In this paper we propose a novel, unsupervised method for (multi-)document summarization. This approach relies on the strength of word associations in the set of documents.",Document summarization based on word associations,"In the age of big data, automatic methods for creating summaries of documents become increasingly important. In this paper we propose a novel, unsupervised method for (multi-)document summarization. In an unsupervised and language-independent fashion, this approach relies on the strength of word associations in the set of documents to be summarized. The summaries are generated by picking sentences which cover the most specific word associations of the document(s). We measure the performance on the DUC 2007 dataset. Our experiments indicate that the proposed method is the best-performing unsupervised summarization method in the state-of-the-art that makes no use of human-curated knowledge bases.", 
"Many methods, including supervised and unsupervised algorithms, have been developed for extractive document summarization. In this paper, we present a Conditional Random Fields (CRF) based framework to keep the merits of the above two kinds of approaches while avoiding their disadvantages. We compared our proposed approach with eight existing methods on an open benchmark data set. The results show that our approach can improve the performance by more than 7.1% and 12.1%.",Document Summarization using Conditional Random Fields,"Many methods, including supervised and unsupervised algorithms, have been developed for extractive document summarization. Most supervised methods consider the summarization task as a twoclass classification problem and classify each sentence individually without leveraging the relationship among sentences. The unsupervised methods use heuristic rules to select the most informative sentences into a summary directly, which are hard to generalize. In this paper, we present a Conditional Random Fields (CRF) based framework to keep the merits of the above two kinds of approaches while avoiding their disadvantages. What is more, the proposed framework can take the outcomes of previous methods as features and seamlessly integrate them. The key idea of our approach is to treat the summarization task as a sequence labeling problem. In this view, each document is a sequence of sentences and the summarization procedure labels the sentences by 1 and 0. The label of a sentence depends on the assignment of labels of others. We compared our proposed approach with eight existing methods on an open benchmark data set. The results show that our approach can improve the performance by more than 7.1% and 12.1% over the best supervised baseline and unsupervised baseline respectively in terms of two popular metrics F1 and ROUGE-2. Detailed analysis of the improvement is presented as well.", 
Single document summarization techniques have potential to simplify information consumption on mobile phones. In this paper we present a language independent single-document summarization method. We map document sentences to semantic concepts in Wikipedia and select sentences for the summary.,Document Summarization using Wikipedia,"Although most of the developing world is likely to first access the Internet through mobile phones, mobile devices are constrained by screen space, bandwidth and limited attention span. Single document summarization techniques have the potential to simplify information consumption on mobile phones by presenting only the most relevant information contained in the document. In this paper we present a language independent single-document summarization method. We map document sentences to semantic concepts in Wikipedia and select sentences for the summary based on the frequency of the mapped-to concepts. Our evaluation on English documents using the ROUGE package indicates our summarization method is competitive with the state of the art in single document summarization.", 
"Duplicate records elimination is a challenging data cleansing task. In this paper, we present a duplicate records elimination approach to improve the quality of data. We propose a deep learning-based approach for duplicate records detection using a sentence embeddings model. We also propose an algorithm for duplicated records correction. We evaluate our proposal on heart disease problem using Cleveland heart disease dataset.",Does data cleaning improve heart disease prediction,"quality has become an important issue. This issue becomes more and more important in medicine area, w effective decision making is high. In this context, the need for data cleaning to improve data quality is becoming crucial. Duplicate records elimination is a challenging data cleansing task. In this paper, we present a duplicate records elimination approach to improve the quality of data. We propose a deep learning-based approach for duplicate records detection using a sentence embeddings model. Also, we propose an algorithm for duplicated records correction. Then, we apply the proposed duplicate records elimination approach to analyse the effect of data cleaning on the quality of decisions. We evaluate our proposal on heart disease problem using Cleveland heart disease dataset. Experiments show that the classification performance improves upon the application of the duplicate records elimination approach on datasets compared to that of datasets with duplicate records. Data quality has become an important issue. This issue becomes more and more important in medicine area, where the need for effective decision making is high. In this context, the need for data cleaning to improve data quality is becoming crucial. Duplicate records elimination is a challenging data cleansing task. In this paper, we present a duplicate records elimination approach to improve the quality of data. We propose a deep learning-based approach for duplicate records detection using a sentence embeddings model. Also, we propose an algorithm for duplicated records correction. Then, we apply the proposed duplicate records elimination approach to analyse the effect of data cleaning on the quality of decisions. We evaluate our proposal on heart disease problem using Cleveland heart disease dataset. Experiments show that the classification performance improves upon the application of the duplicate records elimination approach on datasets compared to that of datasets with duplicate records.", 
"Code comments convey information about the programmers' intention in a more explicit but less rigorous manner than source code. This information can assist programmers in various tasks, such as code comprehension, reuse, and maintenance. Researchers analyzed more than 450 000 comments across 136 popular open-source software systems.",Does your code need comment,"Code comments convey information about the programmers' intention in a more explicit but less rigorous manner than source code. This information can assist programmers in various tasks, such as code comprehension, reuse, and maintenance. To better understand the properties of the comments existing in the source code, we analyzed more than 450 000 comments across 136 popular open-source software systems coming different domains. We found that the methods involving header comments and internal comments were shown low percentages in software systems, ie, 4.4% and 10.27%, respectively. As an application of our findings, we propose an automatic approach to determine whether a method needs a header comment, known as commenting necessity identification. Specifically, we identify the important factors for determining the commenting necessity of a method and extract them as structural features, syntactic features, and textual features. Then, by applying machine learning techniques and noise-handling techniques, we achieve a precision of 88.5% on eight open-source software from GitHub. The encouraging experimental results demonstrate the feasibility and effectiveness of our approach.", 
No method is able to work on all domains of text documents. Process first categorises the source text and then applies the respective category's optimal set of rules or weights or method.,Domain Independent Framework for Automatic Text Summarization,"Due to the exponential growth of documents on internet, users want all the relevant data at one place without any hassle. This led to the growth of Automatic Text Summarization. For this purpose a number of methods have been proposed by researchers but no method is able to work on all domains of text documents. Some methods which work for News domain may fail in Medical domain to give efficient results. In this paper we proposed a domain independent framework for Automatic Text Summarization. The Process first categorises the source text and then it applies the respective category’s optimal set of rules or weights or method. The major advantage of framework is that it can be applicable for both extractive and abstractive text summarization.", 
"Text summarization in the biomedical domain has largely been limited to extractive approaches. In this work, we propose a deep-reinforced, abstractive summarization model that is capable of reading biomedical publication abstracts and producing summaries in the form of a one sentence headline.",Domain-Aware Abstractive Text Summarization for Medical Documents,"Text summarization in the biomedical domain has largely been limited to extractive approaches. Abstractive approaches, using deep learning, have recently been successful for summarizing general-domain documents, such as news articles, but have not been applied to domain specific documents due to the difficulty for neural models to learn domain specific knowledge. In this work, we propose a deep-reinforced, abstractive summarization model that is capable of reading biomedical publication abstracts and producing summaries in the form of a one sentence headline, or title. We introduce novel reinforcement learning reward metrics based on biomedical expert tools, such as the UMLS Metathesaurus and MeSH, and show that our model is capable of producing domain-aware, abstractive summaries. We also introduce a reward metric based on TF-IDF and show that our model can also learn domain specific information without the use of expert tools.", 
"Electroencephalography (EEG) monitors brain activity during sleep and is used to identify sleep disorders. We propose a novel deep learning architecture called Dreem One Shot Event Detector (DOSED) DOSED jointly predicts locations, durations and types of events in EEG time series. It relies on a convolutional neural network that builds a feature representation from raw EEG signals.",DOSED A deep learning approach to detect multiple sleep micro-events in EEG signal,"Electroencephalography (EEG) monitors brain activity during sleep and is used to identify sleep disorders. In sleep medicine, clinicians interpret raw EEG signals in so-called sleep stages, which are assigned by experts to every 30s window of signal. For diagnosis, they also rely on shorter prototypical micro-architecture events which exhibit variable durations and shapes, such as spindles, K-complexes or arousals. Annotating such events is traditionally performed by a trained sleep expert, making the process time consuming, tedious and subject to inter-scorer variability. To automate this procedure, various methods have been developed, yet these are event-specific and rely on the extraction of hand-crafted features. We propose a novel deep learning architecture called Dreem One Shot Event Detector (DOSED). DOSED jointly predicts locations, durations and types of events in EEG time series. The proposed approach, applied here on sleep related micro-architecture events, is inspired by object detectors developed for computer vision such as YOLO and SSD. It relies on a convolutional neural network that builds a feature representation from raw EEG signals, as well as two modules performing localization and classification respectively. The proposed approach is tested on 4 datasets and 3 types of events (spindles, K-complexes, arousals) and compared to the current state-of-the-art detection algorithms. Results demonstrate the versatility of this new approach and improved performance compared to the current state-of-the-art detection methods.", 
Driver behavior monitoring system as Intelligent Transportation Systems (ITS) have been widely exploited to reduce the traffic accidents risk. Most previous methods for monitoring the driver behavior are rely on computer vision techniques. Such methods suffer from violation of privacy and the possibility of spoofing. This paper presents a novel yet efficient deep learning method for analyzing driver behavior.,Driver behavior detection and classification using deep convolutional neural networks,"Driver behavior monitoring system as Intelligent Transportation Systems (ITS) have been widely exploited to reduce the traffic accidents risk. Most previous methods for monitoring the driver behavior are rely on computer vision techniques. Such methods suffer from violation of privacy and the possibility of spoofing. This paper presents a novel yet efficient deep learning method for analyzing the driver behavior. We have used the driving signals, including acceleration, gravity, throttle, speed, and Revolutions Per Minute (RPM) to recognize five types of driving styles, including normal, aggressive, distracted, drowsy, and drunk driving. To take the advantages of successful deep neural networks on images, we learn a 2D Convolutional Neural Network (CNN) on images constructed from driving signals based on recurrence plot technique. Experimental results confirm that the proposed method can efficiently detect the driver behavior.", 
"The proposed method employs a dual encoder including the primary and the secondary encoders. The two level encodings are combined and fed into the decoder to generate more diverse summary. The experimental results on two challenging datasets (i.e., CNN/DailyMail and DUC 2004) demonstrate that our dual encoding model performs against existing methods.",Dual Encoding for Abstractive Text Summarization,"Recurrent neural network-based sequence-to-sequence attentional models have proven effective in abstractive text summarization. In this paper, we model abstractive text summarization using a dual encoding model. Different from the previous works only using a single encoder, the proposed method employs a dual encoder including the primary and the secondary encoders. Specifically, the primary encoder conducts coarse encoding in a regular way, while the secondary encoder models the importance of words and generates more fine encoding based on the input raw text and the previously generated output text summarization. The two level encodings are combined and fed into the decoder to generate more diverse summary that can decrease repetition phenomenon for long sequence generation. The experimental results on two challenging datasets (i.e., CNN/DailyMail and DUC 2004) demonstrate that our dual encoding model performs against existing methods.",  
"Early diagnosis of Alzheimer's disease (AD) is a proceeding hot issue along with a sharp upward trend in the incidence rate. Recently, early diagnosis of AD employing Electroencephalogram (EEG) as a specific hallmark has been an increasingly significant hot topic area. How to extract more abstract features for better generalization still remains tremendously troubling.",Early Alzheimer's disease diagnosis based on EEG spectral images using deep learning,"Early diagnosis of Alzheimer’s disease (AD) is a proceeding hot issue along with a sharp upward trend in the incidence rate. Recently, early diagnosis of AD employing Electroencephalogram (EEG) as a specific hallmark has been an increasingly significant hot topic area. In consideration of the limited size of available EEG spectral images, how to extract more abstract features for better generalization still remains tremendously troubling. In this paper, we demonstrate that it can be settled well with multi-task learning strategy based on discriminative convolutional high-order Boltzmann Machine with hybrid feature maps. First, differently from our original model — Contractive Slab and Spike Convolutional Deep Boltzmann Machine (CssCDBM), we directly conduct EEG spectral image classification via inducing label layer, resulting in a discriminative version of CssCDBM, referred to as DCssCDBM. This demonstrates DCssCDBM can be extended well into the classification model instead of feature extractor alone previously. Then, the most important approach innovation is that we train our DCssCDBM with multi-task learning framework via EEG spectral images based Identification and verification tasks for overfitting reduction for the first time, which could increase the inter-subject variations and reduce the intra-subject variations respectively, both of which are essential to early diagnosis of AD. The proposed method shows the better ability of high-level representations extraction and demonstrates the advanced results over several state-of-the-art methods.", 
"Cardiovascular disease (CVD) is an important factor in life since it may cause the death of human by effecting the heart and blood vessels of the body. Early detection of this disease is necessary for securing patients life. A stacked-GRU based Recurrent Neural Network model, abbreviated as, stk-G, is proposed in this paper.",Early Detection of Heart Disease Using Gated Recurrent Neural Network,"Cardiovascular disease (CVD) is an important factor in life since it may cause the death of human by effecting the heart and blood vessels of the body. Early detection of this disease is necessary for securing patients life. For this purpose, an automated tool is proposed in this paper for detecting patients with CVD and assisting health care systems also. A stacked-GRU based Recurrent Neural Network model, abbreviated as, stk-G, is proposed in this paper that considers interfering factors from past health records while detecting patients with cardiac problems. This proposed model is compared with two benchmark classifiers known as Support Vector Machine (SVM) and K-Nearest Neighbour (K-NN). The comparative analysis concludes that the proposed model offers enhanced efficiency for heart disease prediction. A promising result is given by the proposed method with an accuracy of 84.37%, F1-Score of 0.84 and MSE of 0.16.", 
Study compares machine learning-based prediction models to commonly used regression models for prediction of undiagnosed T2DM. Simple regression model performed with the lowest average RMSE of 0.838. Highest level of variable selection stability over time was observed with LightGBM models. No clinically relevant improvement when more sophisticated models were used.,Early detection of type 2 diabetes mellitus using machine learning-based prediction models,"Most screening tests for T2DM in use today were developed using multivariate regression methods that are often further simplifed to allow transformation into a scoring formula. The increasing volume of electronically collected data opened the opportunity to develop more complex, accurate prediction models that can be continuously updated using machine learning approaches. This study compares machine learning-based prediction models (i.e. Glmnet, RF, XGBoost, LightGBM) to commonly used regression models for prediction of undiagnosed T2DM. The performance in prediction of fasting plasma glucose level was measured using 100 bootstrap iterations in diferent subsets of data simulating new incoming data in 6-month batches. With 6 months of data available, simple regression model performed with the lowest average RMSE of 0.838, followed by RF (0.842), LightGBM (0.846), Glmnet (0.859) and XGBoost (0.881). When more data were added, Glmnet improved with the highest rate (+ 3.4%). The highest level of variable selection stability over time was observed with LightGBM models. Our results show no clinically relevant improvement when more sophisticated prediction models were used. Since higher stability of selected variables over time contributes to simpler interpretation of the models, interpretability and model calibration should also be considered in development of clinical prediction models.", 
"The connected vehicle eco-approach and departure (EAD) application for signalized intersections has been widely studied. EAD is deemed to be effective in terms of reducing energy consumption and greenhouse gas emissions. The proposed system can also reduce 7% of CO, 18% of HC, and 13% of NOx for all trips.",Eco-Approach and Departure (EAD) Application for Actuated Signals in Real-World Traffic,"The connected vehicle eco-approach and departure (EAD) application for signalized intersections has been widely studied and is deemed to be effective in terms of reducing energy consumption and both greenhouse gas and other criteria pollutant emissions. Prior studies have shown that tangible environmental benefits can be gained by communicating the driver with the signal phase and timing (SPaT) information of the upcoming traffic signals with fixed time control to the driver. However, similar applications to actuated signals pose a significant challenge due to their randomness to some extent caused by vehicle actuation. Based on the framework previously developed by the authors, a real-world testing has been conducted along the El Camino Real corridor in Palo Alto, CA, USA, to evaluate the system performance in terms of energy savings and emissions reduction. Strategies and algorithms are designed to be adaptive to the dynamic uncertainty for actuated signal and real-world traffic. It turns out that the proposed EAD system can save 6% energy for the trip segments when activated within DSRC ranges and 2% energy for all trips. The proposed system can also reduce 7% of CO, 18% of HC, and 13% of NOx for all trips. Those results are compatible with the simulation results and validate the previously developed EAD framework.", 
Edges provide a sparse yet informative representation of an image. The number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We show results that are significantly more accurate than the current state-of-the-art and faster to compute.,Edge Boxes Locating Object Proposals from Edges,"The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box’s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96% object recall at overlap threshold of 0.5 and over 75% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.", 
A privacy-preserving authentication framework that combines fifth-generation communication technology (5G) and edge computing technology is proposed. The proposed framework is different from that used in the previous 802.11p-based inter-vehicle communication network architecture. The authentication protocol proposed in this study can be divided into two parts.,Edge Computing-based Privacy-preserving Authentication Framework and Protocol for 5G-enabled Vehicular Networks,"Based on the functional characteristics of intervehicle communication networks under the current framework, a privacy-preserving authentication framework that combines fifthgeneration communication technology (5G) and edge computing technology is proposed. The proposed framework is different from that used in the previous 802.11p-based inter-vehicle communication network architecture, and we use device-to-device technology to achieve communication between vehicles. One difference between a 5G-enabled model and a traditional model for vehicle ad hoc networks is that secure communication between vehicles in a 5G-enabled model is challenging. The authentication protocol proposed in this study can be divided into two parts. The first part involves authenticating and selecting an edge computing vehicle, wherein a fuzzy logic mathematical method is used in the selection process. The second part is the mutual authentication between edge computing and ordinary vehicles. In this process, the exchange of security information between vehicles in a group is possible. Simultaneously, the identity privacy and traceability of the vehicle are ensured. Moreover, the proposed signature scheme is proved to be secure under a random oracle model. Performance evaluation results demonstrate that our proposed scheme has relatively lower computational and communication overhead than existing schemes.", 
"This paper proposes a novel extractive graph-based framework ""EdgeSumm"" that relies on four proposed algorithms. EdgeSumm is general for any document genre and unsupervised so it does not require any training data. It gets the highest ROUGE scores on DUC2001. For DUC2002, the proposed framework outperforms the state-of-the-art ATS systems.","EdgeSumm, Graph-based framework for automatic text summarization","Searching the Internet for a certain topic can become a daunting task because users cannot read and comprehend all the resulting texts. Automatic Text summarization (ATS) in this case is clearly beneficial because manual summarization is expensive and time-consuming. To enhance ATS for single documents, this paper proposes a novel extractive graph-based framework “EdgeSumm” that relies on four proposed algorithms. The first algorithm constructs a new text graph model representation from the input document. The second and third algorithms search the constructed text graph for sentences to be included in the candidate summary. When the resulting candidate summary still exceeds a user-required limit, the fourth algorithm is used to select the most important sentences. EdgeSumm combines a set of extractive ATS methods (namely graph-based, statistical-based, semantic-based, and centrality-based methods) to benefit from their advantages and overcome their individual drawbacks. EdgeSumm is general for any document genre (not limited to a specific domain) and unsupervised so it does not require any training data. The standard datasets DUC2001 and DUC2002 are used to evaluate EdgeSumm using the widely used automatic evaluation tool: Recall-Oriented Understudy for Gisting Evaluation (ROUGE). EdgeSumm gets the highest ROUGE scores on DUC2001. For DUC2002, the evaluation results show that the proposed framework outperforms the state-of-the-art ATS systems by achieving improvements of 1.2% and 4.7% over the highest scores in the literature for the metrics of ROUGE-1 and ROUGE-L respectively. In addition, EdgeSumm achieves very competitive results for the metrics of ROUGE-2 and ROUGE-SU4.", 
Deep Belief Network (DBN) and Stacked AutoEncoders (SAE) as Classifiers with encouraging performance accuracy. One of the designed SAE models outperforms the performance of DBN and the models presented in existing research by an impressive error rate of 1.1%.,EEG Based Eye State Classification using Deep Belief Network and Stacked AutoEncoder,"A Brain-Computer Interface (BCI) provides an alternative communication interface between the human brain and a computer. The Electroencephalogram (EEG) signals are acquired, processed and machine learning algorithms are further applied to extract useful information. During EEG acquisition, artifacts are induced due to involuntary eye movements or eye blink, casting adverse effects on system performance. The aim of this research is to predict eye states from EEG signals using Deep learning architectures and present improved classifier models. Recent studies reflect that Deep Neural Networks are trending state of the art Machine learning approaches. Therefore, the current work presents the implementation of Deep Belief Network (DBN) and Stacked AutoEncoders (SAE) as Classifiers with encouraging performance accuracy. One of the designed SAE models outperforms the performance of DBN and the models presented in existing research by an impressive error rate of 1.1% on the test set bearing accuracy of 98.9%. The findings in this study, may provide a contribution towards the state of the art performance on the problem of EEG based eye state classification.", 
Recognition of epileptic seizure type is essential for the neurosurgeon to understand the cortical connectivity of the brain. This study attempts to classify seven variants of seizures with non-seizure EEG through the application of convolutional neural networks. CNN based approach outperformed conventional feature and clustering based approaches. It can be concluded that the EEG based classification of seizure type using CNN model could be used in pre-surgical evaluation for treating patients with epilepsy.,EEG based multi-class seizure type classification using convolutional neural network and transfer learning,"Recognition of epileptic seizure type is essential for the neurosurgeon to understand the cortical connectivity of the brain. Though automated early recognition of seizures from normal electroencephalogram (EEG) was existing, no attempts have been made towards the classification of variants of seizures. Therefore, this study attempts to classify seven variants of seizures with non-seizure EEG through the application of convolutional neural networks (CNN) and transfer learning by making use of the Temple University Hospital EEG corpus. The objective of our study is to perform a multiclass classification of epileptic seizure type, which includes simple partial, complex partial, focal non-specific, generalized non-specific, absence, tonic, and tonic–clonic, and non-seizures. The 19 channels EEG time series was converted into a spectrogram stack before feeding as input to CNN. The following two different modalities were proposed using CNN: (1) Transfer learning using pretrained network, (2) Extract image features using pretrained network and classify using the support vector machine classifier. The following ten pretrained networks were used to identify the optimal network for the proposed study: Alexnet, Vgg16, Vgg19, Squeezenet, Googlenet, Inceptionv3, Densenet201, Resnet18, Resnet50, and Resnet101. The highest classification accuracy of 82.85% (using Googlenet) and 88.30% (using Inceptionv3) was achieved using transfer learning and extract image features approach respectively. Comparison results showed that CNN based approach outperformed conventional feature and clustering based approaches. It can be concluded that the EEG based classification of seizure type using CNN model could be used in pre-surgical evaluation for treating patients with epilepsy.", 
"A classification algorithm of electroencephalogram (EEG) based on sparse representation and convolution neural network is proposed. The method has a better recognition accuracy than that of the traditional SRC algorithm. Using the dataset downloaded from the website of BCI competition III, experiments show that the recognition accuracy of the method is over 80.",EEG Classification Based on Sparse Representation and Deep Learning,"For brain computer interfaces (BCIs) research, the classification of motor imagery brain signals is a major and challenging step. Based on the traditional sparse representation classification, a classification algorithm of electroencephalogram (EEG) based on sparse representation and convolution neural network is proposed by this paper. For the EEG signal, firstly, the features of the signal are obtained through the common spatial pattern (CSP) algorithm, and then the redundant dictionary with sparse representation is constructed based on these features. The sparse representation of the EEG signal is completed and the sparse features can be obtained. Finally, the sparse features are transformed into two dimensional signals, and the convolution neural network is used to complete the classification of EEG signals. Using the dataset downloaded from the website of BCI competition III (dataset IVa), for two types of EEG signals, the experiments show that the recognition accuracy of the method is over 80, and the recognition accuracy is better than that of the traditional SRC algorithm.", 
"Driver fatigue is the main cause of traffic accidents, which bring great harm to society and families. This paper proposes to use deep convolutional neural networks to predict the mental states of drivers from electroencephalography (EEG) signals.",EEG classification of driver mental states by deep learning,"Driver fatigue is attracting more and more attention, as it is the main cause of traffic accidents, which bring great harm to society and families. This paper proposes to use deep convolutional neural networks, and deep residual learning, to predict the mental states of drivers from electroencephalography (EEG) signals. Accordingly we have developed two mental state classification models called EEG-Conv and EEG-Conv-R. Tested on intra- and inter-subject, our results show that both models outperform the traditional LSTM- and SVM-based classifiers. Our major findings include (1) Both EEG-Conv and EEG-Conv-R yield very good classification performance for mental state prediction; (2) EEG-Conv-R is more suitable for inter-subject mental state prediction; (3) EEG-Conv-R converges more quickly than EEG-Conv. In summary, our proposed classifiers have better predictive power and are promising for application in practical brain-computer interaction.", 
Proposed CNN-VAE classification framework for MI electroencephalogram (EEG) signals. It combines a convolutional neural network (CNN) architecture with a Variational autoencoder (VAE) for classification. It outperforms the best classification method in the literature for BCI Competition IV dataset 2b.,EEG Classification of Motor Imagery Using a Novel Deep Learning Framework,"Successful applications of brain-computer interface (BCI) approaches to motor imagery (MI) are still limited. In this paper, we propose a classification framework for MI electroencephalogram (EEG) signals that combines a convolutional neural network (CNN) architecture with a variational autoencoder (VAE) for classification. The decoder of the VAE generates a Gaussian distribution, so it can be used to fit the Gaussian distribution of EEG signals. A new representation of input was developed by combining the time, frequency, and channel information from the EEG signal, and the CNN-VAE method was designed and optimized accordingly for this form of input. In this network, the classification of the extracted CNN features is performed via the deep network VAE. Our framework, with an average kappa value of 0.564, outperforms the best classification method in the literature for BCI Competition IV dataset 2b with a 3% improvement. Furthermore, using our own dataset, the CNN-VAE framework also yields the best performance for both three-electrode and five-electrode EEGs and achieves the best average kappa values 0.568 and 0.603, respectively. Our results show that the proposed CNN-VAE method raises performance to the current state of the art.", 
Mu rhythm is a spontaneous neural response occurring during a motor imagery (MI) task. It has been increasingly applied to the design of brain–computer interface (BCI) Extreme learning machine (ELM) has proven to be more efficient than support vector machine for MI-related EEG classification. SBELM-based algorithm is able to automatically control the model complexity and exclude redundant hidden neurons.,EEG classification using sparse Bayesian extreme learning machine for brain-computer interface,"Mu rhythm is a spontaneous neural response occurring during a motor imagery (MI) task and has been increasingly applied to the design of brain–computer interface (BCI). Accurate classification of MI is usually rather difficult to be achieved since mu rhythm is very weak and likely to be contaminated by other background noises. As an extension of the single layer feedforward network, extreme learning machine (ELM) has recently proven to be more efficient than support vector machine that is a benchmark for MI-related EEG classification. With probabilistic inference, this study introduces a sparse Bayesian ELM (SBELM)-based algorithm to improve the classification performance of MI. SBELM is able to automatically control the model complexity and exclude redundant hidden neurons by combining advantageous of both ELM and sparse Bayesian learning. The effectiveness of SBELM for MI-related EEG classification is validated on a public dataset from BCI Competition IV IIb in comparison with several other competing algorithms. Superior classification accuracy confirms that the proposed SBELM-based algorithm is a promising candidate for performance improvement of an MI BCI.", 
This paper proposes a simple Convolutional Neural Network (CNN) program to classify epileptic seizure. The resulting accuracy is 72.49% and the loss is 0.576. This result has a better learning time than GoogleNet and smaller loss result than AlexNet.,EEG dataset classification using CNN method,"This paper proposes a simple Convolutional Neural Network (CNN) program to classify epileptic seizure. The diagnosis of epileptic seizure involves the identification and different characteristic of the Electroencephalography (EEG) signal. As such, it needs a method for identifying and classifying epileptic seizure. Deep learning is part of a neural network that has the ability and pattern to identify and classify epileptic seizure. CNN has been demonstrated high performance on image classification and pattern detection. In this paper, we combine the continuous wavelet transform (CWT) and CNN to classify epileptic seizure. This experiment uses the wavelet transform to convert signal data of EEG to time-frequency domain images. The output of the wavelet transform is an image that will classify into five attributes. In this experiment, we develop a simple program that will compare with other CNN approach (AlexNet and GoogleNet). The results of this experiment are two kinds of data, accuracy, and loss. The resulting accuracy is 72.49%, and the loss is 0.576. This result has a better learning time than GoogleNet and smaller loss result than AlexNet.", 
"SincNet is an efficient classifier for speaker recognition, but it has some drawbacks in dealing with EEG signals classification. In this paper, we improve and propose a SincNet-based classifier, Sinc net-R, which consists of three convolutional layers and three deep neural network (DNN) layers. We then make use of Sincnet-R to test the classification accuracy and robustness by emotional EEG signals.",EEG Emotion Classification Using an Improved SincNet-Based Deep Learning Model,"Deep learning (DL) methods have been used increasingly widely, such as in the fields of speech and image recognition. However, how to design an appropriate DL model to accurately and efficiently classify electroencephalogram (EEG) signals is still a challenge, mainly because EEG signals are characterized by significant differences between two different subjects or vary over time within a single subject, non-stability, strong randomness, low signal-to-noise ratio. SincNet is an efficient classifier for speaker recognition, but it has some drawbacks in dealing with EEG signals classification. In this paper, we improve and propose a SincNet-based classifier, SincNet-R, which consists of three convolutional layers, and three deep neural network (DNN) layers. We then make use of SincNet-R to test the classification accuracy and robustness by emotional EEG signals. The comparable results with original SincNet model and other traditional classifiers such as CNN, LSTM and SVM, show that our proposed SincNet-R model has higher classification accuracy and better algorithm robustness.", 
Proposed system uses deep learning to detect pathologies in brain signals. Brain signals captured in the form of EEG signals can indicate whether a person suffers from a pathology or not. The proposed system with the deep CNN model and fusion achieves 87.96% accuracy.,EEG Pathology Detection based on Deep Learning,"With the advancement of machine learning technologies, particularly deep learning, automated systems to assist human life are flourishing. In this paper, we propose an automatic electroencephalogram (EEG) pathology detection system based on deep learning. Various types of pathologies can affect brain signals. Thus, the brain signals captured in the form of EEG signals can indicate whether a person suffers from a pathology or not. In the proposed system, raw EEG signals are processed in the form of a spatio-temporal representation. The spatio-temporal form of the EEG signals is the input to a convolutional neural network (CNN). Two different CNN models, namely, a shallow model and a deep model, are investigated using transfer learning. A fusion strategy based on a multilayer perceptron is also investigated. Experimental results on the Temple University Hospital EEG Abnormal Corpus v2.0.0 show that the proposed system with the deep CNN model and fusion achieves 87.96% accuracy, which is better than some reported accuracy rates on the same corpus.", 
The EEG (Electroencephalogram) signal indicates the electrical activity of the brain. They are highly random in nature and may contain useful information about the brain state. It is very difficult to get useful information from these signals directly in the time domain just by observing them. They can be extracted for the diagnosis of different diseases using advanced signal processing techniques.,EEG Signal Analysis A Survey,"The EEG (Electroencephalogram) signal indicates the electrical activity of the brain. They are highly random in nature and may contain useful information about the brain state. However, it is very difficult to get useful information from these signals directly in the time domain just by observing them. They are basically non-linear and nonstationary in nature. Hence, important features can be extracted for the diagnosis of different diseases using advanced signal processing techniques. In this paper the effect of different events on the EEG signal, and different signal processing methods used to extract the hidden information from the signal are discussed in detail. Linear, Frequency domain, time - frequency and non-linear techniques like correlation dimension (CD), largest Lyapunov exponent (LLE), Hurst exponent (H), different entropies, fractal dimension(FD), Higher Order Spectra (HOS), phase space plots and recurrence plots are discussed in detail using a typical normal EEG signal.", 
Brain Computer Interface (BCI) systems are the devices which are proposed to help the disabled. People who are incapable of making motor response to communicate with computer using brain signal. Aim of BCI is to interpret brain activity into digital form which acts as a command for a computer.,EEG SIGNAL ANALYSIS FOR BCI INTERFACE A REVIEW,"Brain Computer Interface (BCI) systems are the devices which are proposed to help the disabled; people who are incapable of making motor response to communicate with computer using brain signal. The aim of BCI is to interpret brain activity into digital form which acts as a command for a computer. One key challenge in current BCI research is how to extract features of random time-varying EEG signals and its classification as accurately as possible. Feature extraction techniques are used to extract the features which represent a unique property obtained from pattern of brain signal. Earlier EEG analysis was restricted to visual inspection only. The visual inspection of the signal is very subjective and hardly allows any standardization or statistical analysis. Hence, several different techniques were intended in order to quantify the information of the brain signal. Many linear and non-linear methods for feature extraction exist. The purpose of this paper is to give a brief introduction to the EEG signals and BCI system. The paper also includes a review on the conventional methods that are used for feature extraction of the signal.", 
"Emotion detection will give a clear idea about customer satisfaction in e-learning, marketing, entertainments and behavior of criminals in law. Angry, Happy, Sad and Relax are the emotions classified using Kohonen neural networks.",EEG signal based Modified Kohonen Neural Networks for Classification of Human Mental Emotions,"Identifying human emotions is very important in human machine interaction(HMI). These emotions will affect communication between people, and their mood. Emotion detection will give a clear idea about customer satisfaction in e-learning, marketing, entertainments and behavior of criminals in law. Artificial neural networks are essential for machine learning and emotion detection. The emotions are detected from EEG signals which can give better performance to audio and facial signals. In this work, several modified Kohonen neural networks are proposed for human emotion classification. EEG signals from DEAP Database are used as input for ANN to detect the human emotions. Angry, Happy, Sad and Relax are the emotions classified using Kohonen Neural Networks. Experimental results show promising results for the proposed approaches.", 
"Nonlinear, noisy and outlier characteristics of electroencephalography (EEG) signals inspire the employment of fuzzy logic. This paper introduces an approach to classify motor imagery EEG signals using an interval type-2 fuzzy logic system (IT2FLS) and wavelet transformation. The proposed approach yields great accuracy and requires low computational cost.",EEG signal classification for BCI applications by wavelets and interval type-2 fuzzy logic systems,"The nonlinear, noisy and outlier characteristics of electroencephalography (EEG) signals inspire the employment of fuzzy logic due to its power to handle uncertainty. This paper introduces an approach to classify motor imagery EEG signals using an interval type-2 fuzzy logic system (IT2FLS) in a combination with wavelet transformation. Wavelet coefficients are ranked based on the statistics of the receiver operating characteristic curve criterion. The most informative coefficients serve as inputs to the IT2FLS for the classification task. Two benchmark datasets, named Ia and Ib, downloaded from the brain-computer interface (BCI) competition II, are employed for the experiments. Classification performance is evaluated using accuracy, sensitivity, specificity and F-measure. Widely-used classifiers, including feedforward neural network, support vector machine, k-nearest neighbours, AdaBoost and adaptive neuro-fuzzy inference system, are also implemented for comparisons. The wavelet-IT2FLS method considerably dominates the comparable classifiers on both datasets, and outperforms the best performance on the Ia and Ib datasets reported in the BCI competition II by 1.40% and 2.27% respectively. The proposed approach yields great accuracy and requires low computational cost, which can be applied to a real-time BCI system for motor imagery data analysis.", 
"In this article, we propose a new EEG signal classification method based on Relevance Vector Machine (RVM) and AR model. It can well separate the ictal EEG signals from the inter-ictal ones, this is very important in the diagnosis of epilepsy. The results indicate that: (1) features extracted based on AR models can well represent the EEG signals in the task of epilepsy diagnosis.",EEG Signal Classification for Epilepsy Diagnosis based on AR Model and RVM,"In this article, we propose a new EEG signal classification method based on Relevance Vector Machine (RVM) and AR model. It can well separate the ictal EEG signals from the inter-ictal ones, this is very important in the diagnosis of epilepsy. Our studies can be divided into three parts: firstly, EEG features were extracted from the signals based on AR models, and then the performance of these features was evaluated; secondly, according to the performance of the features, feature selection was introduced between feature extraction and classifiers; finally, RVM was implemented with different AR models, different kernel widths, and different subsets of the features in order to get an overview of the method. The results indicate that: (1) features extracted based on AR models can well represent the EEG signals in the task of EEG signal classification for epilepsy diagnosis; (2) feature selection is needed between feature extraction and classifiers; (3) the method based on RVM and AR model can well differentiate the two types of EEG signals.", 
"Brain-computer interface (BCI) is linking the brain activity to computer, which allows a person to control devices directly with his brain waves. Controlling devices using BCI is a crucial aid for people suffering from severe disabilities. BCIs can replace human to control robots working in dangerous.",EEG Signal Classification for Real-Time Brain-Computer Interface Applications-A Review,"Brain-computer interface (BCI) is linking the brain activity to computer, which allows a person to control devices directly with his brain waves and without any use of his muscles. Recent advances in real-time signal processing have made BCI a feasible alternative for controlling robot and for communication as well. Controlling devices using BCI is a crucial aid for people suffering from severe disabilities and more than that, BCIs can replace human to control robots working in dangerous or uncongenial situations. Effective BCIs demand for accurate and real-time EEG signals processing. This paper is to review the current state of research and to compare the performance of different algorithms for real-time classification of BCI-based electroencephalogram signals.", 
The affective computing aims to detect emotional states during the interaction between the user and the machine. This work aims to establish a relationship between EEG signals and the user opinion about the usability of some Facebook privacy features. EEG signals related to emotional states can be applied to the context of software usability providing more resources to the validation process.,EEG Signal Classification in Usability Experiments,"The affective computing aims to detect emotional states during the interaction between the user and the machine allowing the use of this information in decision-making processes. EEG signals related to emotional states can be applied to the context of software usability providing more resources to the validation process and the identification of the degree of user satisfaction. This work aims to establish a relationship between EEG signals and the user opinion about the usability of some Facebook privacy features. Based on the assumption that there are variation in brain activity during the execution of tasks labeled as ""easy"" or ""difficult"", a performance evaluation was done based on a Linear Discriminant Analysis (LDA) and Support Vector Machines (SVM) classifiers. The Mean Power Spectral Density, in 7 frequency bands, from 8 electrodes in F, C, P, and O areas were used as features. The classification rates showed a small advantage of the SVM when all the 28 variables were used. However, when the 13 variables pointed by the Mann-Whitney U test were used, LDA showed good discrimination capability. The electrodes in F and C areas, related with cognition and motor functions, rejected null hypothesis in almost all frequency bands during the execution of the tasks, showing that it is possible to recognize the studied emotional states. Despite the fact that this was a preliminary study, it showed the feasibility of using the EEG as a potential source of information to be added to software usability testing.", 
"The contribution describes the design, optimization and verification of the off-line single-trial movement classification system. The classification system utilizes hidden information stored in the characteristic shapes of human brain activity (EEG signal) The great variability of EEG potentials requires using of context information and hence the classifier based on Hidden Markov Models.",EEG Signal Classification Introduction to the Problem,"The contribution describes the design, optimization and verification of the off-line single-trial movement classification system. Four types of movements are used for the classification: the right index finger extension vs. flexion as well as the right shoulder (proximal) vs. right index finger (distal) movement. The classification system utilizes hidden information stored in the characteristic shapes of human brain activity (EEG signal). The great variability of EEG potentials requires using of context information and hence the classifier based on Hidden Markov Models (HMM). The suitable parameterization, model structure as well as training and classification process are suggested on the base of spectral analysis results and experience with the speech recognition. The training and the classification are performed with the disjoint sets of EEG realizations. Classification experiments are performed with 10 randomly chosen sets of EEG realizations. The final average score of the distal/proximal movement classification is 80%; the standard deviation of classification results is 9%. The classification of the extension / flexion gives comparable results.", 
"More and more people with disabilities use prosthetic limbs to compensate for the function of normal upper limbs. In this paper, a new EEG signal classification method is proposed to identify the upper limb movements. The model designed in this paper has a classification accuracy rate of 93.22% on the WAY-EEG-GAL dataset.",EEG signal classification method based on feature priority analysis and CNN,"In recent years, with the advancement of technology, more and more people with disabilities use prosthetic limbs to compensate for the function of normal upper limbs, so the demand for the upper limb movement recognition has increased. In addition, with the development of brain-computer interfaces, a large number of academic institutions are dedicated to identifying the upper limb movements through EEG signals. In this paper, a new EEG signal classification method is proposed to identify the upper limb movements. Firstly, the importance of EEG detection electrodes is sorted by random forest algorithm, and the higher priority electrodes are screened out. Then the wavelet transform method is used to extract the features of the original data that have been screened out. After that, the EEG signals are classified according to the extracted features by using convolutional neural network. The model designed in this paper has a classification accuracy rate of 93.22% on the WAY-EEG-GAL dataset, which is an effective EEG signal classification model.", 
"In this paper, we propose a method to classify electroencephalogram (EEG) signal recorded from left- and righthand movement imaginations. We use a technique of complexity measure based on fractal analysis to reveal feature patterns in the EEG signal. Experimental results can be considerably applied in a brain-computer interface (BCI) application.",EEG Signal Classification Method Based on Fractal Features and Neural Network,"In this paper, we propose a method to classify electroencephalogram (EEG) signal recorded from left- and righthand movement imaginations. Three subjects (two males and one female) are volunteered to participate in the experiment. We use a technique of complexity measure based on fractal analysis to reveal feature patterns in the EEG signal. Effective algorithm, namely, detrended fluctuation analysis (DFA) has been selected to estimate embedded fractal dimension (FD) values between relaxing and imaging states of the recorded EEG signal. To show the waveform of FDs, we use a windowing-based method or called time-dependent fractal dimension (TDFD) and the Kullback-Leibler (K-L) divergence. Two feature parameters; KL divergence and different expected values are proposed to be input variables of the classifier. Finally, featured data are classified by a three-layer feed-forward neural network based on a simple backpropagation algorithm. Experimental results can be considerably applied in a brain-computer interface (BCI) application and show that the proposed method is more effective than the conventional method by improving average classification rates of 87.5% and 88.3% for left- and right-hand movement imagery tasks, respectively.", 
"Current brain–computer interface (BCI) systems are based on imagined speech. This means that these systems are controlled only by thinking about a speech without verbally expressing it. Imagined speech recognition using electroencephalogram (EEG) signals is more convenient than electrocorticogram (ECoG) So, we propose an approach for EEG classification of imagined speech with high accuracy and efficiency.",EEG signal classification of imagined speech based on Riemannian distance of correntropy spectral density,"Several current brain–computer interface (BCI) systems are based on imagined speech. This means that these systems are controlled only by thinking about a speech without verbally expressing it. Imagined speech recognition using electroencephalogram (EEG) signals is much more convenient than other methods such as electrocorticogram (ECoG), due to its easy, non-invasive recording. So, we proposed an approach for EEG classification of imagined speech with high accuracy and efficiency. In this work, correntropy spectral density (CSD) matrices are evaluated for EEG signals obtained from different channels, and the distances between these matrices are considered as measures for imagined speech recognition. Riemannian distance benefits from simplicity and accuracy and it has achieved high scores in BCI competitions. Also, in this work, channel selection and frequency band detection during imagined speech is evaluated with statistical methods. The “Kara One” database is used in this research that includes EEG signals of eight subjects during imagined speech of four English words. We evaluated the proposed approach in comparison with the results of other imagined speech classification methods. Average classification accuracy of the proposed method is 90.25% during imagined speech for all subjects in KARA One database. The simulation results of this paper show the efficiency and accuracy of Riemannian distance of CSD and the superiority of the proposed method over other methods for imagined speech classification.", 
"Current brain–computer interface (BCI) systems are based on imagined speech. This means that these systems are controlled only by thinking about a speech without verbally expressing it. Imagined speech recognition using electroencephalogram (EEG) signals is more convenient than electrocorticogram (ECoG) So, we propose an approach for EEG classification of imagined speech with high accuracy and efficiency.",EEG Signal Classification Using Convolutional Neural Networks on Combined Spatial and Temporal Dimensions for BCI Systems,"EEG signal classification is an important task to build an accurate Brain Computer Interface (BCI) system. Many machine learning and deep learning approaches have been used to classify EEG signals. Besides, many studies have involved the time and frequency domain features to classify EEG signals. On the other hand, a very limited number of studies combine the spatial and temporal dimensions of the EEG signal. Brain dynamics are very complex across different mental tasks, thus it is difficult to design efficient algorithms with features based on prior knowledge. Therefore, in this study, we utilized the 2D AlexNet Convolutional Neural Network (CNN) to learn EEG features across different mental tasks without prior knowledge. First, this study adds spatial and temporal dimensions of EEG signals to a 2D EEG topographic map. Second, topographic maps at different time indices were cascaded to populate a 2D image for a given time window. Finally, the topographic maps enabled the AlexNet to learn features from the spatial and temporal dimensions of the brain signals. The classification performance was obtained by the proposed method on a multiclass dataset from BCI Competition IV dataset 2a. The proposed system obtained an average classification accuracy of 81.09%, outperforming the previous state-of-the-art methods by a margin of 4% for the same dataset. The results showed that converting the EEG classification problem from a (1D) time series to a (2D) image classification problem improves the classification accuracy for BCI systems. Also, our EEG topographic maps enabled CNN to learn subtle features from spatial and temporal dimensions, which better represent mental tasks than individual time or frequency domain features.", 
New method based on empirical mode decomposition (EMD) for classification of seizure and seizure-free EEG signals. EMD method decomposes the EEG signal into a set of narrowband amplitude and frequency modulated (AM-FM) components.,EEG Signal Classification Using Empirical Mode Decomposition and Support Vector Machine,"In this paper, we present a new method based on empirical mode decomposition (EMD) for classification of seizure and seizure-free EEG signals. The EMD method decomposes the EEG signal into a set of narrowband amplitude and frequency modulated (AM-FM) components known as intrinsic mode functions (IMFs). The method proposes the use of the area parameter and mean frequency estimation of IMFs in the classification of the seizure and seizure-free EEG signals. These parameters have been used as an input in least squares support vector machine (LS-SVM), which provides classification of seizure EEG signals from seizure-free EEG signals. The classification accuracy for classification of seizure and seizure-free EEG signals obtained by using proposed method is 98.33% for second IMF with radial basis function kernel of LS-SVM.", 
"Epilepsy is one of the most common and diverse set of chronic neurological disorders. Electroencephalogram (EEG) signal processing technique plays a significant role in detection and prediction of epileptic seizure. This paper presents a new approach for seizure detection to analysis preictal and interictal EEG signals. The propose method outperforms the state-of-the-art method in terms of sensitivity, specificity and accuracy.",EEG Signal Classification using Frequency Band Analysis towards Epileptic Seizure Prediction,"Epilepsy is one of the most common and diverse set of chronic neurological disorders characterized by an abnormal excessive or synchronous neuronal activity in the brain that is termed “seizure”, affecting about 50 million individuals worldwide. Electroencephalogram (EEG) signal processing technique plays a significant role in detection and prediction of epileptic seizure. Recently, many research works have been devoted to detect/predict of epileptic seizure based on analysis of EEG signals. Even though remarkable works have been conducted on seizure detection/prediction, experimental results are not mature enough in terms of sensitivity, specificity, and accuracy. In this paper we present a new approach for seizure detection to analysis preictal (before seizure onset) and interictal (period between seizures) EEG signals by extracting different features from gamma frequency band by decomposing the signals using discrete wavelet transformation. Note that the detection of preictal and interictal EEG signals leads to predict the epileptic seizure. Experimental results demonstrate that the propose method outperforms the state-of-the-art method in terms of sensitivity, specificity and accuracy to classify seizure by analyzing EEG signals to the benchmark dataset in different brain locations.", 
Neural network (NN) finds role in variety of applications due to combined effect of feature extraction and classification availability. Two-layer LSTM and four-layer improved NN deep learning algorithms are proposed to improve the performance in EEG classification. Statistical features are extracted for input EEG collected from Bonn database.,EEG signal classification using LSTM and improved neural network algorithms,"Neural network (NN) finds role in variety of applications due to combined effect of feature extraction and classification availability in deep learning algorithms. In this paper, we have chosen SVM, logistic regression machine learning algorithms and NN for EEG signal classification. Two-layer LSTM and four-layer improved NN deep learning algorithms are proposed to improve the performance in EEG classification. Novelty lies in one-dimensional gradient descent activation functions with radial basis operations used in the initial layers of improved NN which help in achieving better performance. Statistical features namely mean, standard deviation, kurtosis and skewness are extracted for input EEG collected from Bonn database and then applied for various classification techniques. Accuracy, precision, recall and F1 score are the performance metrics used for analyzing the algorithms. Improved NN and LSTM give better performance compared to all other architectures. The simulations are carried out with variety of activation functions, optimizers and loss models to analyze the performance using Python in keras.", 
Classification of EEG signals is one of the biggest problems in Brain Computer Interface (BCI) systems. This paper presents a BCI system based on using the EEG signals associated with five mental tasks.,EEG Signal Classification Using Neural Network and Support Vector Machine in Brain Computer Interface,"Classification of EEG signals is one of the biggest problems in Brain Computer Interface (BCI) systems. This paper presents a BCI system based on using the EEG signals associated with five mental tasks (baseline, math, mental letter composing, geometric figure rotation and visual counting). EEG data for these five cognitive tasks from one subject were taken from the Colorado University database. Wavelet Transform (WT), Fast Fourier Transform (FFT) and Principal Component Analysis (PCA) were used for features extraction. Artificial Neural Network (ANN) trained by a standard back propagation algorithm and Support Vector Machines (SVMs) were used for classifying different combinations mental tasks. Experimental results show the classification accuracies achieved with the three used feature extraction techniques and the two classification techniques.", 
"Independent component analysis can be used to improve the performance of brain–computer interface (BCI) systems. In this paper, a new method is proposed for EEG signal classification in BCI systems by using nonlinear ICA algorithm. The effectiveness of the proposed method is evaluated by using the classification of EEG signals.",EEG SIGNAL CLASSIFICATION USING NONLINEAR INDEPENDENT COMPONENT ANALYSIS,"One of the preprocessors can be used to improve the performance of brain–computer interface (BCI) systems is independent component analysis (ICA). ICA is a signal processing technique in which observed random data are transformed into components that are statistically independent from each other. This suggests the possibility of using ICA to separate different independent brain activities during motor imagery into separate components. However, there is no guarantee for linear combination of brain sources in EEG signals. Thus the identification of nonlinear dynamic of EEG signals should be taken into consideration. In this paper, a new method is proposed for EEG signal classification in BCI systems by using nonlinear ICA algorithm. The effectiveness of the proposed method is evaluated by using the classification of EEG signals. The tasks to be discriminated are the imaginative hand movement and the resting state. The results demonstrate that the proposed method performed well in several experiments on different subjects and can improve the classification accuracy in the BCI systems.", 
"Brain Computer Interface (BCI) is the method of communicating the human brain with an external device. It uses the brain signals to take actions, control, actuate and communicate with the world directly using brain integration with peripheral devices and systems. Brain waves are in necessitating to eradicate noises and to extract the valuable features.",EEG Signal Classification using Principal Component Analysis with Neural Network in Brain Computer Interface Applications,"Brain Computer Interface (BCI) is the method of communicating the human brain with an external device. People who are incapable to communicate conventionally due to spinal cord injury are in need of Brain Computer Interface. Brain Computer Interface uses the brain signals to take actions, control, actuate and communicate with the world directly using brain integration with peripheral devices and systems. Brain waves are in necessitating to eradicate noises and to extract the valuable features. Artificial Neural Network (ANN) is a functional pattern classification technique which is trained all the way through the error Back-Propagation algorithm. In this paper in order to classify the mental tasks, the brain signals are trained using neural network and also using Principal Component Analysis with Artificial Neural Network. Principal Component Analysis (PCA) is a dominant tool for analyzing data and finding patterns in it. In Principal Component Analysis, data compression is possible and it projects higher dimensional data to lower dimensional data. By using Principal Component Analysis with Neural Network, the redundant data in the dataset is eliminated first and the obtained data is trained using Neural Network. EEG data for five cognitive tasks from five subjects are taken from the Colorado University database. Pattern classification is applied for the data of all tasks of one subject using Neural Network and also using Principal Component Analysis with Neural Network. Finally it is observed that the correctly classified percentage of data is better in Principal Component Analysis with Neural Network compared to Neural Network alone.", 
"Support vector machine (SVM) has been used widely for classification of electroencephalogram (EEG) signals for the diagnosis of neurological disorders such as epilepsy and sleep disorders. SVM shows good generalization performance for high dimensional data due to its convex optimization problem. In our approach, the data points are generated by selecting universum from the EEG dataset itself. This removes the effect of outliers on the generation of universum data.",EEG signal classification using universum support vector machine,"Support vector machine (SVM) has been used widely for classification of electroencephalogram (EEG) signals for the diagnosis of neurological disorders such as epilepsy and sleep disorders. SVM shows good generalization performance for high dimensional data due to its convex optimization problem. The incorporation of prior knowledge about the data leads to a better optimized classifier. Different types of EEG signals provide information about the distribution of EEG data. To include prior information in the classification of EEG signals, we propose a novel machine learning approach based on universum support vector machine (USVM) for classification. In our approach, the universum data points are generated by selecting universum from the EEG dataset itself which are the interictal EEG signals. This removes the effect of outliers on the generation of universum data. Further, to reduce the computation time, we use our approach of universum selection with universum twin support vector machine (UTSVM) which has less computational cost in comparison to traditional SVM. For checking the validity of our proposed methods, we use various feature extraction techniques for different datasets consisting of healthy and seizure signals. Several numerical experiments are performed on the generated datasets and the results of our proposed approach are compared with other baseline methods. Our proposed USVM and proposed UTSVM show better generalization performance compared to SVM, USVM, Twin SVM (TWSVM) and UTSVM. The proposed UTSVM has achieved highest classification accuracy of 99 % for the healthy and seizure EEG signals.", 
Mixture of experts (ME) is modular neural network architecture for supervised learning. A double-loop Expectation-Maximization (EM) algorithm has been introduced to the ME network structure for detection of epileptic seizure.,EEG signal classification using wavelet feature extraction and a mixture of expert model,"Mixture of experts (ME) is modular neural network architecture for supervised learning. A double-loop Expectation-Maximization (EM) algorithm has been introduced to the ME network structure for detection of epileptic seizure. The detection of epileptiform discharges in the EEG is an important component in the diagnosis of epilepsy. EEG signals were decomposed into the frequency subbands using discrete wavelet transform (DWT). Then these sub-band frequencies were used as an input to a ME network with two discrete outputs: normal and epileptic. In order to improve accuracy, the outputs of expert networks were combined according to a set of local weights called the ‘‘gating function’’. The invariant transformations of the ME probability density functions include the permutations of the expert labels and the translations of the parameters in the gating functions. The performance of the proposed model was evaluated in terms of classification accuracies and the results confirmed that the proposed ME network structure has some potential in detecting epileptic seizures. The ME network structure achieved accuracy rates which were higher than that of the standalone neural network model.", 
"This paper describes the application of neural network models for classification of electroencephalogram (EEG) signals. Decision making was performed in two stages: initially, a feature extraction scheme using the wavelet transform (WT) has been applied.",EEG Signal Classification Using Wavelet Feature Extraction and Neural Networks,"Decision Support Systems have been utilised since 1960, providing physicians with fast and accurate means towards more accurate diagnoses and increased tolerance when handling missing or incomplete data. This paper describes the application of neural network models for classification of electroencephalogram (EEG) signals. Decision making was performed in two stages: initially, a feature extraction scheme using the wavelet transform (WT) has been applied and then a learning-based algorithm classifier performed the classification. The performance of the neural model was evaluated in terms of training performance and classification accuracies and the results confirmed that the proposed scheme has potential in classifying the EEG signals.", 
"Study compares four representations of EEG signals and their classification by a two-layer neural network. The neural network is implemented on a CNAPS server (128 processor, SIMD architecture) by Adaptive Solutions, Inc.",EEG Signal Classification with Different Signal Representations,"If several mental states can be reliably distinguished by recognizing patterns in EEG, then a paralyzed person could communicate to a device like a wheelchair by composing sequences of these mental states. In this article, we report on a study comparing four representations of EEG signals and their classification by a two-layer neural network with sigmoid activation functions. The neural network is implemented on a CNAPS server (128 processor, SIMD architecture) by Adaptive Solutions, Inc., gaining a 100-folddecreasein training time over a Sun Sparc 10 for a large number of Ridden units.", 
"Affective states classification has become an important part of the Brain-Computer Interface (HCI) study. In this study, DBNs are trained on the narrow-band spectral features extracted from multichannel EEG recordings. The proposed framework using Deep Belief Networks not only provided better classification performance, but also significantly lower the number of labeled data required.",EEG-Based Affect States Classification using Deep Belief Networks,"Affective states classification has become an important part of the Brain-Computer Interface (HCI) study. In recent years, affective computing systems using physiological signals, such as ECG, GSR and EEG has shown very promising results. However, like many other machine learning studies involving physiological signals, the bottle neck is always around the database acquisition and the annotation process. To investigate potential ways to address this small sample problem, this paper introduces a Deep Belief Networks (DBN) based learning system for the EEG-based affective processing system. Through the greedy-Iayer pretraining using unlabeled data as weIl as a supervised fine-tuning process, the DBN-based approaches significantly reduced the number of labeled samples required. The DBN methods also acted as an application specific feature selector, by examining the weight vector between the input feature vector and the first invisible layer, we can gain much needed insights on the spatial or spectral locations of the most discriminating features. In this study, DBNs are trained on the narrow-band spectral features extracted from multichannel EEG recordings. To evaluate the efficacy of the proposed DBN-based learning system, we carried out an subject-independent affective states classification experiments on the DEAP database to classify 2-dimensional affect states. As a baseline to the proposed DBN approach, the same classification problem was also carried out using support vector machines (SVMs) and one way ANOVA based feature selection process. The classification results shown that the proposed framework using Deep Belief Networks not only provided better classification performance, but also significantly lower the number of labeled data required to train such machine learning systems.", 
"In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.",EEG-BASED EMOTION CLASSIFICATION USING DEEP BELIEF NETWORKS,"In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data, especially for images and speech. In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input. A hidden markov model (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, SVM and Graph regularized Extreme Learning Machine (GELM). The average accuracies of DBN-HMM, DBN, GELM, SVM, and KNN in our experiments are 87.62%, 86.91%, 85.67%, 84.08%, and 69.66%, respectively. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.", 
Deep learning network (DLN) is used to discover unknown feature correlation between input signals that is crucial for the learning task. DLN is capable of classifying three di?erent levels of valence and arousal with accuracy of 49.52% and 46.03% respectively.,EEG-Based Emotion Recognition Using Deep Learning Network with Principal Component Based Covariate Shift Adaptation,"Automatic emotion recognition is one of the most challenging tasks. To detect emotion from nonstationary EEG signals, a sophisticated learning algorithm that can represent high-level abstraction is required. Tis study proposes the utilization of a deep learning network (DLN) to discover unknown feature correlation between input signals that is crucial for the learning task. The DLN is implemented with a stacked autoencoder (SAE) using hierarchical feature learning approach. Input features of the network are power spectral densities of 32-channel EEG signals from 32 subjects. To alleviate overfitting problem, principal component analysis (PCA) is applied to extract the most important components of initial input features. Furthermore, covariate shift adaptation of the principal components is implemented to minimize the nonstationary e?ect of EEG signals. Experimental results show that the DLN is capable of classifying three di?erent levels of valence and arousal with accuracy of 49.52% and 46.03%, respectively. Principal component based covariate shift adaptation enhances the respective classification accuracy by 5.55% and 6.53%. Moreover, DLN provides better performance compared to SVM and naive Bayes classifiers.", 
Neural Networks are far more capable of identifying patterns and do the classification more effectively. This paper showcases various activation functions and their impact on overall accuracy of classification using a dataset of 84 samples of EEG signals. Accuracy is the measure considered for the comparison of the effectiveness of neural networks.,Effect of Different Activation Functions on EEGEffect of Different Activation Functions on EEG Signal Classification based on Neural Networks,"In recent years, EEG (Electroencephalography) has grabbed significant research attention towards giving it as an input to the computing devices. To identify hidden patterns, EEG signals is not straightforward, and in order to be able to use it as a reliable input method, adequate accuracy is required. While machine learning techniques are there for numeric, categorical and similar types of data, Neural Networks are far more capable of identifying patterns and do the classification more effectively. Activation functions are integral part of the neural networks and selection of the activation function is a choice for the neural network designer. This paper showcases various activation functions and their impact on overall accuracy of classification using a dataset of 84 samples of EEG signals using ANN (Artificial Neural Network) and CNN (Convolutional Neural Network). Total 6 different activation functions considered here at all levels of ANN and CNN and respective results are analyzed. Activation functions at different layers of both ANN and CNN are compared such as fully connected layers and convolution layers. Accuracy is the measure considered for the comparison of the effectiveness of neural networks.", 
"A large number of extractive summarization techniques have been developed in the past decade. We show that using a combination of several different sentence similarity measures, rather than only one, significantly improves performance of the resultant meta-system. Even simple ensemble techniques prove to be very effective in improving overall performance.",Effective aggregation of various summarization techniques,"A large number of extractive summarization techniques have been developed in the past decade, but very few enquiries have been made as to how these differ from each other or what are the factors that actually affect these systems. Such meaningful comparison if available can be used to create a robust ensemble of these approaches, which has the possibility to consistently outperform each individual summarization system. In this work we examine the roles of three principle components of an extractive summarization technique: sentence ranking algorithm, sentence similarity metric and text representation scheme. We show that using a combination of several different sentence similarity measures, rather than only one, significantly improves performance of the resultant meta-system. Even simple ensemble techniques, when used in an informed manner, prove to be very effective in improving the overall performance and consistency of summarization systems. A statistically significant improvement of about 5% to 10% in ROUGE-1 recall was achieved by aggregating various sentence similarity measures. As opposed to this aggregation of several ranking algorithms did not show a significant improvement in ROUGE score, but even in this case the resultant meta-systems were more robust than candidate systems. The results suggest that new extractive summarization techniques should particularly focus on defining a better sentence similarity metric and use multiple sentence similarity scores and ranking algorithms in favour of a particular combination.", 
Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions.,Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques,"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88:7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).", 
"The relevance score of sentence is determined through its comparison with all the other sentences in the document. The weight of each word in the sentence is calculated with account of those features. The weights of features, influencing relevance of words, are determined using genetic algorithms.",Effective Summarization Method of Text Documents,"In this paper, we propose text summarization method that creates text summary by definition of the relevance score of each sentence and extracting sentences from the original documents. While summarization this method takes into account weight of each sentence in the document. The essence of the method suggested is in preliminary identification of every sentence in the document with characteristic vector of words, which appear in the document, and calculation of relevance score for each sentence. The relevance score of sentence is determined through its comparison with all the other sentences in the document and with the document title by cosine measure. Prior to application of this method the scope of features is defined and then the weight of each word in the sentence is calculated with account of those features. The weights of features, influencing relevance of words, are determined using genetic algorithms.", 
This paper examines the quality of feature set obtained from Wavelet based Energy-entropy with variation of scale and wavelet type. Elliptic bandpass filters are used to discard unwanted signals and also to extract alpha & beta rhythms.,Effects of Wavelets on Quality of Features in Motor Imagery EEG Signal Classification,"This paper examines the quality of feature set obtained from Wavelet based Energy-entropy with variation of scale and wavelet type. Here motor imageryo?ef-right hand movement classification problem has been studied. Elliptic bandpass filters are used to discard unwanted signals and also to extract alpha & beta rhythms. We have implemented wavelet-based energy-entropy with three level of decomposition in combination with ten wavelet types (Daubechies). We want to identify the best pair of level of decomposition and wavelet type for EEG based motor imagery classifications. We have verified our study with three classifiers- Naive Bayes, Multi layered Perceptron and Support Vector Machine. The classifiers performance for best wavelet decomposition level is analyzed using evaluation metrics such as accuracy, F-measure and area under ROC.", 
"Using data-driven models for solving text summarization or similar tasks has become very common in the last years. Most studies report basic accuracy scores only, and nothing is known about the ability of the proposed models to improve when trained on more data.","Efficiency Metrics for Data-Driven Models, A Text Summarization Case Study","Using data-driven models for solving text summarization or similar tasks has become very common in the last years. Yet most of the studies report basic accuracy scores only, and nothing is known about the ability of the proposed models to improve when trained on more data. In this paper, we define and propose three data efficiency metrics: data score efficiency, data time deficiency and overall data efficiency. We also propose a simple scheme that uses those metrics and apply it for a more comprehensive evaluation of popular methods on text summarization and title generation tasks. For the latter task, we process and release a huge collection of 35 million abstract-title pairs from scientific articles. Our results reveal that among the tested models, the Transformer is the most efficient on both tasks.", 
"In VANET, the privacy of the query becomes a serious concern for location-based services (LBS) In this scheme, the vehicle requests for services from the location server without revealing the query content. The location server sends all Point of Interests (POIs) to the vehicle, but the vehicle can only obtain the request content.",Efficient and Secure Location-Based Services scheme in VANET,"In VANET, the privacy of the query becomes a serious concern for location-based services (LBS). An LBS scheme must, also, not breach the location privacy of the user vehicle. In order to protect query privacy of the vehicle user, the existing state-of-the-art schemes either reduce the accuracy of LBS or insert a trusted third party (TTP) between the vehicle user and the location server hosting the LBS scheme. Cryptographic constructs such as public-key encryption, Oblivious Transfer (OT), Private Information Retrieval (PIR), and homomorphic encryption are used to remove the TTP or increase the accuracy of the previous schemes. The problem with these existing schemes is that some of them are either inefficient or insecure. In order to address the security and privacy issues, we propose an efficient privacy-preserving mechanism for protecting the query privacy of the user, information content of the location server, and location privacy-preserving of the vehicle in the LBS scheme for VANET. In this scheme, the vehicle requests for services from the location server without revealing the query content to the location server. The location server sends all Point of Interests (POIs) to the vehicle, but the vehicle can only obtain the requested query content. The query privacy of the user and content privacy of the location server is preserved in the exchange. The proposed variant of the OT extension protocol is computationally and communicationally efficient and uses a lattice-based ring-Learning With Errors (ring-LWE) scheme as a Base-OT.", 
Single-trial motor imagery classification is a crucial aspect of brain–computer applications. Riemannian geometry-based feature extraction methods are effective when designing these types of motor-imagery-based brain– computer interface applications. Deep-learning methods are superior when the data availability is abundant and while there is a large number of features. Our proposed method improved the classification accuracy for several subjects that comprised the well-known BCI competition IV 2a dataset.,Efficient Classification of Motor Imagery Electroencephalography Signals Using Deep Learning Methods,"Single-trial motor imagery classification is a crucial aspect of brain–computer applications. Therefore, it is necessary to extract and discriminate signal features involving motor imagery movements. Riemannian geometry-based feature extraction methods are effective when designing these types of motor-imagery-based brain–computer interface applications. In the field of information theory, Riemannian geometry is mainly used with covariance matrices. Accordingly, investigations showed that if the method is used after the execution of the filterbank approach, the covariance matrix preserves the frequency and spatial information of the signal. Deep-learning methods are superior when the data availability is abundant and while there is a large number of features. The purpose of this study is to a) show how to use a single deep-learning-based classifier in conjunction with BCI (brain–computer interface) applications with the CSP (common spatial features) and the Riemannian geometry feature extraction methods in BCI applications and to b) describe one of the wrapper feature-selection algorithms, referred to as the particle swarm optimization, in combination with a decision tree algorithm. In this work, the CSP method was used for a multiclass case by using only one classifier. Additionally, a combination of power spectrum density features with covariance matrices mapped onto the tangent space of a Riemannian manifold was used. Furthermore, the particle swarm optimization method was implied to ease the training by penalizing bad features, and the moving windows method was used for augmentation. After empirical study, the convolutional neural network was adopted to classify the pre-processed data. Our proposed method improved the classification accuracy for several subjects that comprised the well-known BCI competition IV 2a dataset.", 
"Wearable health monitoring systems (WHMSs) will play an increasingly important role in future e-healthcare. Given its sensitivity, the health data should be protected against unauthorized access. It is critical to design an end-to-end mutual authentication protocol that enables secure communication.",Efficient end-to-end authentication protocol for wearable health monitoring systems,"Wearable health monitoring systems (WHMSs) will play an increasingly important role in future e-healthcare and enable smart and ubiquitous healthcare services. Given its sensitivity, the health data should be protected against unauthorized access. As a result, it is critical to design an end-to-end mutual authentication protocol that enables secure communication between the wearable sensor and medical professionals. Recently, Amin et al. proposed an anonymity preserving mutual authentication protocol for WHMSs. However, we identify that their protocol suffers from stolen mobile device attack, desynchronization attack, and sensor key exposure. Then we put forward an improved end-to-end authentication protocol based on quadratic residues. Comprehensive security analysis is conducted to show that the proposed protocol fixes these flaws of Amin et al.’s protocol and satisfies all desired requirements. The comparison with these existing protocols demonstrates that our protocol provides a practical end-to-end security solution for WHMSs.", 
"A new Turkish text summarization system that combines structural and semantic features. The system uses 5 structural features, 1 of which is newly proposed and 3 are semantic features extracted from Turkish Wikipedia links. The features are combined using the weights calculated by 2 novel approaches.",Efficient feature integration with Wikipedia-based semantic feature extraction for Turkish text summarization,"This study presents a novel hybrid Turkish text summarization system that combines structural and semantic features. The system uses 5 structural features, 1 of which is newly proposed and 3 are semantic features whose values are extracted from Turkish Wikipedia links. The features are combined using the weights calculated by 2 novel approaches. The first approach makes use of an analytical hierarchical process, which depends on a series of expert judgments based on pairwise comparisons of the features. The second approach makes use of the artificial bee colony algorithm for automatically determining the weights of the features. To confirm the significance of the proposed hybrid system, its performance is evaluated on a new Turkish corpus that contains 110 documents and 3 human-generated extractive summary corpora. The experimental results show that exploiting all of the features by combining them results in a better performance than exploiting each feature individually.", 
"This paper aims to develop a convenient interface that provides keyword extraction, summary generation and search engine to users. We apply the proposed summarization method to Korean and English news articles. We think that this interface can help users more efficiently to read the news articles on various mobile devices.",EFFICIENT KEYWORD EXTRACTION AND TEXT SUMMARIZATION FOR READING ARTICLES ON SMART PHONE,"These days, we can connect to the internet from almost anywhere, allowing us to access web content, including newspapers, magazines, blogs and websites, using mobile devices such as a smart phone. However, people sometimes struggle to read and use the contents due to the nature of these devices such as a small display, low display resolution and limited computing resources (low CPU speed and little memory). This paper aims to develop a convenient interface that provides keyword extraction, summary generation and search engine to users. We apply the proposed summarization method to Korean and English news articles and evaluate it using several experiments on single and multiple news article test collections and user-receptiveness tests. Since the proposed method shows a good performance on these experiments and tests, we think that this interface can help users more efficiently to read the news articles on various mobile devices.", 
This paper addresses the problem of Korean text summarization (KTS) and presents a flexible multi- plugin framework. We design a novel KTS algorithm based on key phrase extraction.,Efficient Korean text summarization based on key phrase extraction,"Language big data explosion causes a severe divergence of people’s attention and a serious scarcity of people’s time. This paper addresses the problem of Korean text summarization (KTS) and presents a flexible multi-plugin framework. Within the framework, we design a novel KTS algorithm based on key phrase extraction. Supported by the pluggable components of word stemming and part of speech tagging, the key-phrase-extraction-based KTS algorithm can complete text summarization efficiently. The experimental results show that our KTS algorithm with MMR plugin component can achieve the perfect performance in the Korean summarization task.", 
"The electroencephalogram (EEG) signal is an electrical representation of brain's working. Alcohol can affect whole parts of the body but, it particularly affects the brain, heart, liver, and immune system. EMD method decomposes non-stationary and non-linear signals into a set of stationary intrinsic mode functions (IMFs).",Efficient method for classification of alcoholic and normal EEG signals using EMD,"The electroencephalogram (EEG) signal is an electrical representation of brain’s working that reflects various physiological and pathological activities such as alcoholism. Alcohol can affect whole parts of the body but, it particularly affects the brain, heart, liver, and the immune system; its effects on the brain are called brain disorders. Nowadays, automatic identification of alcoholic subjects based on EEG signals has become one of the challenging problems in biomedical research. In this study, an automatic classification method for classifying alcoholic and normal EEG signals, based on empirical mode decomposition (EMD), is proposed. The uniqueness of EMD method is to decompose non-stationary and non-linear signals into a set of stationary intrinsic mode functions (IMFs) that are band limited signals. These IMFs are transformed into analytic representations by applying the Hilbert transform. From these analytic IMFs, various features namely mean, kurtosis, skewness, entropy, and negentropy are extracted; these features are used as input to least squares support vector machines (LS-SVMs) classifier with radial basis function (RBF) kernel and polynomial kernel. The accuracy results achieved for LS-SVM classifier with polynomial and RBF kernels are found to be 96.67 and 97.92%, respectively, which are found to be better as compared with other state-of-the-art methods.", 
"Algorithm first detects low-level regions that could potentially belong to the object. It then performs a full-object segmentation through propagation. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%.",Efficient object detection and segmentation for fine-grained recognition,"We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.", 
Cardiovascular diseases (CVD) are among the most common serious illnesses affecting human health. Identifying risk factors using machine learning models is a promising approach. We propose a model that incorporates different methods to achieve effective prediction of heart disease. Our proposed model produced the highest accuracy while using RFBM and Relief feature selection methods (99.05%).,Efficient Prediction of Cardiovascular Disease Using Machine Learning Algorithms With Relief and LASSO Feature Selection Techniques,"Cardiovascular diseases (CVD) are among the most common serious illnesses affecting human health. CVDs may be prevented or mitigated by early diagnosis, and this may reduce mortality rates. Identifying risk factors using machine learning models is a promising approach. We would like to propose a model that incorporates different methods to achieve effective prediction of heart disease. For our proposed model to be successful, we have used efficient Data Collection, Data Pre-processing and Data Transformation methods to create accurate information for the training model. We have used a combined dataset (Cleveland, Long Beach VA, Switzerland, Hungarian and Stat log). Suitable features are selected by using the Relief, and Least Absolute Shrinkage and Selection Operator (LASSO) techniques. New hybrid classifiers like Decision Tree Bagging Method (DTBM), Random Forest Bagging Method (RFBM), K-Nearest Neighbors Bagging Method (KNNBM), AdaBoost Boosting Method (ABBM), and Gradient Boosting Boosting Method (GBBM) are developed by integrating the traditional classifiers with bagging and boosting methods, which are used in the training process. We have also instrumented some machine learning algorithms to calculate the Accuracy (ACC), Sensitivity (SEN), Error Rate, Precision (PRE) and F1 Score (F1) of our model, along with the Negative Predictive Value (NPR), False Positive Rate (FPR), and False Negative Rate (FNR). The results are shown separately to provide comparisons. Based on the result analysis, we can conclude that our proposed model produced the highest accuracy while using RFBM and Relief feature selection methods (99.05%).", 
"In this work, we modify the Weighted TF_IDF (Term Frequency Inverse Document Frequency) algorithm. We compare the modified algorithm with that of the existing algorithms of TextRank Algorithm, Luhn's Algorithm and others. We find that the proposed algorithm would highly be useful for blind people.",Efficient text summarization method for blind people using text mining techniques,"Owing to the phenomenal growth in communication technology, most of us hardly have time to read books. This habit of reading is slowly diminishing because of the busy lives of people. For visually challenged people, the situation is even worse. In order to address this impedes, we develop a better and more accurate methodology than the existing ones. In this work, in order to save the efforts for reading the complete text every time, we modify the Weighted TF_IDF (Term Frequency Inverse Document Frequency) algorithm to summarize books into relevant keywords. Then, we compare the modified algorithm with that of the existing algorithms of TextRank Algorithm, Luhn’s Algorithm, LexRank Algorithm, Latent Semantic Analysis(LSA). From the comparative analysis, we find that Weighted TF_IDF is an efficient algorithm to automate text summarization and produce an effective summary which is then converted from text to speech. Thus, the proposed algorithm would highly be useful for blind people.", 
Textual content summarization is the system of extracting essential facts from the given facts. Text summarization has been used in lots of application like business evaluation and marketplace overview.,EFFICIENT TEXT SUMMARIZER USING POINT TO GENERATOR TECHNIQUE,"Text summarization is the system of extracting essential facts from the given facts and writing it within the form of precis. Many people locate it tough to read the whole passage of records so that it will gather the essential key factors, so we use textual content summarization to reduce the burden of analyzing big passages. Text summarization is been used in lots of application like business evaluation and marketplace overview. On the entire, by using textual content summarization we get the statistics wished simply by using studying the summary.", 
"In our model, vehicles act as ants and they choose their itineraries thanks to a pheromone map affected by the phenomenon of evaporation. The presented algorithms are evaluated in real world traffic networks and by modeling and simulating extreme cases.",Efficient Vehicular Crowdsourcing Models in VANET for Disaster Management,"Route planning in a vehicular network is a well known problem. Static solutions for finding a shortest path have proven their efficiency, however in a dynamic network such as a vehicular network, they are confronted to dynamic costs (travel time, consumption, waiting time, ...) and time constraints (traffic peaks, ghost traffic jam, accidents ...). This is a practical problem faced by several services providers on traffic information who want to offer a realistic computation of a shortest path. This paper propose a model based on the communication between vehicles (Vehicle to Vehicle: V2V) to reduce the time spend by travels taking into account the travel time registered and exchanged between vehicles in real time. In our model, vehicles act as ants and they choose their itineraries thanks to a pheromone map affected by the phenomenon of evaporation. The presented algorithms are evaluated in real world traffic networks and by modeling and simulating extreme cases such as accidents, act of terrorism and disasters.", 
Automatic text summarization is the process of generating a summary by condensing text document by a computer machine. Main issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize the quality of summary.,Efficient Voting-Based Extractive Automatic Text Summarization Using Prominent Feature Set,"Automatic text summarization (ATS) is the process of generating a summary by condensing text document by a computer machine. In this paper, we explored voting-based extractive approaches for text summarization. The main issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize the quality of summary. Voting-based methods are sensitive to initial ranking process. We proposed reciprocal ranking-based sentence scoring approach that alleviates the feature weighting and initial ranking problem. The proposed approach uses a specific prominent set of features for initial ranking that further enhance the performance. Experimental results on Document Understating Conference 2002 data-set using ROUGE evaluation matrices shows that our proposed method performs better as compared to other voting-based methods.", 
"In this work, we used single electrooculogram (EOG) signal to perform automatic sleep scoring. Deep belief network (DBN) and combination of DBN and Hidden Markov Models are employed to discriminate sleep stages.",Electrooculogram based Sleep Stage Classification Using Deep Belief Network,"In this work, we used single electrooculogram (EOG) signal to perform automatic sleep scoring. Deep belief network (DBN) and combination of DBN and Hidden Markov Models (HMM) are employed to discriminate sleep stages. Under the leave-one-out protocol, the average accuracy of DBN and DBN-HMM are 77.7% and 83.3% for all sleep stages, respectively. On the other hand, we found the EOG signal not only contribute to identify stages of Awake and rapid eye movement, also contribute to discriminate stage 2 and slow wave sleep stage.", 
"Automatic text summarization has emerged as a technique for accessing only to useful information. So far, some approaches have got good results combining different strategies with language-dependent knowledge. This paper presents a competitive method based on an EM clustering algorithm for improving the quality of the automatic summaries.",EM Clustering Algorithm for Automatic Text Summarization,"Automatic text summarization has emerged as a technique for accessing only to useful information. In order to known the quality of the automatic summaries produced by a system, in DUC 2002 (Document Understanding Conference) has developed a standard human summaries called gold collection of 567 documents of single news. In this conference only five systems could outperforms the baseline heuristic in single extractive summarization task. So far, some approaches have got good results combining different strategies with language-dependent knowledge. In this paper, we present a competitive method based on an EM clustering algorithm for improving the quality of the automatic summaries using practically non language-dependent knowledge. Also, a comparison of this method with three text models is presented.", 
"SemEval- 2019 Task 6 was OffensEval: Identifying and Categorizing Offensive Language in Social Media. The task was divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification.",Embeddia at SemEval-2019 Task 6 Detecting Hate with Neural Network and Transfer Learning Approaches,"SemEval-2019 Task 6 was OffensEval: Identifying and Categorizing Offensive Language in Social Media. The task was further divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification. In this paper, we present the approaches used by the Embeddia team, who qualified as fourth, eighteenth and fifth on the three sub-tasks. A different model was trained for each sub-task. For the first sub-task, we used a BERT model fine-tuned on the provided dataset, while for the second and third tasks we developed a custom neural network architecture which combines bag-of-words features and automatically generated sequence-based features. Our results show that combining automatically and manually crafted features fed into a neural architecture outperform transfer learning approach on more unbalanced datasets.", 
"Internet of Vehicles is a specific application of Internet of Things technology in intelligent transportation systems. US government has spent hundreds of millions of dollars on DSRC development. DSRC is a dedicated short-range communication developed and tested by automotive technology suppliers. In the face of the 5G era, the dispute between DSRC and LTE standards has become a hot topic in the field of car networking.",Emerging technologies for 5G-enabled vehicular networks,"Internet of Vehicles is a specific application of Internet of Things technology in intelligent transportation systems, and has attracted attention of relevant research institutions, automobile manufacturers and communication technology suppliers all over the world. The US government has spent hundreds of millions of dollars on DSRC development, which is a technology that can help achieve V2V and V2I communications, and enable vehicles to communicate with intelligent traffic lights to mitigate traffic congestion and accidents or bad weather on the road. Automakers such as GM and Toyota have well deployed Wi-Fi-based DSRC technology. But with the continuous development of 5G and D2D communications, cellular technology tends to be a strong candidate for V2X communications. .Although 5G communication technology does not yet have a unified standard, its related concepts and trends have been recognized by industry area. Automobile manufacturers represented by Ford and BMW expect to achieve a leap in automotive networking through 5G. In the face of the 5G era, the dispute between DSRC and LTE standards has become a hot topic in the field of car networking. DSRC, a dedicated short-range communication developed and tested by automotive technology suppliers is under threat, and the next generation of 5G technology might be the final answer. Based on this point, this paper firstly introduces the development history of the Internet of Vehicles communication standard, and analyzes the advantages and disadvantages of DSRC and cellular network communication technology. Then, the three core elements of the Internet of Vehicles (Node Performance, local Network, and Internet of Things) are discussed for the development trend of vehicle networking communication technology in the context of 5G. Finally, this chapter explains the impact of the development of 5G communication technology on the future development of vehicle networking, and proposes the possible development directions of future vehicle networking technologies.", 
"Human Computer Interaction (HCI) system has become an increasingly important part of our daily lives. HCI determines the effective utilization of the available information flow of the computing, communication, and display technologies. In recent years, there has been a tremendous interest in introducing intuitive interfaces that can recognize the user's body movements and translate them into machine commands.",EMG Signal Classification for Human Computer Interaction A Review,"With the ever increasing role of computerized machines in society, Human Computer Interaction (HCI) system has become an increasingly important part of our daily lives. HCI determines the effective utilization of the available information flow of the computing, communication, and display technologies. In recent years, there has been a tremendous interest in introducing intuitive interfaces that can recognize the user's body movements and translate them into machine commands. For the neural linkage with computers, various biomedical signals (biosignals) can be used, which can be acquired from a specialized tissue, organ, or cell system like the nervous system. Examples include Electro-Encephalogram (EEG), Electrooculogram (EOG), and Electromyogram (EMG). Such approaches are extremely valuable to physically disabled persons. Many attempts have been made to use EMG signal from gesture for developing HCI. EMG signal processing and controller work is currently proceeding in various direction including the development of continuous EMG signal classification for graphical controller, that enables the physically disabled to use word processing programs and other personal computer software, internet. It also enable manipulation of robotic devices, prosthesis limb, I/O for virtual reality games, physical exercise equipments etc. Most of the developmental area is based on pattern recognition using neural networks. The EMG controller can be programmed to perform gesture recognition based on signal analysis of groups of muscles action potential. This review paper is to discuss the various methodologies and algorithms used for EMG signal classification for the purpose of interpreting the EMG signal into computer command.", 
Emotion is closely related to healthy and abnormal mood is the alarm of our body. We propose emotional patches and combine it with the deep belief network(DBN) to achieve high-precision emotion classification. The experimental result demonstrates that our method achieves the highest classification accuracy and outperform the state-of-the-art emotion classification approaches.,Emotion Classification Using Deep Neural Networks and Emotional Patches,"Emotion is closely related to healthy and abnormal mood is the alarm of our body. This paper is concentrated on the objective and accurate emotion classification using EEG signal. We propose emotional patches and combine it with the deep belief network(DBN) to achieve high-precision emotion classification. DBN is able to fit the distribution of the EEG signal and mapping the extracted feature to the higher-level characteristics space where we can easily perform high-precision classification. Compared with the other method, our method uses the emotional patches which have considered the temporal information of emotion and reduce the influence of noise. In addition, our model doesn’t need to be trained twice to complete higher classification accuracy. We divide the EEG signal and choose the vital ? frequency band where we perform feature extraction. Based on the SJTU Emotion EEG Dataset(SEED), we perform the emotion classification experiment and compare our method with the commonly used classifiers such as SVM, LR and CCA etc. The experimental result demonstrates that our method achieves the highest classification accuracy and outperform the state-of-the-art emotion classification approaches based on EEG.", 
Emotion classification finds application in brain–computer interface systems for the assistance of disabled persons. Electroencephalogram (EEG) signal plays a vital role because it provides immediate response to every state of change in the human brain.,Emotion classification using EEG signals based on tunable-Q wavelet transform,"Emotion is a most instinctive feeling of a human. Emotion classification finds application in brain–computer interface systems for the assistance of disabled persons. To recognise the emotional state, electroencephalogram (EEG) signal plays a vital role because it provides immediate response to every state of change in the human brain. Here, the utility of tunable-Q wavelet transform (TQWT) is explored for the classification of different emotions EEG signals. TQWT decomposes EEG signal into subbands and time-domain features are extracted from subbands. The extracted features are used as an input to extreme learning machine classifier for the classification of happy, fear, sad, and relax emotions. Experimental results of the proposed method show better four emotions classification performance when compared with the other existing methods.", 
"In this research we propose to use EEG signal to classify two emotions (i.e., positive and negative) elicited by pictures. With power spectrum features, the accuracy rate of SVM classifier is about 85.41%.",Emotion Classification using Minimal EEG Channels and Frequency Bands,"In this research we propose to use EEG signal to classify two emotions (i.e., positive and negative) elicited by pictures. With power spectrum features, the accuracy rate of SVM classifier is about 85.41%. Considering each pair of channels and different frequency bands, it shows that frontal pairs of channels give a better result than the other area and high frequency bands give a better result than low frequency bands. Furthermore, we can reduce number of pairs of channels from 7 to 5 with almost the same accuracy and can cut low frequency bands in order to save computation time. All of these are beneficial to the development of emotion classification system using minimal EEG channels in real-time.", 
Emotion detection is playing a very important role in our life. Roman Urdu is the most widely used language on social media platforms for communication. There are many useful applications of the emotional analysis of a text. We developed a comprehensive corpus of 18k sentences that are gathered from di?erent domains. The results showed that the SVM model achieves a better F-measure score.,Emotion Detection in Roman Urdu Text using Machine Learning,"Emotion detection is playing a very important role in our life. People express their emotions in di?erent ways i.e., face expression, gestures, speech, and text. This research focuses on detecting emotions from the Roman Urdu text. Previously, A lot of work has been done on di?erent languages for emotion detection but there is limited work done in Roman Urdu. Therefore, there is a need to explore Roman Urdu as it is the most widely used language on social media platforms for communication. One major issue for the Roman Urdu is the absence of benchmark corpora for emotion detection from text because language assets are essential for di?erent natural language processing (NLP) tasks. There are many useful applications of the emotional analysis of a text such as improving the quality of products, dialog systems, investment trends, mental health. In this research, to focus on the emotional polarity of the Roman Urdu sentence we develop a comprehensive corpus of 18k sentences that are gathered from di?erent domains and annotate it with six di?erent classes. We applied di?erent baseline algorithms like KNN, Decision tree, SVM, and Random Forest on our corpus. After experimentation and evaluation, the results showed that the SVM model achieves a better F-measure score.", 
"Proposed system is evaluated using two audio-visual emotional databases, one of which is Big Data. The Big Data comprises of speech and video. In the proposed system, a speech signal is first processed in the frequency domain to obtain a Mel-spectrogram, which can be treated as an image.",Emotion Recognition Using Deep Learning Approach from Audio-Visual Emotional Big Data,"This paper proposes an emotion recognition system using a deep learning approach from emotional Big Data. The Big Data comprises of speech and video. In the proposed system, a speech signal is first processed in the frequency domain to obtain a Mel-spectrogram, which can be treated as an image. Then this Mel-spectrogram is fed to a convolutional neural network (CNN). For video signals, some representative frames from a video segment are extracted and fed to the CNN. The outputs of the two CNNs are fused using two consecutive extreme learning machines (ELMs). The output of the fusion is given to a support vector machine (SVM) for final classification of the emotions. The proposed system is evaluated using two audio-visual emotional databases, one of which is Big Data. Experimental results confirm the effectiveness of the proposed system involving the CNNs and the ELMs.", 
This work proposes an innovative method that hybridizes the principal component analysis (PCA) and t-statistics for feature extraction. The proposed method has been applied on the SEED dataset (SJTU Emotion EEG Dataset) that yielded significant channels and features for getting higher classification accuracy.,Employing PCA and t-statistical approach for feature extraction andclassification of emotion from multichannel EEG signal,"To achieve a highly efficient brain-computer interface (BCI) system regarding emotion recognition from electroencephalogram (EEG) signal, the most crucial issues are feature extractions and classifier selection. This work proposes an innovative method that hybridizes the principal component analysis (PCA) and t-statistics for feature extraction. This work contributes to successfully implement spatial PCA to reduce signal dimensionality and to select the suitable features based on the t-statistical inferences among the classes. The proposed method has been applied on the SEED dataset (SJTU Emotion EEG Dataset) that yielded significant channels and features for getting higher classification accuracy. With extracted features, four classifiers– support vector machine (SVM), artificial neural network (ANN), linear discriminant analysis (LDA), and k-nearest neighbor (k-NN) method were applied to classify the emotional states. The classifiers showed slightly different classification accuracies compared to each other. ANN and SVM showed the highest classification accuracy (86.57 ± 4.08 and 85.85 ± 5.72) in case of subject dependent approach. On the other hand, the proposed method provides 84.3% and 77.1% classification accuracy with ANN and SVM, respectively in case of subject independent approach. Eventually, the proposed method and its outcomes demonstrate that this proposal is better than the several existing methods in emotion recognition.", 
"This paper investigates the infrastructure to vehicle and infrastructure to cloud connectivity and reliability in the vehicular ad hoc networks (VANET) area of Intelligent Transportation Systems (ITS) A key focus of this work is to investigate protocols that will enhance real-time, robust and reliable communication methods.",End to End VANET IoT Communications A 5G Smart Cities Case Study Approach,"This paper investigates the infrastructure to vehicle and infrastructure to cloud connectivity and reliability in the vehicular ad hoc networks (VANET) area of Intelligent Transportation Systems (ITS). A key focus of this work is to investigate protocols that will enhance real-time, robust and reliable communication methods, and complement autonomous vehicles’ navigation experiences within smart cities. The main areas of study include highway infrastructure that include the Wireless Sensor Networks (WSN) to the Cloud (web service) and vice-versa. The pertinent cloud-based data will be communicated to subscribed vehicles (with password access) to complete the V2I and I2V communication cycle. The data collected from the WSN is communicated to the cloud via XML over XMPP, zero configuration, and mDNS protocols. The use of the XMPP protocol to communicate data to the cloud data repository represents a novel approach to IoT harmonization for this particular infrastructure to cloud/I2V application.", 
"WhatsApp messaging service has emerged as the most popular messaging app on mobile devices today. It uses end-to-end encryption which makes government and secret services efforts to combat organized crime, terrorists, and child pornographers technically impossible. Governments would like a ""backdoor"" into such apps, to use in accessing messages.",End-to-End Encryption in Messaging Services and National Security—Case of WhatsApp,"The ubiquity of instant messaging services on mobile devices and their use of end-to-end encryption in safeguarding the privacy of their users have become a concern for some governments. WhatsApp messaging service has emerged as the most popular messaging app on mobile devices today. It uses end-to-end encryption which makes government and secret services efforts to combat organized crime, terrorists, and child pornographers technically impossible. Governments would like a “backdoor” into such apps, to use in accessing messages and have emphasized that they will only use the “backdoor” if there is a credible threat to national security. Users of WhatsApp have however, argued against a “backdoor”; they claim a “backdoor” would not only be an infringement of their privacy, but that hackers could also take advantage of it. In light of this security and privacy conflict between the end users of WhatsApp and government’s need to access messages in order to thwart potential terror attacks, this paper presents the advantages of maintaining E2EE in WhatsApp and why governments should not be allowed a “backdoor” to access users’ messages. This research presents the benefits encryption has on consumer security and privacy, and also on the challenges it poses to public safety and national security.", 
Deep learning has made significant improvements at many image processing tasks in recent years. The number of parameters in a CNN is too high such that computers require more energy and larger memory size. We propose a novel energy efficient model Binary Weight and Hadamard-transformed Image Network.,Energy Efficient Hadamard Neural Networks,"Deep learning has made significant improvements at many image processing tasks in recent years, such as image classification, object recognition and object detection. Convolutional neural networks (CNN), which is a popular deep learning architecture designed to process data in multiple array form, show great success to almost all detection & recognition problems and computer vision tasks. However, the number of parameters in a CNN is too high such that the computers require more energy and larger memory size. In order to solve this problem, we propose a novel energy efficient model Binary Weight and Hadamard-transformed Image Network (BWHIN), which is a combination of Binary Weight Network (BWN) and Hadamard-transformed Image Network (HIN). It is observed that energy efficiency is achieved with a slight sacrifice at classification accuracy. Among all energy efficient networks, our novel ensemble model outperforms other energy efficient models.", 
There are two main methodologies to perform text summarization -Extraction and Abstraction. Summarization by Extraction involves selection of most frequent words from the original text. abstraction method uses linguistic method to interpret the text. The fuzzy logic is use to classify the sentences based on feature score to get the summary of a whole document.,Enforcing Text Summarization using Fuzzy Logic,"In today’s modern era of information revolution, e-business expands rapidly with a large volume of document. In order to analyze the importance data from the document “Text Summarization” will be useful in serving the need of user. There are two main methodologies to perform text summarization -Extraction and Abstraction. Summarization by Extraction involves selection of most frequent words from the original text, whereas abstraction method uses linguistic method to interpret the text. A rouge and Pyramid method is well utilized to extract text for summarization. Previously adopted text summarization method uses information fragment that has been weighted as an important fragment by human summaries for the text. The main drawback of abstractive method is that, it understands the original text and re-telling it into shorter version. Our proposed system uses extraction method i.e. semantic matching instead of lexical matching. The fuzzy logic is use to classify the sentences based on feature score to get the summary of a whole document.", 
"Machine Learning (ML) techniques are commonly used to solve many problems in data science. In this paper, we used UCI Heart Disease dataset to test ML techniques along with conventional methods. A superlative increase of 2.1% accuracy for anemic classifiers was attained with the help of an ensemble voting based model.",Enhanced Accuracy of Heart Disease Prediction using Machine Learning and Recurrent Neural Networks Ensemble Majority Voting Method,"To solve many problems in data science, Machine Learning (ML) techniques implicates artificial intelligence which are commonly used. The major utilization of ML is to predict the conclusion established on the extant data. Using an established dataset machine determine emulate and spread them to an unfamiliar data sets to anticipate the conclusion. A few classification algorithm’s accuracy prediction is satisfactory, although other perform limited accuracy. Different ML and Deep Learning (DL) networks established on ANN have been extensively recommended for the disclosure of heart disease in antecedent researches. In this paper, we used UCI Heart Disease dataset to test ML techniques along with conventional methods (i.e. random forest, support vector machine, K-nearest neighbor), as well as deep learning models (i.e. long short-term-memory and gated-recurrent unit neural networks). To improve the accuracy of weak algorithms we explore voting based model by combining multiple classifiers. A provisional cogent approach was used to regulate how the ensemble technique can be enforced to improve an accuracy in the heart disease prediction. The strength of the proposed ensemble approach such as voting based model is compelling in improving the prognosis accuracy of anemic classifiers and established adequate achievement in analyze risk of heart disease. A superlative increase of 2.1% accuracy for anemic classifiers was attained with the help of an ensemble voting based model.", 
"Risk factor identification extraction model achieved 0.9073 of F-score, and prediction model achieved. 0.9516 of F -score. The prediction result is better than the most previous methods. The character-level model based on text region embedding can well map risk factors and their labels as a unit into a vector.",Enhanced character-level deep convolutional neural networks for cardiovascular disease prediction,"Electronic medical records contain a variety of valuable medical information for patients. So, when we are able to recognize and extract risk factors for disease from EMRs of patients with cardiovascular disease (CVD), and are able to use them to predict CVD, we have the ability to automatically process clinical texts, resulting in an improved accuracy of supporting doctors for the clinical diagnosis of CVD. In the case where CVD is becoming more worldwide, predictive CVD based on EMRs has been studied by many researchers to address this important aspect of improving diagnostic efficiency. This paper proposes an Enhanced Character-level Deep Convolutional Neural Networks (EnDCNN) model for cardiovascular disease prediction. On the manually annotated Chinese EMRs corpus, our risk factor identification extraction model achieved 0.9073 of F-score, our prediction model achieved 0.9516 of F-score, and the prediction result is better than the most previous methods. The character-level model based on text region embedding can well map risk factors and their labels as a unit into a vector, and downsampling plays a crucial role in improving the training efficiency of deep CNN. What’s more, the shortcut connections with pre-activation used in our model architecture implements dimension-matching free in training.", 
The diagnosis of heart disease has become a difficult medical task in the present medical research. The Enhanced Deep learning assisted Convolutional Neural Network (EDCNN) has been proposed to help doctors. The EDCNN system has been implemented on the Internet of Medical Things Platform (IoMT) Test results show that a flexible design and subsequent tuning of EDCNN hyperparameters can achieve a precision of up to 99.1%.,Enhanced Deep Learning Assisted Convolutional Neural Network for Heart Disease Prediction on the Internet of Medical Things Platform,"The diagnosis of heart disease has become a difficult medical task in the present medical research. This diagnosis depends on the detailed and precise analysis of the patient’s clinical test data on an individual’s health history. The enormous developments in the field of deep learning seek to create intelligent automated systems that help doctors both to predict and to determine the disease with the internet of things (IoT) assistance. Therefore, the Enhanced Deep learning assisted Convolutional Neural Network (EDCNN) has been proposed to assist and improve patient prognostics of heart disease. The EDCNN model is focused on a deeper architecture which covers multi-layer perceptron’s model with regularization learning approaches. Furthermore, the system performance is validated with full features and minimized features. Hence, the reduction in the features affects the efficiency of classifiers in terms of processing time, and accuracy has been mathematically analyzed with test results. The EDCNN system has been implemented on the Internet of Medical Things Platform (IoMT) for decision support systems which helps doctors to effectively diagnose heart patient’s information in cloud platforms anywhere in the world. The test results show compared to conventional approaches such as Artificial Neural Network (ANN), Deep Neural Network (DNN), Ensemble Deep Learning-based smart healthcare system (EDL-SHS), Recurrent neural network (RNN), Neural network ensemble method (NNE), based on the analysis the designed diagnostic system can efficiently determine the risk level of heart disease effectively. Test results show that a flexible design and subsequent tuning of EDCNN hyperparameters can achieve a precision of up to 99.1 %.", 
Deep-learning-based driver-drowsiness detection for brain-computer interface (BCI) using functional near-infrared spectroscopy (fNIRS) is investigated. The passive brain signals from drowsiness were acquired from 13 healthy subjects while driving a car simulator. Deep neural networks (DNN) were pursued to classify the drowsy and alert states. The CNN architecture resulted in an average accuracy of 99.3% showing the model was capable of differentiating the images of drowssy/non-drowsy states.,Enhanced Drowsiness Detection Using Deep Learning-An fNIRS Study,"In this paper, a deep-learning-based driver-drowsiness detection for brain-computer interface (BCI) using functional near-infrared spectroscopy (fNIRS) is investigated. The passive brain signals from drowsiness were acquired from 13 healthy subjects while driving a car simulator. The brain activities were measured with a continuous-wave fNIRS system, in which the prefrontal and dorsolateral prefrontal cortices were focused. Deep neural networks (DNN) were pursued to classify the drowsy and alert states. For training and testing the models, the convolutional neural networks (CNN) were used on color map images to determine the best suitable channels for brain activity detection in 0?1, 0?3, 0?5, and 0?10 second time windows. The average accuracies (i.e., 82.7, 89.4, 93.7, and 97.2% in the 0?1, 0?3, 0?5, and 0?10 sec time windows, respectively) using DNNs from the right dorsolateral prefrontal cortex were obtained. The CNN architecture resulted in an average accuracy of 99.3%, showing the model to be capable of differentiating the images of drowsy/non-drowsy states. The proposed approach is promising for detecting drowsiness and in accessing the brain location for a passive BCI.", 
An approach of automatic Bangla text summarization is presented here by enhancing an existing keyphrase-based method. The enhancement is accomplished with three steps as folIows. Performance is measured with ROUGE (Recall Oriented Understudy for Gisting Evaluation) automatic evaluation package.,Enhancement of keyphrase-based approach of automatic Bangla text summarization,"An approach of automatic Bangla text summarization is presented here by enhancing an existing keyphrase-based method. The enhancement is accomplished with three steps as folIows: (i) modifying the keyphrases selection process, (ii) including the first sentence in summary if it contains any title word and (iii) counting numerical figure which is presented in digits and words for sentence scoring. Step by step performance analysis of our proposed approach is portrayed for two datasets. Performance is measured with ROUGE (Recall Oriented Understudy for Gisting Evaluation) automatic evaluation package. The results, based on ROUGE-l and ROUGE-2 scores, show that the proposed enhancement has significant influence for Bangla text summarization over existing keyphrase-based method.", 
"Method for generating text summary for a given biomedical concept, e.g., H1N1 disease, from multiple documents based on semantic relation extraction. Results are better than the MEAD system, a well-known tool for text summarization.",Enhancing Biomedical Text Summarization Using Semantic Relation Extraction,"Automatic text summarization for a biomedical concept can help researchers to get the key points of a certain topic from large amount of biomedical literature efficiently. In this paper, we present a method for generating text summary for a given biomedical concept, e.g., H1N1 disease, from multiple documents based on semantic relation extraction. Our approach includes three stages: 1) We extract semantic relations in each sentence using the semantic knowledge representation tool SemRep. 2) We develop a relation-level retrieval method to select the relations most relevant to each query concept and visualize them in a graphic representation. 3) For relations in the relevant set, we extract informative sentences that can interpret them from the document collection to generate text summary using an information retrieval based method. Our major focus in this work is to investigate the contribution of semantic relation extraction to the task of biomedical text summarization. The experimental results on summarization for a set of diseases show that the introduction of semantic knowledge improves the performance and our results are better than the MEAD system, a well-known tool for text summarization.", 
Clustering in the vehicular ad-hoc network (VANET) is crucial for enhancing the stability of the collaborative environment. A novel clustering algorithm based on the Spectral Clustering algorithm and the improved force-directed algorithm is designed.,Enhancing Clustering Stability in VANET A Spectral Clustering Based Approach,"Vehicles can establish a collaborative environment cognition through sharing the original or processed sensor data from the vehicular sensors and status map. Clustering in the vehicular ad-hoc network (VANET) is crucial for enhancing the stability of the collaborative environment. In this paper, the problem for clustering is innovatively transformed into a cutting graph problem. A novel clustering algorithm based on the Spectral Clustering algorithm and the improved force-directed algorithm is designed. It takes the average lifetime of all clusters as an optimization goal so that the stability of the entire system can be enhanced. A series of close-to-practical scenarios are generated by the Simulation of Urban Mobility (SUMO). The numerical results indicate that our approach has superior performance in maintaining whole cluster stability.", 
NetSum is a new approach to automatic summarization based on neural nets. We extract a set of features from each sentence that helps identify its importance in the document. We apply novel features based on news search query logs and Wikipedia entities.,Enhancing single-document summarization by combining RankNet and third-party sources,"We present a new approach to automatic summarization based on neural nets, called NetSum. We extract a set of features from each sentence that helps identify its importance in the document. We apply novel features based on news search query logs and Wikipedia entities. Using the RankNet learning algorithm, we train a pair-based sentence ranker to score every sentence in the document and identify the most important sentences. We apply our system to documents gathered from CNN.com, where each document includes highlights and an article. Our system significantly outperforms the standard baseline in the ROUGE-1 measure on over 70% of our document set.", 
"Automatic Text Summarization (ATS) systems enable users to get the gist of information and knowledge in a short time. Deep neural networks have proven their ability to achieve excellent performance in many real-world Natural Language Processing and computer vision applications. In this paper, we seek to enhance the quality of ATS by integrating unsupervised deep neural network techniques with word embedding approach. We show that the ensemble methods with Word2Vec representation surpass all the investigated models.",Enhancing unsupervised neural networks based text summarization with word embedding and ensemble learning,"The vast amounts of data being collected and analyzed have led to invaluable source of information, which needs to be easily handled by humans. Automatic Text Summarization (ATS) systems enable users to get the gist of information and knowledge in a short time in order to make critical decisions quickly. Deep neural networks have proven their ability to achieve excellent performance in many real-world Natural Language Processing and computer vision applications. However, it still lacks attention in ATS. The key problem of traditional applications is that they involve high dimensional and sparse data, which makes it difficult to capture relevant information. One technique for overcoming these problems is learning features via dimensionality reduction. On the other hand, word embedding is another neural network technique that generates a much more compact word representation than a traditional Bag-of-Words (BOW) approach. In this paper, we are seeking to enhance the quality of ATS by integrating unsupervised deep neural network techniques with word embedding approach. First, we develop a word embedding based text summarization, and we show that Word2Vec representation gives better results than traditional BOW representation. Second, we propose other models by combining word2vec and unsupervised feature learning methods in order to merge information from different sources. We show that unsupervised neural networks models trained on Word2Vec representation give better results than those trained on BOW representation. Third, we also propose three ensemble techniques. The first ensemble combines BOW and word2vec using a majority voting technique. The second ensemble aggregates the information provided by the BOW approach and unsupervised neural networks. The third ensemble aggregates the information provided by Word2Vec and unsupervised neural networks. We show that the ensemble methods improve the quality of ATS, in particular the ensemble based on word2vec approach gives better results. Finally, we perform different experiments to evaluate the performance of the investigated models. We use two kind of datasets that are publically available for evaluating ATS task. Results of statistical studies affirm that word embedding-based models outperform the summarization task compared to those based on BOW approach. In particular, ensemble learning technique with Word2Vec representation surpass all the investigated models.", 
"Best structure for semantic information is still undecided, and graph-based representations enjoy a healthy following. In this document a combination of both approaches is outlined, and its application for extractive text summarization is described. Different configurations for the semantic graphs are used and compared.",Enriched Semantic Graphs for Extractive Text Summarization,"Automatic extraction of semantic information from unstructured text has always been an important goal of natural language processing. While the best structure for semantic information is still undecided, graph-based representations enjoy a healthy following. Some of these representations are extracted directly from the text and external knowledge, while others are built from linguistic insight, created from the deep analysis of the surface text. In this document a combination of both approaches is outlined, and its application for extractive text summarization is described. A pipeline for this task has been implemented, and its results evaluated against a collection of documents from the DUC2003 competition. Graph construction is fully automatic, and summary creation is based on the clustering of conceptual nodes. Different configurations for the semantic graphs are used and compared, and their fitness for the task discussed.", 
"Text Summarization is carried out by two main methods, namely, Extraction and Abstraction. Fuzzy Logic rules were used to balance the weights between important and unimportant features based on the feature Extraction.",Enriching Text Summarization using Fuzzy Logic,"Automatic Text Summarization is a process of generating Summary/Head note for the text document. Text Summarization is carried out by two main methods, namely, Extraction and Abstraction. This paper utilizes the extraction process for sentence selection. Here some Feature based sentence scoring techniques also used, which played an important role in text summarization. Finally an analysis is done by comparing the Fuzzy Logic and Neural Networks techniques based upon the Precision, Recall & F-Measure. Fuzzy Logic rules were used to balance the weights between important and unimportant features based on the feature Extraction. The Experimental result shows that fuzzy Logics give an improving result than the Neural Networks.", 
Multi-label text categorization is a finer-grained approach to text classification. It consists of assigning multiple target labels to documents. We propose an ensemble application of convolutional and recurrent neural networks to capture global and local textual semantics.,Ensemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization,"Text categorization, or text classification, is one of key tasks for representing the semantic information of documents. Multi-label text categorization is finer-grained approach to text categorization which consists of assigning multiple target labels to documents. It is more challenging compared to the task of multi-class text categorization due to the exponential growth of label combinations. Existing approaches to multi-label text categorization fall short to extract local semantic information and to model label correlations. In this paper, we propose an ensemble application of convolutional and recurrent neural networks to capture both the global and the local textual semantics and to model high-order label correlations while having a tractable computational complexity. Extensive experiments show that our approach achieves the state-of-the-art performance when the CNN-RNN model is trained using a large-sized dataset.", 
The LSTMS-B method reaches the average precision of 97.13% for learning EEG visual presentations. It greatly outperforms traditional LSTM network and other contrast models. The method combines deep learning and ensemble learning to extract the category-dependent representations of EEG signals.,Ensemble Deep Learning for Automated Visual Classification Using EEG signals,"This paper proposes an automated visual classification framework in which a novel analysis method (LSTMS-B) of EEG signals guides the selection of multiple networks that leads to the improvement of classification performance. The method, called LSTMS-B, combines deep learning and ensemble learning to extract the category-dependent representations of EEG signals. Specifically, it introduces Swish activation function into traditional LSTM which reduces the effect of vanishing gradient and optimize the training process. Besides, the Bagging theory is applied to increase the generalization. The LSTMS-B method reaches the average precision of 97.13% for learning EEG visual presentations, which greatly outperforms traditional LSTM network and other contrast models. Then, to verify its application value, a ResNet-based regression is trained using original images and relevant EEG representations learned before. We use the output of the regression as the features to classify the images, and finally obtain the average classification accuracy of 90.16%.", 
"Heart disease is one of the most common fatal diseases in the world. Early stage detection and treatment can reduce the number of cardiac failures, mortality of heart disease and cost of diagnosis. The main objective of this research is to develop a Robust Intelligent Heart Disease Prediction System.",Ensemble of Multiple Models For Robust Intelligent Heart Disease Prediction System,"Recently heart disease has become the most common fatal diseases in the world. Early stage detection and treatment can reduce the number of cardiac failures, mortality of heart disease and cost of diagnosis. The healthcare industry collects a huge amount of these medical data, but unfortunately, these are not mined. Discovery of hidden patterns and relationships from this data can help effective decision making to predict the risk of heart disease. The main objective of this research is to develop a Robust Intelligent Heart Disease Prediction System (RIHDPS) using some classification algorithms namely, Naive Bayes, Logistic Regression and Neural Network. This article reviewed the effectiveness of clinical decision support systems by ensemble methods of these three algorithms.", 
"Software engineers share experiences with modern technologies using software information sites, such as Stack Overflow. These sites allow developers to label posted content with short descriptions, known as tags. Tags help to improve the organization of questions and simplify the browsing of questions for users. However, tags assigned to objects tend to be noisy and some objects are not well tagged. 14.7% of the questions that were posted in 2015 on Stack overflow needed tag reediting after the initial assignment.",EnTagRec(++)_ An enhanced tag recommendation system for software  information sites,"Software engineers share experiences with modern technologies using software information sites, such as Stack Overflow. These sites allow developers to label posted content, referred to as software objects, with short descriptions, known as tags. Tags help to improve the organization of questions and simplify the browsing of questions for users. However, tags assigned to objects tend to be noisy and some objects are not well tagged. For instance, 14.7% of the questions that were posted in 2015 on Stack Overflow needed tag reediting after the initial assignment. To improve the quality of tags in software information sites, we propose ENTAGREC++, which is an advanced version of our prior work ENTAGREC. Different from ENTAGREC, ENTAGREC++ does not only integrate the historical tag assignments to software objects, but also leverages the information of users, and an initial set of tags that a user may provide for tag recommendation. We evaluate its performance on five software information sites, STACK OVERFLOW, ASK UBUNTU, ASK DIFFERENT, SUPER USER, and FREECODE. We observe that even without considering an initial set of tags that a user provides, it achieves Recall@5 scores of 0.821, 0.822, 0.891, 0.818 and 0.651, and Recall@10 scores of 0.873, 0.886, 0.956, 0.887 and 0.761, on STACK OVERFLOW, ASK UBUNTU, ASK DIFFERENT, SUPER USER, and FREECODE, respectively. In terms of Recall@5 and Recall@10, averaging across the 5 datasets, it improves upon TagCombine, which is the prior state-of-the-art approach, by 29.3% and 14.5% respectively. Moreover, the performance of our approach is further boosted if users provide some initial tags that our approach can leverage to infer additional tags: when an initial set of tags is given, Recall@5 is improved by 10%.", 
"This paper presents some original methods for text summarization of a single source document by extraction. The methods are based on some of our own text segmentation algorithms. For the first time, the relation of Text Entailment between the sentences of a text is used for segmentation and summarization.",ENTAILMENT-BASED LINEAR SEGMENTATION IN SUMMARIZATION,"This paper presents some original methods for text summarization of a single source document by extraction. The methods are based on some of our own text segmentation algorithms. We denote them as logical segmentation because for all these methods (LTT, ArcInt and ArcReal) the score of a sentence is calculated starting from the number of sentences which are entailed by it. For a text (which is a sequence of sentences) the scores form a structure which indicates how the most important sentences alternate with less important ones and organizes the text according to its logical content. The second logical method, Pure Entailment also uses definition of the relation of entailment between two texts. At least to our knowledge, it is for the first time that the relation of Text Entailment between the sentences of a text is used for segmentation and summarization. The third original method applies Dynamic Programming and centering theory to the sentences logically scored as above. The obtained ranked logical segments are used in the summarization. Our methods of segmentation and summarization are applied and evaluated against a manually realized segmentation and summarization of the same text by Donald Richie, “The Koan”.", 
EEG signals of 20 schizophrenic patients and 20 age-matched control participants are analyzed with the objective of classifying the two groups. A classification accuracy of 86% and 90% is obtained by LDA and Adaboost respectively. The proposed technique is compared and contrasted with a recently reported method.,Entropy and complexity measures for EEG signal classification of schizophrenic and control participants,"In this paper, electroencephalogram (EEG) signals of 20 schizophrenic patients and 20 age-matched control participants are analyzed with the objective of classifying the two groups. For each case, 20 channels of EEG are recorded. Several features including Shannon entropy, spectral entropy, approximate entropy, Lempel— Ziv complexity and Higuchi fractal dimension are extracted from EEG signals. Leaveone (participant)-out cross-validation is used for reliable estimate of the separability of the two groups. The training set is used for training the two classifiers, namely, linear discriminant analysis (LDA) and adaptive boosting (Adaboost). Each classifier is assessed using the test dataset. A classification accuracy of 86% and 90% is obtained by LDA and Adaboost respectively. For further improvement, genetic programming is employed to select the best features and remove the redundant ones. Applying the two classifiers to the reduced feature set, a classification accuracy of 89% and 91% is obtained by LDA and Adaboost respectively. The proposed technique is compared and contrasted with a recently reported method and it is demonstrated that a considerably enhanced performance is achieved. This study shows that EEG signals can be a useful tool for discrimination of the schizophrenic and control participants. It is suggested that this analysis can be a complementary tool to help psychiatrists diagnosing schizophrenic patients.", 
"The power of data is located in what they are used to reveal, argues author. We have little understanding of the role played by the emerging industry of data analytics in the interpretation and use of big data. This article draws upon a sample of 34 data analytics companies.",Envisioning the power of data analytics,"It could be argued that the power of data is located in what they are used to reveal. Yet we have little understanding of the role played by the emerging industry of data analytics in the interpretation and use of big data. These data analytics companies act as intermediaries in the digital data revolution. Understanding the social influence of big data requires us to understand the role played by data analytics within organisations of different types. This particular article focuses very specifically upon the way in which data and data analytics are envisioned within the marketing rhetoric of the data analytics industry. It is argued that to understand the spread of data analytics and the adoption of certain analytic strategies, we first need to look at the projection of promises upon that data. The way that data and analytics are imagined shapes their incorporation and appropriation into practices and organisational structures – what I call here the data frontiers. This article draws upon a sample of 34 data analytics companies in order to explore the way in which data analytics are envisioned within that increasingly powerful industry.", 
"Vehicular ad hoc networks (VANET) have an array of important applications in intelligent transport systems. Many applications require reliable, bandwidth-efficient dissemination of traffic and road information. This is a difficult task since inter-vehicular networks often lack continuous end-to-end connectivity.",Epidemic algorithms for reliable and efficient information dissemination in vehicular ad hoc networks,"Vehicular ad hoc networks (VANET), which are created by vehicles equipped with short- and medium-range wireless communication, have an array of important applications in intelligent transport systems. Many of these applications require reliable, bandwidth-efficient dissemination of traffic and road information via ad hoc network technology. This is a difficult task since inter-vehicular networks often lack continuous end-to-end connectivity and are characterised by large variations in node density. A new epidemic algorithm for information dissemination in highly dynamic and intermittently connected VANET is introduced. It is shown through realistic simulations in highway traffic that the proposed algorithm is capable of reliable and efficient information dissemination in VANET in the face of frequent network fragmentation and large density variations.", 
Epilepsy is one of the common diseases in nervous system. The analysis of EEG signals is a hot topic in current research. The proposed algorithm has strong robustness and high epileptic signal recognition rate.,Epilepsy EEG Signal Classification Algorithm Based on Improved RBF,"Epilepsy is a chronic recurrent transient brain dysfunction syndrome. It is characterized by recurrent epilepsy caused by abnormal discharge of brain neurons. Epilepsy is one of the common diseases in nervous system. The analysis of EEG signals is a hot topic in current research. In order to solve the problem of epileptic EEG signals classification accurately, we carry out in-depth research on epileptic EEG signals, analyze features from linear and non-linear perspectives, input them into the improved RBF model to dynamically extract effective features, and introduce one against one strategy classifier to reduce the probability of error classification. Experiments show that the proposed algorithm has strong robustness and high epileptic signal recognition rate.",  
"Study proposes a novel approach to analyse and classify epileptic electroencephalogram (EEG) signals. The proposed method will assist experts to automatically analyse a large volume of EEG data. It will benefit epilepsy research, according to the researchers. The proposal was implemented on a benchmark epileptic EEG database and compared with existing methods.",Epileptic EEG signal classification using optimum allocation based power spectral density estimation,"This study proposes a novel approach blending optimum allocation (OA) technique and spectral density estimation to analyse and classify epileptic electroencephalogram (EEG) signals. This study employs the OA to determine representative sample points from the original EEG data and then applies periodogram (PD), autoregressive (AR), and the mixture of PD and AR to extract the discriminative features from each OA sample group. The obtained feature sets are evaluated by three popular machine learning methods: support vector machine (SVM), quadratic discriminant analysis (QDA), and k-nearest neighbour (k-NN). Several output coding approaches of the SVM classifier are tested for selecting the best feature sets. This scheme was implemented on a benchmark epileptic EEG database for evaluation and also compared with existing methods. The experimental results show that the OA_AR feature set yields better performances by the SVM with an overall accuracy of 100%, and outperforms the state-of-the-art works with a 14.1% improvement. Thus, the findings of this study prove that the proposed OA-based AR scheme has significant potential to extract features from EEG signals. The proposed method will assist experts to automatically analyse a large volume of EEG data and benefit epilepsy research.", 
This paper introduces a new method of the seizure detection based on EEG signal using the short time Fourier transform(STFT) and convolution neural network(CNN) The experimental result on single channel is that the average accuracy is 86%.,Epileptic Seizure Auto-detection Using Deep Learning Method,"Traditional method of epileptic seizure detection could not avoid the process of manually selecting the features. Recently, the development of deep learning technology has provided a new direction. This paper introduces a new method of the seizure detection based on EEG signal using the short time Fourier transform(STFT) and convolution neural network(CNN). And the paper verifies the feasibility of this method through the actual research data and parameter setting. Afterwards, the method of single threshold is adopted to combine the multichannel results. Then, the comparison with the classical method using the support vector machine(SVM) has been done, which shows that the approach presented in this paper is better. And the experimental result on single channel is that the average accuracy is 86%. In addition, the method of the multi-channel could increase the average accuracy to 90% and the average true positive rate(TPR) to 96.5% while decrease the average false positive rate(FPR) to 7%. All of those indexes reveal the high performance and stability of the approach for the epileptic seizure detection.", 
Epilepsy is a neurological disorder characterized by unexpected electrical disturbance of the brain. The electroencephalogram (EEG) is a commonly used signal for detection of epileptic seizures. This paper presents a new method for classification of ictal and seizure-free EEG signals.,Epileptic seizure classification in EEG signals using second-order difference plot of intrinsic mode functions,"Epilepsy is a neurological disorder which is characterized by transient and unexpected electrical disturbance of the brain. The electroencephalogram (EEG) is a commonly used signal for detection of epileptic seizures. This paper presents a new method for classification of ictal and seizure-free EEG signals. The proposed method is based on the empirical mode decomposition (EMD) and the second-order difference plot (SODP). The EMD method decomposes an EEG signal into a set of symmetric and band-limited signals termed as intrinsic mode functions (IMFs). The SODP of IMFs provides elliptical structure. The 95% confidence ellipse area measured from the SODP of IMFs has been used as a feature in order to discriminate seizure-free EEG signals from the epileptic seizure EEG signals. The feature space obtained from the ellipse area parameters of two IMFs has been used for classification of ictal and seizure-free EEG signals using the artificial neural network (ANN) classifier. It has been shown that the feature space formed using ellipse area parameters of first and second IMFs has given good classification performance. Experimental results on EEG database available by the University of Bonn, Germany, are included to illustrate the effectiveness of the proposed method.", 
The electroencephalogram (EEG) signal analysis is a valuable tool in the evaluation of neurological disorders. This paper presents a novel automatic EEG signal classification method for epileptic seizure detection. The experimental results from a publicly available benchmark database demonstrate that the proposed approach provides better classification accuracy than the recently proposed methods in the literature.,Epileptic Seizure Detection Based on Time-Frequency Images of EEG Signals using Gaussian Mixture Model and Gray Level Co-Occurrence Matrix Features,"The electroencephalogram (EEG) signal analysis is a valuable tool in the evaluation of neurological disorders, which is commonly used for the diagnosis of epileptic seizures. This paper presents a novel automatic EEG signal classification method for epileptic seizure detection. The proposed method first employs a continuous wavelet transform (CWT) method for obtaining the time-frequency images (TFI) of EEG signals. The processed EEG signals are then decomposed into five sub-band frequency components of clinical interest since these sub-band frequency components indicate much better discriminative characteristics. Both Gaussian Mixture Model (GMM) features and Gray Level Co-occurrence Matrix (GLCM) descriptors are then extracted from these sub-band TFI. Additionally, in order to improve classification accuracy, a compact feature selection method by combining the ReliefF and the support vector machine-based recursive feature elimination (RFE-SVM) algorithm is adopted to select the most discriminative feature subset, which is an input to the SVM with the radial basis function (RBF) for classifying epileptic seizure EEG signals. The experimental results from a publicly available benchmark database demonstrate that the proposed approach provides better classification accuracy than the recently proposed methods in the literature, indicating the effectiveness of the proposed method in the detection of epileptic seizures.", 
"Epilepsy is a well-known nervous system disorder characterized by seizures. Traditional methods for analyzing an EEG signal for epileptic seizure detection are time-consuming. Recently, several automated seizure detection frameworks using machine learning technique have been proposed to replace these traditional methods. Both techniques explore the subpattern correlation of EEG signals, which helps in decision-making process.",Epileptic seizure detection in EEG signal using machine learning techniques,"Epilepsy is a well-known nervous system disorder characterized by seizures. Electroencephalograms (EEGs), which capture brain neural activity, can detect epilepsy. Traditional methods for analyzing an EEG signal for epileptic seizure detection are time-consuming. Recently, several automated seizure detection frameworks using machine learning technique have been proposed to replace these traditional methods. The two basic steps involved in machine learning are feature extraction and classification. Feature extraction reduces the input pattern space by keeping informative features and the classifier assigns the appropriate class label. In this paper, we propose two e?ective approaches involving subpattern based PCA (SpPCA) and cross-subpattern correlation-based PCA (SubXPCA) with Support Vector Machine (SVM) for automated seizure detection in EEG signals. Feature extraction was performed using SpPCA and SubXPCA. Both techniques explore the subpattern correlation of EEG signals, which helps in decision-making process. SVM is used for classification of seizure and non-seizure EEG signals. The SVM was trained with radial basis kernel. All the experiments have been carried out on the benchmark epilepsy EEG dataset. The entire dataset consists of 500 EEG signals recorded under di?erent scenarios. Seven di?erent experimental cases for classification have been conducted. The classification accuracy was evaluated using tenfold cross validation. The classification results of the proposed approaches have been compared with the results of some of existing techniques proposed in the literature to establish the claim.", 
"Epilepsy is a complex neurological disorder recognized by abnormal synchronization of cerebral neurons, named seizures. In this paper, a methodology for automated seizure detection based on Discrete Wavelet Transform (DWT) is presented.",Epileptic Seizures Classification Based on Long-Term EEG Signal Wavelet Analysis,"Epilepsy is a complex neurological disorder recognized by abnormal synchronization of cerebral neurons, named seizures. During the last decades, significant progress has been done in automated detection and prediction of seizures, aiming to develop personalized closed-loop intervention systems. In this paper, a methodology for automated seizure detection based on Discrete Wavelet Transform (DWT) is presented. Twenty-one intracranial ictal recordings acquired from the database of University Hospital of Freiburg are firstly segmented in 2 s epochs. Then, a five-level decomposition is applied in each segment and five features are extracted from the wavelet coefficients. The extracted feature vector is used to train a Support Vector Machines (SVM) classifier. Average sensitivity and specificity reached above 93% and 99% respectively.", 
"Epilepsy is a common neurological disease that has affected more than 65 million people worldwide. In more than 30% of the cases, people affected by this disease cannot be cured with medicines or surgery. The current study proposes a seizure prediction system that employs deep learning methods. The proposed method has been applied on 24 subjects of scalp EEG dataset of CHBMIT.",Epileptic Seizures Prediction Using Deep Learning Techniques,"Epilepsy is a very common neurological disease that has affected more than 65 million people worldwide. In more than 30 % of the cases, people affected by this disease cannot be cured with medicines or surgery. However, predicting a seizure before it actually occurs can help in its prevention; through therapeutic intervention. Studies have observed that abnormal activity inside the brain begins a few minutes before the start of a seizure, which is known as preictal state. Many researchers have tried to find a way for predicting this preictal state of a seizure but an effective prediction in terms of high sensitivity and specificity still remains a challenge. The current study, proposes a seizure prediction system that employs deep learning methods. This method includes preprocessing of scalp EEG signals, automated features extraction; using convolution neural network and classification with the support of vector machines. The proposed method has been applied on 24 subjects of scalp EEG dataset of CHBMIT resulting in successfully achieving an average sensitivity and specificity of 92.7% and 90.8% respectively.", 
"The scalp electroencephalogram (EEG) based epileptic seizure/non-seizure detection has been comprehensively studied. Yet, few investigation has been paid to the preictal stage detection, which is practically more crucial to epileptics in taking precautions before seizure onset. In this paper, a novel epileptic preictic state classification and seizure detection algorithm based on deep features learned by stacked convolutional neural networks is developed.",Epileptic Signal Classification with Deep EEG Features by Stacked CNNs,"The scalp electroencephalogram (EEG) based epileptic seizure/non-seizure detection has been comprehensively studied and fruitful achievements have been reported in the past. Yet, few investigation has been paid to the preictal stage detection, which is practically more crucial to epileptics in taking precautions before seizure onset. In this paper, a novel epileptic preictal state classification and seizure detection algorithm based on deep features learned by stacked convolutional neural networks (SCNNs) is developed. The mean amplitude of spectrum map (MAS) obtained from the average subband spectra of multichannel EEGs are adopted for representation. The probability feature vectors by stacked CNNs are extracted in the Softmax layer of CNNs, where an adaptive and discriminative feature weighting fusion (AWF) is developed for performance enhancement. Following the deep extraction layer, the effective kernel extreme learning machine (KELM) is adopted for feature learning and epileptic classification. Experiments on the benchmark CHB-MIT database and a real recorded epileptic database are conducted for performance demonstration. Comparisons to many state-of-the-art epileptic classification methods are provided to show the superiority of the proposed SCNN+AWF algorithm.", 
Sentinel-2 vegetation indices were evaluated in context of estimating defoliation of Scots pine stands in western Poland. Regression and classification models were built based on reference data from 50 field plots and Sentinel-2 satellite images. The Green Normalized Difference Vegetation Index and MERIS Terrestrial Chlorophyll Index VIs were found to be most robust defoliations predictors.,Estimating defoliation of Scots pine stands using machine learning methods and vegetation indices of Sentinel 2,"In the presented study, the Sentinel-2 vegetation indices (VIs) were evaluated in context of estimating defoliation of Scots pine stands in western Poland. Regression and classification models were built based on reference data from 50 field plots and Sentinel-2 satellite images from three acquisition dates. Three machine-learning (ML) methods were tested: k-nearest neighbors (kNN), random forest (RF), and support vector machines (SVM). Regression models predicted stands defoliation with moderate accuracy. R2 values for regression models amounted to 0.53, 0.57, 0.57 for kNN, RF and SVM, accordingly. Analogically, the following values of normalized root mean squared error were obtained: 12.2%, 11.9% and 11.6%. Overall accuracies for two-class classification models were 78%, 75%, 78% for kNN, RF and SVM methods. The Green Normalized Difference Vegetation Index and MERIS Terrestrial Chlorophyll Index VIs were found to be most robust defoliation predictors regardless of the ML method. We conclude that Sentinel-2 satellite images provide useful information about forest defoliation and may contribute to forest monitoring systems.", 
"Long-term monitoring of pulse wave velocity can be beneficial in carrying out accurate diagnosis of the underlying conditions. Doppler radar has emerged as a promising technology for contact-less monitoring and assessment of physiological parameters. The results of our feasibility study demonstrate that the arterial pulse waves in the femoral region, arising due to cardiac activity, can be estimated using the Dopplers.",Estimation of Arterial Pulse Wave Velocity from Doppler Radar Measurements-a Feasibility Study,"Pulse wave velocity has emerged as important diagnostic parameter due to its association with various cardiovascular disorders, such as hypertension, vascular aging, and atherosclerosis. Long-term monitoring of pulse wave velocity can be beneficial in carrying out accurate diagnosis of the underlying conditions or even for an early prediction of cardiovascular diseases. Doppler radar has emerged as a promising technology for contact-less monitoring and assessment of physiological parameters. In this study, we aimed at: i) as a first step, assessing the feasibility of measuring arterial pulse waves at the femoral region using the Doppler radar technology, and consequently, ii) estimating the pulse transit time between the heart-femoral regions as well as between the carotid-femoral regions using simultaneous Doppler radar measurements. The results of our feasibility study demonstrate that the arterial pulse waves in the femoral region, arising due to cardiac activity, can be estimated using the Doppler radar technology in a contact-less fashion. Furthermore, simultaneous pulse wave measurements at distinct surface locations using this technique can enable contact-less estimation of the pulse transit time and consequently pulse wave velocity.", 
Biomedical summarization approaches often rely on a similarity measure to model the source document. We examine the impact of the similarity measure on the performance of the summarization methods. exploiting both biomedical concepts and semantic types yields slightly better performance.,Evaluating Different Similarity Measures for Automatic Biomedical Text Summarization,"Automatic biomedical text summarization is maturing and can provide a solution for biomedical researchers to access the information they need efficiently. Biomedical summarization approaches often rely on the similarity measure to model the source document, mainly when they employ redundancy removal or graph structures. In this paper, we examine the impact of the similarity measure on the performance of the summarization methods. We model the document as a weighted graph. Various similarity measures are used to build different graphs based on biomedical concepts, semantic types and a combination of them. We next use the graphs to generate and evaluate the automatic summaries. The results suggest that the selection of the similarity measure has a substantial effect on the quality of the summaries (?37% improvement in ROUGE-2 metric, and ?29% in ROUGE-SU4). The results also demonstrate that exploiting both biomedical concepts and semantic types yields slightly better performance.", 
"Most of the existing work evaluate the effectiveness of reputation management schemes with settled attacking behaviors in their simulation, which cannot represent the real scenarios. In this paper, we propose to consider dynamical and diversity attacking strategies in the simulation. The final state of the simulation could be used to quantify the protection effectiveness of the reputation management scheme.",Evaluating Reputation Management Schemes of Internet of Vehicles Based on Evolutionary Game Theory,"Conducting reputation management is very important for Internet of vehicles. However, most of the existing work evaluate the effectiveness of their schemes with settled attacking behaviors in their simulation, which cannot represent the real scenarios. In this paper, we propose to consider dynamical and diversity attacking strategies in the simulation of reputation management scheme evaluation. To that end, we apply evolutionary game theory to model the evolution process of malicious users’ attacking strategies, and discuss the methodology of the evaluation simulations. We further apply our evaluation method to a reputation management scheme with multiple utility functions, and discuss the evaluation results. The results indicate that our evaluation method is able to depict the evolving process of the dynamic attacking strategies in a vehicular network, and the final state of the simulation could be used to quantify the protection effectiveness of the reputation management scheme.", 
In software evolution a developer must investigate source code to locate entities that must be modified to complete a change task. Haiduc et al. proposed text summarization based approaches to the automatic generation of class and method summaries. In this paper we propose a new topic modeling based approach to source code summarization.,"Evaluating source code summarization techniques, Replication and expansion","During software evolution a developer must investigate source code to locate then understand the entities that must be modified to complete a change task. To help developers in this task, Haiduc et al. proposed text summarization based approaches to the automatic generation of class and method summaries, and via a study of four developers, they evaluated source code summaries generated using their techniques. In this paper we propose a new topic modeling based approach to source code summarization, and via a study of 14 developers, we evaluate source code summaries generated using the proposed technique. Our study partially replicates the original study by Haiduc et al. in that it uses the objects, the instruments, and a subset of the summaries from the original study, but it also expands the original study in that it includes more subjects and new summaries. The results of our study both support the findings of the original and provide new insights into the processes and criteria that developers use to evaluate source code summaries. Based on our results, we suggest future directions for research on source code summarization.", 
"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency. This highly scalable approach substantially outperforms previous models, including those trained with strong supervision.",Evaluating the Factual Consistency of Abstractive Text Summarization,"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.", 
This paper introduces evaluation methods for an Arabic extractive text summarization system. The system integrates Bayesian and Genetic Programming (GP) classification methods in an optimized way to extract the summary sentences. We have introduced methods for evaluating the summary against other human summaries.,Evaluation Approaches for an Arabic Extractive Generic Text Summarization System,"The advance of technology and extensive use of the web has prompt the need to summarization of text documents. Users tend to extract the most informative or indicative information instead of reading the whole original documents. Naturally, automatic text summarization will save time and effort for the users, and will enable them to make decisions in less time. This paper introduces evaluation methods for an Arabic extractive text summarization system. This system integrates Bayesian and Genetic Programming (GP) classification methods in an optimized way to extract the summary sentences. The system is trainable and uses manually annotated corpus. We have introduced methods for evaluating the summary against other human summaries. Moreover, we used human judgement for system output, and finally we tested the system against a commercial Arabic summarization system.", 
New evaluation measure for assessing the quality of a summary. Core of the measure is covered by Latent Semantic Analysis (LSA) which can capture the main topics of a document. Results show a high correlation between human rankings and the LSA-based evaluation measure.,Evaluation measures for text summarization,"We explain the ideas of automatic text summarization approaches and the taxonomy of summary evaluation methods. Moreover, we propose a new evaluation measure for assessing the quality of a summary. The core of the measure is covered by Latent Semantic Analysis (LSA) which can capture the main topics of a document. The summarization systems are ranked according to the similarity of the main topics of their summaries and their reference documents. Results show a high correlation between human rankings and the LSA-based evaluation measure. The measure is designed to compare a summary with its full text. It can compare a summary with a human written abstract as well; however, in this case using a standard ROUGE measure gives more precise results. Nevertheless, if abstracts are not available for a given corpus, using the LSA-based measure is an appropriate choice.", 
"To evaluate an artificial intelligence (AI)–based, automatic coronary artery calcium (CAC) scoring software, using a semi-automatic software as a reference. This observational study included 315 consecutive, non-contrast-enhanced calcium scoring computed tomography (CSCT) scans. There was an excellent correlation and agreement between the automatic software and the semi- automatic software for three CAC scores and the number of calcified lesions. Risk category classification was accurate but showing an overestimation bias tendency.","Evaluation of an AI-based, automatic coronary artery calcium scoring software","To evaluate an artificial intelligence (AI)–based, automatic coronary artery calcium (CAC) scoring software, using a semi-automatic software as a reference. This observational study included 315 consecutive, non-contrast-enhanced calcium scoring computed tomography (CSCT) scans. A semi-automatic and an automatic software obtained the Agatston score (AS), the volume score (VS), the mass score (MS), and the number of calcified coronary lesions. Semi-automatic and automatic analysis time were registered, including a manual double-check of the automatic results. Statistical analyses were Spearman’s rank correlation coefficient (?), intra-class correlation (ICC), Bland Altman plots, weighted kappa analysis (?), and Wilcoxon signed-rank test. The correlation and agreement for the AS, VS, and MS were ? = 0.935, 0.932, 0.934 (p < 0.001), and ICC = 0.996, 0.996, 0.991, respectively (p < 0.001). The correlation and agreement for the number of calcified lesions were ? = 0.903 and ICC = 0.977 (p < 0.001), respectively. The Bland Altman mean difference and 1.96 SD upper and lower limits of agreements for the AS, VS, and MS were ? 8.2 (? 115.1 to 98.2), ? 7.4 (? 93.9 to 79.1), and ? 3.8 (? 33.6 to 25.9), respectively. Agreement in risk category assignment was 89.5% and ? = 0.919 (p < 0.001). The median time for the semi-automatic and automatic method was 59 s (IQR 35–100) and 36 s (IQR 29–49), respectively (p < 0.001). There was an excellent correlation and agreement between the automatic software and the semi-automatic software for three CAC scores and the number of calcified lesions. Risk category classification was accurate but showing an overestimation bias tendency. Also, the automatic method was less time-demanding.", 
Automatic text summarization area has experienced an increasing number of researches and products. This paper reviews the main automatic text summarizing methods based on Rhetorical Structure Theory. All RST-based methods overcome the extractive summarizer and hybrid methods produce worse summaries.,Evaluation of Automatic Text Summarization Methods Based on Rhetorical Structure Theory,"Motivated by governmental, commercial and academic interests, automatic text summarization area has experienced an increasing number of researches and products, which led to a countless number of summarization methods. In this paper, we present a comprehensive comparative evaluation of the main automatic text summarization methods based on Rhetorical Structure Theory (RST), claimed to be among the best ones. We also propose new methods and compare our results to an extractive summarizer, which belongs to a summarization paradigm with severe limitations. To the best of our knowledge, most of our results are new in the area and reveal very interesting conclusions. The simplest RST-based method is among the best ones, although all of them present comparable results. We show that all RST-based methods overcome the extractive summarizer and that hybrid methods produce worse summaries. Finally, we verify that Mann and Thompson strong assumption in summarization and RST research area is not helpful in the way previously imagined.", 
The goal of this paper is to compare summaries generated by different automatic text summarization methods and those generated by human beings. We compared manually-produced summaries and the automatically-produced ones. The results showed that summaries produced by Fuzzy method were much more acceptable and understandable.,Evaluation of Automatic Text Summarizations based on Human Summaries,"The goal of this paper is to compare summaries generated by different automatic text summarization methods and those generated by human beings. To achieve this end, we did two series of experiments: in the first one, we employed automatically produced extractive summaries; in the second one, manually-produced summaries obtained by several English teachers were used. Our automatic summaries were obtained using Fuzzy method and Vector approach. Using Rouge evaluation system, we compared the manually-produced summaries and the automatically-produced ones. Rouge evaluation of generated summaries indicated the superiority of summaries produced by humans over the automatically produced summaries. On the other hand, the comparison between the generated summaries showed that summaries produced by Fuzzy method were much more acceptable and understandable compared to summaries produced by Vector approach. This can provide support for the replacement of manually generated summaries by summaries produced using Fuzzy method in certain cases where real time summaries are needed.", 
In this paper we examine the summary evaluation problem. Text summarization is the oldest and most successful summarization domain. We present results for a comprehensive evaluation summary.,Evaluation of Automatic Video Summarization Systems,"Compact representations of video, or video summaries, data greatly enhances efficient video browsing. However, rigorous evaluation of video summaries generated by automatic summarization systems is a complicated process. In this paper we examine the summary evaluation problem. Text summarization is the oldest and most successful summarization domain. We show some parallels between these to domains and introduce methods and terminology. Finally, we present results for a comprehensive evaluation summary that we have performed.", 
"In this paper, we present and analyze the results of the application of Arabic query-Based Text Summarization System – AQBTSS. We adapted the traditional Vector Space Model and cosine similarity measure to find the most relevant passages extracted form Arabic documents.",Evaluation of Query-Based Arabic Text Summarization System,"In this paper, we present and analyze the results of the application of Arabic Query-Based Text Summarization System – AQBTSS – in an attempt to produce a query-oriented summary for a single Arabic document. For this task, we adapted the traditional Vector Space Model (VSM) and the cosine similarity measure to find the most relevant passages extracted form Arabic document to produce a text summary. We aim at using the short summaries in some Natural Language (NL) tasks such as generating answers for Arabic open domain Question Answering System (AQAS) as well as experimenting with categorizing Arabic scripts. The obtained results indicate that our simple approach for text summarization is promising.", 
"Text summarization techniques have been found to be effective with regard to helping users find relevant information faster. There is an important need to design a personalized text summarization system that takes into account both what a user is currently interested in and how a user perceives information, says the paper. Different users have different assessments with regards to information coverage and the way information is presented in loosely and closely related document sets, it says. This paper aims at studying the impact of a user's cognitive styles when assessing multi-document summaries.",Evaluation of the Impact of User-Cognitive Styles on the Assessment of Text Summarization,"Text summarization techniques have been found to be effective with regard to helping users find relevant information faster. The effectiveness and efficiency of a user’s performance in an information-seeking task can greatly be improved if he/she needs to only look at a summary that includes the relevant information presented in his/her preferred manner. On the other hand, if the main idea is misrepresented and/or omitted altogether from a summary, it may take users more time to solve a target problem or, even worse, lead users to make incorrect decisions. There is an important need to design a personalized text summarization system that takes into account both what a user is currently interested in and how a user perceives information. The latter factor is referred to as a user’s cognitive styles. Although there are some existing approaches that have employed a user’s interests to help in the design of a personalized text summarization system, there has been inadequate focus on exploring cognitive styles. This paper aims at studying the impact of a user’s cognitive styles when assessing multidocument summaries. In particular, we choose two dimensions of a user’s cognitive style—the analytic/wholist and verbal/ imagery dimensions—and study their impacts on how a user assesses a summary that was generated from a set of documents. In particular, the type of a document set refers to whether the set’s content is loosely or closely related. We use a document set type to explore if there are any differences in the users’ assessments of summaries that were generated from sets of different types. The results of this paper show that different users have different assessments with regard to information coverage and the way that information is presented in both loosely and closely related document sets. In addition, we found that the coherency ratings that were given to summaries from the two types of document sets were significantly different between the analytic and wholist groups. This result leads us to investigate the impact of a user’s cognitive styles and the following two factors that directly relate to the coherence of a summary: 1) graph entropy and 2) the percentage of standalone concepts. We found that these two factors and a user’s cognitive styles affect a user’s ratings on the coherency of a summary.", 
Unsupervised techniques uses human generated summary to select features and parameters. Many researches have shown the conflicts in summary generated. Big data analytics for text dataset also recommends unsupervised methods. Graph based sentence scoring method is much efficient than otherunsupervised learning techniques.,Evaluation of unsupervised learning based extractive text summarization technique for large scale review and feedback data,"Supervised techniques uses human generated summary to select features and parameter for summarization. The main problem in this approach is reliability of summary based on human generated parameters and features. Many researches have shown the conflicts in summary generated. Due to diversity of large scale datasets, supervised techniques based summarization also fails to meet the requirements. Big data analytics for text dataset also recommends unsupervised techniques than supervised techniques. Unsupervised techniques based summarization systems finds representative sentences from large amount of text dataset. Co-selection based evaluation measure is applied for evaluating the proposed research work. The value of recall, precision, f-measure and similarity measure are determined for concluding the research outcome for the respective objective. The algorithms like KMeans, MiniBatchKMeans, and Graph based summarization techniques are discussed with all technical details. The results achieved by applying Graph Based Text Summarization techniques with large scale review and feedback data found improvement over previously published results based on sentence scoring using TF and TF-IDF. Graph based sentence scoring method is much efficient than other unsupervised learning techniques applied for extractive text summarization. The execution of graph based algorithm with Spark’s Graph X programming environment will secure execution time for this types of large scale review and feedback dataset which is considered under Big Data Problem.", 
"In this article, we present event graphs, a novel event-based document representation model that filters and structures the information about events described in text. We combine machine learning and rule-based models to extract sentence-level event mentions and determine the temporal relations between them. We present novel models for information retrieval and multi-document summarization.",Event graphs for information retrieval and multi-document summarization,"With the number of documents describing real-world events and event-oriented information needs rapidly growing on a daily basis, the need for efficient retrieval and concise presentation of event-related information is becoming apparent. Nonetheless, the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. In this article, we present event graphs, a novel event-based document representation model that filters and structures the information about events described in text. To construct the event graphs, we combine machine learning and rule-based models to extract sentence-level event mentions and determine the temporal relations between them. Building on event graphs, we present novel models for information retrieval and multi-document summarization. The information retrieval model measures the similarity between queries and documents by computing graph kernels over event graphs. The extractive multi-document summarization model selects sentences based on the relevance of the individual event mentions and the temporal structure of events. Experimental evaluation shows that our retrieval model significantly outperforms well-established retrieval models on event-oriented test collections, while the summarization model outperforms competitive models from shared multi-document summarization tasks.", 
"Text summarization is the process of automatically creating a compressed version of a given document preserving its information content. There are two types of summarization: extractive and abstractive. In this paper, we propose unsupervised document summarization method that creates the summary by clustering and extracting sentences from the original document.",Evolutionary Algorithm for Extractive Text Summarization,"Text summarization is the process of automatically creating a compressed version of a given document preserving its information content. There are two types of summarization: extractive and abstractive. Extractive summarization methods simplify the problem of summarization into the problem of selecting a representative subset of the sentences in the original documents. Abstractive summarization may compose novel sentences, unseen in the original sources. In our study we focus on sentence based extractive document summarization. The extractive summarization systems are typically based on techniques for sentence extraction and aim to cover the set of sentences that are most important for the overall understanding of a given document. In this paper, we propose unsupervised document summarization method that creates the summary by clustering and extracting sentences from the original document. For this purpose new criterion functions for sentence clustering have been proposed. Similarity measures play an increasingly important role in document clustering. Here we’ve also developed a discrete differential evolution algorithm to optimize the criterion functions. The experimental results show that our suggested approach can improve the performance compared to sate-of-the-art summarization approaches.", 
This paper gives a review of the growth and improvement in the techniques of Automatic Text Summarization on implementing Evolutionary Algorithms techniques. We propose a broad set of features that considers additional features in the fitness function.,Evolutionary Algorithms for Extractive Automatic Text Summarization,"Due to the exponential growth of documents on internet, users want all the relevant data at one place without any hassle. This led to the growth of Automatic Text Summarization. For extractive text summarization in which representative sentences from the document itself are selected as summary, various statistical, knowledge based and discourse based methods are proposed by researchers. The goal of this paper is to give a survey on the important techniques and methodologies that are employed using Genetic Algorithms in Automatic Text Summarization. This paper gives a review of the growth and improvement in the techniques of Automatic Text Summarization on implementing Evolutionary Algorithms techniques. We propose a broad set of features that considers additional features in the fitness function.", 
Main problem for generating an extractive automatic text summary (EATS) is to detect the key themes of a text. Unsupervised approaches cluster the sentences of the original text to find the key sentences that take part in an automatic summary. Quality of automatic summary is evaluated using similarity metrics with human-made summaries.,Evolutionary Automatic Text Summarization using Cluster Validation Indexes,"The main problem for generating an extractive automatic text summary (EATS) is to detect the key themes of a text. For this task, unsupervised approaches cluster the sentences of the original text to find the key sentences that take part in an automatic summary. The quality of an automatic summary is evaluated using similarity metrics with human-made summaries. However, the relationship between the quality of the human-made summaries and the internal quality of the clustering is unclear. First, this paper proposes a comparison of the correlation of the quality of a human-made summary to the internal quality of the clustering validation index for finding the best correlation with a clustering validation index. Second, in this paper, an evolutionary method based on the best above internal clustering validation index for an automatic text summarization task is proposed. Our proposed unsupervised method for EATS has the advantage of not requiring information regarding the specific classes or themes of a text, and is therefore domain- and language-independent. The high results obtained by our method, using the most-competitive standard collection for EATS, prove that our method maintains a high correlation with human-made summaries, meeting the specific features of the groups, for example, compaction, separation, distribution, and density.", 
"Automatic clustering problem is difficult and challenging in unsupervised learning owing to the lack of prior domain knowledge. In this paper, we resort to quality metrics and ensemble strategy for the sake of explicit/implicit knowledge discovery. Experiments are conducted from several different aspects and the corresponding analyses are provided.",Evolutionary multi-objective automatic clustering enhanced with quality metrics and ensemble strategy,"Automatic clustering problem, which needs to detect the appropriate clustering without a pre-defined number of clusters (k), is difficult and challenging in unsupervised learning owing to the lack of prior domain knowledge. Despite a rising tendency with the application of evolutionary multi-objective optimization (EMO) techniques for automatic clustering, there still exist some obvious under-explored issues. In this paper, we resort to quality metrics and ensemble strategy for the sake of explicit/implicit knowledge discovery to guide the optimization process. The quality and diversity of solutions defined in terms of cluster validities, as similar to performance indicator for multi-objective optimization, are applied to assist in addressing automatic clustering problems and decreasing unnecessary computational overhead. To be specific, the main components like initialization, reproduction operations, and environmental selection which involved during EMO based automatic clustering are discussed and refined. For the determination of the final partitioning, quality metrics and cluster ensemble strategy are both considered to improve the retrieve system in the unsupervised way. Experiments are conducted from several different aspects and the corresponding analyses are provided, which confirm that the proposals are more efficient and effective for automatic clustering.", 
"Service Location Protocol is the most extensively researched, tested, utilized, flexible, reliable, scalable, lightweight and standardized discovery protocols.",Evolutionary Study of Service Location Protocol,"This paper describes the Evolutionary study of Service Location Protocol. There are many discovery protocols developed by the researchers but SLP is the most extensively researched, tested, utilized, flexible, reliable, scalable, lightweight and standardized discovery protocols. Service Location Protocol is discussed briefly in this paper along with its working. The Evolution of SLP is discussed from its origin to its applicability in different research areas till date.", 
"Hacking trauma is prevalent in forensic cases involving genocide and dismemberment, but research into the identification of this type of trauma is lacking. This study examines characteristics of hacking and blunt force skeletal trauma to determine if there is a point at which blunt force trauma becomes distinguishable from hacking trauma. The entrance widths of the impacts exhibited a significant relationship with the blade angles. V-shaped kerf in the bones was also found to be more visible around the 60° blade angle.",Examination of hacking and blunt force skeletal trauma,"Hacking trauma is prevalent in forensic cases involving genocide and dismemberment, but research into the identification of this type of trauma is lacking. The present study examines characteristics of hacking and blunt force skeletal trauma in order to determine if there is a point at which blunt force trauma becomes distinguishable from hacking trauma. Ten implements with a range of blade angles (i.e., the striking surface of the implement) were used in conjunction with a controlled-force hacking device to impact 100 limb bones of white-tailed deer (Odocoileus virginianus). Observations of the trauma included the occurrence and degree of fragmentation, the entrance widths of the impacts, and composite scores of six hacking characteristics, especially the distinctive V-shaped kerf. ANOVA tests and regression analyses were used to assess the relationships between these characteristics and the blade angles. A significant relationship (p-value = 0.011) was found between the composite hacking scores and the blade angles, indicating that blunt force and hacking trauma can be distinguished. The entrance widths of the impacts exhibited a significant relationship with the blade angles (p-value = 0.037). There was also a significant relationship between the visibility of a V-shaped kerf in the bones (p-value = 0.003), with visibility decreasing around the 60° blade angle. These data should assist in establishing guidelines to differentiate hacking and blunt force skeletal trauma in cases where the implement is on a spectrum between sharp and blunt.", 
Algorithm is based on repeated application of TextRank on a sentence similarity graph. We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge.,ExB Text Summarizer,"We present our state of the art multilingual text summarizer capable of single as well as multi-document text summarization. The algorithm is based on repeated application of TextRank on a sentence similarity graph, a bag of words model for sentence similarity and a number of linguistic pre- and post-processing steps using standard NLP tools. We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge: Multilingual Single-document Summarization and Multilingual Multi-document Summarization.", 
Algorithm is based on repeated application of TextRank on a sentence similarity graph. We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge.,Exceeding chance level by chance The caveat of theoretical chance levels in brain signal classification and statistical assessment of decoding accuracy,"Machine learning techniques are increasingly used in neuroscience to classify brain signals. Decoding performance is reflected by how much the classification results depart from the rate achieved by purely random classification. In a 2-class or 4-class classification problem, the chance levels are thus 50% or 25% respectively. However, such thresholds hold for an infinite number of data samples but not for small data sets. While this limitation is widely recognized in the machine learning field, it is unfortunately sometimes still overlooked or ignored in the emerging field of brain signal classification. Incidentally, this field is often faced with the difficulty of low sample size. In this study we demonstrate how applying signal classification to Gaussian random signals can yield decoding accuracies of up to 70% or higher in two-class decoding with small sample sets. Most importantly, we provide a thorough quantification of the severity and the parameters affecting this limitation using simulations in which we manipulate sample size, class number, cross-validation parameters (k-fold, leave-one-out and repetition number) and classifier type (Linear-Discriminant Analysis, Naïve Bayesian and Support Vector Machine). In addition to raising a red flag of caution, we illustrate the use of analytical and empirical solutions (binomial formula and permutation tests) that tackle the problem by providing statistical significance levels (p-values) for the decoding accuracy, taking sample size into account. Finally, we illustrate the relevance of our simulations and statistical tests on real brain data by assessing noise-level classifications in Magnetoencephalography (MEG) and intracranial EEG (iEEG) baseline recordings.", 
"Text summarization is the process of distilling the most important content from text documents. Computer based automatic abstracting and summarizing has proven to be extremely challenging. This paper reports our experience with applying extractive summarization techniques to process news articles, economic reports.",Experiences with and Reflections on Text Summarization Tools,"Text summarization is a process of distilling the most important content from text documents. While human beings have proven to be extremely capable summarizers, computer based automatic abstracting and summarizing has proven to be extremely challenging tasks. In this paper we report our experience with applying extractive summarization techniques to process news articles, economic reports and nursing narratives. We present analysis of the effect of different summarization methods and parameters on the summarization results. We also compare the performance of the summarizers across the three different document genres. The learned lessons are discussed and the possibilities for applying the theory of Computing with Words in text summarization are elaborated.", 
"Automatic text summarization methods are increasingly needed in different fields of knowledge. Redundancy reduction has been demonstrated as an indispensable criterion. All possible combinations of two, three, and four criteria have been considered. The combination that includes content coverage, redundancy reduction, and relevance obtains the most balanced results.",Experimental analysis of multiple criteria for extractive multi-document text summarization,"Automatic text summarization methods are increasingly needed in different fields of knowledge. In the scientific literature, generic extractive multi-document text summarization can be formulated as an optimization problem which involves several criteria. Only two criteria have been considered simultaneously, i.e., content coverage and redundancy reduction, whereas the other ones, relevance and coherence have been considered separately. Therefore, there is a lack of studies comparing the performance of different criteria. For this reason, a comparative study of the different criteria suitable for generic extractive multidocument text summarization is performed here. All possible combinations of two, three, and four criteria have been considered within a multi-objective optimization context. Experiments have been carried out based on datasets from Document Understanding Conferences (DUC), and the combinations of objective functions have been compared and evaluated with Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Redundancy reduction has been demonstrated as an indispensable criterion, being the coherence the least significant and efficient criterion. The combination that includes content coverage, redundancy reduction, and relevance obtains the most balanced results in terms of average ROUGE and execution time.", 
This paper evaluates the performance of different similarity measures in the context of document summarization. The proposed method is based on evaluation of relevance score of sentence. We present a comprehensive experimental evaluation two different document collection.,Experimental investigating the F-measure as similarity measure for automatic text summarization,"This paper evaluates the performance of different similarity measures in the context of document summarization. For this purpose in this paper a simple and effective sentence extractive technique is used. The proposed method is based on evaluation of relevance score of sentence. Many measures are available for the calculation of inter sentence relationships. To calculate a similarity between sentences we use cosine measure and classical IR F-measure. We present a comprehensive experimental evaluation two different document collection. Our experimental results show that F-measure lead to the best overall results than cosine measure. Keywords: Text mining, summarization, F-measure, cosine measure, sentence relevance score.", 
A class of connected cruise control algorithms with feedback structure originated from human driving behavior would be needed. The algorithms would use beyond-line-of-sight motion information from neighboring human-driven vehicles via vehicle-to-everything (V2X) communication.,Experimental validation of connected automated vehicle design among human-driven vehicles,"In this paper, we present results regarding the experimental validation of connected automated vehicle design. In order for a connected automated vehicle to integrate well with human-dominated traffic, we propose a class of connected cruise control algorithms with feedback structure originated from human driving behavior. We test the connected cruise controllers using real vehicles under several driving scenarios while utilizing beyond-line-of-sight motion information obtained from neighboring human-driven vehicles via vehicle-to-everything (V2X) communication. We experimentally show that the design is robust against variations in human behavior as well as changes in the topology of the communication network. We demonstrate that both safety and energy efficiency can be significantly improved for the connected automated vehicle as well as for the neighboring human-driven vehicles and that the connected automated vehicle may bring additional societal benefits by mitigating traffic waves.", 
We propose novel techniques for contextual advertising using text summarization. ,Experimenting Text Summarization Techniques for Contextual Advertising,"Contextual advertising systems suggest suitable advertisings to users while surfing the Web. Focusing on text summarization, we propose novel techniques for contextual advertising. Comparative experiments between these techniques and existing ones have been performed.", 
"In this paper, we describe our experiments with clause segmentation in producing summaries for the TAC 2008 Update Summarization Track.",Experimenting with Clause Segmentation for Text Summarisation,"In this paper, we describe our experiments with clause segmentation in producing summaries for the TAC 2008 Update Summarization Track. The submitted runs were designed to determine if a heuristic clause segmentation applied before sentence selection would improve summarization results by reducing the need for sentence compression approaches. A baseline summariser was used to test this hypothesis. The TAC results achieved suggest that a slight trend in improvement was detected.", 
Automatic summaries have become very important resources. This work evaluate deep content selection methods for multi-document summarization based on the CST model. Results show that the use of CST model helps to improve informativeness and quality.,Experiments with CST-based Multidocument Summarization,"Recently, with the huge amount of growing information in the web and the little available time to read and process all this information, automatic summaries have become very important resources. In this work, we evaluate deep content selection methods for multi-document summarization based on the CST model (Cross-document Structure Theory). Our methods consider summarization preferences and focus on the overall main problems of multi-document treatment: redundancy, complementarity, and contradiction among different information sources. We also evaluate the impact of the CST model over superficial summarization systems. Our results show that the use of CST model helps to improve informativeness and quality in automatic summaries.", 
"We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset.",Exploiting Image-trained CNN Architectures for Unconstrained Video Classification,"We conduct an in-depth exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED’14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED’14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED’14 from 34:95% to 38:74%. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF-101 dataset.", 
Most of the text summarization research to date has been concerned with the summarization of short documents. Little work has been done on summarizing very long documents. We introduce a new data set specifically designed.,Explorations in Automatic Book Summarization,"Most of the text summarization research carried out to date has been concerned with the summarization of short documents (e.g., news stories, technical reports), and very little work if any has been done on the summarization of very long documents. In this paper, we try to address this gap and explore the problem of book summarization. We introduce a new data set specifically designed for the evaluation of systems for book summarization, and describe summarization techniques that explicitly account for the length of the documents.", 
Generative probabilistic models for multi-document summarization. We construct a sequence of models each injecting more structure into the representation of document set content. We also explore HIERSUM's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.,Exploring Content Models for Multi-Document Summarization,"We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system. We also explore HIERSUM’s capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.", 
"Domain shift has been well explored in many NLP applications, it still has received little attention in the domain of extractive text summarization. The model is underutilizing the nature of the training data due to ignoring the difference in the distribution of training sets and shows poor generalization on the unseen domain.",Exploring Domain Shift in Extractive Text Summarization,"Although domain shift has been well explored in many NLP applications, it still has received little attention in the domain of extractive text summarization. As a result, the model is underutilizing the nature of the training data due to ignoring the difference in the distribution of training sets and shows poor generalization on the unseen domain. With the above limitation in mind, in this paper, we first extend the conventional definition of the domain from categories into data sources for the text summarization task. Then we re-purpose a multidomain summarization dataset and verify how the gap between different domains influences the performance of neural summarization models. Furthermore, we investigate four learning strategies and examine their abilities to deal with the domain shift problem. Experimental results on three different settings show their different characteristics in our new testbed.", 
"In this article, we explore an event detection framework to improve multi-document summarization. Our approach is based on a two-stage single-document method that extracts a collection of key phrases. We explore distributed representations of text in the form of word embeddings, which contributed to improve the summarization results.",Exploring events and distributed representations of text in multi-document summarization,"In this article, we explore an event detection framework to improve multi-document summarization. Our approach is based on a two-stage single-document method that extracts a collection of key phrases, which are then used in a centrality-as-relevance passage retrieval model. We explore how to adapt this single-document method for multi-document summarization methods that are able to use event information. The event detection method is based on Fuzzy Fingerprint, which is a supervised method trained on documents with annotated event tags. To cope with the possible usage of different terms to describe the same event, we explore distributed representations of text in the form of word embeddings, which contributed to improve the summarization results. The proposed summarization methods are based on the hierarchical combination of single-document summaries. The automatic evaluation and human study performed show that these methods improve upon current state-of-the-art multi-document summarization systems on two mainstream evaluation datasets, DUC 2007 and TAC 2009. We show a relative improvement in ROUGE-1 scores of 16% for TAC 2009 and of 17% for DUC 2007.", 
"The recent artificial intelligence studies have witnessed great interest in abstractive text summarization. Motivated by the humanlike reading strategy that follows a hierarchical routine, we propose a novel Hybrid learning model for Abstractive Text Summarization. The model consists of three major components, a knowledge-based attention network, a multitask encoder-decoder network, and a generative adversarial network.",Exploring Human-Like Reading Strategy for Abstractive Text Summarization,"The recent artificial intelligence studies have witnessed great interest in abstractive text summarization. Although remarkable progress has been made by deep neural network based methods, generating plausible and high-quality abstractive summaries remains a challenging task. The human-like reading strategy is rarely explored in abstractive text summarization, which however is able to improve the effectiveness of the summarization by considering the process of reading comprehension and logical thinking. Motivated by the humanlike reading strategy that follows a hierarchical routine, we propose a novel Hybrid learning model for Abstractive Text Summarization (HATS). The model consists of three major components, a knowledge-based attention network, a multitask encoder-decoder network, and a generative adversarial network, which are consistent with the different stages of the human-like reading strategy. To verify the effectiveness of HATS, we conduct extensive experiments on two real-life datasets, CNN/Daily Mail and Gigaword datasets. The experimental results demonstrate that HATS achieves impressive results on both datasets.", 
"Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. We have developed a graph-based method by exploring the links between text to produce coherent summaries.",Exploring Text Links for Coherent Multi-Document Summarization,"Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem, we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience, the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric, and show improvements in readability by human evaluation.", 
This paper explored the innovativeness and adoption categorization in library automation of the Federal Colleges of Education libraries North-West Nigeria. The majority of higher institutions of learning rely very well on the traditional method of library functions and services rather than adopting the order automation.,Exploring the Innovativeness and Adoption Categorization in Library Automation of the Federal Colleges of Education Libraries North-West Nigeria,"This paper explored the innovativeness and adoption categorization in library automation of the Federal Colleges of Education libraries North-West Nigeria. The majority of higher institutions of learning in North-West Nigeria rely very well on the traditional method of library functions and services rather than adopting the order automation. And the yardstick for measuring the effectiveness of institution bodies is the extent to which they are exposed to new things. Therefore, the overall objective of the study was to provide an explanation on the driver problem of lack of adoption for library automation by the academic libraries. The population of the study involved five (5) sampled college librarians, each represented from the five Federal Colleges of Education North-West Nigeria. A descriptive survey method, alongside a questionnaire, was used for data collection. Before the survey, questionnaire items were validated by lecturers in the Department of Library and Information Science, University of Gezira Sudan. A run reliability test (alpha level 0.05) indicated 0.700 Cronbach’s Alpha Coefficient on innovativeness for the adoption of library automation, and 0.993 Cronbach’s Alpha Coefficient on adoption category for library automation. Data collected were analyzed using descriptive statistics of simple frequency count and mean scores. All the college librarian questionnaires were retrieved and used for data analysis.", 
"Transfer learning has emerged as a powerful technique in natural language processing (NLP) The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. This paper introduces a unified framework that converts all text-based language problems into a text-to-text format.",Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.", 
"In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases.",Extract with Order for Coherent Multi-Document Summarization,"In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.", 
"This paper advocates the thesis that the summarizations obtained from a sentence can be categorized. The propounded approach consists of three steps: scanning the whole text for matching keywords, then checking the number of repetition of keywords, and finally ranking each matched category for final result.",Extraction and Classification of Linguistic Text for Text Summarization,"Text summarization is one of the most exigent fields in Natural Language Processing. Classification of text and keyword fetching from a sentence is the most intrinsic part of text summarization. The method propounded in this paper is immensely coherent for classification of Linguistic Sentences using statistical approach. This method can be implemented in Search Engines, Sentimental Analysis, Digital Dictionaries, Text Summarization Automatic Indexing, Filtering, Topic Detection and Report Generation etc [1]. This paper advocates the thesis that the summarizations obtained from a sentence can be categorized, ranking keywords by different fuzzy value as scores. The propounded approach consists of three steps: scanning the whole text for matching keywords, then checking the number of repetition of keywords, and finally ranking each matched category for final result.", 
"This paper describes an algorithm that incorporates k-means clustering, term-frequency inverse-document-frequency and tokenization to perform extraction based text summarization. ",Extraction based approach for text summarization using k-means clustering,"This paper describes an algorithm that incorporates k-means clustering, term-frequency inverse-document-frequency and tokenization to perform extraction based text summarization. The approach used in this paper is an unsupervised learning technique which is accomplished as a part of three step process. Tokenization of Document, Computing Score for each Sentence and Applying Centroid Based Clustering on the Sentences and extracting important Sentences as part of summary.", 
Total size of internet has now turned to 2 petabytes. The increase in the performance and fast accessing of web resources has made a new challenge of browsing among huge data on internet. The research on web has turned its steps towards Browsing among Information (BAI) rather than Browsed for Information (BFI).,Extraction Based Automatic Text Summarization System with HMM Tagger,"A rough estimation of world’s famous search engine Google in year 2010 revealed that the total size of internet has now turned to 2 petabytes. The increase in the performance and fast accessing of web resources has made a new challenge of browsing among huge data on internet. It is hence browsing on web is an under laid topic for researchers. The research on web has turned its steps towards Browsing among Information (BAI) rather than Browsing for Information (BFI).The field of Information Extraction (IE) is offering a huge scope to concise and compact the information enabling the user to decide by mere check at snippets of each link. Automatic text summarization is the process of condensing the source text into a shorter version preserving its information content and overall meaning. In this paper, we propose a frequent term based text summarization technique based on the analysis of Parts of Speech for generating effective and efficient summary.", 
"This paper provides a comparative analysis of various graph based extraction methods for automatic text summarizations. We consider five techniques that include TextRank, LexRank, LSA, Luhn and Edmundson. ","Extraction Based Text Summarization Methods on User's Review Data, A Comparative Study","This paper provides a comparative analysis of various graph based extraction methods for automatic text summarizations using ROUGE on review dataset. We consider five techniques that include TextRank, LexRank, LSA, Luhn, and Edmundson. These methods concentrate on predicting the semantics of an entity. The experimental results on summarizing the users’ opinions show that the LexRank method gives the best performance among all. Generated summaries are understandable and convey informative opinions.", 
This paper presents a summarization technique for text documents exploiting the semantic similarity between sentences. Semantic similarity scores are computed by mapping the sentences on a semantic space using Random Indexing. It presents a computationally efficient way of implicit dimensionality reduction.,Extraction-Based Single-Document Summarization Using Random Indexing,"This paper presents a summarization technique for text documents exploiting the semantic similarity between sentences to remove the redundancy from the text. Semantic similarity scores are computed by mapping the sentences on a semantic space using Random Indexing. Random Indexing, in comparison with other semantic space algorithms, presents a computationally efficient way of implicit dimensionality reduction. It involves inexpensive vector computations such as addition. It thus provides an efficient way to compute similarities between words, sentences and documents. Random Indexing has been used to compute the semantic similarity scores of sentences and graph-based ranking algorithms have been employed to produce an extract of the given text.", 
"This paper presents a novel approach for creating text summaries. Using fuzzy logic and word-net, our model extracts the most relevant sentences from an original document. The approach utilizes fuzzy measures and inference on the extracted textual information.",EXTRACTION-BASED TEXT SUMMARIZATION USING FUZZY ANALYSIS,"Due to the explosive growth of the world-wide web, automatic text summarization has become an essential tool for web users. In this paper we present a novel approach for creating text summaries. Using fuzzy logic and word-net, our model extracts the most relevant sentences from an original document. The approach utilizes fuzzy measures and inference on the extracted textual information from the document to find the most significant sentences. Experimental results reveal that the proposed approach extracts the most relevant sentences when compared to other commercially available text summarizers. Text pre-processing based on word-net and fuzzy analysis is the main part of our work.", 
"This paper proposed an approach for Arabic text summarization. The proposed approach is a graph-based system, which represents the document as a graph where the vertices of the graph are the sentences. A Modified PageRank algorithm is applied with an initial score for each node that is the number of nouns in this sentence. The extracted summary depends on compression ratio, taking into account removing redundancy.",Extractive Arabic Text Summarization Using Modified PageRank Algorithm,"This paper proposed an approach for Arabic text summarization. Text summarization is one of the natural language processing’s applications which is used for reducing the original text amount and retrieving only the important information from the original text. The Arabic language has a complex morphological structure which makes it very difficult to extract nouns to be used as a feature for summarization process. Therefore, Al-Khalil morphological analyzer is used to solve the problem of nouns extraction. The proposed approach is a graph-based system, which represents the document as a graph where the vertices of the graph are the sentences. A Modified PageRank algorithm is applied with an initial score for each node that is the number of nouns in this sentence. More nouns in the sentence mean more information, so nouns count used here as initial rank for the sentence. Edges between sentences are the cosine similarity between the sentences, to get a final summary that contains sentences with more information and well connected with each other. The process of text summarization consists of three major stages: preprocessing stage, features extraction and graph construction stage, and finally applying the Modified PageRank algorithm and summary extraction. The Modified PageRank algorithm used a different number of iterations to find the number returns the best summary results, and the extracted summary depends on compression ratio, taking into account removing redundancy depending on the overlapping between the sentences. To evaluate the performance of this approach EASC Corpus is used as a standard. LexRank and TextRank algorithms were used under the same circumstances, the proposed approach provides better results when compared with other Arabic text summarization techniques. The proposed approach performs efficiently with the number of iteration 10,000.", 
ATS task consists in automatically synthesizing a document to provide a condensed version of it. This study proposes a new method for the ATS task that takes advantage of semantic information to improve keyword detection. Results indicate that the proposed method outperformed previous methods with a standard collection.,Extractive Automatic Text Summarization Based on Lexical-Semantic Keywords,"The automatic text summarization (ATS) task consists in automatically synthesizing a document to provide a condensed version of it. Creating a summary requires not only selecting the main topics of the sentences but also identifying the key relationships between these topics. Related works rank text units (mainly sentences) to select those that could form the summary. However, the resulting summaries may not include all the topics covered in the source text because important information may have been discarded. In addition, the semantic structure of documents has been barely explored in this field. Thus, this study proposes a new method for the ATS task that takes advantage of semantic information to improve keyword detection. This proposed method increases not only the coverage by clustering the sentences to identify the main topics in the source document but also the precision by detecting the keywords in the clusters. The experimental results of this work indicate that the proposed method outperformed previous methods with a standard collection.",  
Automatic text summarization is the process of reducing the text content and retaining the important points of a document. There are two approaches: Extractive and Abstractive. The process of extractive based text. summarization can be divided into two phases: pre-processing and processing.,Extractive Based Automatic Text Summarization,"Automatic text summarization is the process of reducing the text content and retaining the important points of the document. Generally, there are two approaches for automatic text summarization: Extractive and Abstractive. The process of extractive based text summarization can be divided into two phases: pre-processing and processing. In this paper, we discuss some of the extractive based text summarization approaches used by researchers. We also provide the features for extractive based text summarization process. We also present the available linguistic preprocessing tools with their features, which are used for automatic text summarization. The tools and parameters useful for evaluating the generated summary are also discussed in this paper. Moreover, we explain our proposed lexical chain analysis approach, with sample generated lexical chains, for extractive based automatic text summarization. We also provide the evaluation results of our system generated summary. The proposed lexical chain analysis approach can be used to solve different text mining problems like topic classification, sentiment analysis, and summarization.", 
This paper focuses on the extractive based summarization using K-Means Clustering with TFIDF (Term Frequency-Inverse Document Frequency) for summarization. The approaches of automatic text summarization earn a keen interest within the Text Mining and NLP (Natural Language Processing) communities.,Extractive based Text Summarization Using K-Means and TF-IDF,"The quantity of information on the internet is massively increasing and gigantic volume of data with numerous compositions accessible openly online become more widespread. It is challenging nowadays for a user to extract the information efficiently and smoothly. As one of the methods to tackle this challenge, text summarization process diminishes the redundant information and retrieves the useful and relevant information from a text document to form a compressed and shorter version which is easy to understand and timesaving while reflecting the main idea of the discussed topic within the document. The approaches of automatic text summarization earn a keen interest within the Text Mining and NLP (Natural Language Processing) communities because it is a laborious job to manually summarize a text document. Mainly there are two types of text summarization, namely extractive based and abstractive based. This paper focuses on the extractive based summarization using K-Means Clustering with TFIDF (Term Frequency-Inverse Document Frequency) for summarization. The paper also reflects the idea of true K and using that value of K divides the sentences of the input document to present the final summary. Furth more, we have combined the K-means, TF-IDF with the issue of K value and predict the resulting system summary which shows comparatively best results.", 
The exponential growth of the Web documents has constituted the need for automatic document summarization. ExDoS is the first approach to combine both supervised and unsupervised algorithms in a single framework. The model obtains higher ROUGE scores comparing to most state-of-the-art models. The human evaluation also demonstrates that our model is capable of generating informative and readable summaries.,Extractive Document Summarization Based on Dynamic Feature Space Mapping,"The exponential growth of the Web documents has constituted the need for automatic document summarization. In this context, extractive document summarization, i.e., that task of extracting the most relevant information, removing redundancy and presenting the remained data in a coherent and cohesive structure, is a challenging task. In this paper, we propose a novel intelligent approach, namely ExDoS, that harvests benefits of both supervised and unsupervised algorithms simultaneously. To the best of our knowledge, ExDoS is the first approach to combine both supervised and unsupervised algorithms in a single framework and an interpretable manner for document summarization purpose. ExDoS iteratively minimizes the error rate of the classifier in each cluster with the help of dynamic local feature weighting. Moreover, this approach specifies the contribution of features to discriminate each class, which is a challenging issue in the summarization task. Therefore, in addition to summarizing text, ExDoS is also able to measure the importance of each feature in the summarization process. We evaluate our model both automatically (in terms of ROUGE factor) and empirically (human analysis) on the benchmark datasets: the DUC2002 and CNN/DailyMail. Results show that our model obtains higher ROUGE scores comparing to most state-of-the-art models. The human evaluation also demonstrates that our model is capable of generating informative and readable summaries.", 
"We propose a novel methodology for extractive, generic summarization of text documents. The Maximum Independent Set, which has not been used previously in any summarization study, has been utilized. A text processing tool, which we named KUSH, is suggested to preserve semantic cohesion between sentences.",Extractive multi-document text summarization based on graph independent sets,"We propose a novel methodology for extractive, generic summarization of text documents. The Maximum Independent Set, which has not been used previously in any summarization study, has been utilized within the context of this study. In addition, a text processing tool, which we named KUSH, is suggested in order to preserve the semantic cohesion between sentences in the representation stage of introductory texts. Our anticipation was that the set of sentences corresponding to the nodes in the independent set should be excluded from the summary. Based on this anticipation, the nodes forming the Independent Set on the graphs are identified and removed from the graph. Thus, prior to quantification of the effect of the nodes on the global graph, a limitation is applied on the documents to be summarized. This limitation prevents repetition of word groups to be included in the summary. Performance of the proposed approach on the Document Understanding Conference (DUC-2002 and DUC-2004) datasets was calculated using ROUGE evaluation metrics. The developed model achieved a 0.38072 ROUGE performance value for 100-word summaries, 0.51954 for 200-word summaries, and 0.59208 for 400-word summaries. The values reported throughout the experimental processes of the study reveal the contribution of this innovative method.", 
Multi-Objective Artificial Bee Colony (MOABC) algorithm has been designed and implemented for summarizing text. It can be proven to produce more concentrated ROUGE values when the algorithm execution is repeated. The results of the proposed approach show important improvements in the relative dispersion.,Extractive multi-document text summarization using a multi-objective artificial bee colony optimization approach,"Automatic text summarization methods are increasingly needed nowadays. Extractive multi-document summarization approaches aim to obtain the main content of a document collection at the same time that the redundant information is reduced. This can be addressed from an optimization point of view. There is a lack of multi-objective approaches applied in this context. In this paper, a Multi-Objective Artificial Bee Colony (MOABC) algorithm has been designed and implemented for this task. Experiments have been performed based on datasets from Document Understanding Conference (DUC) and model performances have been evaluated with Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics, as is usual in this knowledge field. The results of the proposed approach show important improvements, i.e., in average, 31.09% (8.43%) and 18.63% (6.09%) of improvement in ROUGE-2 (ROUGE-L) have been obtained with respect to the best single-objective and multi-objective results in the scientific literature. Even more, the proposed approach has been proven to produce more concentrated ROUGE values when the algorithm execution is repeated (between 620.63% and 1333.95% of reduction in the relative dispersion, that is, between 6 and 13 times better), leading to more robust results.",  
Multi-document summarization is an optimization problem demanding optimizing more than one objective function concurrently. This is the first attempt to address text summarization problem as a MOO model. Heuristic perturbation and heuristic local repair operators are proposed and injected into the adopted evolutionary algorithm to harness its strength.,Extractive multi-document text summarization using multi-objective evolutionary algorithm based model,"Automatic document summarization technology is evolving and may offer a solution to the problem of information overload. Multi-document summarization is an optimization problem demanding optimizing more than one objective function concurrently. The proposed work considers a balance of two significant objectives: content coverage and diversity while generating a summary from a collection of text documents. Despite the large efforts introduced from several researchers for designing and evaluating performance of many text summarization techniques, their formulations lack the introduction of any model that can give an explicit representation of – coverage and diversity – the two contradictory semantics of any summary. The design of generic text summarization model based on sentence extraction is modeled as an optimization problem redirected into more semantic measure reflecting individually both content coverage and content diversity as an explicit individual optimization models. The proposed two models are then coupled and defined as a multi-objective optimization (MOO) problem. Up to the best of our knowledge, this is the first attempt to address text summarization problem as a MOO model. Moreover, heuristic perturbation and heuristic local repair operators are proposed and injected into the adopted evolutionary algorithm to harness its strength. Assessment of the proposed model is performed using document sets supplied by Document Understanding Conference 2002 ( ) and a comparison is made with other state-of-the-art methods using Recall-Oriented Understudy for Gisting Evaluation ( ) toolkit. Results obtained support strong proof for the effectiveness of the proposed model based on MOO over other state-of-the-art models.", 
A new algorithm called ESDS-GHS-GLO has been proposed for text summarization. It outperforms most of the state-of-the-art methods except MA-SingleDocSum. The algorithm is based on the Global-best Harmony Search metaheuristic and a greedy local search procedure.,Extractive Single-Document Summarization Based on Global-Best Harmony Search and a Greedy Local Optimizer,"Due to the great amount of documents available on the Web, end users need to be able to access information in summary form – keeping the most important information in the document. The methods employed for automatic text summarization generally allocate a score to each sentence in the document, taking into account certain features. The most relevant sentences are then selected, according to the score obtained for each sentence. In this paper, the extractive single document summarization task is treated as a binary optimization problem and, based on the Global-best Harmony Search metaheuristic and a greedy local search procedure, a new algorithm called ESDS-GHS-GLO is proposed. This algorithm optimizes an objective function, which is a lineal normalized combination of the position of the sentence in the document, sentence length, and coverage of the selected sentences in the summary. The proposed method was compared with the state of the art methods MA-SingleDocSum, DE, FEOM, UnifiedRank, NetSum, QCS, CRF, SVM, and Manifold Ranking, using ROUGE measures on the DUC2001 and DUC2002 datasets. The results showed that ESDS-GHS-GLO outperforms most of the state-of-the-art methods except MA-SingleDocSum. ESDS-GHS-GLO obtains promissory results using a fitness function less complex than MA-SingleDocSum, therefore requiring less execution time.", 
Extractive text or speech summarization endeavors to select representative sentences from a source document and assemble them into a concise summary. The recent past has seen a surge of interest in developing deep learning- or deep neural network-based supervised methods for extractive text summarization.,Extractive speech summarization leveraging convolutional neural network techniques,"Extractive text or speech summarization endeavors to select representative sentences from a source document and assemble them into a concise summary, so as to help people to browse and assimilate the main theme of the document efficiently. The recent past has seen a surge of interest in developing deep learning- or deep neural network-based supervised methods for extractive text summarization. This paper presents a continuation of this line of research for speech summarization and its contributions are three-fold. First, we exploit an effective framework that integrates two convolutional neural networks (CNNs) and a multilayer perceptron (MLP) for summary sentence selection. Specifically, CNNs encode a given document-sentence pair into two discriminative vector embeddings separately, while MLP in turn takes the two embeddings of a document-sentence pair and their similarity measure as the input to induce a ranking score for each sentence. Second, the input of MLP is augmented by a rich set of prosodic and lexical features apart from those derived from CNNs. Third, the utility of our proposed summarization methods and several widely-used methods are extensively analyzed and compared. The empirical results seem to demonstrate the effectiveness of our summarization method in relation to several state-of-the-art methods.", 
"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually, we model the relationship between sentences as a semantic text matching problem. We believe the power of this matching-based summarization framework has not been fully exploited.",Extractive Summarization as Text Matching,"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries.", 
The most successful approaches to extractive text summarization seek to maximize bigram coverage. We propose instead to maximize semantic volume. We provide a greedy algorithm based on the Gram-Schmidt process to efficiently.,Extractive Summarization by Maximizing Semantic Volume,"The most successful approaches to extractive text summarization seek to maximize bigram coverage subject to a budget constraint. In this work, we propose instead to maximize semantic volume. We embed each sentence in a semantic space and construct a summary by choosing a subset of sentences whose convex hull maximizes volume in that space. We provide a greedy algorithm based on the Gram-Schmidt process to efficiently perform volume maximization. Our method outperforms the state-of-the-art summarization approaches on benchmark datasets.", 
Text summarization is currently a major research topic in Natural Language Processing. Latent semantic analysis (LSA) is a technique in natural language processing. It is an unsupervised approach which does not need any training or external knowledge. This is the first LSA based Text Summarizer in Myanmar.,Extractive Summarization for Myanmar Language,"Due to increasing availability of online information, tools and mechanisms for automatic summarization of documents is needed. Text summarization is currently a major research topic in Natural Language Processing. There are various approaches to generate text summary. Among them, we proposed Myanmar text summarization using latent semantic analysis (LSA). Latent semantic analysis (LSA) is a technique in natural language processing, and can analyze relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. It is an unsupervised approach which does not need any training or external knowledge. There is no LSA based sentence extraction in Myanmar language. This is the first LSA based Text Summarizer in Myanmar. This paper present generic, extractive and single-document Myanmar text summarization using latent semantic analysis. This paper compare two sentence selection methods (Steinberger and Jezek's approach and Ozay approach) of latent semantic analysis to extract important sentences. We summarize Myanmar news from Myanmar official websites such as 7day daily, iyarwaddy, etc.", 
"In this paper, we propose a text summarization technique by constructing lexical chains. Lexical chains try to identify cohesion links between words by identifying their semantic relationship. They try to link words in a document that are thought to be describing the same concept to gather information.",Extractive Summarization of a Document Using Lexical Chains,"Nowadays, efficient access of information from the text documents with high-degree of semantic information has become more difficult due to diversity of vocabulary and rapid growth of the Internet. Traditional text clustering algorithms are widely used to organize a large text document into smaller manageable groups of sentences, but it does not consider the semantic relationship among the words present in the document. Lexical chains try to identify cohesion links between words by identifying their semantic relationship. They try to link words in a document that are thought to be describing the same concept to gather information. This method of text summarization helps to process the linguistic features of the document which is otherwise ignored in statistical summarization approaches. In this paper, we have proposed a text summarization technique by constructing lexical chains and defining a coherence metric to select the summary sentences.", 
"This work describes the process and quality of automatically generated summaries of clinical trial descriptions using extractive text summarization methods. We generated a novel dataset from trials registered on clinicaltrials.gov. On average, the summaries in this corpus are 25% the length of the detailed descriptions. The TextRank algorithm exhibits the overall best performance with a ROUGE-1 F1 score of 0.3531.",Extractive summarization of clinical trial descriptions,"Text summarization of clinical trial descriptions has the potential to reduce the time required to familiarize oneself with the subject of studies by condensing long-form detailed descriptions to concise, meaning-preserving synopses. This work describes the process and quality of automatically generated summaries of clinical trial descriptions using extractive text summarization methods. We generated a novel dataset from the detailed descriptions and brief summaries of trials registered on clinicaltrials.gov. We executed several text summarization algorithms on the detailed descriptions in this corpus and calculated the standard ROUGE metrics using the brief summaries included in the record as a reference. To investigate the correlation of these metrics with human sentiments, four reviewers assessed the content-completeness of the generated summaries and the helpfulness of both the generated and reference summaries via a Likert scale questionnaire. The filtering stages of the dataset generation process reduce the 277,228 trials registered on clinicaltrials.gov to 101,016 records usable for the summarization task. On average, the summaries in this corpus are 25% the length of the detailed descriptions. Of the evaluated text summarization methods, the TextRank algorithm exhibits the overall best performance with a ROUGE-1 F1 score of 0.3531, ROUGE-2 F1 score of 0.1723, and ROUGE-L F1 score of 0.3003. These scores correlate with the assessment of the helpfulness and content similarity by the human reviewers. Inter-rater agreement for the helpfulness and content similarity was slight and fair respectively (Fleiss’ kappa of 0.12 and 0.22). Extractive summarization is a viable tool for generating meaning-preserving synopses of detailed clinical trial descriptions. Further, the human evaluation has shown that the ROUGE-L F1 score is useful for rating the general quality of generated summaries of clinical trial descriptions in an automated way.", 
This paper proposes a sentence– image classification method based on the multi-modal RNN model. The method is able to mine the hidden sentence–image alignments and to create informative well-aligned multi- modal summaries. Experiments on the extended DailyMail corpora constructed by collecting images and captions from the Web show that our method outperforms 11 baseline text summarization methods.,Extractive summarization of documents with images based on multi-modal RNN,"Rapid growth of multi-modal documents containing images on the Internet expresses strong demand on multi-modal summarization. The challenge is to create a computing method that can uniformly process text and image. Deep learning provides basic models for meeting this challenge. This paper treats extractive multi-modal summarization as a classification problem and proposes a sentence– image classification method based on the multi-modal RNN model. Our method encodes words and sentences with the hierarchical RNN models and encodes the ordered image set with the CNN model and the RNN model, and then calculates the selection probability of sentences and the sentence–image alignment probability through a logistic classifier taking text coverage, text redundancy, image set coverage, and image set redundancy as features. Two methods are proposed to compute the image set redundancy feature by combining the important scores of sentences and the hidden sentence– image alignment. Experiments on the extended DailyMail corpora constructed by collecting images and captions from the Web show that our method outperforms 11 baseline text summarization methods and that adopting the two image-related features in the classification method can improve text summarization. Our method is able to mine the hidden sentence–image alignments and to create informative well-aligned multi-modal summaries.", 
"Several approaches to automatic speech summarization are discussed below, using the ICSI Meetings corpus. We contrast feature-based approaches using prosodic and lexical features with maximal marginal relevance and latent semantic analysis approaches to summarization. ",Extractive summarization of meeting recordings,"Several approaches to automatic speech summarization are discussed below, using the ICSI Meetings corpus. We contrast feature-based approaches using prosodic and lexical features with maximal marginal relevance and latent semantic analysis approaches to summarization. While the latter two techniques are borrowed directly from the field of text summarization, feature-based approaches using prosodic information are able to utilize characteristics unique to speech data. We also investigate how the summarization results might deteriorate when carried out on ASR output as opposed to manual transcripts. All of the summaries are of an extractive variety, and are compared using the software ROUGE.", 
Central to automatic summarization is the notion of similarity between sentences in text. We propose the use of continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity.,Extractive Summarization using Continuous Vector Space Models,Automatic summarization can help users extract the most important pieces of information from the vast amount of text digitized into electronic form everyday. Central to automatic summarization is the notion of similarity between sentences in text. In this paper we propose the use of continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity. We evaluate different compositions for sentence representation on a standard dataset using the ROUGE evaluation measures. Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of continuous word vector representations for automatic summarization., 
"This paper proposes a text summarization approach for factual reports using a deep learning model. The approach consists of three phases: feature extraction, feature enhancement, and summary generation. Experimentation carried out on several articles demonstrates the effectiveness of the proposed approach.",Extractive Summarization using Deep Learning,"This paper proposes a text summarization approach for factual reports using a deep learning model. This approach consists of three phases: feature extraction, feature enhancement, and summary generation, which work together to assimilate core information and generate a coherent, understandable summary. We are exploring various features to improve the set of sentences selected for the summary, and are using a Restricted Boltzmann Machine to enhance and abstract those features to improve resultant accuracy without losing any important information. The sentences are scored based on those enhanced features and an extractive summary is constructed. Experimentation carried out on several articles demonstrates the effectiveness of the proposed approach.", 
It is difficult to identify sentence importance from a single point of view. We propose a learning-based approach to combine various sentence features. Experiments show that the combined features improved summarization performance significantly.,Extractive summarization using supervised and semi-supervised learning,"It is difficult to identify sentence importance from a single point of view. In this paper, we propose a learning-based approach to combine various sentence features. They are categorized as surface, content, relevance and event features. Surface features are related to extrinsic aspects of a sentence. Content features measure a sentence based on content-conveying words. Event features represent sentences by events they contained. Relevance features evaluate a sentence from its relatedness with other sentences. Experiments show that the combined features improved summarization performance significantly. Although the evaluation results are encouraging, supervised learning approach requires much labeled data. Therefore we investigate co-training by combining labeled and unlabeled data. Experiments show that this semi-supervised learning approach achieves comparable performance to its supervised counterpart and saves about half of the labeling time cost.", 
Generative pretrained language models have been proven very successful on a wide range of NLP tasks. This is the first work that applies BERT based architecture to a text summarization task.,Extractive summarization with very deep pretrained language model,"Recent development of generative pretrained language models has been proven very successful on a wide range of NLP tasks, such as text classification, question answering, textual entailment and so on. In this work, we present a two-phase encoder decoder architecture based on Bidirectional Encoding Representation from Transformers(BERT) for extractive summarization task. We evaluated our model by both automatic metrics and human annotators, and demonstrated that the architecture achieves the state-of-the-art comparable result on large scale corpus – ‘CNN/Daily Mail1 As the best of our knowledge’, this is the first work that applies BERT based architecture to a text summarization task and achieved the state-of-the-art comparable result.", 
"Text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. We prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets. We present a new model of summarization, which generalizes existing models in the literature.","Extractive Summarization, Limits, Compression, Generalized Model and Heuristics","Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization problem, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and some existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.", 
"Automatic Text Summarizer serves as one of the best tool for interpreting lengthy textual content. It represents the shorter version of the original document by choosing most important part of text, thus generating its summary. Rule-based method was proposed to select the best sentences.",Extractive text summarization by feature-based sentence extraction using rule-based concept,"World Wide Web is a tremendous source of knowledge. Vast amount of information available over the internet has made the humans suffer from problem of information explosion. Therefore, a good mechanism is required to extract relevant information. This calls a need for an automatic and significant tool that converts lengthy documents into concise form by extracting relevant information from it. Automatic Text Summarizer serves as one of the best tool for interpreting lengthy textual content. It represents the shorter version of the original document by choosing most important part of text, thus generating its summary. It is classified into two categories: abstraction and extraction. This paper highlights on extractive approach. Main aim is to select best sentences by weighting them. We carried out our experiment on 15 documents from DUC 2002 dataset. Each test document was first pre-processed. Then, all the sentences were represented as attribute vector of features by calculating their scores. Rule-based method was proposed to select the best sentences. Results were compared with GSM summarizer and conclusion was drawn that best average recall, precision and f-measure values was obtained for Rule-Based Summarizer.", 
"There is no framework available for automatic Urdu extractive summary generation. The sentences with the highest weights are given importance to be included in the summary. The sentence weight is computed by the sum of the weights of the words in the sentence. In the proposed framework, LW and GW based approaches are modeled for the Urdu language. The extractive summaries are generated by LW and GW based approaches and evaluated with ground-truth summaries.",Extractive Text Summarization Models for Urdu Language,"In the recent few years, a lot of advancement has been made in Urdu linguistics. There are many portals and news websites that are generating a huge amount of data every day. However, there is still no publicly available dataset nor any framework available for automatic Urdu extractive summary generation. In an automatic extractive summary generation, the sentences with the highest weights are given importance to be included in the summary. The sentence weight is computed by the sum of the weights of the words in the sentence. There are two famous approaches to compute the weight of the words in the English language: local weights (LW) approach and global weights (GW) approach. The sensitivity of the weights depends on the contents of the text, the one word may have di?erent weights in a di?erent article, known as LW based approach. Whereas, in the case of GW, the weights of the words are computed from the independent dataset, which implies the weights of all words remain the same in di?erent articles. In the proposed framework, LW and GW based approaches are modeled for the Urdu language. The sentence weight method and the weighted term-frequency method are LW based approaches that compute the weights of the sentences by the sum of important words and the sum of frequencies of the important words, respectively. Whereas, vector space model (VSM) is GW based approach, that computes the weight of the words from the independent dataset, and then remain the same for all types of the text; GW is widely used in the English language for various applications such as information retrieval and text classifcation. The extractive summaries are generated by LW and GW based approaches and evaluated with ground-truth summaries that are obtained by the experts. The VSM is used as a baseline framework for sentence weighting. Experiments show that LW based approaches are better for extractive summary generation. The F-score of the sentence weight method and the weighted term-frequency method are 80% and 76%, respectively. The VSM achieved only 62% accuracy on the same dataset. Both, the datasets with ground-truth, and the code are made publicly available for the researchers.", 
"Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications.",Extractive text summarization system to aid data extraction from full text in systematic review development,"Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. In this study, we developed a text summarization system aimed at enhancing productivity and reducing errors in the traditional data extraction process. We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications. The summaries at the sentence and fragment levels were evaluated in finding common clinical SR data elements such as sample size, group size, and PICO values. We compared the computer-generated summaries with human written summaries (title and abstract) in terms of the presence of necessary information for the data extraction as presented in the Cochrane review’s study characteristics tables. At the sentence level, the computer-generated summaries covered more information than humans do for systematic reviews (recall 91.2% vs. 83.8%, p<0.001). They also had a better density of relevant sentences (precision 59% vs. 39%, p<0.001). At the fragment level, the ensemble approach combining rule-based, concept mapping, and dictionary-based methods performed better than individual methods alone, achieving an 84.7% F-measure. Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system.", 
An approach for generating short and precise summaries for long text documents is proposed. It uses a combination of Restricted Boltzmann Machine and Fuzzy Logic to select important sentences from the text. Various sentence and word level features are used to provide meaningful sentences.,Extractive Text Summarization Using Deep Learning,"An approach for generating short and precise summaries for long text documents is proposed. Lately, the size of information on the internet is increasing. It has become tough for the users to dig into the loads of information to analyze it and draw conclusions. Text summarization solves this problem by generating a summary, selecting sentences which are most important from the document without losing the information. In this work, an approach for Extractive text summarization is designed and implemented for single document summarization. It uses a combination of Restricted Boltzmann Machine and Fuzzy Logic to select important sentences from the text still keeping the summary meaningful and lossless. The text documents used for summarization are in English language. Various sentence and word level features are used to provide meaningful sentences. Two summaries for each document are generated using Restricted Boltzmann Machine and Fuzzy logic. Both summaries are then combined and processed using a set of operations to get the final summary of the document. The results show that the designed approach overcomes the problem of text overloading by generating an effective summary.", 
Text-summarization is a new application of making this task simplified. This paper presents the integration of Restricted Boltzmann Machine and Fuzzy Logic methods.,Extractive text summarization using F-RBM,The Intense growth of data over the internet and process extracting predominating details from it is a time -consuming task. A new application of making this task simplified is text-summarization. This paper presents the integration of Restricted Boltzmann Machine and Fuzzy Logic methods. Both of these processes have different ways of giving precision to a summary but together being part of the unsupervised world has surpassed their working alone to summarize the text. Integrated algorithm i.e. called FRBM (fuzzy Restricted Boltzmann Machine) provides enhanced depiction in the modeling of probability over visible and hidden units than the RBM., 
"Keyword extraction is an important phase in automatic text summarization process. Identifying lexical association between terms is one of the existing techniques proposed for determining the topic of the document. In this paper, we have made use of Lexical association and graph based ranking techniques for retrieving keywords.",Extractive Text Summarization Using Lexical Association and Graph Based Text Analysis,"Keyword extraction is an important phase in automatic text summarization process because it directly affects the relevance of the system generated summary. There are many procedures for extracting keywords, but all of these aim to find the words that directly represent the topic of the document. Identifying lexical association between terms is one of the existing techniques proposed for determining the topic of the document. In this paper, we have made use of lexical association and graph based ranking techniques for retrieving keywords from a source text and subsequently to assign them a relative weight. The individual weights of the extracted keywords are used to rank the sentences in the source text. Our summarization system is tested with DUC 2002 dataset and is found to be effective when compared to the existing context based summarization systems.", 
This paper presents a comparative evaluation of statistical methods in extractive text summarization. Modified weighing method is identified as the best method with 80% efficiency. Thematic weight and emphasize weights are added to conventional weighing method.,Extractive text summarization using modified weighing and sentence symmetric feature methods,"Text Summarization is a process that converts the original text into summarized form without changing the meaning of its contents. It finds its usefulness in many areas when the time to go through a large content is limited. This paper presents a comparative evaluation of statistical methods in extractive text summarization. Top score method is taken to be the bench mark for evaluation. Modified weighing method and modified sentence symmetric feature method are implemented with additional characteristic features to achieve a better performance than the benchmark method. Thematic weight and emphasize weights are added to conventional weighing method and the process of weight updation in sentence symmetric method is also modified in this paper. After evaluating these three methods using the standard measures, modified weighing method is identified as the best method with 80% efficiency.", 
Text Summarization has been an extensively studied problem. We propose a data-driven approach using feedforward neural networks for single document summarization. We train and evaluate the model on standard DUC 2002 dataset.,Extractive Text Summarization using Neural Networks,"Text Summarization has been an extensively studied problem. Traditional approaches to text summarization rely heavily on feature engineering. In contrast to this, we propose a fully data-driven approach using feedforward neural networks for single document summarization. We train and evaluate the model on standard DUC 2002 dataset which shows results comparable to the state of the art models. The proposed model is scalable and is able to produce the summary of arbitrarily sized documents by breaking the original document into fixed sized parts and then feeding it recursively to the network.", 
"The biomedical article is the trendy issue at the moment leading to the huge amount of knowledge created rapidly. In this paper, we propose a new automatic extractive text summarization technique based on graph representation. We compared our results with other text summarizing software using 400 biological review papers.",Extractive Text Summarization Using Ontology and Graph-Based Method,"In recent years, many people started to take care of the physical health. The biomedical article is the trendy issue at the moment leading to the huge amount of knowledge created rapidly. In this paper, we propose a new automatic extractive text summarization technique based on graph representation that makes use of the Unified Medical Language System (UMLS), an ontology knowledge from the National Library of Medicine (NLM). We combined the graph building rules with a distance function between text documents, called Word Mover’s Distance. To prioritize the core sentences, we extracted the summary by using a popular graph-based method from Google, PageRank. We compared our results with other text summarization software using 400 biological review papers as a corpus randomly sampled from PubMed Central. Our approach outperformed the baseline comparators in terms of Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores.", 
"Automatic Text summarization is the technique to identify the most useful and necessary information in a text. In this paper, a novel statistical method to perform an extractive text summarization on single document is demonstrated. The method extraction of sentences, which gives the idea of the input text, is presented.",Extractive Text Summarization Using Sentence Ranking,"Automatic Text summarization is the technique to identify the most useful and necessary information in a text. It has two approaches 1) Abstractive text summarization and 2) Extractive text summarization. An extractive text summarization means an important information or sentence are extracted from the given text file or original document. In this paper, a novel statistical method to perform an extractive text summarization on single document is demonstrated. The method extraction of sentences, which gives the idea of the input text in a short form, is presented. Sentences are ranked by assigning weights and they are ranked based on their weights. Highly ranked sentences are extracted from the input document so it extracts important sentences which directs to a high-quality summary of the input document and store summary as audio.", 
"Text summarization is the process of shortening the documents by preserving the important contents of the text. In this paper, we have proposed an approach to extract a good set of features followed by neural network for supervised extractive summarization.",Extractive Text Summarization Using Word Vector Embedding,"These days, text summarization is an active research field to identify the relevant information from large documents produced in various domains such as finance, news media, academics, politics, etc. Text summarization is the process of shortening the documents by preserving the important contents of the text. This can be achieved through extractive and abstractive summarization. In this paper, we have proposed an approach to extract a good set of features followed by neural network for supervised extractive summarization. Our experimental results on Document Understanding Conferences 2002 dataset show the effectiveness of the proposed method against various online extractive text summarizers.", 
"In this paper we examine whether the performance of a text summarization method depends on the topic of a document. We conclude that although our first hypothesis is not supported, adapting summarization systems to the linguistic properties of input documents benefits the process of summarization.","Extractive Text Summarization, Can We Use the Same Techniques for Any Text","In this paper we address two issues. The first one analyzes whether the performance of a text summarization method depends on the topic of a document. The second one is concerned with how certain linguistic properties of a text may affect the performance of a number of automatic text summarization methods. For this we consider semantic analysis methods, such as textual entailment and anaphora resolution, and we study how they are related to proper noun, pronoun and noun ratios calculated over original documents that are grouped into related topics. Given the obtained results, we can conclude that although our first hypothesis is not supported, since it has been found no evident relationship between the topic of a document and the performance of the methods employed, adapting summarization systems to the linguistic properties of input documents benefits the process of summarization.", 
Extractive summarization is the strategy of concatenating extracts taken from a corpus into a summary. abstractive summarization involves paraphrasing the corpus using novel sentences. The margin by which abstraction outperforms extraction is greater when controversiality is high.,"Extractive vs. NLG-based abstractive summarization of evaluative text,The effect of corpus controversiality","Extractive summarization is the strategy of concatenating extracts taken from a corpus into a summary, while abstractive summarization involves paraphrasing the corpus using novel sentences. We define a novel measure of corpus controversiality of opinions contained in evaluative text, and report the results of a user study comparing extractive and NLG-based abstractive summarization at different levels of controversiality. While the abstractive summarizer performs better overall, the results suggest that the margin by which abstraction outperforms extraction is greater when controversiality is high, providing a context in which the need for generation-based methods is especially great.", 
"A new learning-based encoding method encodes the micro-structures of the face using unsupervised learning techniques. The resulting face representation is compact, highly discriminative, and easy to extract. We propose a pose-adaptive matching method that uses pose-specific classifiers to deal with different pose combinations.",Face Recognition with Learning-based Descriptor,"We present a novel approach to address the representation issue and the matching issue in face recognition (verification). Firstly, our approach encodes the micro-structures of the face by a new learning-based encoding method. Unlike many previous manually designed encoding methods (e.g., LBP or SIFT), we use unsupervised learning techniques to learn an encoder from the training examples, which can automatically achieve very good tradeoff between discriminative power and invariance. Then we apply PCA to get a compact face descriptor. We find that a simple normalization mechanism after PCA can further improve the discriminative ability of the descriptor. The resulting face representation, learning-based (LE) descriptor, is compact, highly discriminative, and easy-to-extract. To handle the large pose variation in real-life scenarios, we propose a pose-adaptive matching method that uses pose-specific classifiers to deal with different pose combinations (e.g., frontal v.s. frontal, frontal v.s. left) of the matching face pair. Our approach is comparable with the state-of-the-art methods on the Labeled Face in Wild (LFW) benchmark (we achieved 84.45% recognition rate), while maintaining excellent compactness, simplicity, and generalization ability across different datasets.", 
Facial expression recognition model is based on transfer features from deep convolutional networks (ConvNets) We train the deep ConvNets through the task of 1580-class face identification on the MSRA-CFW database. We transfer high-level features from the trained deep model to recognize expression. The modified model obviously improves its ability of classification in the occluded condition.,Facial Expression Recognition based on Transfer Learning from Deep Convolutional Networks,"It is well-known that deep models could extract robust and abstract features. We propose a efficient facial expression recognition model based on transfer features from deep convolutional networks (ConvNets). We train the deep ConvNets through the task of 1580-class face identification on the MSRA-CFW database and transfer high-level features from the trained deep model to recognize expression. To train and test the facial expression recognition model on a large scope, we built a facial expression database of seven basic emotion states and 2062 imbalanced samples depending on four facial expression databases (CK+, JAFFE, KDEF, Pain expressions form PICS). Compared with 50.65% recognition rate based on Gabor features with the seven-class SVM and 78.84% recognition rate based on distance features with the seven-class SVM, we achieve average 80.49% recognition rate with the seven-class SVM classifier on the self-built facial expression database. Considering occluded face in reality, we test our model in the occluded condition and demonstrate the model could keep its ability of classification in the small occlusion case. To increase the ability further, we improve the facial expression recognition model. The modified model merges high-level features transferred from two trained deep ConvNets of the same structure and the different training sets. The modified model obviously improves its ability of classification in the occluded condition and achieves average 81.50% accuracy on the self-built facial expression database.",  
"Facial expression recognition has been an active research area in the past 10 years. Growing application areas include avatar animation, neuromarketing and sociable robots. Recognition of facial expressions is not an easy problem for machine learning methods. In this work, we propose a simple solution that uses a combination of Convolutional Neural Network and specific image pre-processing steps.",Facial expression recognition with Convolutional Neural Networks Coping with few data and the training sample order,"Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for machine learning methods, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of Convolutional Neural Network and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CKþ, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods – 96.76% of accuracy in the CKþ database – it is fast to train, and it allows for real time facial expression recognition with standard computers.", 
We propose a method of Faster R-CNN (Faster Regions with Convolutional Neural Network Features) for facial expression recognition. The dataset is provided by Chinese Linguistic Data Consortium (CLDC) which is composed of multimodal emotional audio and video data. Experimental results show the performance and the generalization ability of the FasterR-CNN.,Facial Expression Recognition with Faster R-CNN,"In order to avoid the complex explicit feature extraction process and the problem of low-level data operation involved in traditional facial expression recognition, we proposed a method of Faster R-CNN (Faster Regions with Convolutional Neural Network Features) for facial expression recognition in this paper. Firstly, the facial expression image is normalized and the implicit features are extracted by using the trainable convolution kernel. Then, the maximum pooling is used to reduce the dimensions of the extracted implicit features. After that, RPNs (Region Proposal Networks) is used to generate high-quality region proposals, which are used by Faster R-CNN for detection. Finally, the Softmax classifier and regression layer is used to classify the facial expressions and predict boundary box of the test sample, respectively. The dataset is provided by Chinese Linguistic Data Consortium (CLDC), which is composed of multimodal emotional audio and video data. Experimental results show the performance and the generalization ability of the Faster R-CNN for facial expression recognition. The value of the mAP is around 0.82.", 
"Facial landmark detection has long been impeded by the problems of occlusion and pose variation. We investigate the possibility of improving detection robustness through multi-task learning. We formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence.",Facial Landmark Detection by Deep Multi-task Learning,"Facial landmark detection has long been impeded by the problems of occlusion and pose variation. Instead of treating the detection task as a single and independent problem, we investigate the possibility of improving detection robustness through multi-task learning. Specifically, we wish to optimize facial landmark detection together with heterogeneous but subtly correlated tasks, e.g. head pose estimation and facial attribute inference. This is non-trivial since different tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art method based on cascaded deep model.", 
"We have developed a summarization system, TAS (Technical Article Summarizer), that automatically generates a summary that is tailored to the patient characteristics. We hypothesize that a personalized summary will allow a physician to more quickly find information relevant to patient care.",Facilitating Physicians' Access to Information via Tailored Text Summarization,"We have developed a summarization system, TAS (Technical Article Summarizer), which, when provided with a patient record and journal articles returned by a search, automatically generates a summary that is tailored to the patient characteristics. We hypothesize that a personalized summary will allow a physician to more quickly find information relevant to patient care. In this paper, we present a user study in which subjects carried out a task under three different conditions: using search results only, using a generic summary and search results, and using a personalized summary with search results. Our study demonstrates that subjects do a better job on task completion with the personalized summary, and show a higher level of satisfaction, than under other conditions.", 
"An extractive summarization algorithm selects a subset of the textual units in the input data for inclusion in the summary. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with them.",Fairness of Extractive Text Summarization,"We propose to evaluate extractive summarization algorithms from a completely new perspective. Considering that an extractive summarization algorithm selects a subset of the textual units in the input data for inclusion in the summary, we investigate whether this selection is fair. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with the textual units, and find that the generated summaries often have very di?erent distributions of the said attribute. Specifically, some classes of the textual units are under-represented in the summaries according to the fairness notion of adverse impact. To our knowledge, this is the first work on fairness of summarization, and is likely to open up interesting research problems.", 
Nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from fake facts. We leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation of fake summaries.,Faithful to the Original Fact Aware Neural Abstractive Summarization,"Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.", 
"Electroencephalogram (EEG) is a widely used non-invasive brain signal acquisition technique that measures voltage fluctuations from neuron activities of the brain. EEGs are typically used to diagnose and monitor disorders such as epilepsy, sleep disorders, and brain death. This paper proposes a deep learning based model to detect the presence of the artifacts.",Fast Automatic Artifact Annotator for EEG Signals Using Deep Learning,"Electroencephalogram (EEG) is a widely used non-invasive brain signal acquisition technique that measures voltage fluctuations from neuron activities of the brain. EEGs are typically used to diagnose and monitor disorders such as epilepsy, sleep disorders, and brain death and also to help the advancement of various fields of science such as cognitive science, and psychology. EEG signals usually suffer from a variety of artifacts caused by eye movements, chewing, muscle movements, and electrode pops, which disrupts the diagnosis and hinders precise representation of brain activities. This paper proposes a deep learning based model to detect the presence of the artifacts and to classify the kind of the artifact to help clinicians resolve problems regarding artifacts immediately during the signal collection process. The model is optimized to map the 1-second segments of raw EEG signals to detect 4 different kinds of artifacts and the real signal. The model achieves a 5-class classification accuracy of 67.59%, and a true positive rate of 80% with a 25.82% false alarm for binary artifact classification with time-lapse. The model is lightweight and could potentially be deployed in portable machines.", 
"Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. It trains the very deep VGG16 network 9× faster than R- CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012.",Fast R-CNN,"This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe).", 
"Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. It trains the very deep VGG16 network 9× faster than R- CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012.","Fast, Accurate Detection of 100,000 Object Classes on a Single Machine","Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object’s appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times— four orders of magnitude— when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.", 
"State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network.",Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", 
"Legal Documents were little bit tough to understand and it is too long, hence a headnote, a brief summary of the Legal Document, is a needed one in the current scenario. The generation of headnote is a time consuming process for Advocates as well as Judges for Arguments and for Decision Making.",Feature Extraction Based Legal Document Summarization,"Basically Legal Documents were little bit tough to understand and it is too long, hence a headnote, a brief summary of the Legal Document, is a needed one in the current scenario. In recent days there are number of research work is going on to automate this Head note preparation process. The generation of headnote is a time consuming process for Advocates as well as Judges for Arguments and for Decision Making. The main drawback in the legal field is that, they were not coherent and can’t able to convey the relative relevance of the case. In this paper, Head note is prepared automatically from the legal document. A set of legal documents were extracted from the Website, and used as training data. The present work implements Fuzzy Logic Technique to generate the Headnote summary.", 
"The neurological changes in alertness and drowsiness states can be asses by electroencephalogram (EEG) signals. In this paper, the nonstationary characteristic of the EEG signal is explored by tunable Q-factor wavelet transform (TQWT) The results of the KW-test show that the proposed features are effectively discriminative of the alertness.",Feature extraction method for classification of alertness and drowsiness states EEG signals,"Drowsy driving is one of the major causes of road accidents. The road accidents can be avoided by the discrimination of alertness and drowsiness states of the drives. The neurological changes in alertness and drowsiness states can be asses by electroencephalogram (EEG) signals. In this paper, the nonstationary characteristic of the EEG signal is explored by tunable Q-factor wavelet transform (TQWT). TQWT decomposes the EEG signal into sub-bands, which further used for the extraction of features. Statistical features of the Hjorth mobility such as minimum value, maximum value, mean and standard deviation (SD) are used for characterization of the alertness and drowsiness states. Various classifiers such as decision tree, logistic regression, fine Gaussian support vector machine, weighted KNN, ensemble boosted trees and extreme learning machine (ELM) are considered. The alertness and drowsiness EEG signals discriminative performance of TQWT-based features are assessed by the Kruskal-Wallis (KW) test. The results of KW-test show that the proposed features are effectively discriminative of the alertness and drowsiness states. According to the obtained results, the best accuracy score of 91.842% is produced by the ELM classifier.", 
"The neurological changes in alertness and drowsiness states can be asses by electroencephalogram (EEG) signals. In this paper, the nonstationary characteristic of the EEG signal is explored by tunable Q-factor wavelet transform (TQWT) The results of the KW-test show that the proposed features are effectively discriminative of the alertness.",FEATURE EXTRACTION OF EEG SIGNAL USING WAVELET TRANSFORM FOR AUTISM CLASSIFICATION,"Feature extraction is a process to extract information from the electroencephalogram (EEG) signal to represent the large dataset before performing classification. This paper is intended to study the use of discrete wavelet transform (DWT) in extracting feature from EEG signal obtained by sensory response from autism children. In this study, DWT is used to decompose a filtered EEG signal into its frequency components and the statistical feature of the DWT coefficient are computed in time domain. The features are used to train a multilayer perceptron (MLP) neural network to classify the signals into three classes of autism severity (mild, moderate and severe). The training results in classification accuracy achieved up to 92.3% with MSE of 0.0362. Testing on the trained neural network shows that all samples used for testing is being classified correctly.", 
A good technique for feature extraction is necessary in order to achieve robust classification of signal. Several techniques have been implemented for extracting features in EEG signal. Local Discriminant Bases algorithm is introduced in the present paper as another powerful adaptive feature extraction technique for EEG signal which is not reported elsewhere.,Feature Extraction of Electroencephalogram (EEG) Signal-A Review,"This paper presents a review on signal analysis method for feature extraction of electroencephalogram (EEG) signal. It is an important aspect in signal processing as the result obtained will be used for signal classification. A good technique for feature extraction is necessary in order to achieve robust classification of signal. Considering several techniques have been implemented for extracting features in EEG signal, we only highlight the most commonly used for schizophrenia. The techniques are Hilbert-Huang transform, Principal Component Analysis, Independent Component Analysis and Local Discriminant Bases. Despite of their drawbacks, they can be applied which depends on the aim of a research, parameters and the data collected. Nevertheless, these techniques can be modified so that the new algorithm can overcome the shortcomings of original algorithm or algorithm beforehand. The modified Local Discriminant Bases algorithm is introduced in the present paper as another powerful adaptive feature extraction technique for EEG signal which is not reported elsewhere in investigating schizophrenia.", 
 EEG waveforms carry information about the underlying neural system dynamics and show different features amongst epilepsy syndromes. Highest average accuracy was obtained for 3 classes using 20 features of power spectrum from the sum of 6 IMFs. The data was used as the classification inputs for artificial neural networks and random forest classifiers.,Feature Extraction Using Combination of Intrinsic Mode Functions and Power Spectrum for EEG Signal Classification,"The measurement of brain electrical activity recorded as EEG signals finds most application in epilepsy. EEG waveforms carry information about the underlying neural system dynamics and show different features amongst epilepsy syndromes. In this research, empirical mode decomposition (EMD) and power spectrum were employed to extract the features from EEG dataset of healthy participants, and epilepsy patients with seizure and seizure free conditions. The recorded EEG signals are represented by 500 signal segments from 5 sets of different conditions. The sum of Intrinsic Mode Function (IMF) power spectrum components gave 10 features for 50 components, and 20 features for 25 components, which were used as the classification inputs for artificial neural networks and random forest classifiers. The classifications were carried out for 3, 4, and 5 classes. From the experiments, the highest average accuracy was obtained for 3 classes using 20 features of power spectrum from the sum of 6 IMFs. For use of 6 IMFs, the accuracies had the maximum values of 92.4%, 90.4%, and 78.6% for 3, 4 , and 5 classes respectively. It also improved the accuracy significantly for 5 classes.", 
"Boosting algorithms have been proved effective for multi-label learning. RFBoost statistically outperforms both RFBoost1 and AdaBoost.MH on all datasets. RF boost1 proved more efficient than Adaboost.MH, making it a better alternative for addressing classification problems in real-life applications and expert systems. The paper presents and investigates seven feature ranking methods to improve RFBoost's performance.",Feature ranking for enhancing boosting-based multi-label text categorization,"Boosting algorithms have been proved effective for multi-label learning. As ensemble learning algorithms, boosting algorithms build classifiers by composing a set of weak hypotheses. The high computational cost of boosting algorithms in learning from large volumes of data such as text categorization datasets is a real challenge. Most boosting algorithms, such as AdaBoost.MH, iteratively examine all training features to generate the weak hypotheses, which increases the learning time. RFBoost was introduced to manage this problem based on a rank-and-filter strategy in which it first ranks the training features and then, in each learning iteration, filters and uses only a subset of the highest-ranked features to construct the weak hypotheses. This step ensures accelerated learning time for RFBoost compared to AdaBoost.MH, as the weak hypotheses produced in each iteration are reduced to a very small number. As feature ranking is the core idea of RFBoost, this paper presents and investigates seven feature ranking methods (information gain, chi-square, GSS-coefficient, mutual information, odds ratio, F1 score, and accuracy) in order to improve RFBoost’s performance. Moreover, an accelerated version of RFBoost, called RFBoost1, is also introduced. Rather than filtering a subset of the highest-ranked features, FBoost1 selects only one feature, based on its weight, to build a new weak hypothesis. Experimental results on four benchmark datasets for multi-label text categorization) Reuters-21578, 20-Newsgroups, OHSUMED, and TMC2007(demonstrate that among the methods evaluated for feature ranking, mutual information yields the best performance for RFBoost. In addition, the results prove that RFBoost statistically outperforms both RFBoost1 and AdaBoost.MH on all datasets. Finally, RFBoost1 proved more efficient than AdaBoost.MH, making it a better alternative for addressing classification problems in real-life applications and expert systems.", 
Heart Failure is a clinical syndrome commonly caused by any structural or functional impairment. Fast and accurate mortality prediction is essential to improve the health care of patients and prevent them from death. This paper proposes a feature rearrangement based deep learning system for heart failure mortality prediction. The proposed system is experimentally evaluated on real-world Heart Failure data collected from the EHR system of Shanghai Shuguang Hospital.,Feature rearrangement based deep learning system for predicting heart failure mortality,"Heart Failure is a clinical syndrome commonly caused by any structural or functional impairment. Fast and accurate mortality prediction for Heart Failure is essential to improve the health care of patients and prevent them from death. However, due to the imbalance problem and poor feature representation in Heart Failure data, mortality prediction of Heart Failure is difficult with some simple models. To handle these problems, this study is focused on proposing a fast and accurate Heart Failure mortality prediction framework. This paper proposes a feature rearrangement based deep learning system for heart failure mortality prediction. The proposed framework improves the performance of predicting heart failure mortality by handling imbalance problem and achieving better feature representation. This paper also proposes a method named Feature rearrangement based convolutional layer, which demonstrates that the order of the input features is essential for the convolutional network. The proposed system is experimentally evaluated on real-world Heart Failure data collected from the EHR system of Shanghai Shuguang Hospital, where 10,198 in-patients records are extracted between March 2009 and April 2016. Internal comparison results illustrate that the proposed framework achieves the best performance for Heart Failure mortality prediction. Extensive experimental results compared with other machine learning methods demonstrate that the proposed method has the highest average accuracy and area under the curve while predicting the three goals of in-hospital mortality, 30-day mortality, and 1-year mortality. Finally, top 12 essential clinical features are mined with their chi-square scores, which can help to assist clinicians in the treatment and research of heart failure. The proposed method successfully predict different target in three observation windows. Feature rearrangement based convolutional layer and Focal loss are employed into the proposed framework, which helps promote the prediction accuracy of Heart Failure death. The proposed method is fast and accurate for predicting heart failure mortality, especially for imbalance situation. This paper also provide a reasonable pipeline to model EHRs data and handle imbalance problem in medical data.", 
"Brain-computer interface technology interprets the EEG signals displayed by the human brain. It uses the interpreted information to manipulate the outside world, thereby abandoning the human peripheral nerves and muscle systems. The method has a high accuracy rate for the recognition of motor imagery EEG.",Feature recognition of motor imaging EEG signals based on deep learning,"The brain-computer interface technology interprets the EEG signals displayed by the human brain’s neurological thinking activities through computers and instruments, and directly uses the interpreted information to manipulate the outside world, thereby abandoning the human peripheral nerves and muscle systems. The emergence of brain-computer interface technology has brought practical value to many fields. Based on the mechanism and characteristics of motion imaging EEG signals, this paper designs the acquisition experiment of EEG signals. After removing the anomalous samples, the wavelet-reconstruction method is used to extract the specific frequency band of the motion imaging EEG signal. According to the characteristics of motor imagery EEG signals, the feature recognition algorithm of convolutional neural networks is discussed. After an in-depth analysis of the reasons for choosing this algorithm, a variety of different network structures were designed and trained. The optimal network structure is selected by analyzing the experimental results, and the reasons why the structure effect is superior to other structures are analyzed. The results show that the method has a high accuracy rate for the recognition of motor imagery EEG, and it has good robustness.", 
"In this paper, we propose a scheme for feature selection using linear independent component analysis and mutual information maximization method. The method is theoretically motivated by the fact that the classification error rate is related to the mutual information.",Feature Selection by Independent Component Analysis and Mutual Infornation Maximization in BEG Signal Classification,"Feature selection and dimensionality reduction are important steps in pattern recognition. In this paper, we propose a scheme for feature selection using linear independent component analysis and mutual information maximization method. The method is theoretically motivated by the fact that the classification error rate is related to the mutual information between the feature vectors and the class labels. The feasibility of the principle is illustrated on a synthetic dataset and its performance is demonstrated using EEG signal classification. Experimental results show that this method works well for feature selection.", 
Text Summarization is condensing source text into a shorter version preserving its information content. The importance of sentences is decided based on statistical and linguistic features of sentences. Mathematical regression is used to estimate the text feature weights based on fuzzy scores of sentences of 50 Punjabi news documents.,Features Selection and Weight learning for Punjabi Text Summarization,"This paper concentrates on features selection and weight learning for Punjabi Text Summarization. Text Summarization is condensing the source text into a shorter version preserving its information content. It is the process of selecting important sentences from the original document and concatenating them into shorter form. The importance of sentences is decided based on statistical and linguistic features of sentences. For Punjabi language text Summarization, some of statistical features that often increase the candidacy of a sentence for inclusion in summary are: Sentence length feature, Punjabi Keywords selection feature (TF-ISF approach) and number feature. Some of linguistic features that often increase the candidacy of a sentence for inclusion in summary are: Punjabi sentence headline feature, next line feature, Punjabi noun feature, Punjabi proper noun feature, common English-Punjabi noun feature, cue phrase feature and presence of title keywords in a sentence. Mathematical regression is used to estimate the text feature weights based on fuzzy scores of sentences of 50 Punjabi news documents.",  
"Feature selection, as a preprocessing stage, is a challenging problem in various sciences such as biology, engineering, computer science, and other fields. FeatureSelect is a feature or gene selection software application which is based on wrapper methods. It provides a user-friendly and straightforward method of feature selection for use in any kind of research.",FeatureSelect a software for feature selection based on machine learning approaches,"Feature selection, as a preprocessing stage, is a challenging problem in various sciences such as biology, engineering, computer science, and other fields. For this purpose, some studies have introduced tools and softwares such as WEKA. Meanwhile, these tools or softwares are based on filter methods which have lower performance relative to wrapper methods. In this paper, we address this limitation and introduce a software application called FeatureSelect. In addition to filter methods, FeatureSelect consists of optimisation algorithms and three types of learners. It provides a user-friendly and straightforward method of feature selection for use in any kind of research, and can easily be applied to any type of balanced and unbalanced data based on several score functions like accuracy, sensitivity, specificity, etc. Results: In addition to our previously introduced optimisation algorithm (WCC), a total of 10 efficient, well-known and recently developed algorithms have been implemented in FeatureSelect. We applied our software to a range of different datasets and evaluated the performance of its algorithms. Acquired results show that the performances of algorithms are varying on different datasets, but WCC, LCA, FOA, and LA are suitable than others in the overall state. The results also show that wrapper methods are better than filter methods. Conclusions: FeatureSelect is a feature or gene selection software application which is based on wrapper methods. Furthermore, it includes some popular filter methods and generates various comparison diagrams and statistical measurements.", 
Deep learning methods for classification of electroencephalographic (EEG) recordings have been restricted by the lack of large datasets. We propose a novel privacy-preserving DL architecture named federated transfer learning (FTL) for EEG classification that is based on the federated learning framework. We evaluate the performance of the proposed architecture on the PhysioNet dataset for 2-class motor imagery classification.,Federated Transfer Learning for EEG Signal Classification,"The success of deep learning (DL) methods in the Brain-Computer Interfaces (BCI) field for classification of electroencephalographic (EEG) recordings has been restricted by the lack of large datasets. Privacy concerns associated with EEG signals limit the possibility of constructing a large EEGBCI dataset by the conglomeration of multiple small ones for jointly training machine learning models. Hence, in this paper, we propose a novel privacy-preserving DL architecture named federated transfer learning (FTL) for EEG classification that is based on the federated learning framework. Working with the single-trial covariance matrix, the proposed architecture extracts common discriminative information from multi-subject EEG data with the help of domain adaptation techniques. We evaluate the performance of the proposed architecture on the PhysioNet dataset for 2-class motor imagery classification. While avoiding the actual data sharing, our FTL approach achieves 2% higher classification accuracy in a subject-adaptive analysis. Also, in the absence of multi-subject data, our architecture provides 6% better accuracy compared to other state-of-the-art DL architectures.", 
"The Internet of Vehicles (IoV) aims to provide a new convenient, comfortable, and safe driving way. Increasing reliance on wireless communication, control, and computing technology makes IoV more vulnerable to potential attacks. Authors develop a dual authentication scheme for IoV according to its different scenarios. They claim it provides privacy-preserving and security enhancement without the burden of key management.",Efficient Privacy-Preserving Dual Authentication and Key Agreement Scheme for Secure V2V Communications in an IoV Paradigm,"The Internet of Vehicles (IoV) aims to provide a new convenient, comfortable, and safe driving way, and in turn enables intelligent transportation through wireless communications among road-side units, on-board units (OBUs), phones, and other devices inside a vehicle. However, significantly increasing reliance on wireless communication, control, and computing technology makes IoV more vulnerable to potential attacks, such as remote intrusion, control, and trajectory tracking. Therefore, efficient authentication solutions preventing unauthorized visitors need to be addressed to cope with these issues. Hence, in this paper we focus on the security and privacy-preserving by developing a dual authentication scheme for IoV according to its different scenarios. First, the OBU self-generates an anonymous identity and temporary encryption key to open an authentication session. Second, the legitimacy of the vehicle’s real and anonymous identity can be verified by trust authority (TA). After that, the vehicle’s reputation is evaluated according to its history interactive behavior and the session key for V2V can be finally established. There are three major advantages, including privacy-preserving and security enhancement without a burden of key management in the condition of acceptable time delay range, introducing trust evaluation into authentication protocol, as well as considering the vehicle behavior attributes in the new reputation evaluation method. In addition, we also prove the correctness of this scheme using the Burrows–Abadi–Needham (BAN) logic, and the performance comparison against the existing schemes is given as well.", 
"Fashion pervades different instances of culture, ranging from clothes to language and behavior. This paper aims at investigating through a mixed-method approach cognitive-functional categories of fictive speech acts. We collected 200 texts from Vogue magazine (printed edition) from 2015 to 2018. The findings show that the occurrence of the categories varies per year with opposing trends.",Fictivity in Vogue a cognitive-functional categorization of fictive speech acts in a fashion corpus using corpus linguistics,"Fashion pervades different instances of culture, ranging from clothes to language and behavior. In this paper, we analyze the occurrence of three fictive speech categories (PASCUAL, 2014) as storytelling strategies in sections of North American Vogue magazine. This paper aims at investigating through a mixed-method approach cognitive-functional categories of fictive speech acts based on a corpus analysis of a corpus with fashion texts. We collected 200 texts from Vogue magazine (printed edition) from 2015 to 2018. In a sample of 60 texts, sentential (SENT), intra-sentential (INTRA) and inter-sentential (INTER) fictive acts were manually identified and categorized in each text through tags. The quantitative analysis mapped the counts of each category per year and per text and the qualitative in-depth analysis investigates instances from each category and their interpretation in the light of Fashion and cognitive and corpus linguistics theories. The findings show that the occurrence of the categories of fictive speech acts varies per year with opposing trends, though they tend to co-occur in the same texts. Therefore, fictive speech acts function as recurrent discursive strategies to foster a conversational environment in texts from Vogue magazine.", 
"Scientists need access to figures to validate research findings and to generate new hypotheses. By themselves, figures are nearly always incomprehensible to both humans and machines. The associated text of a figure is scattered throughout its fulltext article and contains redundant information. The FigSum+ systems automatically identify associated texts and remove redundant information, and generate a text summary for every figure in an article.",Figure-Associated Text Summarization and Evaluation,"Biomedical literature incorporates millions of figures, which are a rich and important knowledge resource for biomedical researchers. Scientists need access to the figures and the knowledge they represent in order to validate research findings and to generate new hypotheses. By themselves, these figures are nearly always incomprehensible to both humans and machines and their associated texts are therefore essential for full comprehension. The associated text of a figure, however, is scattered throughout its fulltext article and contains redundant information content. In this paper, we report the continued development and evaluation of several figure summarization systems, the FigSum+ systems, that automatically identify associated texts, remove redundant information, and generate a text summary for every figure in an article. Using a set of 94 annotated figures selected from 19 different journals, we conducted an intrinsic evaluation of FigSum+. We evaluate the performance by precision, recall, F1, and ROUGE scores. The best FigSum+ system is based on an unsupervised method, achieving F1 score of 0.66 and ROUGE-1 score of 0.97.", 
"Fine-grained visual categorization (FGVC) is challenging due in part to the fact that it is often difficult to acquire an enough number of training samples. To employ large models for FGVC without suffering from overfitting, existing methods usually adopt a strategy of pretraining the models using a rich set of auxiliary data. We propose in this paper a new deep FGVC model termed MetaFGNet.",Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data,"Fine-grained visual categorization (FGVC) is challenging due in part to the fact that it is often difficult to acquire an enough number of training samples. To employ large models for FGVC without suffering from overfitting, existing methods usually adopt a strategy of pretraining the models using a rich set of auxiliary data, followed by finetuning on the target FGVC task. However, the objective of pre-training does not take the target task into account, and consequently such obtained models are suboptimal for fine-tuning. To address this issue, we propose in this paper a new deep FGVC model termed MetaFGNet. Training of MetaFGNet is based on a novel regularized meta-learning objective, which aims to guide the learning of network parameters so that they are optimal for adapting to the target FGVC task. Based on MetaFGNet, we also propose a simple yet effective scheme for selecting more useful samples from the auxiliary data. Experiments on benchmark FGVC datasets show the efficacy of our proposed method.", 
"Deep hierarchical networks achieve the best results on benchmarks for object classification and handwritten digit recognition. Feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. NORB is completely trained within five epochs.","Flexible, High Performance Convolutional Neural Networks for Image Classification","We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.", 
"Fog computing is considered a promising technology for real-time big data analytics. It distributes the data processing at the edge of the network, which provides faster responses to ITS application queries and saves the network resources. This paper proposes a novel architecture for real time big data Analytics in the IoV environment. It also discusses the opportunities and challenges that face the implementation of fog computing and real- time bigData analytics in this environment.","Fog Based Intelligent Transportation Big Data Analytics in The Internet of Vehicles Environment: Motivations, Architecture, Challenges and Critical Issues","The intelligent transportation system (ITS) concept was introduced to increase road safety, manage traffic efficiently, and preserve our green environment. Nowadays, ITS applications are becoming more data-intensive and their data are described using the ”5Vs of Big Data”. Thus, to fully utilize such data, big data analytics need to be applied. The Internet of vehicles (IoV) connects the ITS devices to cloud computing centres, where data processing is performed. However, transferring huge amount of data from geographically distributed devices creates network overhead and bottlenecks, and it consumes the network resources. In addition, following the centralized approach to process the ITS big data results in high latency which cannot be tolerated by the delay-sensitive ITS applications. Fog computing is considered a promising technology for real-time big data analytics. Basically, the fog technology complements the role of cloud computing and distributes the data processing at the edge of the network, which provides faster responses to ITS application queries and saves the network resources. However, implementing fog computing and the lambda architecture for real-time big data processing is challenging in the IoV dynamic environment. In this regard, a novel architecture for real-time ITS big data analytics in the IoV environment is proposed in this paper. The proposed architecture merges three dimensions including intelligent computing (i.e. cloud and fog computing) dimension, real-time big data analytics dimension, and IoV dimension. Moreover, this paper gives a comprehensive description of the IoV environment, the ITS big data characteristics, the lambda architecture for real-time big data analytics, several intelligent computing technologies. More importantly, this paper discusses the opportunities and challenges that face the implementation of fog computing and real-time big data analytics in the IoV environment. Finally, the critical issues and future research directions section discusses some issues that should be considered in order to efficiently implement the proposed architecture.", 
"This study presents a forensic process of extracting WhatsApp data from the latest DB encryption used by WhatsApp to secure stored communication data. The steps involve using some open source tools that can be downloaded for free on the internet. Mobile phones are now mostly used to perform e-transactions, social networking and even criminal activities.",Forensic Acquisition of Data from a Crypt 12 Encrypted Database of Whatsapp,"Mobile phone devices have become popular among every age and social grouping in every society and are utilised by lots of people for different purposes. As the design of Mobile phones are continually evolving due to advancement in present technologies, applications that run on them are also being updated to fully utilise features on new devices. Due to the flexibility and portability coupled with applications that make communication easy and accessible, these devices are now mostly used to perform e-transactions, social networking and even criminal activities. One of such applications is WhatsApp which over various versions have tried to maintain the confidentiality and integrity of messages sent and received using WhatsApp. Securing of data from Criminals or unauthorized users called for constant updating of the encryption scheme of the SQLite database which is usually saved on the memory of the device on which it is installed. Over many updates of WhatsApp, the encryption has been changed from db.crypt, db.crypt5, db.crypt7, db.crypt8 to db.crypt12. There is need for forensic expert to constantly update their knowledge so as to get the needed information from the database. This study presents a forensic process of extracting WhatsApp data from db.crypt12, which is the latest SQLite Database encryption used by WhatsApp to secure stored communication data. The steps involve using some open source tools that can be downloaded for free on the internet.", 
"The method used in this study using National Institute of Standards and Technology (NIST) The study produced the information stored in the database in the form of data artifacts. The contents of the conversation chat messages, log history of delivery, phone number, and a conversation then based on these data to get items of digital evidence.",Forensic Analysis of Android-based WhatsApp Messenger Against Fraud Crime Using The National Institute of Standard and Technology Framework,"Mobile devices developed very rapidly along with the development of technology. Improved service users instant messenger applications such as WhatsApp would be very susceptible to crime. Some crimes such as pornography, premeditated murder and fraud, in the case involving technologies that serve as the digital evidence in court, either in the form of conversations, pictures, video recordings, and other chat messages originating from the application WhatsApp. The method used in this study using National Institute of Standards and Technology (NIST). The NIST method has four stages as a reference for the analysis of evidence, namely the collection, examination, analysis and reporting. Stages are used to process the data security of the physical evidence found and to prove the perpetrator. Analysis of the NIST investigation process has stages that have been modified so that it can adjust to the investigation procedure initiated on the seizure of evidence until the discovery process digital data from the evidence in the case of fraudulent crimes using WhatsApp application. The study produced the information stored in the database in the form of data artifacts that the contents of the conversation chat messages, log history of delivery, phone number, and a conversation then based on these data to get items of digital evidence which subsequently became a reference in the proceedings to determine the punishment for the perpetrators of fraud.", 
"WhatsApp is a giant mobile instant message IM application with over 1billion users. The huge usage of IM like WhatsApp through giant smart phone ""Android"" makes the digital forensic researchers to study deeply. The artefacts left behind in the smartphone play very important role in any electronic crime, or any terror attack.",Forensic Analysis of Artifacts of Giant Instant Messaging “WhatsApp”  in Android Smartphone,"WhatsApp is a giant mobile instant message IM application with over 1billion users. The huge usage of IM like WhatsApp through giant smart phone “Android” makes the digital forensic researchers to study deeply. The artefacts left behind in the smartphone play very important role in any electronic crime, or any terror attack. “WhatsApp” as a biggest IM in the globe is considered to be very important resource for information gathering about any digital crime. Recently, end-to-end encryption and many other important features were added and no device forensic analysis or network forensic analysis studies have been performed to the time of writing this paper. This paper explains how can we able to extract the Crypt Key of “WhatsApp” to decrypt the databases and extract precious artefacts resides in the android system without rooting the device. Artefacts that extracted from the last version of WhatsApp have been analysed and correlate to give new valuable evidentiary traces that help in investigating. Many hardware and software tools for mobile and forensics are used to collect as much digital evidence as possible from persistent storage on android device. Some of these tools are commercial like UFED Cellebrite and Andriller, and other are open source tools such as autopsy, adb, WhatCrypt. All of these tools that forensically sound accompanied this research to discover a lot of artefacts resides in android internal storage in WhatsApp application.", 
"WhatsApp & Viber apps allow users to exchange instant messages, share videos, audio's and images via Smartphone's. The increased use of Instant messengers on Android phones has turned to be the goldmine for mobile and computer forensic experts. This paper focuses on conducting forensic data analysis of 2 widely used IMs applications.",Forensic Analysis of Instant Messenger Applications Android Devices,"The modern day Smartphone’s have built in apps like “WhatsApp & Viber” which allow users to exchange instant messages, share videos, audio’s and images via Smartphone’s instead of relying on their desktop Computers or laptop thereby increasing the portability and convenience for a layman smart phone user. An Instant Messenger (IM) can serve as a very useful yet very dangerous platform for the victim and the suspect to communicate. The increased use of Instant messengers on Android phones has turned to be the goldmine for mobile and computer forensic experts. Traces and Evidence left by applications can be held on Android phones and retrieving those potential evidences with right forensic technique is strongly required. This paper focuses on conducting forensic data analysis of 2 widely used IMs applications on Android phones: WhatsApp and Viber. 5 Android phones were analyzed covering 3 different versions of Android OS: Froyo (2.2), GingerBread (2.3.x) and IceCream Sandwich (4.0.x). The tests and analysis were performed with the aim of determining what data and information can be found on the device’s internal memory for instant messengers e.g. chat messaging logs and history, send & received image or video files, etc. Determining the location of data found from FileSystem Extraction of the device was also determined. The experiments and results show that heavy amount of potential evidences and valuable data can be found on Android phones by forensic investigators.", 
"This study considers the social network, Instagram, as the research subject. Analyze the artifacts left on the Instagram application and shows evidence of gathering. Forensic analysis found that different browsers, due to the differences in privacy control, can lead to discrepancies in recording user behaviors on the same social network.",Forensic Analysis of Social Networks Based on Instagram,"The trend in social networking is changing people’s life style. Since both the smart phone and computers are connected to the same tools, the newly developed applications must serve both ends to please the users. Although the previous flourishing social networks such as Facebook, Google+, and LinkedIn, among the other social network sites, still have a high number of users, but their growth rates have gradually flattened. They have been replaced by emerging social networking sites such as Instagram. Therefore, the modes of cybercrime have also changed in accordance with the users’ activities. In order to identify crimes, it is basically necessary to use appropriate forensic techniques to retrieve these traces and evidence. This study considers the social network, Instagram, as the research subject. Analyze the artifacts left on the Instagram application and shows evidence of gathering such as posting pictures, tagging others, leaving comments and liking on Windows 10 and Android platform, respectively. Besides, this study uses an anti-forensic process to explore the differences between the traces that are left on different browsers, browsing environments, and operating systems. Finally, forensic analysis found that different browsers, due to the differences in privacy control, can lead to the discrepancies in recording the user behaviors on the same social network. It proves to be helpful to forensic analysts and practitioners because it assists them in mapping and finding digital evidences of Instagram on Windows 10 PC and Android smart phone.", 
"We present the forensic analysis of the artifacts left on Android devices by WhatsApp Messenger. By using the results discussed in this paper, an analyst will be able to reconstruct the list of contacts and the chronology of the messages that have been exchanged.",Forensic analysis of WhatsApp Messenger on Android smartphones,"We present the forensic analysis of the artifacts left on Android devices by WhatsApp Messenger, the client of the WhatsApp instant messaging system. We provide a complete description of all the artifacts generated by WhatsApp Messenger, we discuss the decoding and the interpretation of each one of them, and we show how they can be correlated together to infer various types of information that cannot be obtained by considering each one of them in isolation. By using the results discussed in this paper, an analyst will be able to reconstruct the list of contacts and the chronology of the messages that have been exchanged by users. Furthermore, thanks to the correlation of multiple artifacts, (s)he will be able to infer information like when a specific contact has been added, to recover deleted contacts and their time of deletion, to determine which messages have been deleted, when these messages have been exchanged, and the users that exchanged them.", 
Android forensics has evolved over time offering significant opportunities and exciting challenges. Android users may not be aware of the security and privacy implications of installing these applications on their phones. In this thesis we will be concentrating on one such application called 'WhatsApp'.,Forensic Analysis of WhatsApp on Android Smartphones,"Android forensics has evolved over time offering significant opportunities and exciting challenges. On one hand, being an open source platform Android is giving developers the freedom to contribute to the rapid growth of the Android market whereas on the other hand Android users may not be aware of the security and privacy implications of installing these applications on their phones. Users may assume that a password-locked device protects their personal information, but applications may retain private information on devices, in ways that users might not anticipate. In this thesis we will be concentrating on one such application called 'WhatsApp', a popular social networking application. We will be forming an outline on how forensic investigators can extract useful information from WhatsApp and from similar applications installed on an Android platform. Our area of focus is extraction and analysis of application user data from non-volatile external storage and the volatile memory (RAM) of an Android device.", 
"Digital evidence plays an important role in cyber crime investigation, as it is used to link persons with criminal activities. It is of extreme importance to guarantee integrity, authenticity, and auditability of digital evidence.",FORENSIC-CHAIN ETHEREUM BLOCKCHAIN BASED DIGITAL FORENSICS CHAIN OF CUSTODY,"Digital evidence plays an important role in cyber crime investigation, as it is used to link persons with criminal activities. Thus it is of extreme importance to guarantee integrity, authenticity, and auditability of digital evidence as it moves along different levels of hierarchy in chain of custody during cyber crime investigation. Blockchain technology’s capability of enabling comprehensive view of transactions (events/actions) back to origination provides enormous promise for the forensic community. In this research we proposed to use a blockchain that can be leveraged for forensic applications in particular bringing integrity and tamper resistance to digital forensics chain of custody.", 
We propose a novel text summarization model based on 0–1 non-linear programming problem. This proposed model covers the main content of the given document(s) through sentence assignment. When comparing our method to several methods.,Formulation of document summarization as a 0–1 nonlinear programming problem,"We proposed a novel text summarization model based on 0–1 non-linear programming problem. This proposed model covers the main content of the given document(s) through sentence assignment. We implemented our model on multi-document summarization task. When comparing our method to several existing summarization methods on an open DUC2001 and DUC2002 datasets, we found that the proposed method could improve the summarization results significantly. The methods were evaluated using ROUGE-1, ROUGE-2 and ROUGE-W metrics.", 
"Next generation communication relies on standardized protocols, heterogeneous architecture and advanced technologies. FPGA has the potential to be resource/power efficient, it can be used for building up constituents of 5G infrastructure. Dynamic reconfgurability and in-field programming features of FPGAs compared to fixed function ASICs help in developing better wireless systems.",FPGA for 5G Re-confgurable Hardware for Next Generation Communication,"Next generation communication relies on standardized protocols, heterogeneous architectures and advanced technologies that are envisioned to bring ubiquitous and seamless connectivity. This evolution of communication will not only improve the performance of the existing networks, but will also enable various applications in other fields while integrating different heterogeneous systems. This massive scaling of mobile communication requires higher bandwidth to operate. 5G promises a robust solution by o?ering ultra-low latency and high bandwidth for data transmission. To provide individuals and companies with a real-time, social, and all connected experience, an end-to-end coordinated architecture which is agile and intelligent has to be designed at each stage. As FPGA has the potential to be resource/power efficient, it can be used for building up constituents of 5G infrastructure. It can accelerate network performance without making a large investment in new hardware. Dynamic reconfgurability and in-field programming features of FPGAs compared to fixed function ASICs help in developing better wireless systems. This article presents various application areas of FPGAs for the upcoming 5G network planning.", 
"We propose a new, ambitious framework for abstractive summarization. It aims at selecting the content of a summary not from sentences, but from an abstract representation. This abstract representation relies on the concept of Information Items (INIT).",Framework for Abstractive Summarization using Text-to-Text Generation,"We propose a new, ambitious framework for abstractive summarization, which aims at selecting the content of a summary not from sentences, but from an abstract representation of the source documents. This abstract representation relies on the concept of Information Items (INIT), which we define as the smallest element of coherent information in a text or a sentence. Our framework differs from previous abstractive summarization models in requiring a semantic analysis of the text. We present a first attempt made at developing a system from this framework, along with evaluation results for it from TAC 2010. We also present related work, both from within and outside of the automatic summarization domain.", 
"Summarization using Reinforcement Learning (ASRL) is superior to the best performing method in DUC2004, according to a new paper. The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.",Framework of automatic text summarization using reinforcement learning,"We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary. We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores. The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.", 
EEG signal in the time domain with high sampled rate faces difficulties for their noise sensitive properties. Common spatial pattern (CSP) is a widely used approach for feature extraction. A frequency domain CSP is proposed by the work to overcome the limitations of the conventional CSP.,Frequency Domain Approach in CSP based Feature Extraction for EEG Signal Classification,"EEG signal in the time domain with high sampled rate faces difficulties for their noise sensitive properties that lead to erroneous feature extraction. Since the feature extraction is one of the most significant steps in EEG signal classification, common spatial pattern (CSP) is a widely used approach for feature extraction. Conventional CSP in the time domain may often fail to maintain the discriminative features between the classes. Therefore, a frequency domain CSP (FCSP) is proposed by the work to overcome the limitations of the conventional CSP. We have applied the conventional and FCSP method on the motor imagery data for feature extraction. The average classification accuracies of the conventional and FCSP method were found 74% and 84%, respectively. Eventually, the proposed scheme outperforms the conventional method by increasing the classification accuracy up to 10%.", 
"Proposed system generates a summary for a given input document based on identification and extraction of important sentences. The system counts nouns and verbs term frequency because they are considered as the most representative to the content of the text. Precision, recall and f-measure ratio are used to evaluate the accuracy of the generated summary.",Frequent term based text summarization for bahasa indonesia,"Text summary helps in understanding the content of a text without having to read the contents of the text as a whole. Automatic text summarization can be used to summarize the text easier. In this paper a frequent term based text summarization for Bahasa Indonesia is designed and implemented in java. The proposed system generates a summary for a given input document based on identification and extraction of important sentences in the document. The system counts nouns and verbs term frequency because they are considered as the most representative to the content of the text. The system also integrated to statistical approach with two underlying concepts such as title of the news article and location of the sentence. The generated summaries were compared with human generated summaries. Precision, recall and f-measure ratio are used to evaluate the accuracy of the generated summary. Assessment of the system summary result quality by respondents is also done by giving a value from 1 to 100. Based on the experimental results, the system is able to produce an effective summary with the average f-measure of 78%, at the compression rate of 30%. The average value of the quality of system summary result provided by respondents is 83,3.", 
"The existing garbage disposal system in India consists of unclassified waste collected from homes. Biodegradable waste is used to generate power, enrich soil and act as food to animals. This process does not harm the earth making it valuable, ecologically safe and helps us to protect our environment.",FRIENDLY WASTE SEGREGATION USING DEEP LEARNING,"Recent enforcement of law by the Indian government for the welfare of sanitation workers has raised the need for an automated system in waste management. The existing garbage disposal system in India consists of unclassified waste collected from homes which are then segregated at a station manually. This segregation of solid waste done by manual labor can bring about many health hazards for the waste sorters in addition to being less efficient, time consuming and not completely feasible due to their large amount. In our paper, we have proposed an automated recognition system using Deep learning algorithm in Artificial Intelligence to classify objects as biodegradable and non-biodegradable, where the system once trained with an initial dataset, can identify objects real-time and classify them almost accurately. Biodegradable waste is used to generate power, enrich soil and act as food to animals. This process does not harm the earth making it valuable, ecologically safe and helps us to protect our environment, rich ecosystem and human inhabitants in future.", 
"Study investigated how assessors with expertise categorized Beaujolais wines from general to more specific levels of categorization. In both perceptual and conceptual conditions, assessors were asked to perform a binary sorting task, followed by a verbalisation task. At each level, with a few exceptions, a clearer separation was observed between the two categories in the conceptual condition.",From perceptual to conceptual categorization of wines What is the effect of expertise,"Wine supply in the French market is structured in an intricate system of categories based on origin. There is very little knowledge about consumers understanding of this complex category system and the sensory styles behind these categories. This study investigated how assessors with di?erent level of expertise categorized Beaujolais wines from general to more specific levels of categorization (grape variety, appellation, and “lieu-dit”) in both perceptual (wines) and conceptual (wine labels) conditions. Based on the literature on expertise, we expected a stronger e?ect in the perceptual condition, in particular for the most specific levels of categorization. For each wine categorization level, three sets of 12 wines were tasted by three panels of 60 assessors: a panel of unfamiliar novices with no much exposure to Beaujolais wines; a panel of familiar novices with regular exposure to Beaujolais wines; and a panel of experts from the Beaujolais. In both perceptual and conceptual conditions, assessors were asked to perform a binary sorting task, followed by a verbalisation task. Data were analysed using DISTATIS. At each level, with a few exceptions, a clearer separation was observed between the two categories in the conceptual condition than in the perceptual condition. Although the experts categorized the labels by grape variety, they did not spontaneously categorize the wines in that way. Finally, we observed a clear expertise e?ect only in the conceptual condition. This result will be discussed in terms of expertise acquisition and categorization processes.", 
"Natural Language Processing (NLP) is now widely integrated into web and mobile applications, enabling natural interactions between humans and computers. There is a large body of NLP studies published in Information Systems (IS) However, a comprehensive review of how NLP research is conceptualized and realized in the context of IS has not been conducted.",From semantics to pragmatics where IS can lead in Natural Language Processing (NLP) research,"Natural Language Processing (NLP) is now widely integrated into web and mobile applications, enabling natural interactions between humans and computers. Although there is a large body of NLP studies published in Information Systems (IS), a comprehensive review of how NLP research is conceptualised and realised in the context of IS has not been conducted. To assess the current state of NLP research in IS, we use a variety of techniques to analyse a literature corpus comprising 356 NLP research articles published in IS journals between 2004 and 2018. Our analysis indicates the need to move from semantics to pragmatics. More importantly, our findings unpack the challenges and assumptions underlying current research trends in NLP. We argue that overcoming these challenges will require a renewed disciplinary IS focus. By proposing a roadmap of NLP research in IS, we draw attention to three NLP research perspectives and present future directions that IS researchers are uniquely positioned to address.", 
Informal style and apparent lack of structure in speech mean that the typical approaches used for text summarization must be extended for use with speech.,From text to speech summarization,"In this paper, we present approaches used in text summarization, showing how they can be adapted for speech summarization and where they fall short. Informal style and apparent lack of structure in speech mean that the typical approaches used for text summarization must be extended for use with speech. We illustrate how features derived from speech can help determine summary content within two ongoing summarization projects at Columbia University.", 
This paper shows that full abstraction can be accomplished in the context of guided summarization.,Fully Abstractive Approach to Guided Summarization,"This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.", 
"Atrial fibrillation (AF) is the most prevalent form of cardiac arrhythmia. Current treatments for AF remain suboptimal due to a lack of understanding of the underlying atrial structures that directly sustain AF. AtriaNet is a 16-layer convolutional neural network (CNN) on 154 3-D LGE-MRIs with a spatial resolution of 0.625 mm. By utilizing computationally efficient batch prediction, Atria net was able to process each atrial structure within 1 min.",Fully Automatic Left Atrium Segmentation From Late Gadolinium Enhanced Magnetic Resonance Imaging Using a Dual Fully Convolutional Neural Network,"Atrial fibrillation (AF) is the most prevalent form of cardiac arrhythmia. Current treatments for AF remain suboptimal due to a lack of understanding of the underlying atrial structures that directly sustain AF. Existing approaches for analyzing atrial structures in 3-D, especially from late gadolinium-enhanced (LGE) magnetic resonance imaging, rely heavily on manual segmentation methods that are extremely labor-intensive and prone to errors. As a result, a robust and automated method for analyzing atrial structures in 3-D is of high interest. We have, therefore, developed AtriaNet, a 16-layer convolutional neural network (CNN), on 154 3-D LGE-MRIs with a spatial resolution of 0.625 mm × 0.625 mm × 1.25 mm from patients with AF, to automatically segment the left atrial (LA) epicardium and endocardium. AtriaNet consists of a multi-scaled, dualpathway architecture that captures both the local atrial tissue geometry and the global positional information of LA using 13 successive convolutions and three further convolutions for merging. By utilizing computationally efficient batch prediction, AtriaNet was able to successfully process each 3-D LGE-MRI within 1 min. Furthermore, benchmarking experiments have shown that AtriaNet has outperformed the state-of-the-art CNNs, with a DICE score of 0.940 and 0.942 for the LA epicardium and endocardium, respectively, and an inter-patient variance of <0.001. The estimated LA diameter and volume computed from the automatic segmentations were accurate to within 1.59 mm and 4.01 cm3 of the ground truths. Our proposed CNN was tested on the largest known data set for LA segmentation, and to the best of our knowledge, it is the most robust approach that has ever been developed for segmenting LGE-MRIs. The increased accuracy of atrial reconstruction and analysis could potentially improve the understanding and treatment of AF.", 
"Convolutional networks are powerful visual models that yield hierarchies of features. We show that they can exceed the state-of-the-art in semantic segmentation. We build ""fully convolutional"" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning.",Fully Convolutional Networks for Semantic Segmentation,"Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.", 
A novel fully textile-integrated antenna based on a slotted short-circuited microstrip line has been designed. The antenna can be manufactured using an industrial loom and a laser prototyping machine.,Fully Textile-Integrated Microstrip-Fed Slot Antenna for Dedicated Short-Range Communications,"A novel fully textile-integrated antenna based on a slotted short-circuited microstrip line has been designed, manufactured and experimentally validated for its use in automobile upholsteries for dedicated short-range communications. The antenna can be manufactured using an industrial loom and a laser prototyping machine, avoiding subsequent treatments, sewing procedures or coatings. The manufactured antenna presents a central working frequency of 5.9 GHz and a 9.3% bandwidth. Good agreement between simulations and measurements has been achieved.", 
Text summarization is the automatic process of creating a short form of an original text. The main goal of an automatic text summarization system is production of a summary which satisfies the user's needs. This paper introduces a new model based on evolutionary algorithms and cellular learning automata. The results show that proposed method outperforms the other methods.,Fuzzy evolutionary cellular learning automata model for text summarization,"Text summarization is the automatic process of creating a short form of an original text. The main goal of an automatic text summarization system is production of a summary which satisfies the user's needs. In this paper, a new model for automatic text summarization is introduced which is based on fuzzy logic system, evolutionary algorithms and cellular learning automata. First, the most important features including word features, similarity measure, and the position and the length of a sentence are extracted. A linear combination of these features shows the importance of each sentence. To calculate similarity measure, a combined method based on artificial bee colony algorithm and cellular learning automata are used. In this method, joint n-grams among sentences are extracted by cellular learning automata and then an artificial bee colony algorithm classifies n-friends in order to extract data and optimize the similarity measure as fitness function. Moreover, a new approach is proposed to adjust the best weights of the text features using particle swarm optimization and genetic algorithm. This method discovers more important and less important text features and then assigns fair weights to them. At last, a fuzzy logic system is used to perform the final scoring. The results of the proposed approach were compared with the other methods including Msword, System19, System21, System28, System31, FSDH, FEOM, NetSum, CRF, SVM, DE, MA-SingleDocSum, Unified Rank and Manifold Ranking using ROUGE-l and ROUGE-2 measures on the DUC2002 dataset. The results show that proposed method outperforms the aforementioned methods.", 
"This paper proposes an automatic text summarization approach based on sentence extraction. It uses fuzzy logic, genetic algorithm, semantic role labeling and their combinations to generate high quality summaries. The proposed approaches are better than other approaches produced by Microsoft Word 2007, Copernic Summarizer, and MANYASPECTS summarizers.",Fuzzy Genetic Semantic Based Text Summarization,"Automatic text summarization is a data reduction process to exclude unnecessary details and present important information in a shorter version. One way to summarize document is by extracting important sentences in the document. To select suitable sentences, a numerical rank is assigned to each sentence based on a sentence scoring approach. Highly ranked sentences are used for the summary. This paper proposed an automatic text summarization approach based on sentence extraction using fuzzy logic, genetic algorithm, semantic role labeling and their combinations to generate high quality summaries. This study explored the benefits of the genetic algorithm in the optimization problem in for feature selection during the training phase and adjusts feature weights during the testing phase. Fuzzy IF-THEN rules were used to balance the weights between important and unimportant features. Conventional extraction methods cannot capture semantic relations between concepts in a text. Therefore, this research investigates the use of the semantic role labeling to capture the semantic contents in sentences and incorporate it into the summarization method. This paper is evaluated in terms of performance using ROUGE toolkit. Experimental results showed that the summaries produced by the proposed approaches are better than other approaches produced by Microsoft Word 2007, Copernic Summarizer, and MANYASPECTS summarizers.", 
Text summarization can be classified into two approaches: extraction and abstraction. This paper focuses on extraction approach. We propose text summarization based on fuzzy logic to improve the quality of the summary. We compared our results with the baseline summarizer and Microsoft Word 2007 summarizers.,Fuzzy Logic Based Method for Improving Text Summarization,"Text summarization can be classified into two approaches: extraction and abstraction. This paper focuses on extraction approach. The goal of text summarization based on extraction approach is sentence selection. One of the methods to obtain the suitable sentences is to assign some numerical measure of a sentence for the summary called sentence weighting and then select the best ones. The first step in summarization by extraction is the identification of important features. In our experiment, we used 125 test documents in DUC2002 data set. Each document is prepared by preprocessing process: sentence segmentation, tokenization, removing stop word, and word stemming. Then, we used 8 important features and calculate their score for each sentence. We proposed text summarization based on fuzzy logic to improve the quality of the summary created by the general statistic method. We compared our results with the baseline summarizer and Microsoft Word 2007 summarizers. The results show that the best average precision, recall, and f-measure for the summaries were obtained by fuzzy method.", 
Automatic methods are needed to efficiently sieve and scavenge useful information from the Internet. The objective of proposed multi-document summarization is to gain good content coverage with information diversity. The proposed statistical feature based model utilizes the fuzzy model to deal with the imprecise and uncertainty of feature weight.,Fuzzy logic based multi document summarization with improved sentence scoring and redundancy removal technique,"Nowadays abundant amount of information is available on Internet which makes it difficult for the users to locate desired information. Automatic methods are needed to efficiently sieve and scavenge useful information from the Internet. Text summarization is identified and accepted as one of the solutions to find desired contents from one or more documents. The objective of proposed multi-document summarization is to gain good content coverage with information diversity. The proposed statistical feature based model utilizes the fuzzy model to deal with the imprecise and uncertainty of feature weight. Redundancy removal using cosine similarity is presented as enrichment to proposed work. The proposed approach is compared with DUC (Document Understanding Conference) participant systems and other summarization systems such as TexLexAn, ItemSum, Yago Summarizer, MSSF and PatSum using ROUGE measure on dataset DUC 2004. The experimental results show that our proposed work achieves a significant performance improvement over the other summarizers.", 
Fuzzy logic is employed to provide multi-level priority assignment to the vehicles contending for channel access. NS-2 simulations prove that this technique improves the packet delivery and minimizes packet loss.,Fuzzy logic controller based priority model for VANET scheduling,"Designing a reliable Vehicular Adhoc Network is challenging, due to the critical constraints in providing reliability with ensured data delivery and minimal latency simultaneously. Here, a fuzzy-based congestion control technique, to minimize latency and packet loss for high priority safety messages is proposed. Fuzzy logic is employed to provide multi-level priority assignment to the vehicles contending for channel access. Based on this priority, the queued messages in the Cluster Head are scheduled. NS-2 simulations prove that this technique improves the packet delivery and minimizes packet loss and channel access delay compared to cognitive radio based implementations at largely densed vehicular network.", 
"Fuzzy Rough Sets are designed for decision-making with uncertainty, imprecision, and incompleteness in data. Two sentences may be equivalent in their meanings despite having different vector space representation. We propose to use Fuzzy rough Sets for the task of sentence similarity-based Text Summarization.",Fuzzy Rough Set-Based Sentence Similarity Measure and its Application to Text Summarization,"Fuzzy Rough Sets are designed for decision-making with uncertainty, imprecision, and incompleteness in data. We propose to use Fuzzy Rough Sets for the task of sentence similarity-based Text Summarization. Text data inherently possess uncertainty, imprecision, and incompleteness for data representation. Two sentences may be equivalent in their meanings despite having different vector space representation while Fuzzy Rough Sets incorporates the meanings of sentences. Fuzzy Rough Set-based sentence similarity for Text Summarization has not been proposed in literature before the present work. The contribution of the research is two-fold, namely (i) Fuzzy Rough Set-based sentence similarities has been proposed and validated on SICK2014 dataset. (ii) The proposed similarities between the sentences are thereby proposed for Single document Text Summarization and evaluated for DUC2002 dataset. Experimental results confirm the applicability and efficiency of using the proposed models for both sentence similarity computations as well as for summarization.", 
The aim of automatic text summarization systems is to select the most relevant information from an abundance of text sources. The sentences were ranked in descending order based on their scores and then the top n sentences were selected as final summary. experiments showed that the incorporation of fuzzy logic with swarm intelligence could play an important role in the selection process.,Fuzzy Swarm Based Text Summarization,"The aim of automatic text summarization systems is to select the most relevant information from an abundance of text sources. A daily rapid growth of data on the internet makes the achieve events of such aim a big challenge. In this study, we incorporated fuzzy logic with swarm intelligence; so that risks, uncertainty, ambiguity and imprecise values of choosing the features weights (scores) could be flexibly tolerated. The weights obtained from the swarm experiment were used to adjust the text features scores and then the features scores were used as inputs for the fuzzy inference system to produce the final sentence score. The sentences were ranked in descending order based on their scores and then the top n sentences were selected as final summary. The experiments showed that the incorporation of fuzzy logic with swarm intelligence could play an important role in the selection process of the most important sentences to be included in the final summary. Also the results showed that the proposed method got a good performance outperforming the swarm model and the benchmark methods. Incorporating more than one technique for dealing with the sentence scoring proved to be an effective mechanism. The PSO was employed for producing the text features weights. The purpose of this process was to emphasize on dealing with the text features fairly based on their importance and to differentiate between more and less important features. The fuzzy inference system was employed to determine the final sentence score, on which the decision was made to include the sentence in the summary or not.", 
"In this paper, we introduce a different hybrid model for automatic text summarization problem. We use diversity-based method to filter similar sentences and select the most diverse ones. We also use fuzzy logic to make the risks, uncertainty, ambiguity and imprecise values of the text features weights flexibly tolerated.",Fuzzy swarm diversity hybrid model for text summarization,"High quality summary is the target and challenge for any automatic text summarization. In this paper, we introduce a different hybrid model for automatic text summarization problem. We exploit strengths of different techniques in building our model: we use diversity-based method to filter similar sentences and select the most diverse ones, differentiate between the more important and less important features using the swarm-based method and use fuzzy logic to make the risks, uncertainty, ambiguity and imprecise values of the text features weights flexibly tolerated. The diversity-based method focuses to reduce redundancy problems and the other two techniques concentrate on the scoring mechanism of the sentences. We presented the proposed model in two forms. In the first form of the model, diversity measures dominate the behavior of the model. In the second form, the diversity constraint is no longer imposed on the model behavior. That means the diversity-based method works same as fuzzy swarm-based method. The results showed that the proposed model in the second form performs better than the first form, the swarm model, the fuzzy swarm method and the benchmark methods. Over results show that combination of diversity measures, swarm techniques and fuzzy logic can generate good summary containing the most important parts in the document.", 
This method uses senses rather than raw words to lessen the problem that sentences of the same or similar semantic meaning but written in synonyms are treated differently. Semantic clustering is used to avoid selecting redundant key sentences.,Fuzzy-Rough Set Aided Sentence Extraction Summarization,"In this paper, a novel method is proposed to extract key sentences of a document as its summary by estimating the relevance of sentences through the use of fuzzy-rough sets. This method uses senses rather than raw words to lessen the problem that sentences of the same or similar semantic meaning but written in synonyms are treated differently. Also included is semantic clustering, used to avoid selecting redundant key sentences. A prototype of this automatic text summarization scheme is constructed and an intrinsic method with criteria widely used in information-retrieval systems is employed for measuring the summary quality. The results of applying the prototype to datasets with manually-generated summaries are shown.", 
"This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. The approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword, sentence centrality and resemblance to the title. The results of the proposed approach are promising, especially the GMM approach.","GA, MR, FFNN, PNN and GMM based models for automatic text summarization","This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. This approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword, sentence centrality, sentence resemblance to the title, sentence inclusion of name entity, sentence inclusion of numerical data, sentence relative length, Bushy path of the sentence and aggregated similarity for each sentence to generate summaries. First, we investigate the effect of each sentence feature on the summarization task. Then we use all features in combination to train genetic algorithm (GA) and mathematical regression (MR) models to obtain a suitable combination of feature weights. Moreover, we use all feature parameters to train feed forward neural network (FFNN), probabilistic neural network (PNN) and Gaussian mixture model (GMM) in order to construct a text summarizer for each model. Furthermore, we use trained models by one language to test summarization performance in the other language. The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles. The results of the proposed approach are promising, especially the GMM approach.", 
Product reviews possess critical information regarding customers' concerns and their experience with the product. Previous studies of deriving useful information from customer reviews focused mainly on numerical and categorical data. Existing methods of opinion mining in processing customer reviews concentrates on counting positive and negative comments of review writers.,Gather customer concerns from online product reviews– A text summarization approach,"Product reviews possess critical information regarding customers’ concerns and their experience with the product. Such information is considered essential to firms’ business intelligence which can be utilized for the purpose of conceptual design, personalization, product recommendation, better customer understanding, and finally attract more loyal customers. Previous studies of deriving useful information from customer reviews focused mainly on numerical and categorical data. Textual data have been somewhat ignored although they are deemed valuable. Existing methods of opinion mining in processing customer reviews concentrates on counting positive and negative comments of review writers, which is not enough to cover all important topics and concerns across different review articles. Instead, we propose an automatic summarization approach based on the analysis of review articles’ internal topic structure to assemble customer concerns. Different from the existing summarization approaches centered on sentence ranking and clustering, our approach discovers and extracts salient topics from a set of online reviews and further ranks these topics. The final summary is then generated based on the ranked topics. The experimental study and evaluation show that the proposed approach outperforms the peer approaches, i.e. opinion mining and clustering-summarization, in terms of users’ responsiveness and its ability to discover the most important topics.", 
"In a graph, a generalized minimum dominating set is a set of vertices and weighted edges. We treat the generalized MDS problem in the present paper by a replica-symmetric spin glass theory. We carry out a preliminary test of the statistical physics-inspired method to this automatic text summarization problem.",Generalized minimum dominating set and application in automatic text summarization,"For a graph formed by vertices and weighted edges, a generalized minimum dominating set (MDS) is a vertex set of smallest cardinality such that the summed weight of edges from each outside vertex to vertices in this set is equal to or larger than certain threshold value. This generalized MDS problem reduces to the conventional MDS problem in the limiting case of all the edge weights being equal to the threshold value. We treat the generalized MDS problem in the present paper by a replica-symmetric spin glass theory and derive a set of belief-propagation equations. As a practical application we consider the problem of extracting a set of sentences that best summarize a given input text document. We carry out a preliminary test of the statistical physics-inspired method to this automatic text summarization problem.", 
Understanding gene regulations is fundamental to biomedical research. summarizing all the existing knowledge about a gene based on literature is highly desirable to help biologists digest the literature. We propose a two-stage approach to generate such a summary for a given gene – first retrieving articles and then extracting sentences for each specified semantic aspect. We evaluate the proposed methods using a test set with 20 genes.,"Generating gene summaries from biomedical literature, A study of semi-structured summarization","Most knowledge accumulated through scientific discoveries in genomics and related biomedical disciplines is buried in the vast amount of biomedical literature. Since understanding gene regulations is fundamental to biomedical research, summarizing all the existing knowledge about a gene based on literature is highly desirable to help biologists digest the literature. In this paper, we present a study of methods for automatically generating gene summaries from biomedical literature. Unlike most existing work on automatic text summarization, in which the generated summary is often a list of extracted sentences, we propose to generate a semi-structured summary which consists of sentences covering specific semantic aspects of a gene. Such a semi-structured summary is more appropriate for describing genes and poses special challenges for automatic text summarization. We propose a two-stage approach to generate such a summary for a given gene – first retrieving articles about a gene and then extracting sentences for each specified semantic aspect. We address the issue of gene name variation in the first stage and propose several different methods for sentence extraction in the second stage. We evaluate the proposed methods using a test set with 20 genes. Experiment results show that the proposed methods can generate useful semi-structured gene summaries automatically from biomedical literature, and our proposed methods outperform general purpose summarization methods. Among all the proposed methods for sentence extraction, a probabilistic language modeling approach that models gene context performs the best.", 
Word Embeddings are low-dimensional distributed representations that encompass a set of language modeling and feature learning techniques from Natural Language Processing. Extreme Learning Machine (ELM) proposed using an ELM for generating word embeddings. ELM-based Word Embedding slightly outperforms state-of-the-art methods: Word2Vec and GloVe models.,Generating Word Embeddings from an Extreme Learning Machine for Sentiment Analysis and Sequence Labeling Tasks,"Word Embeddings are low-dimensional distributed representations that encompass a set of language modeling and feature learning techniques from Natural Language Processing (NLP). Words or phrases from the vocabulary are mapped to vectors of real numbers in a low-dimensional space. In previous work, we proposed using an Extreme Learning Machine (ELM) for generating word embeddings. In this research, we apply the ELM-based Word Embeddings to the NLP task of Text Categorization, specifically Sentiment Analysis and Sequence Labeling. The ELM-based Word Embeddings utilizes a count-based approach similar to the Global Vectors (GloVe) model, where the word-context matrix is computed then matrix factorization is applied. A comparative study is done with Word2Vec and GloVe, which are the two popular state-of-the-art models. The results show that ELM-based Word Embeddings slightly outperforms the aforementioned two methods in the Sentiment Analysis and Sequence Labeling tasks. In addition, only one hyperparameter is needed using ELM whereas several are utilized for the other methods. ELM-based Word Embeddings are comparable to the state-of-the-art methods: Word2Vec and GloVe models. In addition, the count-based ELM model have word similarities to both the count-based GloVe and the predict-based Word2Vec models, with subtle differences.", 
"In this paper, we propose an adversarial process for abstractive text summarization. We simultaneously train a generative model G and a discriminative model D. We show that our model is able to generate more abstractive, readable and diverse summaries.",Generative Adversarial Network for Abstractive Text Summarization,"In this paper, we propose an adversarial process for abstractive text summarization, in which we simultaneously train a generative model G and a discriminative model D. In particular, we build the generator G as an agent of reinforcement learning, which takes the raw text as input and predicts the abstractive summarization. We also build a discriminator which attempts to distinguish the generated summary from the ground truth summary. Extensive experiments demonstrate that our model achieves competitive ROUGE scores with the state-of-the-art methods on CNN/Daily Mail dataset. Qualitatively, we show that our model is able to generate more abstractive, readable and diverse summaries.", 
In this paper we present an approach for applying generative adversarial networks in abstractive text summarization tasks with a novel time-decay attention mechanism. The data generator is modeled as a stochastic policy in reinforcement learning. The discriminator aims to estimate the probability that a summary came from the training data.,Generative Adversarial Network with Policy Gradient for Text Summarization,"Abstractive text summarization is the task of generating meaningful summary from a given document (short or long). This is a very challenging task for longer documents, since they suffer from repetitions (redundancy) when the given document is long and the generated summary should contain multi-sentences. In this paper we present an approach for applying generative adversarial networks in abstractive text summarization tasks with a novel time-decay attention mechanism. The data generator is modeled as a stochastic policy in reinforcement learning. The generator’s goal is to generate summaries which are difficult to be discriminated from real summaries. The discriminator aims to estimate the probability that a summary came from the training data rather than the generator to guide the training of the generative model. This framework corresponds to a minimax two-player game. Qualitatively and quantitatively experimental results (human evaluations and ROUGE scores) show that our model can generate more relevant, less repetitive, grammatically correct, preferable by humans and is promising in solving the abstractive text summarization task.", 
"Sentence scores are calculated using their surface-level features, and summaries are created by extracting the highest ranked sentences. We use features such as term frequency, key phrase (KP), centrality, title similarity and sentence position. This paper presents one of the first Turkish summarization systems, and its results are promising.",Generic Text Summarization for Turkish,"In this paper, we propose a generic text summarization method that generates summaries of Turkish texts by ranking sentences according to their scores. Sentence scores are calculated using their surface-level features, and summaries are created by extracting the highest ranked sentences from the original documents. To extract sentences which form a summary with an extensive coverage of the main content of the text and less redundancy, we use features such as term frequency, key phrase (KP), centrality, title similarity and sentence position. The sentence rank is computed using a score function that uses its feature values and the weights of the features. The best feature weights are learned using machine-learning techniques with the help of human-constructed summaries. Performance evaluation is conducted by comparing summarization outputs with manual summaries of two newly created Turkish data sets. This paper presents one of the first Turkish summarization systems, and its results are promising. We introduce the usage of KP as a surface-level feature in text summarization, and we show the effectiveness of the centrality feature in text summarization. The effectiveness of the features in Turkish text summarization is also analyzed in detail.", 
"Sentence scores are calculated using their surface-level features, and summaries are created by extracting the highest ranked sentences. We use features such as term frequency, key phrase (KP), centrality, title similarity and sentence position. This paper presents one of the first Turkish summarization systems, and its results are promising.",Generic text summarization using probabilistic latent semantic indexing,"This paper presents a strategy to generate generic summary of documents using Probabilistic Latent Semantic Indexing. Generally a document contains several topics rather than a single one. Summaries created by human beings tend to cover several topics to give the readers an overall idea about the original document. Hence we can expect that a summary containing sentences from better part of the topic spectrum should make a better summary. PLSI has proven to be an effective method in topic detection. In this paper we present a method for creating extractive summary of the document by using PLSI to analyze the features of document such as term frequency and graph structure. We also show our results, which was evaluated using ROUGE, and compare the results with other techniques, proposed in the past.",  
Automatic text summarization has become a relevant topic due to the information overload. This automatization aims to help humans and machines to deal with the vast amount of text data offered on the web.,Genetic Clustering Algorithm for Extractive Text Summarization,"Automatic text summarization has become a relevant topic due to the information overload. This automatization aims to help humans and machines to deal with the vast amount of text data (structured and un-structured) offered on the web and deep web. In this paper a novel approach for automatic extractive text summarization called SENCLUS is presented. Using a genetic clustering algorithm, SENCLUS clusters the sentences as close representation of the text topics using a fitness function based on redundancy and coverage, and applies a scoring function to select the most relevant sentences of each topic to be part of the extractive summary. The approach was validated using the DUC2002 data set and ROUGE summary quality measures. The results shows that the approach is representative against the state of the art methods for extractive automatic text summarization.", 
"Existing graph based summarization methods treat sentence as bag of words, rely on content similarity measure and did not consider semantic relationships between sentences. This paper introduces a genetic semantic graph based approach for multi-document abstractive summarization. Experimental results reveal that the proposed approach performs better than other summarization systems.",Genetic semantic graph approach for multi-document abstractive summarization,"The aim of automatic multi-document abstractive summarization is to create a compressed version of the source text and preserves the salient information. Existing graph based summarization methods treat sentence as bag of words, rely on content similarity measure and did not consider semantic relationships between sentences. These methods may fail in determining redundant sentences that are semantically equivalent. This paper introduces a genetic semantic graph based approach for multi-document abstractive summarization. Semantic graph from the document set is constructed in such a way that the graph nodes represent the predicate argument structures (PASs), extracted automatically by employing semantic role labeling (SRL); and the edges of graph correspond to semantic similarity weight determined from PAS-to-PAS semantic similarity, and PAS-to-document set relationship. The PAS-to-document set relationship is represented by different features, weighted and optimized by genetic algorithm. The salient graph nodes (PASs) are ranked based on modified graph based ranking algorithm. In order to reduce redundancy, we utilize maximal marginal relevance (MMR) to re-ranks the PASs and use language generation to generate summary sentences from the top ranked PASs. Experiment of this study is carried out using DUC-2002, a standard corpus for text summarization. Experimental results reveal that the proposed approach performs better than other summarization systems.", 
Video streaming is one of the challenging issues in vehicular ad-hoc networks (VANETs) The selection process of the next relay vehicle is based on a correlated formula of QoE and quality of service (QoS) factors. The proposed GeoQoE-Vanet outperforms both GPSR and GPSR-2P protocols.,GeoQoE-Vanet QoE-Aware Geographic Routing Protocol for Video Streaming over Vehicular Ad-hoc Networks,"Video streaming is one of the challenging issues in vehicular ad-hoc networks (VANETs) due to their highly dynamic topology and frequent connectivity disruptions. Recent developments in the routing protocol methods used in VANETs have contributed to improvements in the quality of experience (QoE) of the received video. One of these methods is the selection of the next-hop relay vehicle. In this paper, a QoE-aware geographic protocol for video streaming over VANETs is proposed. The selection process of the next relay vehicle is based on a correlated formula of QoE and quality of service (QoS) factors to enhance the users’ QoE. The simulation results show that the proposed GeoQoE-Vanet outperforms both GPSR and GPSR-2P protocols in providing the best end-user QoE of video streaming service.", 
"Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization. These models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. We use a hybrid pointer-generator network that can copy words from the source text via pointing.","Get To The Point, Summarization with Pointer-Generator Networks","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", 
"In neural abstractive summarization, the conventional sequence-to-sequence model often suffers from repetition and semantic irrelevance. We propose a global encoding framework that controls the information flow from the encoder to the decoder based on the global information of the source context.",Global Encoding for Abstractive Summarization,"In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition.", 
We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores. The experimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries.,Global optimization under length constraint for neural text summarization,"We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70% overlength summaries on CNN/Daily and 7.8% on long summary of Mainichi, compared to the approximately 20% to 50% on CNN/Daily Mail and 10% to 30% on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The experimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries.", 
VANETs are becoming increasingly important for emerging cooperative intelligent transport systems. Global navigation satellite system (GNSS) is a proven technology to provide precise timing information in many distributed systems. The availability of GNSS time synchronization is characterized by almost 100% in experiments in high-rise urban streets.,GNSS Time Synchronization in Vehicular Ad-Hoc Networks Benefits and Feasibility,"Time synchronization is critical for the operation of distributed systems in networked environments. It is also demanded in vehicular ad-hoc networks (VANETs), which, as a special type of wireless networks, are becoming increasingly important for emerging cooperative intelligent transport systems. Global navigation satellite system (GNSS) is a proven technology to provide precise timing information in many distributed systems. It is well recognized to be the primary means for vehicle positioning and velocity determination in VANETs. However, GNSS-based time synchronization is not well understood for its role in the coordination of various tasks in VANETs. To address this issue, this paper examines the requirements, potential benefits, and feasibility of GNSS time synchronization in VANETs. The availability of GNSS time synchronization is characterized by almost 100% in our experiments in high-rise urban streets, where the availability of GNSS positioning solutions is only 80%. Experiments are also conducted to test the accuracy of time synchronization with 1-PPS signals output from consumer-grade GNSS receivers. They have shown 30-ns synchronization accuracy between two receivers of different models. All these experimental results demonstrate the feasibility of GNSS time synchronization for stringent VANET applications.",  
"A deep convolutional neural network architecture achieves new state of the art for classification and detection, say researchers. The main hallmark is the improved utilization of the computing resources inside the network.",Going Deeper with Convolutions,"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", 
"GoWvis represents any piece of text inputted by the user as a graph-of-words. It leverages graph degeneracy and community detection to generate an extractive summary. The entire analysis can be fully customized via the tuning of many text preprocessing, graph building.","Gowvis, a web application for graph-of-words-based text visualization and summarization","We introduce GoWvis, an interactive web application that represents any piece of text inputted by the user as a graph-of-words and leverages graph degeneracy and community detection to generate an extractive summary (keyphrases and sentences) of the inputted text in an unsupervised fashion. The entire analysis can be fully customized via the tuning of many text preprocessing, graph building, and graph mining parameters. Our system is thus well suited to educational purposes, exploration and early research experiments. The new summarization strategy we propose also shows promise.", 
"GPT-3 is an AI engine that generates text in response to a prompt given to it by a human user. It does not understand the language that it produces, at least not as philosophers understand such things. And yet its output is in many cases astonishingly like human language.",GPT-3 Waterloo or Rubicon Here be Dragons,"GPT-3 is an AI engine that generates text in response to a prompt given to it by a human user. It does not understand the language that it produces, at least not as philosophers understand such things. And yet its output is in many cases astonishingly like human language. How is this possible? Think of the mind as a high-dimensional space of signifieds, that is, meaning-bearing elements. Correlatively, text consists of one-dimensional strings of signifiers, that is, linguistic forms. GPT-3 creates a language model by examining the distances and ordering of signifiers in a collection of text strings and computes over them so as to reverse engineer the trajectories texts take through that space. Peter Gärdenfors’ semantic geometry provides a way of thinking about the dimensionality of mental space and the multiplicity of phenomena in the world, about how mind mirrors the world. Yet artificial systems are limited by the fact that they do not have a sensorimotor system that has evolved over millions of years. They do have inherent limits.", 
One of the most pressing problems for forensic investigators is the huge amount of data to analyze per case. In this paper we propose to use file deduplication across devices as well as file whitelisting rigorously in investigations.,Gradually Improving the Forensic Process,"At the time of writing, one of the most pressing problems for forensic investigators is the huge amount of data to analyze per case. Not only the number of devices increases due to the advancing computerization of everydays life, but also the storage capacity of each and every device raises into multi-terabyte storage requirements per case for forensic working images. In this paper we improve the standardized forensic process by proposing to use file deduplication across devices as well as file whitelisting rigorously in investigations, to reduce the amount of data that needs to be stored for analysis as early as during data acquisition. These improvements happen in an automatic fashion and completely transparent to the forensic investigator. They furthermore be added without negative effects to the chain of custody or artefact validity in court, and are evaluated in a realistic use case.", 
"The system is found to perform well in terms of precision, recall and F-measure with various input documents. Based on the frequency of words occurrence in the input document, the sentences are ranked and the ranks are used to identify the important sentences. The relevance between each sentence in the document with other sentences is found using semantic similarity.",Graph Based Technique for Hindi Text Summarization,"Automatic Summarization is the process of generating or extracting the important sentences from the given input document. Since there are many such systems for English language so this proposed system is mainly focused on the Hindi language. The basic idea of this summarization system is to identify the important sentences and also to extract them based on its relevance with other sentences. In case of summarization the sentences in the summarized document should be meaningful and relevant to each other, which are achieved using sentential semantic analysis. For finding the relation between each sentence and also to analyze for the importance, the Graph based approach is found to be more appropriate. Based on the frequency of words occurrence in the input document, the sentences are ranked and the ranks are used to identify the important sentences in the document. The relevance between each sentence in the document with other sentences is found using semantic similarity. There may be same information conveyed by two different sentences whose semantic similarity score is very high. Such kind of sentences has to be kept only once in the output. For which an analysis has been performed over various semantically similar sentences. Finally, the identified relevant sentences are merged using the rank and the semantic analysis of the sentences. These identified sentences are rearranged to provide a proper meaningful summarized text to avoid textual continuity in the output text. The system is found to perform well in terms of precision, recall and F-measure with various input documents.", 
"The main idea is to rank Maximal Frequent Sequences (MFS) in order to identify the most important information in a text. MFS are considered as nodes of a graph in term selection step, and then ranked in term weighting step using a graph-based algorithm. The longer is MFS, the better are the results.",Graph Ranking on Maximal Frequent Sequences for Single Extractive Text Summarization,"We suggest a new method for the task of extractive text summarization using graph-based ranking algorithms. The main idea of this paper is to rank Maximal Frequent Sequences (MFS) in order to identify the most important information in a text. MFS are considered as nodes of a graph in term selection step, and then are ranked in term weighting step using a graph-based algorithm. We show that the proposed method produces results superior to the-state-of-the-art methods; in addition, the best sentences were found with this method. We prove that MFS are better than other terms. Moreover, we show that the longer is MFS, the better are the results. If the stop-words are excluded, we lose the sense of MFS, and the results are worse. Other important aspect of this method is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, and languages.", 
Text Summarization is an emerging technique for understanding the main purpose of any kind of documents. This paper presents innovative unsupervised methods for automatic sentence extraction using graph-based ranking algorithms.,Graph-Based Algorithms for Text Summarization,"Summarization is a brief and accurate representation of input text such that the output covers the most important concepts of the source in a condensed manner. Text Summarization is an emerging technique for understanding the main purpose of any kind of documents. To visualize a large text document within a short duration and small visible area like PDA screen, summarization provides a greater flexibility and convenience. This paper presents innovative unsupervised methods for automatic sentence extraction using graph-based ranking algorithms and shortest path algorithm.", 
"A new unsupervised approach for Automatic Text Summarization (ATS) employing Fuzzy Logic (FL) to optimize summary generation is proposed. The approach is extractive, graph-based and is applicable to both single and multiple documents. Using the DUC 2004 dataset, our experimental results demonstrate that GFLES is able to generate comprehensive text summaries.",Graph-Based Fuzzy Logic for Extractive Text Summarization (GFLES),"High accuracy models for automatic text summarization are increasingly required due to the overwhelming growth of available text on the internet. In this paper, a new unsupervised approach for Automatic Text Summarization (ATS) employing Fuzzy Logic (FL) to optimize summary generation is proposed. The approach is extractive, graph-based and is applicable for both single and multiple documents. Our Graph-based Fuzzy Logic Extractive Text Summarization (GFLES) approach consists of five main stages: text pre-processing, text representation using graph model, features extraction during graph construction, sentence clustering using our previously developed Graph-based Growing Self-Organizing Map (G-GSOM) and lastly, sentences are ranked using FL and a summary is generated. GFLES is distinctive from other approaches by its four advantages: 1) employment of the graph model for features extraction, 2) a better substitute of Vector Space Model (VSM), 3) appreciating the importance of sub-topics of text by taking them into account before generating the summary by clustering sentences and 4) applicable to both single and multiple documents. Moreover, the use of FL on each cluster at the same time provides a foundation for further development of GFLES to be applied on big data. Using the DUC 2004 dataset, our experimental results demonstrate that GFLES is able to generate comprehensive text summaries. These results establish the potential of GFLES as a high accuracy model for automatic text summarization.", 
"Both approaches are based on the graph-based syntactic representation of text and web documents. In the supervised approach, we train classification algorithms on a summarized collection of documents. The supervised classification provides the highest keyword identification accuracy, while the highest F-measure is reached with a simple degree-based ranking.",Graph-Based Keyword Extraction for Single-Document Summarization,"In this paper, we introduce and compare between two novel approaches, supervised and unsupervised, for identifying the keywords to be used in extractive summarization of text documents. Both our approaches are based on the graph-based syntactic representation of text and web documents, which enhances the traditional vector-space model by taking into account some structural document features. In the supervised approach, we train classification algorithms on a summarized collection of documents with the purpose of inducing a keyword identification model. In the unsupervised approach, we run the HITS algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords. Our experiments on a collection of benchmark summaries show that given a set of summarized training documents, the supervised classification provides the highest keyword identification accuracy, while the highest F-measure is reached with a simple degree-based ranking. In addition, it is sufficient to perform only the first iteration of HITS rather than running it to its convergence.", 
"In this project, I propose a timestamped graph (TSG) model that is motivated by human writing and reading processes. I show how text units in this model emerge over time. In this model, the graphs used by LexRank and TextRank are specific instances of the timestamps graph with particular parameters.",Graph-Based Methods for Automatic Text Summarization,"With the rapid growth of World Wide Web, a huge amount of information is available and accessible online. People do not have time to read everything, and yet they have to make critical decisions based on whatever information is available. Automatic text summarization is a technique to assist people to digest the vast amount of information online. Recently, a number of graph-based approaches, such as LexRank and TextRank, has been suggested for text summarization. These approaches assume a static graph which does not model how the input texts emerge. A suitable evolutionary text graph model may impart a better understanding of the texts and improve the summarization process. In this project, I propose a timestamped graph (TSG) model that is motivated by human writing and reading processes, and show how text units in this model emerge over time. In this model, the graphs used by LexRank and TextRank are specific instances of the timestamped graph with particular parameter settings. I apply timestamped graphs on the standard DUC multi-document text summarization task and achieve comparable results to the state of the art.", 
"We propose a neural multi-document summarization (MDS) system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input features. We then use a greedy heuristic to extract salient sentences while avoiding redundancy.",Graph-based Neural Multi-Document Summarization,"We propose a neural multi-document summarization (MDS) system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences while avoiding redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.", 
The method has been developed using modified TextRank computed based on the concept of PageRank. Modified inverse sentence frequency-cosine similarity is used to give different weightage to different words in the sentence. The performance evaluation of proposed summarization technique shows the effectiveness of the method.,Graph-Based Text Summarization Using Modified TextRank,"Nowadays, the efficient access of enormous amounts of information has become more difficult due to the rapid growth of the Internet. To manage the vast information, we need efficient and effective methods and tools. In this paper, a graph-based text summarization method has been described which captures the aboutness of a text document. The method has been developed using modified TextRank computed based on the concept of PageRank defined for each page in the Web pages. The proposed method constructs a graph with sentences as the nodes and similarity between two sentences as the weight of the edge between them. Modified inverse sentence frequency-cosine similarity is used to give different weightage to different words in the sentence, whereas traditional cosine similarity treats the words equally. The graph is made sparse and partitioned into different clusters with the assumption that the sentences within a cluster are similar to each other and sentences of different cluster represent their dissimilarity. The performance evaluation of proposed summarization technique shows the effectiveness of the method.",  
"Ground-based cloud image categorization is a critical but challenging task that has not been well addressed. In the recent years, the deep Convolutional Neural Network has achieved the promising results in lots of computer vision and image understanding fields. We propose ""DeepCloud"" as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features.",Ground-based Cloud Image Categorization using Deep Convolutional Features,"Accurate ground-based cloud image categorization is a critical but challenging task that has not been well addressed. One of the essential issues that affect the performance is to extract the representative visual features. Nearly all of the existing methods rely on the hand-crafted descriptors (e.g., LBP, CENTRIST and SIFT). Their limited discriminative power indeed leads to the unsatisfactory performance. To alleviate this, we propose “DeepCloud” as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features. In the recent years, the deep Convolutional Neural Network (CNN) has achieved the promising results in lots of computer vision and image understanding fields. Nevertheless, it has not been applied to cloud image classification yet. Thus, we actually pay the first effort to fill this blank. Since cloud image classification can be attributed to a multi-instance learning problem, simply employing the convolutional features within CNN cannot achieve the promising result. To address this, Fisher Vector (FV) encoding is applied to executing the spatial feature aggregation and high-dimensional feature mapping on the raw deep convolutional features. Moreover, the hierarchical convolutional layers are used simultaneously to capture the fine textural characteristics and high-level semantic information in the unified manner. To further leverage the performance, a cloud pattern mining and selection method is also proposed. It targets at finding the discriminative local patterns to better distinguish the different kinds of clouds. The experiments on a challenging ground-based cloud image dataset demonstrate the superiority of our proposition over the state-of-the-art methods.", 
Expertise in text summarization can be lacking when source text and summary are long and based on logical reasoning. This paper proposes a framework for group-based text. summarization that clusters semantically related sentences into groups based on. Semantic Link Network (SLN) and then ranks the groups and concatenates the top-ranked ones into a summary.,Grouping sentences as better language unit for extractive text summarization,"Most existing methods for extractive text summarization aim to extract important sentences with statistical or linguistic techniques and concatenate these sentences as a summary. However, the extracted sentences are usually incoherent. The problem becomes worse when the source text and the summary are long and based on logical reasoning. The motivation of this paper is to answer the following two related questions: What is the best language unit for constructing a summary that is coherent and understandable? How is the extractive summarization process based on the language unit? Extracting larger language units such as a group of sentences or a paragraph is a natural way to improve the readability of summary as it is rational to assume that the original sentences within a larger language unit are coherent. This paper proposes a framework for group-based text summarization that clusters semantically related sentences into groups based on Semantic Link Network (SLN) and then ranks the groups and concatenates the top-ranked ones into a summary. A two-layer SLN model is used to generate and rank groups with semantic links including the is-part-of link, sequential link, similar-to link, and cause–effect link. The experimental results show that summaries composed by group or paragraph tend to contain more key words or phrases than summaries composed by sentences and summaries composed by groups contain more key words or phrases than those composed by paragraphs especially when the average length of source texts is from 7000 words to 17,000 words which is the usual length of scientific papers. Further, we compare seven clustering algorithms for generating groups and propose five strategies for generating groups with the four types of semantic links.", 
"Neural network models, based on the attentional encoder-decoder model, have good capability in abstractive text summarization. These models are hard to be controlled in the process of generation, which leads to a lack of key information. We propose a guiding generation model that combines the extractive method and the abstractive method.",Guiding generation for abstractive text summarization based on key information guide network,"Neural network models, based on the attentional encoder-decoder model, have good capability in abstractive text summarization. However, these models are hard to be controlled in the process of generation, which leads to a lack of key information. We propose a guiding generation model that combines the extractive method and the abstractive method. Firstly, we obtain keywords from the text by a extractive model. Then, we introduce a Key Information Guide Network (KIGN), which encodes the keywords to the key information representation, to guide the process of generation. In addition, we use a prediction-guide mechanism, which can obtain the long-term value for future decoding, to further guide the summary generation. We evaluate our model on the CNN/Daily Mail dataset. The experimental results show that our model leads to significant improvements.", 
"Deep Neural Networks (DNNs) often require millions of expensive floating-point operations for each input classification. This overhead limits the applicability of DNNs to low-power, embedded platforms and incurs high cost in data centers. We propose a novel approach to map floating-points to 8-bit dynamic fixed-point networks. We show our method achieves significant power and energy savings while increasing the classification accuracy.","Hardware-Software Codesign of Accurate, Multiplier-free Deep Neural Networks","While Deep Neural Networks (DNNs) push the state-of-the-art in many machine learning applications, they often require millions of expensive floating-point operations for each input classification. This computation overhead limits the applicability of DNNs to low-power, embedded platforms and incurs high cost in data centers. This motivates recent interests in designing low-power, low-latency DNNs based on fixed-point, ternary, or even binary data precision. While recent works in this area offer promising results, they often lead to large accuracy drops when compared to the floating-point networks. We propose a novel approach to map floating-point based DNNs to 8-bit dynamic fixed-point networks with integer power-of-two weights with no change in network architecture. Our dynamic fixed-point DNNs allow different radix points between layers. During inference, power-of-two weights allow multiplications to be replaced with arithmetic shifts, while the 8-bit fixed-point representation simplifies both the buffer and adder design. In addition, we propose a hardware accelerator design to achieve low-power, low-latency inference with insignificant degradation in accuracy. Using our custom accelerator design with the CIFAR-10 and ImageNet datasets, we show that our method achieves significant power and energy savings while increasing the classification accuracy.",  
5G mobile communication technology will greatly promote the development of vehicular ad hoc networks (VANETs) A hybrid D2D message authentication (HDMA) scheme is proposed for 5G-enabled VANets. Security analysis shows that HDMA is robust to resist various security attacks.,HDMA Hybrid D2D Message Authentication Scheme for 5G-Enabled VANETs,"The fifth-generation (5G) mobile communication technology with higher capacity and data rate, ultra-low device to device (D2D) latency, and massive device connectivity will greatly promote the development of vehicular ad hoc networks (VANETs). Meantime, new challenges such as security, privacy and efficiency are raised. In this article, a hybrid D2D message authentication (HDMA) scheme is proposed for 5G-enabled VANETs, in which a novel group signature-based algorithm is used for mutual authentication between vehicle to vehicle (V2V) communication. In addition, a pre-computed lookup table is adopted to reduce the computation overhead of modular exponentiation operation. Security analysis shows that HDMA is robust to resist various security attacks, and performance analysis also points out that, the authentication overhead of HDMA is more efficient than some traditional schemes with the help of the pre-computed lookup table in V2V and vehicle to infrastructure (V2I) communication.", 
Heart disease is one of the major causes of mortality worldwide. Early heart disease diagnosis can prevent deaths caused by late diagnosis. This study proposes an effective heart disease prediction model for a clinical decision support system (CDSS) The proposed model outperformed other models and previous study results by achieving accuracy of 95.90% and 98.40% for Statlog and Cleveland datasets.,HDPM An Effective Heart Disease Prediction Model for a Clinical Decision Support System,"Heart disease, one of the major causes of mortality worldwide, can be mitigated by early heart disease diagnosis. A clinical decision support system (CDSS) can be used to diagnose the subjects’ heart disease status earlier. This study proposes an effective heart disease prediction model (HDPM) for a CDSS which consists of Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to detect and eliminate the outliers, a hybrid Synthetic Minority Over-sampling Technique-Edited Nearest Neighbor (SMOTE-ENN) to balance the training data distribution and XGBoost to predict heart disease. Two publicly available datasets (Statlog and Cleveland) were used to build the model and compare the results with those of other models (naive bayes (NB), logistic regression (LR), multilayer perceptron (MLP), support vector machine (SVM), decision tree (DT), and random forest (RF)) and of previous study results. The results revealed that the proposed model outperformed other models and previous study results by achieving accuracies of 95.90% and 98.40% for Statlog and Cleveland datasets, respectively. In addition, we designed and developed the prototype of the Heart Disease CDSS (HDCDSS) to help doctors/clinicians diagnose the patients’/subjects’ heart disease status based on their current condition. Therefore, early treatment could be conducted to prevent the deaths caused by late heart disease diagnosis.", 
Heart disease diagnosis depends on complex combination of clinical and pathological data. This paper develops a system that can assist medical professionals in predicting heart disease status. The accuracy of prediction is near 80% and the system has a user-friendly design. The HDPS system developed in this study is a novel approach that can be used in the classification of heart disease.,HDPS Heart Disease Prediction System,"The diagnosis of heart disease in most cases depends on a complex combination of clinical and pathological data. Because of this complexity, there exists a significant amount of interest among clinical professionals and researchers regarding the efficient and accurate prediction of heart disease. In this paper, we develop a heart disease predict system that can assist medical professionals in predicting heart disease status based on the clinical data of patients. Our approaches include three steps. Firstly, we select 13 important clinical features, i.e., age, sex, chest pain type, trestbps, cholesterol, fasting blood sugar, resting ecg, max heart rate, exercise induced angina, old peak, slope, number of vessels colored, and thal. Secondly, we develop an artificial neural network algorithm for classifying heart disease based on these clinical features. The accuracy of prediction is near 80%. Finally, we develop a user-friendly heart disease predict system (HDPS). The HDPS system will be consisted of multiple features, including input clinical data section, ROC curve display section, and prediction performance display section (execute time, accuracy, sensitivity, specificity, and predict result). Our approaches are effective in predicting the heart disease of a patient. The HDPS system developed in this study is a novel approach that can be used in the classification of heart disease.", 
"Using machine learning algorithms to measure blood pressure at a continuous rate is a feasible way to provide models and analysis for telemedicine monitoring data and predicting blood pressure. In conclusion, applying the GBDT is the best method for predicting the blood pressure of multiple individuals. With the inclusion of data such as age, body fat, ratio, and height, algorithm accuracy improves.",Health Data Driven on Continuous Blood Pressure Prediction Based on Gradient Boosting Decision Tree Algorithm,"Diseases related to issues with blood pressure are becoming a major threat to human health. With the development of telemedicine monitoring applications, a growing number of corresponding devices are being marketed, such as the use of remote monitoring for the purposes of increasing the autonomy of the elderly and thus encouraging a healthier and longer health span. Using machine learning algorithms to measure blood pressure at a continuous rate is a feasible way to provide models and analysis for telemedicine monitoring data and predicting blood pressure. For this paper, we applied the gradient boosting decision tree (GBDT) while predicting blood pressure rates based on the human physiological data collected by the EIMO device. EIMO equipment-specific signal acquisition includes ECG and PPG. In order to avoid over-fitting, the optimal parameters are selected via the cross-validation method. Consequently, our method has displayed a higher accuracy rate and better performance in calculating the mean absolute error evaluation index than methods, such as the traditional least squares method, ridge regression, lasso regression, ElasticNet, SVR, and KNN algorithm. When predicting the blood pressure of a single individual, calculating the systolic pressure displays an accuracy rate of above 70% and above 64% for calculating the diastolic pressure with GBDT, with the prediction time being less than 0.1 s. In conclusion, applying the GBDT is the best method for predicting the blood pressure of multiple individuals: with the inclusion of data such as age, body fat, ratio, and height, algorithm accuracy improves, which in turn indicates that the inclusion of new features aids prediction performance.", 
"EQ-5D index scores in Indonesian type 2 diabetes mellitus (T2DM) outpatients. 907 participants completed the five-level Indonesian version of the EQ-5d. Male participants had a higher index score compared to females, and the highest percentage of self-reported health problems was in the pain/ discomfort dimension. Housewives were found to experience more T2DM-related pain/discomfort and anxiety/depression.",Health-related quality of life in Indonesian type 2 diabetes mellitus outpatients measured with the Bahasa version of EQ-5D,"To present EuroQol-5D (EQ-5D) index scores in Indonesian type 2 diabetes mellitus (T2DM) outpatients and to investigate the associations between EQ-5D and socio-demographic characteristics and clinical condition. Socio-demographic data were collected by interviewing participants, clinical data were obtained from treating physicians and self-reporting. Participants originated from primary and secondary care facilities in the Java and Sulawesi regions. Ordinal regression analysis was conducted with the quintiles of the EQ-5D index scores as the dependent variable to investigate the multivariate association with the participants’ socio-demographic characteristics and clinical condition. 907 participants completed the five-level Indonesian version of the EQ-5D. The mean age of the participants was 59.3 (SD 9.7), and 57% were female. The overall EQ-5D index score was 0.77 (0.75–0.79). Male participants had a higher EQ-5D index score compared to females, and the highest percentage of self-reported health problems was in the pain/ discomfort dimension (61%). Factors identified as being significantly associated with lower EQ-5D index scores were: (i) treatment in secondary care, (ii) lower educational level, (iii) dependency on caregivers, (iv) not undergoing T2DM therapy, and (v) being a housewife. This study provides estimates of EQ-5D index scores that can be used in health economic evaluations. As housewives were found to experience more T2DM-related pain/discomfort and anxiety/depression, targeted approaches to reduce these problems should be aimed specifically at this group of patients. Potential approaches could involve disease-specific-counselors (health literacy partners) who provide routine monitoring of T2DM therapy as well as improved health promotion among T2DM communities.", 
The method of identifying infarcts makes use of convolutional neural networks. These have been trained with a specially prepared set of images that contain people simulating a heart attack. The promising results in the classification of infarcts show 91.75% accuracy and 92.85% sensitivity.,Heart Attack Detection in Colour Images Using Convolutional Neural Networks,"Cardiovascular diseases are the leading cause of death worldwide. Therefore, getting help in time makes the difference between life and death. In many cases, help is not obtained in time when a person is alone and suffers a heart attack. This is mainly due to the fact that pain prevents him/her from asking for help. This article presents a novel proposal to identify people with an apparent heart attack in colour images by detecting characteristic postures of heart attack. The method of identifying infarcts makes use of convolutional neural networks. These have been trained with a specially prepared set of images that contain people simulating a heart attack. The promising results in the classification of infarcts show 91.75% accuracy and 92.85% sensitivity.", 
"This paper is based on an improved three-dimensional U-net convolutional neural network deep learning algorithm for heart coronary artery segmentation for disease risk prediction. In the experiment, it found that simple data expansion may be detrimental to the test data. The model training effect of the centerline preprocessing is superior to the original data.",Heart Coronary Artery Segmentation and Disease Risk Warning Based on a Deep Learning Algorithm,"This paper is based on an improved three-dimensional U-net convolutional neural network deep learning algorithm for heart coronary artery segmentation for disease risk prediction, and it is practical with multiple data sets under two backgrounds without centerline and with the centerline. By using a new local feature to extract the ventricular information, and using the deep belief network to extract the features to regress the contour coordinates of the biventricular. Combining features and deep belief networks and training regression networks can not only extract high-level information but also accurately divide the left and right ventricles at a small computational cost. The performance of segmentation based on the dice coefficient compared between the two datasets. The results show that the model training effect of the centerline preprocessing is superior to the original data. The experimental results show that the best effect reaches the dice coefficient of 0.8291. In the experiment, it found that simple data expansion may be detrimental to the test data. From the training curve, it is believed that with the improvement of the quality of training data, the performance of coronary artery segmentation can be further improved, and it is of great significance to provide doctors and patients with more accurate and efficient opinions and suggestions in clinical practice to improve the quality of diagnosis and treatment. The purpose of assisting experts in real-time diagnosis and analysis achieved.", 
"In healthcare, data analysis can save lives by improving the medical diagnosis. Different data mining tools are available for researchers, and used to conduct studies and experiments. Matlab was the best performing tool, and Matlab's Artificial Neural Network model the best-performing technique.",Heart disease classification using data mining tools and machine learning techniques,"Nowadays, in healthcare industry, data analysis can save lives by improving the medical diagnosis. And with the huge development in software engineering, different data mining tools are available for researchers, and used to conduct studies and experiments. For this, we have decided to compare six common data mining tools: Orange, Weka, RapidMiner, Knime, Matlab, and Scikit-Learn, using six machine learning techniques: Logistic Regression, Support Vector Machine, K Nearest Neighbors, Artificial Neural Network, Naïve Bayes, and Random Forest by classifying heart disease. The dataset used in this study has 13 features, one target variable, and 303 instances in which 139 suffers from cardiovascular disease and 164 are healthy subjects. Three performance measures were used to compare the performance of the techniques in each tool: the accuracy, the sensitivity, and the specificity. The results showed that Matlab was the best performing tool, and Matlab’s Artificial Neural Network model was the best performing technique. We concluded this research by plotting the Receiver operating characteristic curve of Matlab and by giving several recommendations on which tool to choose taking into account the users experience in the field of data mining.", 
This paper analyzes the detection of heart disease using machine learning algorithms and python programming. Heart disease is a common and dangerous disease caused by fat containment. We have observed a dataset consists of 12 parameters and 70000 individual data values.,Heart Disease Detection Using Machine Learning,"This paper analyzes the detection of heart disease using machine learning algorithms and python programming. Over the post decades, heart disease is common and dangerous disease caused by fat containment. This disease occurs due to over pressure in the human body. Using different types of parameters in the dataset we can predict the cardiac-disease. We have observed a dataset consists of 12 parameters and 70000 individual data values to analyze the performance of patients. The main objective of the paper is to get a better accuracy to detect the heart-disease using algorithms in which the target output counts that a person having heart disease or not.", 
"Heart disease is one of the complex diseases and globally many people suffered from this disease. On time and efficient identification of heart disease plays a key role in healthcare, particularly in the field of cardiology. In this article, we propose an efficient and accurate system to diagnosis heart disease. The system is based on machine learning techniques.",Heart Disease Identification Method Using Machine Learning Classification in E-Healthcare,"Heart disease is one of the complex diseases and globally many people suffered from this disease. On time and efficient identification of heart disease plays a key role in healthcare, particularly in the field of cardiology. In this article, we proposed an efficient and accurate system to diagnosis heart disease and the system is based on machine learning techniques. The system is developed based on classification algorithms includes Support vector machine, Logistic regression, Artificial neural network, K-nearest neighbor, Naïve bays, and Decision tree while standard features selection algorithms have been used such as Relief, Minimal redundancy maximal relevance, Least absolute shrinkage selection operator and Local learning for removing irrelevant and redundant features. We also proposed novel fast conditional mutual information feature selection algorithm to solve feature selection problem. The features selection algorithms are used for features selection to increase the classification accuracy and reduce the execution time of classification system. Furthermore, the leave one subject out cross-validation method has been used for learning the best practices of model assessment and for hyperparameter tuning. The performance measuring metrics are used for assessment of the performances of the classifiers. The performances of the classifiers have been checked on the selected features as selected by features selection algorithms. The experimental results show that the proposed feature selection algorithm (FCMIM) is feasible with classifier support vector machine for designing a high-level intelligent system to identify heart disease. The suggested diagnosis system (FCMIM-SVM) achieved good accuracy as compared to previously proposed methods. Additionally, the proposed system can easily be implemented in healthcare for the identification of heart disease.", 
"Heart disease is one of the complex diseases and globally many people suffered from this disease. On time and efficient identification of heart disease plays a key role in healthcare, particularly in the field of cardiology. In this article, we propose an efficient and accurate system to diagnosis heart disease. The system is based on machine learning techniques.",Heart Disease Prediction and Classification Using Machine Learning Algorithms Optimized by Particle Swarm Optimization and Ant Colony Optimization,"The prediction of heart disease is one of the areas where machine learning can be implemented. Optimization algorithms have the advantage of dealing with complex non-linear problems with a good flexibility and adaptability. In this paper, we exploited the Fast Correlation-Based Feature Selection (FCBF) method to filter redundant features in order to improve the quality of heart disease classification. Then, we perform a classification based on different classification algorithms such as K-Nearest Neighbour, Support Vector Machine, Naïve Bayes, Random Forest and a Multilayer Perception | Artificial Neural Network optimized by Particle Swarm Optimization (PSO) combined with Ant Colony Optimization (ACO) approaches. The proposed mixed approach is applied to heart disease dataset; the results demonstrate the efficacy and robustness of the proposed hybrid method in processing various types of data for heart disease classification. Therefore, this study examines the different machine learning algorithms and compares the results using different performance measures, i.e. accuracy, precision, recall, f1-score, etc. A maximum classification accuracy of 99.65% using the optimized model proposed by FCBF, PSO and ACO. The results show that the performance of the proposed system is superior to that of the classification technique presented above.", 
Heart disease has gained a great deal of attention among various life-threatening diseases. Heart disease can be detected or diagnosed by different medical tests by considering various internal factors. Support Vector Machine (SVM) outperforms the others with greater accuracy (95%) than any other machine learning approach.,Heart Disease Prediction based on External Factors A Machine Learning Approach,"Technology has immensely changed the world over the last decade. As a consequence, the life of the people is undergoing multiple changes that directly have positive and negative effects on health. Less physical activity and a lot of virtual involvements are pushing people into various healthrelated issues and heart disease is one of them. Currently, it has gained a great deal of attention among various life-threatening diseases. Heart disease can be detected or diagnosed by different medical tests by considering various internal factors. However, this type of approach is not only time-consuming but also expensive. Concurrently, there are very few studies conducted on heart disease prediction based on external factors. To bridge this gap, we proposed a heart disease prediction model based on the machine learning approach which enables predicting heart disease with 95% accuracy. To acquire the best result, 6 distinct machine learning classifiers (Decision Tree, Random Forest, Naive Bayes, Support Vector Machine, Quadratic Discriminant, and Logistic Regression) were used. At the same time, sklearn.ensemble.ExtraTreesClassifier has been used to extract relevant features to improve predictive accuracy and control over-fitting. Findings reveal that Support Vector Machine (SVM) outperforms the others with greater accuracy (95%).", 
The purpose of this paper is to develop a cost effective treatment using data mining technologies for facilitating data base decision support system. Almost all the hospitals use some hospital management system to manage healthcare in patients. Most of the systems rarely use the huge clinical data where vital information is hidden.,Heart disease Prediction System Using data Mining Techniques,"In today’s modern world cardiovascular disease is the most lethal one. This disease attacks a person so instantly that it hardly gets any time to get treated with. So diagnosing patients correctly on timely basis is the most challenging task for the medical fraternity. A wrong diagnosis by the hospital leads to earn a bad name and loosing reputation. At the same time treatment of the said disease is quite high and not affordable by most of the patients particularly in India. The purpose of this paper is to develop a cost effective treatment using data mining technologies for facilitating data base decision support system. Almost all the hospitals use some hospital management system to manage healthcare in patients. Unfortunately most of the systems rarely use the huge clinical data where vital information is hidden. As these systems create huge amount of data in varied forms but this data is seldom visited and remain untapped. So, in this direction lots of efforts are required to make intelligent decisions. The diagnosis of this disease using different features or symptoms is a complex activity. In this paper using varied data mining technologies an attempt is made to assist in the diagnosis of the disease in question.", 
Large amount of data is generated in medical organisations but not properly used. This paper presents a classifier approach for detection of heart disease. It shows how Naive Bayes can be used for classification purpose.,Heart Disease Prediction System using Naive Bayes,"As large amount of data is generated in medical organisations (hospitals, medical centers) but as this data is not properly used. There is a wealth of hidden information present in the datasets. This unused data can be converted into useful data. For this purpose we can use different data mining techniques. This paper presents a classifier approach for detection of heart disease and shows how Naive Bayes can be used for classification purpose. In our system, we will categories medical data into five categories namely no, low, average, high and very high. Also, if unknown sample comes then the system will predict the class label of that sample. Hence two basic functions namely classification (training) and prediction (testing) will be performed. Accuracy of the system is depends on algorithm and database used.", 
Machine learning algorithm to think like a human being is making this concept so important and versatile. The non-linear tendency of the Cleveland heart disease dataset was exploited for applying Random Forest to get an accuracy of 85.81%.,Heart Disease Prediction System Using Random Forest,"The scope of Machine Learning algorithms are increasing in predicting various diseases. The nature of machine learning algorithm to think like a human being is making this concept so important and versatile. Here the challenge of increasing the accuracy of Heart disease prediction is taken upon. The non-linear tendency of the Cleveland heart disease dataset was exploited for applying Random Forest to get an accuracy of 85.81%. The method of predicting heart diseases using Random Forest with well-set attributes fetches us more accuracy. Random Forest was built by training 303 instances of data and authentication of accuracy was done using 10-fold cross validation. By the proposed algorithm for heart disease prediction, many lives could be saved in the future.", 
"In this paper, we aim to predict accuracy, whether the individual is at risk of a heart disease. This prediction will be done by applying machine learning algorithms on training data. The accuracy obtained using the developed model ranges between 85 and 88%.",Heart Disease Prediction Using CNN Algorithm,"In this paper, we aim to predict accuracy, whether the individual is at risk of a heart disease. This prediction will be done by applying machine learning algorithms on training data that we provide. Once the person enters the information that is requested, the algorithm is applied and the result is generated. Obviously, the accuracy is expected to decrease when the medical data itself are incomplete. We implement the prediction model over real-life hospital data. We propose to use convolutional neural network algorithm as a disease risk prediction algorithm using structured and perhaps even on unstructured patient data. The accuracy obtained using the developed model ranges between 85 and 88%. We have proposed further by applying other machine learning algorithms over the training data to predict the risk of diseases, comparing their accuracies so that we can deduce the most accurate one. Attributes can also be modified in an attempt to improve the accuracy further.", 
Heart diseases have emerged as the number one cause of deaths. Heart disease is accountable for deaths in all age groups and is common among males and females. This paper aims to report about taking advantage of the various data mining techniques and develop prediction models for heart disease survivability.,Heart Disease Prediction Using Data Mining Techniques,Studies have shown that heart diseases have emerged as the number one cause of deaths. Heart disease is accountable for deaths in all age groups and is common among males and females. A good solution to this problem is to be able to predict what a patient’s health status will be like in the future so the doctors can start treatment much sooner which will yield better results. It’s a lot better than acting at the last minute where the patient is already at risk and hence the prediction of heart disease is widely researched area. A lot of research and technological advancement has been recorded in similar fields. This paper aims to report about taking advantage of the various data mining techniques and develop prediction models for heart disease survivability., 
"In this paper, we will discuss heart disease. We proposed a model for heart disease prediction. Heart Disease UCI dataset will be used to demonstrate Talos Hyper-parameter optimization is more efficient.",Heart Diseases Prediction using Deep Learning Neural Network Model,"Deep learning plays an important role in the field of medical science in solving health issues and diagnosing various diseases. So in this paper, we will discuss heart disease. We proposed a model for heart disease prediction. Heart Disease is on of key area where Deep Neural Network can be used so we can improve the overall quality of the classification of heart disease. The classification can be performed on the various ways like KNN, SVM, Naïve Bayes, Random Forest. Heart Disease UCI dataset will be used to demonstrate Talos Hyper-parameter optimization is more efficient than others.", 
Coronary artery disease (CAD) is one of the most common types of heart disease. Corona virus can cause inflammation of the heart muscle. Heart disease is the top killer that has affected both urban and rural population. In this contemporary life style there is an urgent need of a system which will predict accurately the possibility of getting heart disease in an early stage. The aim of this paper is to design a robust system which works efficiently and will able to predict possibility of heart failure accurately.,HEART FAILURE PREDICTION USING MACHINE LEARNING TECHNIQUES,"In this modern era people are very busy and working hard in order to satisfying their materialistic needs and not able to spend time for themselves which leads to physical stress and mental disorder. There are also reports that heart suffer because of global pandemic corona virus. Inflammation of the heart muscle can be caused by corona virus. Thus heart disease is very common now a day’s particularly in urban areas because of excess mental stress due to corona virus. As a result Heart disease has become one of the most important factors for death of men and women in the so called material world. It has emerged as the top killer that has affected both urban and rural population. CAD (Coronary artery disease) is one of the most common types of heart disease. In the medical field predicting the heart disease has become a very complicated and challenging task, requires patient previous health records and in some cases they even need Genetic information as well. So, in this contemporary life style there is an urgent need of a system which will predict accurately the possibility getting heart disease. Predicting a Heart Disease in early stage will save many people’s Life. There were many heart disease prediction systems available at present, the Authors have been researched well and proposed different Classification and prediction algorithms but each one has its own limitations. The main objective of this paper is to overcome the limitations and to design a robust system which works efficiently and will able to predict the possibility of heart failure accurately. This paper uses the data set from the UCI repository and having 13 important attributes. This work is implemented using many algorithms such as SVM, Naïve Bayes, Logistic Regression, Decision Tree and KNN. It is found that SVM gave the best result with accuracy up to 85.2%. A comparative statement of all the algorithms also presented in the implementation part of the paper. This research also uses model validation technique to design a best suitable model fitting in the current scenario.", 
"The electrocardiogram (ECG) has been widely used in the diagnosis of heart disease. Arrhythmia can be classified into many types, including life-threatening and non-life-threatening. Accurate detection of arrhythmic types can effectively prevent heart disease and reduce mortality.",Heartbeat classification using Deep Residual Convolutional Neural Network from 2-lead electrocardiogram,"The electrocardiogram (ECG) has been widely used in the diagnosis of heart disease such as arrhythmia due to its simplicity and non-invasive nature. Arrhythmia can be classified into many types, including life-threatening and non-life-threatening. Accurate detection of arrhythmic types can effectively prevent heart disease and reduce mortality. In this study, a novel deep learning method for classification of cardiac arrhythmia according to deep residual network (ResNet) is presented. We developed a 31-layer one-dimensional (1D) residual convolutional neural network. The algorithm includes four residual blocks, each of which consists of three 1D convolution layers, three batch normalization (BP) layers, three rectified linear unit (ReLU) layers, and an ""identity shortcut connections"" structure. In addition, we propose to use 2-lead ECG signals in combination with deep learning methods to automatically identify five different types of heartbeats. We have obtained an average accuracy, sensitivity and positive predictivity of 99.06%, 93.21% and 96.76% respectively for single-lead ECG heartbeats. In the 2-lead datasets, the results show that the deep ResNet model has high classification performance, achieving an accuracy of 99.38%, sensitivity of 94.54%, and specificity of 98.14%. The proposed method can be used as an adjunct tool to assist clinicians in their diagnosis.", 
"This paper examines the effectiveness of well-known summarization heuristics when applied to the task of generating single-document summary extracts of variable length. The original text  documents and their summaries were scored by different human judges based on soft metrics like topic-coverage, relative coherence, novelty and information content.",Heuristics based automatic text summarization of unstructured text,"Automatic Text Summarization is a specialized text mining task of generating  a  summary  or  abstract  from  single  or  multiple  input text  documents.  Various  heuristic  and  semi-supervised  learning methods  have  been  explored  by  researchers  in  this  field  to generate  generic  as  well  as  user-oriented  summaries. This  paper examines the  effectiveness of  well-known  summarization heuristics when applied to the task of generating single-document summary extracts of variable length. For evaluating the quality of the  summaries,  the  original  text  documents  and their  summaries were scored by different human judges based on soft metrics like topic-coverage,  relative  coherence,  novelty  and  information content;  and  their  scores  were  statistically  compared. It  was experimentally  verified  that  in  65%  of  the  documents there was less than 10% variance between the scores assigned to the original texts and their summaries.", 
Hierarchical Fine-Tuning based CNN is competitive with the state-of-the-art CNN based methods. Method leverages the hierarchical relations between the categories to tackle the data sparsity problem.,HFT-CNN Learning Hierarchical Category Structure for Multi-label Short Text Categorization,"We focus on the multi-label categorization task for short texts and explore the use of a hierarchical structure (HS) of categories. In contrast to the existing work using non-hierarchical flat model, the method leverages the hierarchical relations between the categories to tackle the data sparsity problem. The lower the HS level, the worse the categorization performance. Because lower categories are fine-grained and the amount of training data per category is much smaller than that in an upper level. We propose an approach which can effectively utilize the data in the upper levels to contribute categorization in the lower levels by applying a Convolutional Neural Network (CNN) with a finetuning technique. The results using two benchmark datasets show that the proposed method, Hierarchical Fine-Tuning based CNN (HFTCNN) is competitive with the state-of-the-art CNN based methods.", 
Recent advances in ATS are overwhelmingly contributed by deep learning techniques. The human reading cognition is essential for reading comprehension and logical thinking. We propose a Hierarchical Human-like deep neural network for ATS inspired by the process of how humans comprehend an article and write the corresponding summary.,Hierarchical Human-Like Deep Neural Networks for Abstractive Text Summarization,"Developing an abstractive text summarization (ATS) system that is capable of generating concise, appropriate, and plausible summaries for the source documents is a long-term goal of artificial intelligence (AI). Recent advances in ATS are overwhelmingly contributed by deep learning techniques, which have taken the state-of-the-art of ATS to a new level. Despite the significant success of previous methods, generating high-quality and human-like abstractive summaries remains a challenge in practice. The human reading cognition, which is essential for reading comprehension and logical thinking, is still relatively new territory and underexplored in deep neural networks. In this article, we propose a novel Hierarchical Human-like deep neural network for ATS (HH-ATS), inspired by the process of how humans comprehend an article and write the corresponding summary. Specifically, HH-ATS is composed of three primary components (i.e., a knowledge-aware hierarchical attention module, a multitask learning module, and a dual discriminator generative adversarial network), which mimic the three stages of human reading cognition (i.e., rough reading, active reading, and postediting). Experimental results on two benchmark data sets (CNN/Daily Mail and Gigaword) demonstrate that HH-ATS consistently and substantially outperforms the compared methods.", 
"Each image is expressed as a bag of orderless pairs. Each image includes a local feature vector encoded over a visual dictionary, and its corresponding side information from priors or contexts. The side information is used for hierarchical clustering of the encoded local features. A hierarchical matching kernel is derived as the weighted sum of the similarities over the encoded features pooled within clusters at different levels.",Hierarchical Matching with Side Information for Image Classification,"In this work, we introduce a hierarchical matching framework with so-called side information for image classification based on bag-of-words representation. Each image is expressed as a bag of orderless pairs, each of which includes a local feature vector encoded over a visual dictionary, and its corresponding side information from priors or contexts. The side information is used for hierarchical clustering of the encoded local features. Then a hierarchical matching kernel is derived as the weighted sum of the similarities over the encoded features pooled within clusters at different levels. Finally the new kernel is integrated with popular machine learning algorithms for classification purpose. This framework is quite general and flexible, other practical and powerful algorithms can be easily designed by using this framework as a template and utilizing particular side information for hierarchical clustering of the encoded local features. To tackle the latent spatial mismatch issues in SPM, we design in this work two exemplar algorithms based on two types of side information: object confidence map and visual saliency map, from object detection priors and within-image contexts respectively. The extensive experiments over the Caltech-UCSD Birds 200, Oxford Flowers 17 and 102, PASCAL VOC 2007, and PASCAL VOC 2010 databases show the state-of-the-art performances from these two exemplar algorithms.", 
"In this paper, we propose a universal solution to web search and web browsing on handheld devices for visually impaired people.",Hierarchical Soft Clustering and Automatic Text Summarization for Accessing the Web on Mobile Devices for Visually Impaired People,"In this paper, we propose a universal solution to web search and web browsing on handheld devices for visually impaired people. For this purpose, we propose (1) to automatically cluster web page results and (2) to summarize all the information in web pages so that speech-to-speech interaction is used efficiently to access information.", 
"Researchers have shown that human abstractors extract sentences for summaries based on the hierarchical structure of documents. In general, a document exhibits a well-defined hierarchical structure that can be described as fractals. fractal summarization is promising and outperforms current summarization techniques, authors say.",Hierarchical summarization of large documents,"Many automatic text summarization models have been developed in the last decades. Related research in information science has shown that human abstractors extract sentences for summaries based on the hierarchical structure of documents; however, the existing automatic summarization models do not take into account the human abstractor’s behavior of sentence extraction and only consider the document as a sequence of sentences during the process of extraction of sentences as a summary. In general, a document exhibits a well-defined hierarchical structure that can be described as fractals— mathematical objects with a high degree of redundancy. In this article, we introduce the fractal summarization model based on the fractal theory. The important information is captured from the source document by exploring the hierarchical structure and salient features of the document. A condensed version of the document that is informatively close to the source document is produced iteratively using the contractive transformation in the fractal theory. The fractal summarization model is the first attempt to apply fractal theory to document summarization. It significantly improves the divergence of information coverage of summary and the precision of summary. User evaluations have been conducted. Results have indicated that fractal summarization is promising and outperforms current summarization techniques that do not consider the hierarchical structure of documents.", 
This study is a part of the growing body of research on large-scale text summarization. It is an experimental approach and it differs from earlier works in the sense that it generates topic hierarchy. The documents are labeled with topics using latent Dirichlet allocation (LDA) and are automatically organized in a lattice structure.,Hierarchical Summarization of Text Documents Using Topic Modeling and Formal Concept Analysis,"Availability of large collection of text documents triggers the need for large-scale text summarization. Identification of topics and organization of documents are important for analysis and exploration of textual data. This study is a part of the growing body of research on large-scale text summarization. It is an experimental approach and it differs from earlier works in the sense that it generates topic hierarchy which simultaneously provides a hierarchical structure to the document corpus. The documents are labeled with topics using latent Dirichlet allocation (LDA) and are automatically organized in a lattice structure using formal concept analysis (FCA). This groups the semantically related documents together. The lattice is further converted to a tree for better visualization and easy exploration of data. To signify the effectiveness of the approach, we have carried out the experiment and evaluation on 20 Newsgroup Dataset. Results depict that the presented method is considerably successful in forming topic hierarchy.", 
"In this paper, we develop a neural summarization model which can effectively process multiple input documents. Our model augments a previously proposed Transformer architecture. We represent cross-document relationships via an attention mechanism.",Hierarchical Transformers for Multi-Document Summarization,"In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments a previously proposed Transformer architecture (Liu et al., 2018) with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.", 
Proposed method transforms the original EEG signal into a spatial pattern and applies the RBF feature selection method to generate robust feature. Classification is performed by the SVM and our experimental result shows that the classification accuracy of the proposed method reaches 90%.,High Accuracy Classification of EEG Signal,"Improving classification accuracy is a key issue to advancing brain computer interface (BCI) research from laboratory to real world applications. This article presents a high accuracy EEG signal classification method using single trial EEG signal to detect left and right finger movement. We apply an optimal temporal filter to remove irrelevant signal and subsequently extract key features from spatial patterns of EEG signal to perform classification. Specifically, the proposed method transforms the original EEG signal into a spatial pattern and applies the RBF feature selection method to generate robust feature. Classification is performed by the SVM and our experimental result shows that the classification accuracy of the proposed method reaches 90% as compared to the current reported best accuracy of 84%.", 
"Classification of Electroencephalogram (EEG) data for imagined motor movements has been a challenge. The number of data samples in a class of interest, e.g. a specific action, is a small fraction of the total data. Building a robust classifier when learning from a highly unbalanced dataset is difficult.",High performance EEG signal classification using classifiability and the Twin SVM,"Classification of Electroencephalogram (EEG) data for imagined motor movements has been a challenge in the design and development of Brain Computer Interfaces (BCIs). There are two principle challenges. The first is the variability in the recorded EEG data, which manifests across trials as well as across individuals. Consequently, features that are more discriminative need to be identified before any pattern recognition technique can be applied. The second challenge is in the pattern recognition domain. The number of data samples in a class of interest, e.g. a specific action, is a small fraction of the total data, which is composed of samples corresponding to all actions of all users. Building a robust classifier when learning from a highly unbalanced dataset is very difficult; minimizing the classification error typically causes the larger class to overwhelm the smaller one. We show that the combination of ‘classifiability’ for selecting the optimal frequency band and the use of the Twin Support Vector Machine (Twin SVM) for classification, yields significantly improved generalization. On benchmark BCI Competition datasets, the proposed approach often yields up to 20% improvement over the state-of-the-art.", 
"Information extraction and query-oriented summarization method are applied here to reply people's query. There are few effective and commonly used methods on filtering the redundancy and noise of the raw data, which results in the poor quality of the reply. Experimental results show that the research is effective in filtering the noise and redundancy.",High quality information extraction and query-oriented summarization for automatic query-reply in social network,"In this paper, we propose a new method for automatic query-reply in social network. Information extraction and query-oriented summarization method are applied here to reply people’s query. There are few effective and commonly used methods on filtering the redundancy and noise of the raw data, which results in the poor quality of the reply. Due to the characteristics of social network messages, we pay more attention to reducing the noise and eliminating the redundancy of the messages to ensure the quality of the final reply. First, we propose an information extraction method to extract high quality information from social network messages, which is based on time-frequency transformation. Second, query-oriented text summarization is implemented to generate a brief and concise summary as the final reply, which is based on the scoring, ranking and selection of sentences of high quality social network messages produced by previous step. Experimental results show that the research is effective in filtering the redundancy and noise of social network messages, the final query-reply results outperform other commonly used methods’ results in both automatic evaluation and manual evaluation. Through our approach, noise and redundancy of social network messages are effectively filtered. Certainly, our method improves the quality of the reply for people’s query.", 
"The larger the training set, the higher the impact of the dimensionality on the accuracy. High-dimensional signatures are important to obtain state-of-the-art results on large datasets. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement.",High-Dimensional Signature Compression for Large-Scale Image Classification,"We address image classification on a large-scale, i.e. when a large number of images and classes are involved. First, we study classification accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and / or an increase in CPU cost. We report results on two large databases – ImageNet and a dataset of 1M Flickr images – showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7%, a relative improvement of 160% with respect to the state-of-the-art.", 
Proposed single element is a newly designed fractal antenna which is right-handed circularly polarized (RHCP) The antenna array is designed by a single-layer microstrip structure with a compact size of 151.70 × 43.50 mm2. The performance of the proposed RHCP antenna is suitable for the DSRC-band application.,High-Gain and Circularly Polarized Fractal Antenna Array for Dedicated Short Range Communication Systems,"In this paper, a low-profile fractal antenna and its array for DSRC-band applications have been proposed. The proposed single element is a newly designed fractal antenna which is right-handed circularly polarized (RHCP) and derived from the Koch-snowflake 1st-iteration. Moreover, a diagonal slot defect in the ground plane has been implemented for resonating the structure at the desired frequency and, to get a low cross-polarization over the operating frequency. The compact feed-network of the array is designed using a Wilkinson power-divider. A single element and a 4 × 1 antenna array are designed, prototyped, and verified. The antenna array is designed by a single-layer microstrip structure with a compact size of 151.70 × 43.50 mm2. According to the experimental results, the single element and the antenna array have S11 of ?15.27 dB and ?13.95 dB, and RHCP gain of 6.14 dBic and 11.98 dBic, respectively. Moreover, the computed radiation efficiencies of single element and array are 78.17% and 71.50%, respectively, while CP bandwidths of single element and array are 49.00 MHz and 58.00 MHz, respectively. The performance of the proposed RHCP antenna is suitable for the DSRC-band application.", 
"Deep hierarchical networks achieve the best results on benchmarks for object classification and handwritten digit recognition. Feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. NORB is completely trained within five epochs.",High-Performance Neural Networks for Visual Object Classification,"We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.", 
"Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. The new approach gives near-perfect separation on the original MIT pedestrian database. Fine-scale gradients, fine orientation binning, relatively coarse spatial binning are all important for good results.",Histograms of Oriented Gradients for Human Detection,"We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.", 
"Q&A platforms such as Stack Overflow allow users to ask and answer questions about a wide variety of programming topics. These platforms accumulate a large amount of knowledge, including hundreds of thousands of lines of source code. Developers can benefit from the source code by copying or learning from (parts of) it. In 31.5% of the studied files, developers needed to modify source code from Stack Overflows to make it work in their own projects. The top 3 barriers that make it difficult for developers to reuse code are: too much code modification required to fit in their projects, incomprehensive code, and low code quality.",How do developers utilize source code from stack overflow,"Technical question and answer Q&A platforms, such as Stack Overflow, provide a platform for users to ask and answer questions about a wide variety of programming topics. These platforms accumulate a large amount of knowledge, including hundreds of thousands lines of source code. Developers can benefit from the source code that is attached to the questions and answers on Q&A platforms by copying or learning from (parts of) it. By understanding how developers utilize source code from Q&A platforms, we can provide insights for researchers which can be used to improve next-generation Q&A platforms to help developers reuse source code fast and easily. In this paper, we first conduct an exploratory study on 289 files from 182 open-source projects, which contain source code that has an explicit reference to a Stack Overflow post. Our goal is to understand how developers utilize code from Q&A platforms and to reveal barriers that may make code reuse more difficult. In 31.5% of the studied files, developers needed to modify source code from Stack Overflow to make it work in their own projects. The degree of required modification varied from simply renaming variables to rewriting the whole algorithm. Developers sometimes chose to implement an algorithm from scratch based on the descriptions from Stack Overflow answers, even if there was an implementation readily available in the post. In 35.5% of the studied files, developers used Stack Overflow posts as an information source for later reference. To further understand the barriers of reusing code and to obtain suggestions for improving the code reuse process on Q&A platforms, we conducted a survey with 453 open-source developers who are also on Stack Overflow. We found that the top 3 barriers that make it difficult for developers to reuse code from Stack Overflow are: (1) too much code modification required to fit in their projects, (2) incomprehensive code, and (3) low code quality. We summarized and analyzed all survey responses and we identified that developers suggest improvements for future Q&A platforms along the following dimensions: code quality, information enhancement & management, data organization, license, and the human factor. For instance, developers suggest to improve the code quality by adding an integrated validator that can test source code online, and an outdated code detection mechanism. Our findings can be used as a roadmap for researchers and developers to improve code reuse.", 
Clustering is considered an important issue in Vehicular Ad-hoc Networks (VANETs) There is still a lack of understanding of how relative mobility between cluster members and the cluster head affects the throughput of VANET clusters.,How relative mobility affects the aggregate throughput of VANET cluster,"Clustering is considered an important issue in Vehicular Ad-hoc Networks (VANETs) to ensure the network robustness and throughput. Nevertheless, in the literature, there is still a lack of understanding of how relative mobility between cluster members and the cluster head affects the throughput of VANET clusters. In this letter, we present a model to characterize the throughput of VANET clusters, by taking into account the relative mobility effect, which implicitly accounts for the Doppler effect. The numerical analysis shows that there is a large gap between the throughput of the model with/without consideration of relative mobility, which indicates the necessity to develop clustering algorithms that minimize intra-cluster mobility.", 
"Until recently, most mobile messaging apps did not protect confidentiality or integrity of the messages. Press releases about mass surveillance performed by intelligence services such as NSA and GCHQ motivated many people to use alternative messaging solutions. A messaging app that claims to provide secure instant messaging and has attracted a lot of attention is TEXTSECURE.",How Secure is TextSecure,"Instant Messaging has gained popularity by users for both private and business communication as low-cost short message replacement on mobile devices. However, until recently, most mobile messaging apps did not protect confidentiality or integrity of the messages. Press releases about mass surveillance performed by intelligence services such as NSA and GCHQ motivated many people to use alternative messaging solutions to preserve the security and privacy of their communication on the Internet. Initially fueled by Facebook’s acquisition of the hugely popular mobile messaging app WHATSAPP, alternatives claiming to provide secure communication experienced a significant increase of new users. A messaging app that claims to provide secure instant messaging and has attracted a lot of attention is TEXTSECURE. Besides numerous direct installations, its protocol is part of Android’s most popular aftermarket firmware CYANOGENMOD. TEXTSECURE’s successor Signal continues to use the underlying protocol for text messaging. In this paper, we present the first complete description of TEXTSECURE’s complex cryptographic protocol, provide a security analysis of its three main components (key exchange, key derivation and authenticated encryption), and discuss the main security claims of TEXTSECURE. Furthermore, we formally prove that—if key registration is assumed to be secure—TEXTSECURE’s push messaging can indeed achieve most of the claimed security goals.", 
"Text summarization and classification are core techniques to analyze a huge amount of text data in the big data environment. As the need to read texts on smart phones, tablets and television as well as personal computers continues to grow, both of them become more important. We propose an effective integrated learning framework using both of summary and category information.",How to Improve Text Summarization and Classification by Mutual Cooperation on an Integrated Framework,"Text summarization and classification are core techniques to analyze a huge amount of text data in the big data environment. Moreover, as the need to read texts on smart phones, tablets and television as well as personal computers continues to grow, text summarization and classification techniques become more important and both of them do essential processes for text analysis in many applications. Traditional text summarization and classification techniques have individually been considered as different research fields in this literature. However, we find out that they can help each other as text summarization makes use of category information from text classification and text classification does summary information from text summarization. Therefore, we propose an effective integrated learning framework using both of summary and category information in this paper. In this framework, the feature-weighting method for text summarization utilizes a language model to combine feature distributions in each category and text, and one for text classification does the sentence importance scores estimated from the text summarization. In the experiments, the performances of the integrated framework are better than ones of individual text summarization and classification. In addition, the framework has some advantages of easy implementation and language independence because it is based on only simple statistical approaches and POS tagger.", 
Deep neural networks trained on natural images learn features similar to Gabor filters and color blobs. Such first-layer features appear general in that they are applicable to many datasets and tasks. Transferability of features decreases as the distance between the base task and target task increases. Initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers after fine-tuning.,How transferable are features in deep neural networks,"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", 
This paper analyzes the existed methods for automatic summarization evaluation. It introduces a new evaluation method based on HowNet. The original tests have shown that the new method can touch the semantic meaning of the summarizations to some extent.,HowNet based evaluation for Chinese text summarization,"With the rapid development of text summarization, evaluation methods for automatic summarization system is becoming more and more important in Natural Language Processing, which can promote development of text summarization greatly. This paper analyzes the existed methods for automatic summarization evaluation, and introduces a new evaluation method based on HowNet. The original tests have shown that the new method can touch the semantic meaning of the summarizations to some extent, thus reflect the quality metrics, for example, the sequence of sentences and conciseness much better. With more improvement in future, automatic evaluations will correlate with human assessments highly.", 
EEG has not been extensively applied to the design of Human Activity Recognition (HAR) systems. HAR involves classifying activities of an individual that are captured by sensory technologies. This paper introduces a deep learning-based framework for classifying EEG artifacts. The deep-learning approach proposed by the FCEA improves raw data processing to obtain a better generalization performance.,Human activity recognition using deep electroencephalography learning,"Electroencephalography (EEG) signals can be contaminated by the noise raised from a non-cerebral artifact and vary in magnitude due to physiological differences between individuals. Hence, EEG has not been extensively applied to the design of Human Activity Recognition (HAR) systems. HAR involves classifying the activities of an individual that are captured by sensory technologies. To address this issue, this paper introduces a deep learning-based framework for classifying EEG artifacts (FCEA), based on a person’s physiological activity. The FCEA proposes an approach for designing a processing pipeline involving a Convolutional Neural Network and a Long Short-Term Memory Recurrent Neural Network, in order to classify raw EEG signals based on the EEG artifacts. To demonstrate the performance of the FCEA, a 3-class dataset of EEG activities was created. A consumer-grade EEG wearable device was used to collect the data from two prefrontal EEG channels, from 8 participants whilst speaking loudly, reading printed text and watching a TV program. These activities include jaw-clenching and head and eye movements, that are part of a wide range of daily human activities. Moreover, these activities cannot easily be detected by using the sensory technologies that have commonly been used in HAR. The comparative performance analysis results demonstrate that the 1- and 2-channel models trained by the FCEA outperform competitive state-of-the-art deep-learning models for HAR and raw EEG data. The deep-learning approach proposed by the FCEA improves raw data processing to obtain a better generalization performance.", 
"This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. ",Human Curation and Convnets Powering Item-to-Item Recommendations on Pinterest,"This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, the activity of users organizing content, are highly effective when used in conjunction with contentbased ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from convnets, in improving the user engagement rate of our item-to-item recommendation system.", 
This paper concentrates on hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. It combines the features of Hindi summarizer as suggested by CDAC Noida. This algorithm performs well at 30% compression ratio for both intrinsic and extrinsic measures of summary evaluation. F-Score equal to 92.56% which is reasonably good.,Hybrid Algorithm for Multilingual Summarization of Hindi and Punjabi Documents,"This paper concentrates on hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. It combines the features of Hindi summarizer as suggested by CDAC Noida and Punjabi summarizer as suggested by Gupta and Lehal in 2012. In addition to this, it also suggests some new features for summarizing Hindi and Punjabi multilingual text. It is first time that this multilingual text summarizer has been proposed which supports both Hindi and Punjabi text. Nine features used in this algorithm for summarizing multilingual Hindi and Punjabi text are: 1) Key phrase extraction 2) Font feature 3) Nouns and Verbs Extraction 4) Position feature 5) Cue-phrase feature 6) Negative keywords extraction 7) Named Entities extraction 8) Relative length feature 9) extraction of number data. For each sentence, scores of each feature is calculated and then machine learning based mathematical regression is applied for identifying weights of these nine features. Sentence final-scores are calculated from feature weight equations. Top scored sentences in proper order (in same order as in input) are selected for final summary. Default summary is made at 30% compression ratio. This algorithm performs well at 30% compression ratio for both intrinsic and extrinsic measures of summary evaluation. This algorithm has been thoroughly tested on 30 Hindi-Punjabi documents and reports F-Score equal to 92.56% which is reasonably good.", 
"Heart disease is one of the significant reason of death and disability. The shortage of Doctors, experts and ignoring patient symptoms lead to big challenge. This paper proposes new heart disease prediction system that combine all techniques into one single algorithm.",Hybrid Approach for Heart Disease Prediction Using Data Mining Techniques,"Heart disease is one of the significant reason of death and disability. The shortage of Doctors, experts and ignoring patient symptoms lead to big challenge that may cause death, disability to the patient. Therefore, we need expert system that serve as an analysis tool to discover hidden information and patterns in hear disease medical data. Data mining is a cognitive procedure of discovering the hidden approach patterns from large data set. The available massive data can used to extract useful information and relate all attributes to make a decision. Various techniques listed and tested here to understand the accuracy level of each. In previous studies, researchers expressed their effort on finding best prediction model. This paper proposes new heart disease prediction system that combine all techniques into one single algorithm, it called hybridization. The result confirm that accurate diagnose can be taken by using a combined model from all techniques.", 
"Summarizer using soft computing techniques namely, Fuzzy logic system and deep neural system is proposed. The summary is based on extractive summarization technique. Restricted boltzman Machine (RBM) is used in the deep neural network and fuzzy rule base on the sentences.",Hybrid auto text summarization using deep neural network and fuzzy logic system,"Amount of text for a particular topic is increasing at an exponential rate each day, causing availability of bulk text data, some of which will be relevant to our search while some data may be irrelevant. In such case we need to summarize the text document, so as to understand the key elements of the document. A Hybrid Automatic Summarizer using soft computing techniques namely, Fuzzy logic system and deep neural system is proposed. The summary is based on extractive summarization technique. Restricted boltzman Machine (RBM) is used in the deep neural network and fuzzy rule base on the sentences for feature extraction using a sentence matrix. Evaluation of hybridized systems gave better summary as compared to the individual systems.", 
"The main goal of a text summarization is to present the main ideas in a document in less space. The proposed method is using compression 50%, 30% and 20% to create candidate of the summary. The result shows that proposed method affect significant increasing cohesion degree after evaluated in the tTest.",Hybrid Keyword Extraction Algorithm and Cosine Similarity for Improving Sentences Cohesion in Text Summarization,"As the amount of online information increases, systems that can automatically summarize text in a document become increasingly desirable. The main goal of a text summarization is to present the main ideas in a document in less space. In the create text summarization, there are two procedures i.e. extraction and abstraction procedure. One of extraction procedure is using keyword extraction algorithm which is easier and common but has problem in the lack of cohesion or correlation between sentences. The cohesion between sentences can be applied by using a cosine similarity method. In this study, a hybrid keyword extraction algorithm and cosine similarity for improving sentences cohesion in text summarization has been proposed. The proposed method is using compression 50%, 30% and 20% to create candidate of the summary. The result shows that proposed method affect significant increasing cohesion degree after evaluated in the tTest. The result also shows that 50% compression ratio obtains the best result with Recall, Precision, and F-Measure are 0.761, 0.43 and 0.54 respectively; since summary with compression ratio 50% has higher intersection with human summary than another compression ratio.", 
Latent semantic analysis has been used successfully for extractive text summarization for years. Random indexing-based summarization inherently uses graph-based ranking techniques. We propose a hybrid technique of latent semantic analysis and random indexing.,Hybrid Latent Semantic Analysis and Random Indexing Model for Text Summarization,"Latent semantic analysis has been used successfully for extractive text summarization for years, while random indexing-based summarization has been recently proposed in the literature for text summarization. The random indexing-based summarization inherently uses graph-based ranking techniques. In this paper, we propose a hybrid technique of latent semantic analysis and random indexing for text summarization. Further, we have performed experiments to compare the results with several related baseline methods. The effectiveness of the hybrid method so developed is evident from the relative increase in the results over the baseline LSA-based technique.", 
Audit records are important for investigating suspicious actions against transactional databases. Admissibility as digital evidence depends on satisfying Chain of Custody (CoC) properties. We propose a forensically-aware distributed database architecture that implements these properties as functional requirements.,Hybrid Logical Clocks for Database Forensics,"Database audit records are important for investigating suspicious actions against transactional databases. Their admissibility as digital evidence depends on satisfying Chain of Custody (CoC) properties during their generation, collection and preservation in order to prevent their modification, guarantee action accountability, and allow third-party verification. However, their production has relied on auditing capabilities provided by commercial database systems which may not be effective if malicious users (or insiders) misuse their privileges to disable audit controls, and compromise their admissibility. Hence, in this paper, we propose a forensically-aware distributed database architecture that implements CoC properties as functional requirements to produce admissible audit records. The novelty of our proposal is the use of hybrid logical clocks, which compared with a previous centralised vector-clock architecture, has evident advantages as it (i) allows for more accurate provenance and causality tracking of insider actions, (ii) is more scalable in terms of system size, and (iii) although latency is higher (as expected in distributed environments), 70 per cent of user transactions are executed within acceptable latency intervals.",  
Hybrid MemNet is a data-driven end-to-end deep network. It learns the continuous unified representation of a document before generating its summary. It jointly captures local and global sentential information along with the notion of summary worthy sentences.,Hybrid MemNet for Extractive Summarization,"Extractive text summarization has been an extensive research problem in the field of natural language understanding. While the conventional approaches rely mostly on manually compiled features to generate the summary, few attempts have been made in developing data-driven systems for extractive summarization. To this end, we present a fully data-driven end-to-end deep network which we call as Hybrid MemNet for single document summarization task. Œe network learns the continuous unified representation of a document before generating its summary. It jointly captures local and global sentential information along with the notion of summary worthy sentences. Experimental results on two different corpora confirm that our model shows significant performance gains compared with the state-of-the-art baselines.", 
Summarization is the technique of shirking the original text document in such a way that its meaning is not altered. Summarization techniques have become important for information retrieval. It is impossible for a human to extract relevant information from enormous amount of data in a time-bound situation.,"Hybrid Text Summarization, A Survey","Text summarization is the technique of shirking the original text document in such a way that its meaning is not altered. Summarization techniques have become important for information retrieval as large volumes of data are available on Internet and it is impossible for a human to extract relevant information from enormous amount of data in a time-bound situation. Thus, automatic text summarizer is a tool for reducing the information available on Internet by providing nonredundant and salient sentence extracted from a single or multiple text documents. Text summarization has two approaches: extractive and abstractive. Extractive approach generates the summary by selecting subsets of words, sentences, and phrases of text documents whereas abstractive approach understands the main idea of the document and then represents that idea in a natural language using natural language generation technique to create summaries. This paper represents a Survey of Automatic Hybrid Text Summarization.", 
This paper presents a Hybrid-based Single-document Extractive Arabic Text Summarization approach based on Genetic Algorithm.,Hybrid-based Arabic single-document text summarization approach using genatic algorithm,"Text summarization is one of the most paramount applications of the field of natural language processing. This paper presents a Hybrid-based Single-document Extractive Arabic Text Summarization approach based on Genetic Algorithm. The proposed summarization approach was evaluated using EASC corpus, and ROUGE evaluation method to determine the accuracy of the proposed approach. The result showed that our method outperformed many state-of-the art methods.",  
"Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. The information in this layer may be too coarse spatially to allow precise localization. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks.",Hypercolumns for Object Segmentation and Fine-grained Localization,"Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation, where we improve state-of-the-art from 49.7 mean APr to 60.0, keypoint localization, where we get a 3.3 point boost over, and part labeling, where we show a 6.6 point gain over a strong baseline.", 
Deep learning can be used to identify post-stroke patients using convolutional neural networks. The accuracy of the testing data was 90% with amplitude and Beta features compared to 70% without. The experimental results also showed that adaptive moment estimation model was more stable compared to Stochastic gradient descent.,Identification of post-stroke EEG signal using wavelet and convolutional neural networks,"Post-stroke patients need ongoing rehabilitation to restore dysfunction caused by an attack so that a monitoring device is required. EEG signals reflect electrical activity in the brain, which also informs the condition of post-stroke patient recovery. However, the EEG signal processing model needs to provide information on the post-stroke state. The development of deep learning allows it to be applied to the identification of post-stroke patients. This study proposed a method for identifying post-stroke patients using convolutional neural networks (CNN). Wavelet is used for EEG signal information extraction as a feature of machine learning, which reflects the condition of post-stroke patients. This feature is Delta, Alpha, Beta, Theta, and Mu waves. Moreover, the five waves, amplitude features are also added according to the characteristics of the post-stroke EEG signal. The results showed that the feature configuration is essential as distinguish. The accuracy of the testing data was 90% with amplitude and Beta features compared to 70% without amplitude or Beta. The experimental results also showed that adaptive moment estimation (Adam) optimization model was more stable compared to Stochastic gradient descent (SGD). But SGD can provide higher accuracy than the Adam model.", 
"This research is contextualized in the teaching of computer programming. This research presents the A-Learn EvId method, having as the main differential the evaluation of high-level skills instead of technical aspects.",Identifying Evidences of Computer Programming Skills Through Automatic Source Code Evaluation,"This research is contextualized in the teaching of computer programming. Continuous assessment of source codes produced by students on time is a challenging task for teachers. The literature presents different methods for automatic evaluation of source code, mostly focusing on technical aspects. This research presents the A-Learn EvId method, having as the main differential the evaluation of high-level skills instead of technical aspects. The following results are highlighted: updating the state of the art through systematic mapping; a set of 37 skills identifiable through 9 automatic source code evaluation strategies; construction of datasets totaling 8651 source codes.", 
"Recent advances have paved the way for increasingly accurate seizure preictal state detection algorithms. To develop seizure forecasting for broad clinical and ambulatory use, less complex and invasive modalities are needed. We apply deep learning to a large epilepsy data set containing multi-day, simultaneous recordings of EKG, ECoG, and EEG, using a variety of feature sets.","Identifying signal-dependent information about the preictal state A comparison across ECoG, EEG and EKG using deep learning","The inability to reliably assess seizure risk is a major burden for epilepsy patients and prevents developing better treatments. Recent advances have paved the way for increasingly accurate seizure preictal state detection algorithms, primarily using electrocorticography (ECoG). To develop seizure forecasting for broad clinical and ambulatory use, however, less complex and invasive modalities are needed. Algorithms using scalp electroencephalography (EEG) and electrocardiography (EKG) have also achieved better than chance performance. But it remains unknown how much preictal information is in ECoG versus modalities amenable to everyday use – such as EKG and single channel EEG - and how to optimally extract that preictal information for seizure prediction. We apply deep learning - a powerful method to extract information from complex data - on a large epilepsy data set containing multi-day, simultaneous recordings of EKG, ECoG, and EEG, using a variety of feature sets. We use the relative performance of our algorithms to compare the preictal information contained in each modality. We find that single-channel EKG contains a comparable amount of preictal information as scalp EEG with up to 21 channels and that preictal information is best extracted not with standard heart rate measures, but from the power spectral density. We report that preictal information is not preferentially contained in EEG or ECoG channels within the seizure onset zone. Collectively, these insights may help to devise future prospective, minimally invasive long-term epilepsy monitoring trials with single-channel EKG as a particularly promising modality.", 
"Stroke is widely considered as the second most common cause of mortality. EHRs routinely contain several thousands of features and most of them are redundant and irrelevant. Age, average glucose level, heart disease, and hypertension are the most essential attributes for detecting stroke in patients.",Identifying Stroke Indicators Using Rough Sets,"Stroke is widely considered as the second most common cause of mortality. The adverse consequences of stroke have led to global interest and work for improving the management and diagnosis of stroke. Various techniques for data mining have been used globally for accurate prediction of occurrence of stroke based on the risk factors that are associated with the electronic health care records (EHRs) of the patients. In particular, EHRs routinely contain several thousands of features and most of them are redundant and irrelevant that need to be discarded to enhance the prediction accuracy. The choice of feature-selection methods can help in improving the prediction accuracy of the model and efficient data management of the archived input features. In this paper, we systematically analyze the various features in EHR records for the detection of stroke. We propose a novel rough-set based technique for ranking the importance of the various EHR records in detecting stroke. Unlike the conventional rough-set techniques, our proposed technique can be applied on any dataset that comprises binary feature sets. stroke. We evaluated our proposed method in a publicly available dataset of EHR, and concluded that age, average glucose level, heart disease, and hypertension were the most essential attributes for detecting stroke in patients. Furthermore, we benchmarked the proposed technique with other popular feature-selection techniques. We obtained the best performance in ranking the importance of individual features in detecting stroke.", 
Human brain has developed mechanisms to efficiently decode sensory information according to perceptual categories of high prevalence in the environment. In this work we train Random Forest classification models to decode eight perceptual categories from broad spectrum of human intracranial signals. We show that network selectivity for a single or multiple categories in sensory and non-sensory cortices is related to specific patterns of power increases and decreases in both low and high frequency bands.,Identifying task-relevant spectral signatures of perceptual categorization in the human cortex,"Human brain has developed mechanisms to efficiently decode sensory information according to perceptual categories of high prevalence in the environment, such as faces, symbols, objects. neural activity produced within localized brain networks has been associated with the process that integrates both sensory bottom-up and cognitive top-down information processing. Yet, how specifically the different types and components of neural responses reflect the local networks’ selectivity for categorical information processing is still unknown. In this work we train Random Forest classification models to decode eight perceptual categories from broad spectrum of human intracranial signals (4–150 Hz, 100 subjects) obtained during a visual perception task. We then analyze which of the spectral features the algorithm deemed relevant to the perceptual decoding and gain the insights into which parts of the recorded activity are actually characteristic of the visual categorization process in the human brain. We show that network selectivity for a single or multiple categories in sensory and non-sensory cortices is related to specific patterns of power increases and decreases in both low (4–50 Hz) and high (50–150 Hz) frequency bands. By focusing on task-relevant neural activity and separating it into dissociated anatomical and spectrotemporal groups we uncover spectral signatures that characterize neural mechanisms of visual category perception in human brain that have not yet been reported in the literature.", 
"Millimeter-wave (mmWave) radar is widely used in vehicles for applications such as adaptive cruise control and collision avoidance. We propose an IEEE 802.11adbased radar for long-range radar (LRR) applications at the 60GHz unlicensed band. We exploit the preamble of a single-carrier physical layer frame, which consists of Golay complementary sequences with good correlation properties.",IEEE 802.11ad-based Radar An Approach to Joint Vehicular Communication-Radar System,"Millimeter-wave (mmWave) radar is widely used in vehicles for applications such as adaptive cruise control and collision avoidance. In this paper, we propose an IEEE 802.11adbased radar for long-range radar (LRR) applications at the 60 GHz unlicensed band. We exploit the preamble of a single-carrier physical layer frame, which consists of Golay complementary sequences with good correlation properties that make it suitable for radar. This system enables a joint waveform for automotive radar and a potential mmWave vehicular communication system based on the mmWave consumer wireless local area network standard, allowing hardware reuse. To formulate an integrated framework of vehicle-to-vehicle communication and LRR, we make typical assumptions for LRR applications, incorporating the full duplex radar operation. This new feature is motivated by the recent development of systems with sufficient isolation and self-interference cancellation. We develop single- and multi-frame radar receiver algorithms for target detection as well as range and velocity estimation for both single and multi-target scenarios. Our proposed radar processing algorithms leverage channel estimation and time-frequency synchronization techniques used in a conventional IEEE 802.11ad receiver with minimal modifications. Analysis and simulations show that in a single-target scenario, a gigabits-per-second data rate is achieved simultaneously with cm-level range accuracy and cm/s-level velocity accuracy. The target vehicle is detected with a high probability (above 99.99%) at a low false alarm rate of 10?6 for an equivalent isotropically radiated power of 40 dBm up to a vehicle separation distance of about 200 m. The proposed IEEE 802.11ad-based radar meets the minimum accuracy/resolution requirement of range and velocity estimates for LRR applications.", 
Dedicated short-range communications (DSRCs) and cellular V2X (C-V2X) are two present-day technologies. These RATs fall short of supporting communication requirements of many advanced vehicular applications. Both the DSRC and C-V 2X are undergoing extensive enhancements to support advanced applications. These can supplement today's vehicular sensors in enabling autonomous driving.,IEEE 802.11bd & 5G NR V2X Evolution of Radio Access Technologies for V2X Communications,"With the rising interest in autonomous vehicles, developing radio access technologies (RATs) that enable reliable and low-latency vehicular communications has become of paramount importance. Dedicated short-range communications (DSRCs) and cellular V2X (C-V2X) are two present-day technologies that are capable of supporting day-1 vehicular applications. However, these RATs fall short of supporting communication requirements of many advanced vehicular applications, which are believed to be critical in enabling fully autonomous vehicles. Both the DSRC and C-V2X are undergoing extensive enhancements in order to support advanced vehicular applications that are characterized by high reliability, low latency, and high throughput requirements. These RAT evolutions—the IEEE 802.11bd for the DSRC and NR V2X for C-V2X—can supplement today’s vehicular sensors in enabling autonomous driving. In this paper, we survey the latest developments in the standardization of 802.11bd and NR V2X. We begin with a brief description of the two present-day vehicular RATs. In doing so, we highlight their inability to guarantee the quality of service requirements of many advanced vehicular applications. We then look at the two RAT evolutions, i.e., the IEEE 802.11bd and NR V2X, outline their objectives, describe their salient features, and provide an in-depth description of key mechanisms that enable these features. While both, the IEEE 802.11bd and NR V2X, are in their initial stages of development, we shed light on their preliminary performance projections and compare and contrast the two evolutionary RATs with their respective predecessors.", 
Automatic text summarization is an active research field. This paper presents an automatic extractive Arabic summarization system. The system does not require learning and employs Rhetorical Structure Theory and a sentence scoring scheme.,Ikhtasir - A user selected compression ratio Arabic text summarization system,"Automatic text summarization is an active research field. The rapid growth of the Web, and the associated information overloading, has injected new life into this research area. In certain languages there has been plenty of research in automatic text summarization. Arabic is not one of them. In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the summary. The system does not require learning and employs Rhetorical Structure Theory (RST) along with a sentence scoring scheme, where individual sentences are scored. For output, sentences are selected with an objective of maximizing the overall score of the summary whose size is within the user selected compression ratio. For evaluation, system generated summaries of various lengths were compared against those performed by a professional human. Experiments on sample texts show our system outperforms some of the other existing systems including those that require learning.", 
"Ilastik is an easy-to-use interactive tool that brings machine-learning-based (bio)image analysis to end users. It contains pre-defined workflows for image segmentation, object classification, counting and tracking. Users adapt the workflows to the problem at hand by interactively providing sparse training annotations.",ilastik interactive machine learning for (bio)image analysis,"We present ilastik, an easy-to-use interactive tool that brings machine-learning-based (bio)image analysis to end users without substantial computational expertise. It contains pre-defined workflows for image segmentation, object classification, counting and tracking. Users adapt the workflows to the problem at hand by interactively providing sparse training annotations for a nonlinear classifier. ilastik can process data in up to five dimensions (3D, time and number of channels). Its computational back end runs operations on-demand wherever possible, allowing for interactive prediction on data larger than RAM. Once the classifiers are trained, ilastik workflows can be applied to new data from the command line without further user interaction. We describe all ilastik workflows in detail, including three case studies and a discussion on the expected performance.", 
"Estimating geographic information from an image is an excellent, difficult high-level computer vision problem. We represent the estimated image location as a probability distribution over the Earth's surface. We show that geolocation estimates can provide the basis for numerous other image understanding tasks.",IM2GPS estimating geographic information from a single image,"Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally – on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we will leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earth’s surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification.", 
"Computer vision and machine learning methods were applied to the challenge of automatic microstructure recognition. Two classification tasks were completed, and involved distinguishing between micrographs that depict dendritic morphologies from those that do not contain this particular microstructural feature. Results demonstrate that deep learning algorithms can successfully be applied to micrograph recognition tasks. This work is a step towards applying these established methods to more sophisticated materials recognition or characterization tasks.",Image driven machine learning methods for microstructure recognition,"Computer vision and machine learning methods were applied to the challenge of automatic microstructure recognition. Here, a case study on dendritic morphologies was performed. Two classification tasks were completed, and involved distinguishing between micrographs that depict dendritic morphologies from those that do not contain this particular microstructural feature (Task 1), and from those micrographs identified as depicting dendrites, different cross-sectional views (longitudinal or transverse) were identified (Task 2). Data sets were comprised of images taken over a range of magnifications, from materials with different compositions and varying orientations of microstructural features. Feature extraction and dimensionality reduction were performed prior to training machine learning algorithms to classify microstructural image data. Visual bag of words, texture and shape statistics, and pretrained convolutional neural networks (deep learning algorithms) were used for feature extraction. Classification was then performed using support vector machine, voting, nearest neighbors, and random forest models. For each model, classification was completed using full (original size) and reduced feature vectors for each feature extraction method tested. Performance comparisons were done to evaluate all possible combinations of feature extraction, selection, and classifiers for the task of micrograph classification. Results demonstrate that pre-trained neural networks represent microstructure image data well, and when used for feature extraction yield the highest classification accuracies for the majority of classifier and feature selection methods tested. Thus, deep learning algorithms can successfully be applied to micrograph recognition tasks. Maximum classification accuracies of 91.85  4.25% and 97.37  3.33% for Tasks 1 and 2 respectively, were achieved. This work is a broad investigation of computer vision and machine learning methods that acts as a step towards applying these established methods to more sophisticated materials recognition or characterization tasks. The approach presented here could offer improvements over established stereological measurements by removing the requirement of expert knowledge (bias) for interpretation of image data prior to characterization.", 
"ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers.",ImageNet A Large-Scale Hierarchical Image Database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.", 
"ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers.",ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", 
"ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and advances in object recognition that have been possible as a result.",ImageNet Large Scale Visual Recognition Challenge,"The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.", 
"Detecting epileptic EEG signal automatically and accurately is significant in evaluating patients with epilepsy. In this study, the immune clonal algorithm (ICA) is employed to perform automatic feature selection. The classification accuracy based on selected features is significantly higher than that on original features.",IMMUNE CLONAL ALGORITHM BASED FEATURE SELECTION FOR EPILEPTIC EEG SIGNAL CLASSIFICATION,"Detecting epileptic EEG signal automatically and accurately is significant in evaluating patients with epilepsy. In this study, the immune clonal algorithm (ICA) is employed to perform automatic feature selection, reducing the number of features the classifier deals with and improving the classification accuracy. In the experiment, EEG signal was decomposed into five sub-band components by a discrete wavelet transform. Features were extracted as input to train three classifiers (NB, SVM, KNN and LDA) to judge whether the EEG signal was epileptic or not. Then, ICA was introduced to select a feature subset to train the classifiers. Experimental results show that the classification accuracy based on selected features is significantly higher than that on original features. We also analyzed the relative importance of each feature.", 
The article provides a general analysis of the IEEE 802.15.7 standard for optical communications using visible light. It discusses how the standard's specifications apply to visible light communication use in vehicular networking applications.,Impact of IEEE 802.15.7 Standard on Visible Light Communications Usage in Automotive Applications,"As the demand for wireless communication technologies is exponentially increasing, using the visible light electromagnetic spectral region is a logical option. This article provides a general analysis of the IEEE 802.15.7 standard for optical communications using visible light, focusing on the PHY I, intended for outdoor applications. Moreover, the article discusses how the standard’s specifications apply to visible light communication use in vehicular networking applications. The article concentrates on the standard’s applicability in this domain, and points out the fact that it has not been widely accepted, as most of the VLC developers are focused on decreasing system complexity and implementation costs, rather than complying with the standard’s requirements. Furthermore, the article points out the necessity of a novel vehicular applications VLC specific standard, which should focus on the requirements imposed in this specific area.", 
"Classifying Arabic documents is lagging behind similar works in other languages. In this paper, we present seven deep learning-based algorithms to classify the Arabic documents. Attention mechanism and Bidirectional learning gave outstanding performance with Arabic text categorization. Our best performance is F-score D 97:96%, achieved using the Att-GRU model with stem-based algorithm with no stemming. For Word2Vec, both skip-gram and bag-of-words (CBOW) perform well with either stemming strategies.",Impact of Stemming and Word Embedding on Deep Learning-Based Arabic Text Categorization,"Document classification is a classical problem in information retrieval, and plays an important role in a variety of applications. Automatic document classification can be defined as content-based assignment of one or more predefined categories to documents. Many algorithms have been proposed and implemented to solve this problem in general, however, classifying Arabic documents is lagging behind similar works in other languages. In this paper, we present seven deep learning-based algorithms to classify the Arabic documents. These are: Convolutional Neural Network (CNN), CNN-LSTM (LSTM D Long Short-Term Memory), CNN-GRU (GRU D Gated Recurrent Units), BiLSTM (Bidirectional LSTM), BiGRU, Att-LSTM (Attention-based LSTM), and Att-GRU. And for word representation, we applied the word embedding technique (Word2Vec). We tested our approach on two large datasets–with six and eight categories–using ten-fold cross-validation. Our objective was to study how the classification is affected by the stemming strategies and word embedding. First, we looked into the effects of different stemming algorithms on the document classification with different deep learning models. We experimented with eleven different stemming algorithms, broadly falling into: root-based and stem-based, and no stemming. We performed ANOVA test on the classification results using the different stemmers, which helps assure if the results are significant. The results of our study indicate that stem-based algorithms perform slightly better compared to root-based algorithms. Among the deep learning models, the Attention mechanism and the Bidirectional learning gave outstanding performance with Arabic text categorization. Our best performance is F-score D 97:96%, achieved using the Att-GRU model with stem-based algorithm. Next, we looked into different controlling parameters for word embedding. For Word2Vec, both skip-gram and bag-of-words (CBOW) perform well with either stemming strategies. However, when using a stem-based algorithm, skip-gram achieves good results with a vector of smaller dimension, while CBOW requires a larger dimension vector to achieve a similar performance.", 
Stemming is a process of reducing inflected words to their stem or root from a generally written word form. Arabic text summarization has increasingly become an important task in natural language processing area (NLP).,Impact of stemming on Arabic text summarization,"Stemming is a process of reducing inflected words to their stem or root from a generally written word form. This process is used in many text mining application as a feature selection technique. Moreover, Arabic text summarization has increasingly become an important task in natural language processing area (NLP). Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. Khoja, Larekey and Alkhalil’s stemmer) on the text summarization performance for Arabic language. The evaluation of the proposed system, with the three different stemmers and without stemming, on the dataset used shows that the best performance was achieved by Khoja stemmer in term of recall, precision and F1-measure. The evaluation also shows that the performances of the proposed system are significantly improved by applying the stemming process in the pre-processing stage.", 
Automated software testing is a critical enabler for modern software development. It is of high importance that impediments related to test automation are prevented and removed quickly. We have performed a systematic literature review of reported impediments.,Impediments for software test automation A systematic literature review,"Automated software testing is a critical enabler for modern software development, where rapid feedback on the product quality is expected. To make the testing work well, it is of high importance that impediments related to test automation are prevented and removed quickly. An enabling factor for all types of improvement is to understand the nature of what is to be improved. We have performed a systematic literature review of reported impediments related to software test automation to contribute to this understanding. In this paper, we present the results from the systematic literature review: The list of identified publications, a categorization of identified impediments, and a qualitative discussion of the impediments proposing a socio-technical system model of the use and implementation of test automation.", 
"Three approaches to text summarization are based on semantic nets, fuzzy logic and evolutionary programming respectively. Semantic net approach performs better than the MS Word summarizer as far as the semantics of the original text was concerned. The second approach based on fuzzy logic results in an efficient system since fuzzy logic mimics decision making of humans. Third system showed promising results in precision and F-measure than all the other approaches.",Implementation and evaluation of evolutionary connectionist approaches to automated text summarization,"Text summarization takes care of choosing the most significant portions of text and generates coherent summaries that express the main intent of the given document. This study aims to compare the performances of the three text summarization systems developed by the authors with some of the existing Summarization systems available. These three approaches to text summarization are based on semantic nets, fuzzy logic and evolutionary programming respectively. All the three represent approaches to achieve connectionism. First approach performs Part of Speech (POS) tagging, semantic and pragmatic analysis and cohesion. The second system under discussion was a new extraction based automated system for text summarization using a decision module that employs fuzzy concepts. Third system under consideration was based on a combination of evolutionary, fuzzy and connectionist techniques. Semantic net approach performs better than the MS Word summarizer as far as the semantics of the original text was concerned. To compare our summaries with those of the well known MS Word, Intellexer and Copernic summarizers, we use DUC’s human generated summaries as the bench-mark. The results were very encouraging. The second approach based on fuzzy logic results in an efficient system since fuzzy logic mimics decision making of humans. Third system showed promising results as far as precision and F-measure are concerned than all the other approaches. Our first approach used WordNet, a lexical database for English. Unlike other dictionaries, WordNet does not include information about etymology, pronunciation and the forms of irregular verbs and contains only limited information about usage. To overcome this limitation, we developed a new text summarizer based on fuzzy logic. As Text summarization application requires learning ability based on activation, we utilize ANN attribute through a connectionist model to achieve the best results.", 
The use of technology makes information spread more quickly compared to conventional methods. The obstacle in exchanging information is the problem of information / data security. The system is designed using the Linear Congruential Generator (LCG) algorithm and the Most Significant Bit (MSB) as a security medium.,"Implementation of Linear Congruential Generator (LCG) Algorithm, Most Significant Bit (MSB) and Fibonacci Code in Compression and Security Messages Using Images","The use of technology makes information spread more quickly compared to conventional methods that humans used before they knew technology. However, there is information that is only intended for certain people or institutions because the information is very confidential. The obstacle in exchanging information is the problem of information / data security. Because of that, an electronic message security algorithm which is quite welldeveloped is cryptographic (cryptography) and steganographic (steganography) algorithms. The development of increasingly advanced technology and the addition of more and more computer users also cause data to accumulate and transfer data from one device to another. The data is generally compressed first so that the exchange process does no take too long. Therefore, we need a system that can secure messages while reducing image data to be more efficient in the process of transmission and delivery. To implement this problem the system is designed using the Linear Congruential Generator (LCG) algorithm and the Most Significant Bit (MSB) as a security medium and using the Fibonacci Code algorithm as compression of the image. In implementation the system has a weakness in compressing images that not dominate colors because the size of the data becomes larger. In addition, the compression ratio on the variation of text length has decreased and has increased in the image variation experiments where if the compression ratio decreases, indicating better compression.", 
"Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. However, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. We focus on the relatively mature methods of support vector machines, single decision trees and artificial neural networks.",Implementation of machine learning classification in remote sensing an applied review,"Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.", 
This paper uses a semantic technique by adopting a Rhetorical Structure Theory (RST) for summarization purpose. The quality of RST summarization suffers when dealing with large documents. This paper proposes a new hybrid summarization model for Arabic text which mingles two sub-models.,Improve the Automatic Summarization of Arabic Text Depending on Rhetorical Structure Theory,"This paper uses a semantic technique by adopting a Rhetorical Structure Theory (RST) for summarization purpose, to discover the most significant paragraphs based on functional and semantic criteria. However, the quality of RST summarization suffers when dealing with large documents. This paper proposes a new hybrid summarization model for Arabic text, which mingles two sub-models: The first sub-model produces a primary summary by using Rhetorical Structure Theory for identifying a range of the most significant parts of the text (the nucleus). Then the second sub-model ranks the significant parts in the primary rhetorical-summary based on the cosine similarity feature. To evaluate the proposed model, a prototype was developed on a range of articles, which have been classified into three groups different in size. The final output summary was evaluated in relation to its manual counterpart. In terms of enhancement of the rhetorical-summary precision, the experiment shows that proposed model HSM average precision is 71.6%, superior over the primary rhetorical-summary precision 56.3%.", 
Automatic code summarization is a rapidly expanding research area. Strong consensus is developing that using structural information as input leads to improved performance. We present an approach that uses a graph-based neural architecture that better matches the default structure of the AST to generate summaries.,Improved Code Summarization via a Graph Neural Network,"Automatic source code summarization is the task of generating natural language descriptions for source code. Automatic code summarization is a rapidly expanding research area, especially as the community has taken greater advantage of advances in neural network and AI technologies. In general, source code summarization techniques use the source code as input and outputs a natural language description. Yet a strong consensus is developing that using structural information as input leads to improved performance. The first approaches to use structural information ?attened the AST into a sequence. Recently, more complex approaches based on random AST paths or graph neural networks have improved on the models using ?attened ASTs. However, the literature still does not describe the using a graph neural network together with source code sequence as separate inputs to a model. Therefore, in this paper, we present an approach that uses a graph-based neural architecture that better matches the default structure of the AST to generate these summaries. We evaluate our technique using a data set of 2.1 million Java method-comment pairs and show improvement over four baseline techniques, two from the software engineering literature, and two from machine learning literature.", 
"Feature extraction for automatic classification of EEG signals typically relies on time frequency representations of the signal. Techniques such as cepstral-based filter banks or wavelets are popular analysis techniques in many signal processing applications including EEG classification. We demonstrate that a variant of a standard filter bank-based approach, coupled with first and second derivatives, provides a substantial reduction in the overall error rate.",Improved EEG Event Classification Using Differential Energy,"Feature extraction for automatic classification of EEG signals typically relies on time frequency representations of the signal. Techniques such as cepstral-based filter banks or wavelets are popular analysis techniques in many signal processing applications including EEG classification. In this paper, we present a comparison of a variety of approaches to estimating and postprocessing features. To further aid in discrimination of periodic signals from aperiodic signals, we add a differential energy term. We evaluate our approaches on the TUH EEG Corpus, which is the largest publicly available EEG corpus and an exceedingly challenging task due to the clinical nature of the data. We demonstrate that a variant of a standard filter bank-based approach, coupled with first and second derivatives, provides a substantial reduction in the overall error rate. The combination of differential energy and derivatives produces a 24% absolute reduction in the error rate and improves our ability to discriminate between signal events and background noise. This relatively simple approach proves to be comparable to other popular feature extraction approaches such as wavelets, but is much more computationally efficient.", 
Automatic text summarization is to compress the larger original text into shorter text. Summarization by extraction involves identifying important features and extracting sentences based on their scores. In this paper fuzzy logic method is proposed for improvement in the extraction of summary sentences.,Improvement of text summarization using fuzzy logic based method,"Automatic text summarization is undergoing wide research and gaining importance as the availability of online information is increasing. Automatic text summarization is to compress the larger original text into shorter text called as summary. Abstraction and Extraction are the two main methods to carry out text summarization. Our approach uses extractive method. Summarization by extraction involves identifying important features and extracting sentences based on their scores. 30 documents from news based URLs are used as input. After preprocessing the input document, eight features are used to calculate their score for each sentence. In this paper fuzzy logic method is proposed for improvement in the extraction of summary sentences.", 
Text summarization aims to shorten long text documents into a human readable form that contains the most important facts. The level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries.,Improving Abstraction in Text Summarization,"Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.", 
"A high-quality summarization system usually depends on strong encoder which can refine important information. In this paper, we propose an aggregation mechanism based on the Transformer model to address the challenge of long text representation. Our model can review history information to make encoder hold more memory capacity.",Improving Abstractive Text Summarization with History Aggregation,"Recent neural sequence to sequence models have provided feasible solutions for abstractive summarization. However, such models are still hard to tackle long text dependency in the summarization task. A high-quality summarization system usually depends on strong encoder which can refine important information from long input texts so that the decoder can generate salient summaries from the encoder’s memory. In this paper, we propose an aggregation mechanism based on the Transformer model to address the challenge of long text representation. Our model can review history information to make encoder hold more memory capacity. Empirically, we apply our aggregation mechanism to the Transformer model and experiment on CNN/DailyMail dataset to achieve higher quality summaries compared to several strong baseline models on the ROUGE metrics.", 
This paper presents two different approaches to automatic captioning of geo-tagged images. The graph-based method uses text cohesion techniques to identify information relevant to a location. ,Improving Automatic Image Captioning Using Text Summarization Techniques,This paper presents two different approaches to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image’s location: a graph-based and a statistical-based approach. The graph-based method uses text cohesion techniques to identify information relevant to a location. The statistical-based technique relies on different word or noun phrases frequency counting for identifying pieces of information relevant to a location. Our results show that summaries generated using these two approaches lead indeed to higher ROUGE scores than n-gram language models reported in previous work., 
"Most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework. The actor network provides the confidence of predicting the next word according to current state. The critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations.",Improving Automatic Source Code Summarization via Deep Reinforcement Learning,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, su?ering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the e?ectiveness of our proposed model when compared with some state-of-the-art methods.", 
Text summarization is the process of reducing the size of a text document to create a summary. It can be applied to summarize the original document by decreasing the importance or removing part of the content. This paper shows that it can improve the performance of classical text clustering algorithms.,Improving Clustering Quality by Automatic Text Summarization,"Automatic text summarization is the process of reducing the size of a text document, to create a summary that retains the most important points of the original document. It can thus be applied to summarize the original document by decreasing the importance or removing part of the content. The contribution of this paper in this field is twofold. First we show that text summarization can improve the performance of classical text clustering algorithms, in particular by reducing noise coming from long documents that can negatively affect clustering results. Moreover, the clustering quality can be used to quantitatively evaluate different summarization methods. In this regards, we propose a new graph-based summarization technique for keyphrase extraction, and use the Classic4 and BBC NEWS datasets to evaluate the improvement in clustering quality obtained using text summarization.", 
"This paper presents an improvement of classification performance for electroencephalography-based driver fatigue classification. The system employs autoregressive (AR) modeling as the features extraction algorithm and sparse-deep belief networks as the classification algorithm. Compared to other classifiers, sparse-DBN is a semi supervised learning method which combines unsupervised learning for modeling features in the pre-training layer and supervised learning for classification in the following layer.",Improving EEG-Based Driver Fatigue Classification Using Sparse-Deep Belief Networks,"This paper presents an improvement of classification performance for electroencephalography (EEG)-based driver fatigue classification between fatigue and alert states with the data collected from 43 participants. The system employs autoregressive (AR) modeling as the features extraction algorithm, and sparse-deep belief networks (sparse-DBN) as the classification algorithm. Compared to other classifiers, sparse-DBN is a semi supervised learning method which combines unsupervised learning for modeling features in the pre-training layer and supervised learning for classification in the following layer. The sparsity in sparse-DBN is achieved with a regularization term that penalizes a deviation of the expected activation of hidden units from a fixed low-level prevents the network from overfitting and is able to learn low-level structures as well as high-level structures. For comparison, the artificial neural networks (ANN), Bayesian neural networks (BNN), and original deep belief networks (DBN) classifiers are used. The classification results show that using AR feature extractor and DBN classifiers, the classification performance achieves an improved classification performance with a of sensitivity of 90.8%, a specificity of 90.4%, an accuracy of 90.6%, and an area under the receiver operating curve (AUROC) of 0.94 compared to ANN (sensitivity at 80.8%, specificity at 77.8%, accuracy at 79.3% with AUC-ROC of 0.83) and BNN classifiers (sensitivity at 84.3%, specificity at 83%, accuracy at 83.6% with AUROC of 0.87). Using the sparse-DBN classifier, the classification performance improved further with sensitivity of 93.9%, a specificity of 92.3%, and an accuracy of 93.1% with AUROC of 0.96. Overall, the sparse-DBN classifier improved accuracy by 13.8, 9.5, and 2.5% over ANN, BNN, and DBN classifiers, respectively.", 
Multi document summarization is a process to produce a single summary from a set of related documents. The performance of a summarization system heavily depends on the sentence similarity measure used. This paper presents an enhanced method for computing sentence similarity aiming for improving multi-document summarization performance.,Improving graph based multidocument text summarization using an enhanced sentence similarity measure,"Multi document summarization is a process to produce a single summary from a set of related documents collected from heterogeneous sources. Since the documents may contain redundant information, the performance of a multi document summarization system heavily depends on the sentence similarity measure used for removing redundant sentences from the summary. For graph based multi document summarization where existence of an edge between a pair of sentences is determined based on how much two sentences are similar to each other, the sentence similarity measure also plays an important role. This paper presents an enhanced method for computing sentence similarity aiming for improving multi-document summarization performance. Experiments using two different datasets show the effectiveness of the proposed sentence similarity measure in improving the performance of a graph based multi-document summarization system.", 
"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task.",Improving language understanding by generative pre-training,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).", 
"Pointer Generators have been the de facto standard for modern summarization systems. This architecture faces two major drawbacks: the pointer is limited to copying the exact words and ignoring possible inflections or abstractions. In this paper, we address these problems by allowing the model to ""edit"" pointed tokens instead of always hard copying them.",Improving latent alignment in text summarization by generalizing the pointer generator,"Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks: Firstly, the pointer is limited to copying the exact words while ignoring possible inflections or abstractions, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the model to “edit” pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned relation embedding. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higherquality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.", 
Multi-document summarization has reached its bottleneck due to lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. TCSum can achieve the state-of-the-art performance without using any hand-crafted features.,Improving Multi-Document Summarization via Text Classification,"Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.", 
Text summarization technique deals with the compression of large document into shorter version of text. Extraction based text summarization involves selecting sentences of high relevance (rank) from the document based on word and sentence features. This is modeled using Fuzzy Inference System. The summary of the document is created based upon the level of importance of the sentences in the document.,Improving Performance of Text Summarization,"Today, the tremendous information is available on the internet; it is difficult to get the information fast and most efficiently. There are so many text materials available on the internet, in order to extract the most relevant information from it, we need a good mechanism. Text summarization technique deals with the compression of large document into shorter version of text. Text summarizations choose the most significant part of text and create coherent summaries that state the main purpose of the given document. Extraction based text summarization involves selecting sentences of high relevance (rank) from the document based on word and sentence features and put them together to generate summary. This is modeled using Fuzzy Inference System. The summary of the document is created based upon the level of the importance of the sentences in the document. This paper focuses on the Fuzzy logic Extraction approach for text summarization and the semantic approach of text summarization using Latent Semantic Analysis.",  
Sentence compression is a valuable task in the framework of text summarization. We propose a new method that used Grid Model and dynamic programming to calculate n-grams for generating the best sentence compression.,Improving Quality of Vietnamese Text Summarization Based on Sentence Compression,"Sentence compression is a valuable task in the framework of text summarization. In previous works, the sentence is reduced by removing redundant words or phrases from original sentence and tries to remain information. In this paper, we propose a new method that used Grid Model and dynamic programming to calculate n-grams for generating the best sentence compression. These reduced sentences are combined to text summarization. The experimental results showed that our method really effective and the text is grammatically, coherence and concise.", 
Current Chinese social media text summarization models are based on an encoder-decoder framework. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. experiments show that the proposed model outperforms baseline systems on a social media corpus.,Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization,"Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.", 
The problem of extractive text summarization for a collection of documents is defined as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved. We consider two summarization models–supervised and unsupervised–enhanced by integrating the topic knowledge into their standard lexicon-based models. experimental results show that utilizing topic knowledge improves the summarization quality.,Improving Summarization Quality with Topic Modeling,"The problem of extractive text summarization for a collection of documents is defined as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved in the best possible way. In this paper we describe different applications of topic modeling as it relates to summarization. We consider two summarization models–supervised and unsupervised–enhanced by integrating the topic knowledge into their standard lexicon-based models. Both summarizers strive to cover as much information of the input documents as possible when generating the summaries. The supervised summarizer operates in a standard rank-and-select-sentences manner of extractive summarization, where the best linear combination of multiple sentence features is learned by a genetic algorithm (GA). The unsupervised summarizer models the summarization task as an optimization problem. As is the case with most existing summarization approaches, both original models measure information coverage by lexical units and were enriched by topic knowledge, providing a new measure for the informaition coverage. The experimental results show that utilizing topic knowledge improves the summarization quality.", 
This paper presents a novel hybrid approach for generating abstractive text summaries. It combines fuzzy logic rules with bidirectional long short-term memory (Bi-LSTM) to produce abstractive summary. The FLSTM model is compared with other state-of-the-art models and the empirical results suggested that the proposed FL STM model outperforms all other models.,Improving Text Summarization using Ensembled Approach based on Fuzzy with LSTM,"Abstractive text summarization using attentional recurrent neural network (sequence-to-sequence) models have proven to be very effective. In this paper, a novel hybrid approach is presented for generating abstractive text summaries by combining fuzzy logic rules (which selects extractive sentences) with bidirectional long short-term memory (Bi-LSTM) which further produces abstractive summary. Bi-LSTM uses attention mechanism and Adam optimizer for updating network weights. The proposed approach utilizes fuzzy measures and inference to extract textual information from the document to find the most relevant sentences. These relevant sentences are given as input to Bi-LSTM to produce an abstractive summary of the significant sentences. The proposed FLSTM model is evaluated using ROUGE toolkit. The experiment is performed on standard datasets (i.e., DUC and CNN/daily mail). Another salient feature of this work is merging of DUC 2003–2004, DUC 2006–2007 datasets to generate a larger dataset to achieve better results. The FLSTM model is compared with other state-of-the-art models, and the empirical results suggested that the proposed FLSTM model outperforms all other models.", 
"In today's digital era, it becomes a challenge for netizens to find specific information on the internet. Many web-based documents are retrieved and it is not easy to digest all the retrieved information. Automatic text summarization is a process that identifies the important points from all the related documents.",Improving text summarization using neuro-fuzzy approach,"In today’s digital era, it becomes a challenge for netizens to find specific information on the internet. Many web-based documents are retrieved and it is not easy to digest all the retrieved information. Automatic text summarization is a process that identifies the important points from all the related documents to produce a concise summary. In this paper, we propose a text summarization model based on classification using neuro-fuzzy approach. The model can be trained to filter high-quality summary sentences. We then compare the performance of our proposed model with the existing approaches, which are based on fuzzy logic and neural network techniques. ANFIS showed improved results compared to the previous techniques in terms of average precision, recall and F-measure on the Document Understanding Conference (DUC) data corpus.", 
Text Summarization and categorization have always been two of the most demanding information retrieval tasks. We present the keyword extraction techniques to explore the effects that part of speech tagging.,Improving Text Summarization Using Noun Retrieval Techniques,"Text Summarization and categorization have always been two of the most demanding information retrieval tasks. Deploying a generalized, multifunctional mechanism that produces good results for both of the aforementioned tasks seems to be a panacea for most of the text-based, information retrieval needs. In this paper, we present the keyword extraction techniques, exploring the effects that part of speech tagging has on the summarization procedure of an existing system.", 
We introduce a supervised model for predicting word importance. Our model is superior to prior approaches for identifying words used in human summaries.,Improving the estimation of word importance for news multi-document summarization,We introduce a supervised model for predicting word importance that incorporates a rich set of features. Our model is superior to prior approaches for identifying words used in human summaries. Moreover we show that an extractive summarizer using these estimates of word importance is comparable in automatic evaluation with the state-of-the-art., 
This paper proposes on extraction based system for Single /multi-document summarization. It uses combination of both LSA (Latent Semantic Analysis) and FL (Fuzzy Logic).,Improving the performance for single and multi-document text summarization via lsa & fl,"The automation of the process of summarizing documents assumes a basic part in numerous applications. Automatic Text Summarization has been deliberate on keep hold of the essential data without concerning the archive quality. This paper proposes on extraction based system for Single /multi-document summarization. It uses combination of both LSA (Latent Semantic Analysis) and FL (Fuzzy Logic) methods on fusion o f various features like Preprocessing, feature extraction, classification to generate better quality summary.",  
"Each document of the train set is summarized automatically and two approaches to text categorization are proposed. In the first approach, the text summarization is directly used for feature selection and categorization. The second approach, each summary is used to select and weight features.",Improving the Performance of Text Categorization Using Automatic Summarization,"In order to reduce the dimensionality of feature vector space and reduce the computing complexity of categorization, each document of the train set is summarized automatically and two approaches to text categorization based on these summaries are proposed: in the first approach, the text summarization is directly used for feature selection and categorization instead of the original text; in the second approach, each summary is used to select and weight features for each document, and free texts are classified using KNN algorithm. Experimental results show that the two proposed methods using automatic summarization can not only reduce the time of classifier training, but also improve the performance of text categorization.", 
Cardiovascular disease is a substantial cause of mortality and morbidity in the world. Data mining transforms huge amounts of raw data generated by the health industry into useful information. ETC outperforms other models and achieves 0.9262 accuracy value with SMOTE in prediction of heart patient's survival.,Improving the Prediction of Heart Failure Patients' Survival Using SMOTE and Effective Data Mining Techniques,"Cardiovascular disease is a substantial cause of mortality and morbidity in the world. In clinical data analytics, it is a great challenge to predict heart disease survivor. Data mining transforms huge amounts of raw data generated by the health industry into useful information that can help in making informed decisions. Various studies proved that significant features play a key role in improving performance of machine learning models. This study analyzes the heart failure survivors from the dataset of 299 patients admitted in hospital. The aim is to find significant features and effective data mining techniques that can boost the accuracy of cardiovascular patient’s survivor prediction. To predict patient’s survival, this study applies nine classification models: Decision Tree (DT), Adaptive boosting classifier (AdaBoost), Logistic Regression (LR), Stochastic Gradient classifier (SGD), Random Forest (RF), Gradient Boosting classifier (GBM), Extra Tree Classifier (ETC), Gaussian Naive Bayes classifier (G-NB) and Support Vector Machine (SVM). The imbalance class problem is handled by Synthetic Minority Oversampling Technique (SMOTE). Furthermore, machine learning models are trained on the highest ranked features selected by RF. The results are compared with those provided by machine learning algorithms using full set of features. Experimental results demonstrate that ETC outperforms other models and achieves 0.9262 accuracy value with SMOTE in prediction of heart patient’s survival.", 
"Extractive Summarization, extracts the most applicable sentences from the main document, while keeping the most vital information in the document. This paper introduces a hybrid graph based technique for single-document extractive summarization. The proposed method was evaluated using ROUGE measures on the dataset DUC2002. The effectiveness of this technique is measured using the ROUge score.",Improving triangle-graph based text summarization using hybrid similarity function,"Extractive Summarization, extracts the most applicable sentences from the main document, while keeping the most vital information in the document. The Graph-based techniques have become very popular for text summarisation. This paper introduces a hybrid graph based technique for single-document extractive summarization. Methods/Statistical Analysis: Prior research that utilised the graph-based approach for extractive summarisation deployed one function for computing the necessary summary. Nonetheless, in our work, we have recommended an innovative hybrid similarity function (H), for estimation purpose. This function hybridises four distinct similarity measures: cosine similarity (sim1), Jaccard similarity (sim2), word alignment-based similarity (sim3) and the window-based similarity measure (sim4). The method uses a trainable summarizer, which takes into account several features. The effect of these features on the summarization task is investigated. Findings: By combining, the traditional similarity measures (Cosine and Jaccard) with dynamic programming approaches (word alignment-based and the window-based) for calculating the similarity between two sentences, more common information were extracted and helped to find the best sentences to be extracted in the final summary. The proposed method was evaluated using ROUGE measures on the dataset DUC2002. The experimental results showed that specific combinations of features could give higher efficiency. It also showed that some features have more effect than others on the summary creation. Applications/Improvements: The performance of this new method has been tested using the DUC 2002 data set. The effectiveness of this technique is measured using the ROUGE score, and the results are promising when compared with some existing techniques.", 
Deep convolutional networks have been central to the largest advances in image recognition. Inception architecture has been shown to achieve very good performance at relatively low computational cost. Residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge.,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning","Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.", 
"In this paper, we address the problem of indoor semantic segmentation by incorporating the depth information into the convolutional neural network and conditional random field of a neural network architecture. Comparative experiments show that the proposed DFCN-DCRF architecture achieves competitive performance compared with state-of-the-art methods.",Incorporating Depth into both CNN and CRF for Indoor Semantic Segmentation,"In this paper, we address the problem of indoor semantic segmentation by incorporating the depth information into the convolutional neural network and conditional random field of a neural network architecture. The architecture combines a RGB-D fully convolutional neural network (DFCN) with a depth-sensitive fully-connected conditional random field (DCRF). In the DFCN module, the depth information is incorporated into the early layers using a fusion structure which is followed by several dilated convolution layers for contextual reasoning. Later in the DCRF module, a depth-sensitive fully-connected conditional random field (DCRF) is proposed and combined with the previous DFCN output to refine the preliminary result. Comparative experiments show that the proposed DFCN-DCRF architecture achieves competitive performance compared with state-of-the-art methods.", 
We have explored the usefulness of incorporating speech and discourse features in an automatic speech summarization system. We hypothesize that such a system can outperform solely text-based methods inherited from the field of text summarization.,Incorporating speaker and discourse features into speech summarization,"We have explored the usefulness of incorporating speech and discourse features in an automatic speech summarization system applied to meeting recordings from the ICSI Meetings corpus. By analyzing speaker activity, turn-taking and discourse cues, we hypothesize that such a system can outperform solely text-based methods inherited from the field of text summarization. The summarization methods are described, two evaluation methods are applied and compared, and the results clearly show that utilizing such features is advantageous and efficient. Even simple methods relying on discourse cues and speaker activity can outperform text summarization approaches.", 
"This paper focuses on the problem of short text summarization on the comment stream of a specific message from social network services (SNS) Due to the high popularity of SNS, the quantity of comments may increase at a high rate right after a social message is published. Since distinct users will request the summary at any moment, existing clustering methods cannot be directly applied.","IncreSTS, Towards Real-Time Incremental Short Text Summarization on Comment Streams from Social Network Services","This paper focuses on the problem of short text summarization on the comment stream of a specific message from social network services (SNS). Due to the high popularity of SNS, the quantity of comments may increase at a high rate right after a social message is published. Motivated by the fact that users may desire to get a brief understanding of a comment stream without reading the whole comment list, we attempt to group comments with similar content together and generate a concise opinion summary for this message. Since distinct users will request the summary at any moment, existing clustering methods cannot be directly applied and cannot meet the real-time need of this application. In this paper, we model a novel incremental clustering problem for comment stream summarization on SNS. Moreover, we propose IncreSTS algorithm that can incrementally update clustering results with latest incoming comments in real time. Furthermore, we design an at-a-glance visualization interface to help users easily and rapidly get an overview summary. From extensive experimental results and a real case demonstration, we verify that IncreSTS possesses the advantages of high efficiency, high scalability, and better handling outliers, which justifies the practicability of IncreSTS on the target problem.", 
Recurrent Neural Network (RNN) has experienced success in summarizing abstractive texts for English and Chinese texts. The Bidirectional Gated Recurrent Unit (BiGRU) RNN architecture is used so that the resulted summaries are influenced by the surrounding words.,Indonesian Abstractive Text Summarization Using Bidirectional Gated Recurrent Unit,"Abstractive text summarization is more challenging than the extractive one since it is performed by paraphrasing the entire contents of the text, which has a higher difficulty. But, it produces a more natural summary and higher inter-sentence cohesion. Recurrent Neural Network (RNN) has experienced success in summarizing abstractive texts for English and Chinese texts. The Bidirectional Gated Recurrent Unit (BiGRU) RNN architecture is used so that the resulted summaries are influenced by the surrounding words. In this research, such a method is applied for Bahasa Indonesia to improve the text summarizations those are commonly developed using some extractive methods with low inter-sentence cohesion. An evaluation on a dataset of Indonesian journal documents shows that the proposed model is capable of summarizing the overall contents of testing documents into some summaries with high similarities to the provided abstracts. The proposed model resulting success in understanding source text for generating summarization.", 
Automated Text Summarization (ATS) is a computer-based application to produce a summary from an article. The quality of summarization result depends on the type and the structure of the article. System will produce a good summary if article is an argumentation type.,Indonesian Automated Text Summarization,"Automated Text Summarization (ATS) is a computer-based application to produce a summary from an article but it still keeps an accurate main point from the content of the article. In this research, we build an Indonesian ATS application using a virtual graph concept, calculation weight of sentence and weight of relation among sentences, Deductive – Inductive method in Indonesian Language and Exhaustive Shortest Path Algorithm to provide a summarization path from the first sentence to the last sentence on every paragraph in the article. The test result shows that the quality of summarization result depends on the type and the structure of the article. System will produce a good summary if the type of the article is an argumentation type and the article's structure has many paragraphs in which each paragraph has more than two sentences.", 
"The majority of researches on automatic summarization techniques are applied to common languages such as Chinese and English. In this paper, we propose a new and efficient extraction-based automatic text summarization method based on sentence similarity clustering. This method not only ensures the integrity, criticality and importance of the summary, but also reduces the information redundancy.",Indonesian Automatic Text Summarization Based on A New Clustering Method in Sentence Level,"With the development of the Internet, the amount of information grows exponentially, and the automatic text summarization technology becomes more and more important. At present, the majority of researches on automatic summarization techniques are applied to common languages such as Chinese and English, but it is few in low resource language. In this paper, we constructed an automatic summary dataset of Indonesian language and conducts related research on Indonesian automatic abstracts. And in this paper, we propose a new and efficient extraction-based automatic text summarization method based on sentence similarity clustering. Based on the idea of clustering, this paper considers the semantics of sentences and we clusters sentences according to the similarity between sentences and sentences. According to the rules we extracts the abstracts and finally obtains the summarization results. This method not only ensures the integrity, criticality and importance of the summary, but also reduces the information redundancy of the summary. In the evaluation, our method achieved good results and exceeded all the baselines in the indexes of ?1 score of ROUGE-1,ROUGE-2,ROUGE-3.", 
"In this paper we present a system for generating summary by sentence extraction. We use text features to determine the weight of sentence. We also investigate the effect of semantic feature, using latent semantic analysis, on the task.",Indonesian Text Summarization based on Naïve Bayes Method,"In this paper we present a system for generating summary by sentence extraction. To determine the weight of sentence, we use text features, such as sentence position, sentence relative length, average term frequency, keyword extraction, key phrase extraction, sentence similarity to the title, sentence centrality, inclusion of numerical data, inclusion of entity name, and inclusion of news emphasize words. We also investigate the effect of semantic feature, using latent semantic analysis, on the summarization task. Our experiments show that semantic feature increases precision and F-measure by 9.8% and 2.4% respectively in case of 20% Compression Rate.", 
Scene categorization to indoor vs outdoor may be approached by using low-level features for inferring high-level information about the image. Bayesian network could be used to integrate knowledge from low- level and semantic features.,Indoor vs Outdoor Classification of Consumer Photographs Using Low-Level and Semantic Features,"Scene categorization to indoor vs outdoor may be approached by using low-level features for inferring high-level information about the image. Low-level features such as color and texture have been used extensively in image understanding research, however, they cannot solve the problem completely. In this paper, we propose the use of a Bayesian network for integrating knowledge from low-level and semantic features for indoor vs outdoor classification of images. Using ground truth data for sky and grass detection, we demonstrate that the classification performance can be significantly improved when semantic features are employed in the classification process.", 
"Indonesian text summarization is a challenging task in the NLP community. The problem is even worse for low- resource languages such as Indonesian. In this paper, we present INDOSUM, a new benchmark dataset. The dataset consists of news articles and manually constructed summaries.","Indosum, A New Benchmark Dataset for Indonesian Text Summarization","Automatic text summarization is generally considered as a challenging task in the NLP community. One of the challenges is the publicly available and large dataset that is relatively rare and difficult to construct. The problem is even worse for low-resource languages such as Indonesian. In this paper, we present INDOSUM, a new benchmark dataset for Indonesian text summarization. The dataset consists of news articles and manually constructed summaries. Notably, the dataset is almost 200x larger than the previous Indonesian summarization dataset of the same domain. We evaluated various extractive summarization approaches and obtained encouraging results which demonstrate the usefulness of the dataset and provide baselines for future research. The code and the dataset are available online under permissive licenses.", 
"Systems that could automatically summarize opinions would be immensely useful. This work is built upon past work of extractive summarization methods. It uses Telugu, a south Indian regional language, as the language of study.",Information extraction by an abstractive text summarization for an Indian regional language,"The Internet provides many sources of different opinions, expressed through user reviews of products, blogs, and forum discussions. Systems which could automatically summarize these opinions would be immensely useful for those who wish to use this information to make decisions. The previous work in automatic summarization has completely focused on extractive summarization, in which key sentences are identified from the source text and extracted to form the output. An alternative solution is abstractive summarization in which the information from the source text is first extracted into the form of abstract data which is then post processed to infer the most important message from the original text. This work is built upon past work of extractive summarization methods to create abstractive summaries by creating new sentences in it. This paper conveys the methodology for the abstractive summarization process and its evaluation considering Telugu, a south Indian regional language, as the language of study.", 
"Information Extraction is a method for filtering information from large volumes of text. It is a limited task than full text understanding. In this paper, a model for summarization from large documents using a novel approach has been proposed.",Information retrieval by text summarization for an Indian regional language,"The Information Extraction is a method for filtering information from large volumes of text. Information Extraction is a limited task than full text understanding. In full text understanding, we aspire to represent in an explicit fashion about all the information in a text. In contrast, in Information Extraction, we delimit in advance, as part of the specification of the task and the semantic range of the output. In this paper, a model for summarization from large documents using a novel approach has been proposed. Extending the work for an Indian regional language (Kannada) and various analyses of results were discussed.", 
Inside-Outside Net is an object detector that exploits information both inside and outside the region of interest. It uses skip pooling to extract information at multiple scales and levels of abstraction. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9%.,Inside-Outside Net Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,"It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won “Best Student Entry” and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.", 
Instagram and WhatsApp are two social media and networking services introduced in 2010. We present an overview of the various applications of Instagram and WhatsApp in health and healthcare.,Instagram and WhatsApp in Health and Healthcare An Overview,"Instagram and WhatsApp are two social media and networking services introduced in 2010. They are currently subsidiaries of Facebook, Inc., California, USA. Using evidence from the published literature and case reports indexed in PubMed and other sources, we present an overview of the various applications of Instagram and WhatsApp in health and healthcare. We also briefly describe the main issues surrounding the uses of these two apps in health and medicine.", 
We propose a new weight adjustment factor that is applied to a weighted support vector machine (SVM) as a weak learner of the AdaBoost algorithm. Numerical experiments show that the proposed method outperforms existing approaches in terms of F-measure and area under the receiver operating characteristic curve.,Instance categorization by support vector machines to adjust weights in AdaBoost for imbalanced data classification,"To address class imbalance in data, we propose a new weight adjustment factor that is applied to a weighted support vector machine (SVM) as a weak learner of the AdaBoost algorithm. Different factor scores are computed by categorizing instances based on the SVM margin and are assigned to related instances. The SVM margin is used to define borderline and noisy instances, and the factor scores are assigned to only borderline instances and positive noise. The adjustment factor is then employed as a multiplier to the instance weight in the AdaBoost algorithm when learning a weighted SVM. Using 10 real classimbalanced datasets, we compare the proposed method to a standard SVM and other SVMs combined with various sampling and boosting methods. Numerical experiments show that the proposed method outperforms existing approaches in terms of F-measure and area under the receiver operating characteristic curve, which means that the proposed method is useful for relaxing the class-imbalance problem by addressing well-known degradation issues such as overlap, small disjunct, and data shift problems.", 
"In brain-computer interface (BCI), variations across sessions/subjects result in differences in the properties of potential of the brain. This may lead to variations in feature distribution of electroencephalogram (EEG) across subjects, which greatly reduces the generalization ability of a classifier. We propose an instance transfer subject-independent (ITSD) framework combined with a convolutional neural network (CNN) to improve the classification accuracy of the model during motor imagery (MI) task.",Instance Transfer Subject-Dependent Strategy for Motor Imagery Signal Classification Using Deep Convolutional Neural Networks,"In the process of brain-computer interface (BCI), variations across sessions/subjects result in differences in the properties of potential of the brain. This issue may lead to variations in feature distribution of electroencephalogram (EEG) across subjects, which greatly reduces the generalization ability of a classifier. Although subject-dependent (SD) strategy provides a promising way to solve the problem of personalized classification, it cannot achieve expected performance due to the limitation of the amount of data especially for a deep neural network (DNN) classification model. Herein, we propose an instance transfer subject-independent (ITSD) framework combined with a convolutional neural network (CNN) to improve the classification accuracy of the model during motor imagery (MI) task. The proposed framework consists of the following steps. Firstly, an instance transfer learning based on the perceptive Hash algorithm is proposed to measure similarity of spectrogram EEG signals between different subjects. Then, we develop a CNN to decode these signals after instance transfer learning. Next, the performance of classifications by different training strategies (subject-independent- (SI-) CNN, SD-CNN, and ITSD-CNN) are compared. To verify the effectiveness of the algorithm, we evaluate it on the dataset of BCI competition IV-2b. Experiments show that the instance transfer learning can achieve positive instance transfer using a CNN classification model. Among the three different training strategies, the average classification accuracy of ITSD-CNN can achieve 94:7 ± 2:6 and obtain obvious improvement compared with a contrast model ðp < 0:01Þ. Compared with other methods proposed in previous research, the framework of ITSD-CNN outperforms the state-of-the-art classification methods with a mean kappa value of 0.664.", 
"This paper presents Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. Our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems.",Instance-aware Semantic Segmentation via Multi-task Network Cascades,"Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.", 
Proposed approach divides the network into several logical zones. The zone structure is formed virtually based on position information. Each logical zone may have a zone directory agent to manage registered services. simulation results show that the proposed architecture can provide a high average data hit ratio and low message overhead.,Integrated service discovery architecture for heterogeneous networks,"The discovery and management of desired network services present significant challenges for mobile networks. Based on the Service Location Protocol, this paper proposes an integrated service discovery architecture for vehicular ad hoc networks. The proposed approach divides the network into several logical zones. The zone structure is formed virtually based on position information. Each logical zone may have a zone directory agent to manage registered services from service providers. The proposed architecture considers both vehicle-to-infrastructure and vehicle-to-vehicle communication modes and introduces roadside directory agents and vehicle directory agents to reduce deployment costs. We also introduce a substitute query technique, cache mechanism, and backup mechanism to improve the request hit ratio and reduce the message overhead of the substitute query. Finally, we implement the proposed mechanisms in Network Simulator version 2, with simulation results showing that the proposed architecture can provide a high average data hit ratio and low message overhead.", 
Identifying intravenous immunoglobulin-resistant patients is essential for the prompt and optimal treatment of Kawasaki disease. Data-driven approaches have the potential to identify the high-risk individuals by capturing the complex patterns of real-world data. This study highlights the integration of co-clustering and supervised learning methods for incomplete clinical data mining.,Integrating Co-Clustering and Interpretable Machine Learning for the Prediction of Intravenous Immunoglobulin Resistance in Kawasaki Disease,"Identifying intravenous immunoglobulin-resistant patients is essential for the prompt and optimal treatment of Kawasaki disease, suggesting the need for effective risk assessment tools. Data-driven approaches have the potential to identify the high-risk individuals by capturing the complex patterns of real-world data. To enable clinically applicable prediction of intravenous immunoglobulin resistance addressing the incompleteness of clinical data and the lack of interpretability of machine learning models, a multistage method is developed by integrating data missing pattern mining and intelligible models. First, co-clustering is adopted to characterize the block-wise data missing patterns by simultaneously grouping the clinical features and patients to enable (a) group-based feature selection and missing data imputation and (b) patient subgroup-specific predictive models considering the availability of data. Second, feature selection is performed using the group Lasso to uncover group-specific risk factors. Third, the Explainable Boosting Machine, which is an interpretable learning method based on generalized additive models, is applied for the prediction of each patient subgroup. The experiments using real-world Electronic Health Records demonstrate the superior performance of the proposed framework for predictive modeling compared with a set of benchmark methods. This study highlights the integration of co-clustering and supervised learning methods for incomplete clinical data mining, and promotes data-driven approaches to investigate predictors and effective algorithms for decision making in healthcare.", 
"With the explosive growth of information on the Internet, it becomes more and more important to improve the efficiency of information acquisition. EA-LTS is a two- phase approach to long text summarization. In the extraction phase, it conceives a hybrid sentence similarity measure by combining sentence vector and Levenshtein distance.",Integrating Extractive and Abstractive Models for Long Text Summarization,"With the explosive growth of information on the Internet, it becomes more and more important to improve the efficiency of information acquisition. Automatic text summarization provides a good means for quick acquisition of information through compression and refinement. While existing methods for automatic text summarization achieve elegant performance on short sequences, however, they are facing the challenges of low efficiency and accuracy when dealing with long text. In this paper, we present a two-phase approach towards long text summarization, namely, EA-LTS. In the extraction phase, it conceives a hybrid sentence similarity measure by combining sentence vector and Levenshtein distance, and integrates it into graph model to extract key sentences. In the abstraction phase, it constructs a recurrent neural network based encoder-decoder, and devises pointer and attention mechanisms to generate summaries. We test our model on a real-life long text corpora, collected from sina.com; experimental results verify the accuracy and validity of the proposed method, which is demonstrated to be superior to state-of-the-art methods.", 
"Natural language processing (NLP) and machine learning (ML) techniques have shown to successfully extract insights from radiology reports. The codependency effects of NLP and ML in this context have not been well-studied. The best accuracy is achieved when both algorithms are optimized concurrently, authors say. The interplay between ML and NLP algorithms and their effect on interpretation accuracy is complex.",Integrating Natural Language Processing and Machine Learning Algorithms to Categorize Oncologic Response in Radiology Reports,"A significant volume of medical data remains unstructured. Natural language processing (NLP) and machine learning (ML) techniques have shown to successfully extract insights from radiology reports. However, the codependent effects of NLP and ML in this context have not been wellstudied. Between April 1, 2015 and November 1, 2016, 9418 cross-sectional abdomen/pelvis CT and MR examinations containing our internal structured reporting element for cancer were separated into four categories: Progression, Stable Disease, Improvement, or No Cancer. We combined each of three NLP techniques with five ML algorithms to predict the assigned label using the unstructured report text and compared the performance of each combination. The three NLP algorithms included term frequency-inverse document frequency (TF-IDF), term frequency weighting (TF), and 16-bit feature hashing. The ML algorithms included logistic regression (LR), random decision forest (RDF), one-vs-all support vector machine (SVM), one-vs-all Bayes point machine (BPM), and fully connected neural network (NN). The best-performing NLP model consisted of tokenized unigrams and bigrams with TF-IDF. Increasing N-gram length yielded little to no added benefit for most ML algorithms. With all parameters optimized, SVM had the best performance on the test dataset, with 90.6 average accuracy and F score of 0.813. The interplay between ML and NLP algorithms and their effect on interpretation accuracy is complex. The best accuracy is achieved when both algorithms are optimized concurrently.", 
"Speech contains additional information than text that can be valuable for automatic speech summarization. We evaluate how to effectively use acoustic/prosodic features for extractive meeting summarization, and how to integrate prosodic features with lexical and structural information for further improvement.",Integrating prosodic features in extractive meeting summarization,"Speech contains additional information than text that can be valuable for automatic speech summarization. In this paper, we evaluate how to effectively use acoustic/prosodic features for extractive meeting summarization, and how to integrate prosodic features with lexical and structural information for further improvement. To properly represent prosodic features, we propose different normalization methods based on speaker, topic, or local context information. Our experimental results show that using only the prosodic features we achieve better performance than using the non-prosodic information on both the human transcripts and recognition output. In addition, a decision-level combination of the prosodic and non-prosodic features yields further gain, outperforming the individual models.", 
"The commercial adaptation of VANET to achieve secure Intelligent Transportation System (ITS) heavily depends on the security guarantees for the end-users and consumers. With the emergence of 5th Generation (5G) networks, it is imperative to integrate 5G and vehicular networks. To achieve a seamless integration, various design and implementation issues must be addressed.",Integration of VANET and 5G Security A review of design and implementation issues,"The commercial adaptation of Vehicular Ad hoc NETwork (VANET) to achieve secure Intelligent Transportation System (ITS) heavily depends on the security guarantees for the end-users and consumers. Current VANET security standards address most of the security challenges faced by the vehicular networks. However, with the emergence of 5th Generation (5G) networks, and the demand for a range of new applications and services through vehicular networks, it is imperative to integrate 5G and vehicular networks. To achieve a seamless integration, various design and implementation issues related to 5G and VANETs must be addressed. We focus on the security issues that need to be considered in order to enable the secure integration of 5G and VANETs. More precisely, we conduct in-depth study of the current security issues, solutions, and standards used in vehicular networks and then we identify the security gaps in the existing VANET security solutions. We investigate the security features of 5G networks and discuss how they can be leveraged in vehicular networks to enable a seamless and efficient integration. We also propose a security architecture for vehicular networks wherein the current VANET security standards and 5G security features coexist to support secure VANET applications. Finally, we discuss some future challenges and research directions for 5G-enabled secure vehicular networks.", 
There is a wealth of data available within the health care systems. There is a lack of effective analysis tools to discover hidden relationships in data. The aim of this work is to design a UI to enter the patient record and predict whether the patient is having Heart disease or not.,Intelligent and Effective Heart Disease Prediction System using Weighted Associative Classifiers,"The healthcare environment is still ‘information rich’ But ‘knowledge poor’. There is a wealth of data available within the health care systems. However, there is a lack of effective analysis tools to discover hidden relationships in data. The aim of this work is to design a GUI based Interface to enter the patient record and predict whether the patient is having Heart disease or not using Weighted Association rule based Classifier. The prediction is performed from mining the patient’s historical data or data repository. In Weighted Associative Classifier (WAC), different weights are assigned to different attributes according to their predicting capability. It has already been proved that the Associative Classifiers are performing well than traditional classifiers approaches such as decision tree and rule induction. Further from experimental results it has been found that WAC is providing improved accuracy as compare to other already existing Associative Classifiers. Hence the system is using WAC as a Data mining technique to generate rule base. The system has been implemented in java Platform and trained using benchmark data from UCI machine learning repository. The system is expandable for the new dataset.", 
"In this paper, we propose a novel approach for extracting the most relevant sentences from an original document. The approach utilizes fuzzy measures and inference to find the most significant sentences.",Intelligent Extractive Text Summarization Using Fuzzy Inference Systems,"Abstract- With the growing size of textual information on the world-wide web, automatic data summarization becomes essential for both web users as well as data base and search engine developers. In this paper, we propose a novel approach for extracting the most relevant sentences from an original document to form a summary. The approach utilizes fuzzy measures and inference to find the most significant sentences. Experimental results reveal that the proposed approach extracts the more relevant sentences when compared with two commercially available text summarizers.", 
"Intelligent Heart Disease Prediction System (IHDPS) uses data mining techniques. IHDPS can answer complex ""what if"" queries which traditional decision support systems cannot. Using medical profiles such as age, sex, blood pressure and blood sugar it can predict the likelihood of patients getting a heart disease.",Intelligent Heart Disease Prediction System Using Data Mining Techniques,"The healthcare industry collects huge amounts of healthcare data which, unfortunately, are not “mined” to discover hidden information for effective decision making. Discovery of hidden patterns and relationships often goes unexploited. Advanced data mining techniques can help remedy this situation. This research has developed a prototype Intelligent Heart Disease Prediction System (IHDPS) using data mining techniques, namely, Decision Trees, Naïve Bayes and Neural Network. Results show that each technique has its unique strength in realizing the objectives of the defined mining goals. IHDPS can answer complex “what if” queries which traditional decision support systems cannot. Using medical profiles such as age, sex, blood pressure and blood sugar it can predict the likelihood of patients getting a heart disease. It enables significant knowledge, e.g. patterns, relationships between medical factors related to heart disease, to be established. IHDPS is Web-based, user-friendly, scalable, reliable and expandable. It is implemented on the .NET platform.", 
The dynamic nature of transportation traffic and increased data bandwidth demands are the major obstacles to achieve high transmission rate in Vehicular-to-Anything (V2X) Networks. This paper presents a novel Software Defined Networking (SDN)-controlled and Cognitive Radio (CR)-enabled V2X routing approach to achieve ultra-high data rate. The approach is based on three features enabled machine learning approach. It supports intelligent switching between two 5G technologies: millimeter-wave (mmWave) and terahertz (THz).,Intelligent super-fast Vehicle-to-Everything 5G communications with predictive switching between mmWave and THz links,"With the incoming of 5G communications, Vehicular Networks have the hope to achieve ultra-high data transmission rate with extremely low end-to-end delay. However, the dynamic nature of transportation traffic and increased data bandwidth demands are the major obstacles to achieve high transmission rate in Vehicular-to-Anything (V2X) Networks. To overcome these obstacles, this paper presents a novel Software Defined Networking (SDN)-controlled and Cognitive Radio (CR)-enabled V2X routing approach to achieve ultra-high data rate, by using predictive V2X routing that supports the intelligent switching between two 5G technologies: millimeter-wave (mmWave) and terahertz (THz). To improve the network management, Road Side units (RSUs) are used to segregate the V2X network into different clusters. Stability-aware clustering (SAC) scheme is also used for cluster formations. Our intelligent V2X is based on three features enabled machine learning approach: (1) To predict future 3D positions of the vehicles in the Cluster Heads (CHs) using Deep Neural Network with Extended Kalman Filter (DNN-EKF) algorithm for real-time, high-resolution prediction. (2) For THz communications, 0.3 THz to 3 THz band is selected for short-distance super-fast data transmissions. The THz band detection is performed by the CR-enabled Road Side Units (cRSUs). A Genetic Algorithm (GA)-based Improved Fruit Fly (GA-IFF) scheme is proposed to achieve an optimal route selection in THz communications. (3) In mmWavebased V2X communications, optimal beam selection is performed by the multi-type2 fuzzy inference system (M-T2FIS). By using these three intelligent designs approaches, we are able to achieve ultrahighrate and minimized transmission delay for short-range (in THz bands) and middle-range (in mmWave) communications. Finally, the proposed SDN-controlled, CR-enabled V2X Network is modeled and tested for performance evaluations with the metrics of delivery ratio, routing delay, protocol overhead, and data rate.", 
"IoT stands for Internet of things which is defined as intercommunication of heterogeneous smart devices acting as a single network. In IoT, every device is connected via internet with different devices having different capabilities within a network. Smart city is implemented through smart nodes in real scenarios to make digital environment.",Intelligent Traffic Monitoring and Guidance System for Smart City,"IoT stands for Internet of things which is defined as an intercommunication of heterogeneous smart devices acting as a single network. In IoT, every device is connected via internet with different devices having different capabilities within a network. IoT is implemented almost in every field of life which is an extension of Wireless Sensor Networks (WSNs). Smart city is implemented through smart nodes in real scenarios to make digital environment in all field of life including smart parking, banking, sewerage system, waste management, environmental monitoring, smart transport and healthcare centers. However, there is a very little practical use of IoT and smart cities infrastructure in real life. In this paper, we have focused on intelligent traffic monitoring system using graph theory and formal methods. Our proposed model has various nodes that are assumed within a city including roads, objects and traffic signals to make a collective intelligent traffic guidance and monitoring system. Many operations including finding shortest path in terms of time and distance, finding specific area within city and finding safest and low rush ways towards the destination. The graph based model is transformed into a formal model using Vienna Development Method-Specification Language (VDM-SL). The proof of correctness is provided by various facilities available in the VDM-SL toolbox.", 
"We are building an interactive, visual text analysis tool that aids users in analyzing a large collection of text. We provide an effective visual metaphor that transforms complex and even imperfect text summarization results into a comprehensible visual summary of texts. We offer users a set of flexible visual interaction tools as the alternatives to compensate for the deficiencies of current text summarizing techniques.","Interactive, topic-based visual text summarization and analysis","We are building an interactive, visual text analysis tool that aids users in analyzing a large collection of text. Unlike existing work in text analysis, which focuses either on developing sophisticated text analytic techniques or inventing novel visualization metaphors, ours is tightly integrating state-of-the-art text analytics with interactive visualization to maximize the value of both. In this paper, we focus on describing our work from two aspects. First, we present the design and development of a time-based, visual text summary that effectively conveys complex text summarization results produced by the Latent Dirichlet Allocation (LDA) model. Second, we describe a set of rich interaction tools that allow users to work with a created visual text summary to further interpret the summarization results in context and examine the text collection from multiple perspectives. As a result, our work offers two unique contributions. First, we provide an effective visual metaphor that transforms complex and even imperfect text summarization results into a comprehensible visual summary of texts. Second, we offer users a set of flexible visual interaction tools as the alternatives to compensate for the deficiencies of current text summarization techniques. We have applied our work to a number of text corpora and our evaluation shows the promise of the work, especially in support of complex text analyses.", 
"Electroencephalography (EEG) is a convenient, non-intrusive way of capturing brain signals. We propose an ensemble learning algorithm for automatically computing the most discriminative subset of EEG channels for internal emotion recognition. The algorithm is useful in reducing the amount of data while improving computational efficiency and classification accuracy.",Internal Emotion Classification Using EEG Signal with Sparse Discriminative Ensemble,"Among various physiological signal acquisition methods for the study of human brain, EEG (Electroencephalography) is more effective. EEG provides a convenient, non-intrusive and accurate way of capturing brain signals in multiple channels at fine temporal resolution. We propose an ensemble learning algorithm for automatically computing the most discriminative subset of EEG channels for internal emotion recognition. Our method describes an EEG channel using kernel-based representations computed from the training EEG recordings. For ensemble learning, we formulate a graph embedding linear discriminant objective function using the kernel representations. The objective function is efficiently solved via sparse non-negative Principal Component Analysis (PCA) and the final classifier is learned using the sparse projection coefficients. Our algorithm is useful in reducing the amount of data while improving computational efficiency and classification accuracy at the same time. Experiments on publicly available EEG dataset demonstrate the superiority of the proposed algorithm over the compared methods.", 
Internet of Things (IoT) consists of smart devices that communicate with each other. Smart devices can have wired or wireless connection. Many different wireless communication technologies and protocols can be used to connect the smart device. This paper will be an attempt to review different communication protocols in IoT.,Internet of Things (IoT) Communication Protocols  Review,"Internet of Things (IoT) consists of smart devices that communicate with each other. It enables these devices to collect and exchange data. Besides, IoT has now a wide range of life applications such as industry, transportation, logistics, healthcare, smart environment, as well as personal, social gaming robot, and city information. Smart devices can have wired or wireless connection. As far as the wireless IoT is the main concern, many different wireless communication technologies and protocols can be used to connect the smart device such as Internet Protocol Version 6 (IPv6), over Low power Wireless Personal Area Networks (6LoWPAN), ZigBee, Bluetooth Low Energy (BLE), Z-Wave and Near Field Communication (NFC). They are short range standard network protocols, while SigFox and Cellular are Low Power Wide Area Network (LPWAN).standard protocols. This paper will be an attempt to review different communication protocols in IoT. In addition, it will compare between commonly IoT communication protocols, with an emphasis on the main features and behaviors of various metrics of power consumption security spreading data rate, and other features. This comparison aims at presenting guidelines for the researchers to be able to select the right protocol for different applications.", 
"The term Internet of Things (IoT) is used as an umbrella that covers several topics, related to the application of technological means to monitor, measure and act upon the environment. Data exchanged within and among IoT frameworks are growing exponentially, and the pervasiveness of such systems brings them to come in possession of very sensitive information.","Internet of things reference architectures, security and interoperability A survey","The term Internet of Things (IoT) is used as an umbrella that covers several topics, related to the application of technological means to monitor, measure and act upon the environment. As a result, it is difficult to determine a univocal architecture to identify as a reference and several scenarios, involving different sensors, smart devices, networks or gateways, can unfold. The data exchanged within and among IoT frameworks are growing exponentially, and the pervasiveness of such systems brings them to come in possession of very sensitive information: as a consequence, Security and Privacy have become a hot topic on the IoT scenery. Furthermore, due to the great variety of technological solutions which are currently available, interoperability issues are bound to arise, especially when no standard API interface, or communication protocol, has been officially adopted. This paper provides a review of the most common architectural solutions available today to shape an IoT system, ranging from already standardized architecture to commercial ones. Elements from such architectures have been compared, analysed and mapped one against the other to determine a stable reference for Security and Interoperability analysis. Current solutions in the Security and API Interoperability domains for IoT have been also analysed.", 
"The dramatic increase of urban motorcycle road fatalities has led to significant issues in contemporary traffic management systems. Internet of Vehicle (IoV) is the fastest growing extension module of VANET builds highly compatible cloud based collaboration among heterogeneous entities like vehicles, human and internet services.",Internet of Vehicle based Accident Detection and Management Techniques by using VANET: An Empirical Study,"The dramatic increase of urban motorcycle road fatalities has led to significant issues in contemporary traffic management systems. The impact of synergetic paradigms of VANET envisages sustainable solutions for smart transportation. Furthermore, Internet of Vehicle (IoV) the fastest growing extension module of VANET builds highly compatible cloud based collaboration among heterogeneous entities like vehicles, human and internet services. The proliferation of such IoV enabled device configuration provides scalable, efficient and quality driven applications for smart city ecosystem. This paper focusses on identifying and examining the adoption of wearable embedded smart helmet technology among the motorcyclist that reduces the potential injuries to the head and prevention of accidents due to drunken drive for safe riding. In addition, this study critically evaluated the existing best practices of smart helmet management and issues in terms of software and hardware aspects. Finally, the survey ends by emphasis on analytical insights and future plan.", 
"Vehicles are increasingly being connected to the Internet of Things which enable them to provide ubiquitous access to information to drivers and passengers while on the move. As the number of connected vehicles keeps increasing, new requirements are emerging for seamless, secure, robust, scalable information exchange among vehicles, humans and roadside infrastructures.","Internet of Vehicles Architecture, Protocols, and Security","Today, vehicles are increasingly being connected to the Internet of Things which enable them to provide ubiquitous access to information to drivers and passengers while on the move. However, as the number of connected vehicles keeps increasing, new requirements (such as seamless, secure, robust, scalable information exchange among vehicles, humans and roadside infrastructures) of vehicular networks are emerging. In this context, the original concept of Vehicular Ad-hoc networks (VANETs) is being transformed into a new concept called the Internet of Vehicles (IoV). We discuss the benefits of IoV along with recent industry standards developed to promote its implementation. We further present recently proposed communication protocols to enable the seamless integration and operation of the IoV. Finally, we present future research directions of IoV that require further consideration from the vehicular research community.", 
"Internet of Things (IoT) is a cutting edge technology that connects a plethora of digital devices endowed with several sensing, actuation, and computing capabilities with the Internet. IoT services and big data analytics are enabling smart city initiatives all over the world. These services are transforming cities by improving infrastructure and transportation systems, reducing traffic congestion and providing waste management.",Internet-of-Things-Based Smart Cities Recent Advances and Challenges,"The Internet of Things is a novel cutting edge technology that proffers to connect a plethora of digital devices endowed with several sensing, actuation, and computing capabilities with the Internet, thus offering manifold new services in the context of a smart city. The appealing IoT services and big data analytics are enabling smart city initiatives all over the world. These services are transforming cities by improving infrastructure and transportation systems, reducing traffic congestion, providing waste management, and improving the quality of human life. In this article, we devise a taxonomy to best bring forth a generic overview of the IoT paradigm for smart cities, integrated ICT, network types, possible opportunities and major requirements. Moreover, an overview of the up-to-date e?orts from standard bodies is presented. Later, we give an overview of existing open source IoT platforms for realizing smart city applications followed by several exemplary case studies. In addition, we summarize the latest synergies and initiatives worldwide taken to promote IoT in the context of smart cities. Finally, we highlight several challenges in order to give future research directions.", 
"Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. The ability to interpret what a model has learned is receiving an increasing amount of attention. It is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning.","Interpretable machine learning definitions, methods, and applications","Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant (PDR) framework for discussing interpretations. The PDR framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.", 
Automatic summaries of Micro-Blogs conversations dealing with public figures E-Reputation. These summaries are generated using key-word queries or sample tweet and offer a focused view of the whole Micro-Blog network.,INTWEETIVE TEXT SUMMARIZATION,"The amount of user generated contents from various social medias allows analyst to handle a wide view of conversations on several topics related to their business. Nevertheless keeping up-to-date with this amount of information is not humanly feasible. Automatic Summarization then provides an interesting mean to digest the dynamics and the mass volume of contents. In this paper, we address the issue of tweets summarization which remains scarcely explored. We propose to automatically generated summaries of Micro-Blogs conversations dealing with public figures E-Reputation. These summaries are generated using key-word queries or sample tweet and offer a focused view of the whole Micro-Blog network. Since state-of-the-art is lacking on this point we conduct and evaluate our experiments over the multilingual CLEF RepLab Topic-Detection dataset according to an experimental evaluation process.", 
Healthcare Social Networks (HSNs) offer the possibility to enhance patient care and education. They also present potential risks for patients due to the possible distribution of poor-quality or wrong information. Doctors do not have enough time to read patients' posts and moderate them when required.,Investigating classification supervised learning approaches for the identification of critical patients' posts in a healthcare social network,"Nowadays, Healthcare Social Networks (HSNs) offer the possibility to enhance patient care and education. However, they also present potential risks for patients due to the possible distribution of poor-quality or wrong information along with their bad interpretation. On one hand doctors and practitioners want to promote the exchange of information among patients about a specific disease, but on the other hand they do not have enough time to read patients’ posts and moderate them when required. In this paper, we investigate and compare different supervised learning classifiers that we adopted for the classification of critical patients’ posts who can trigger the intervention of the medical personnel. In particular, by considering different Bayesian, Linear and Support Vector Machine (SVM) classifiers we analyze their accuracy considering different n-grams datasets preparation approaches in order to identify the best approach for the identification of critical patients’ posts in a Healthcare Social Network.", 
"This paper combines contributions by Term Frequency weight, position weight and commendation weight to form an effective extraction based summary. We also show that our summarizer outperforms commercially available summarizers like Copernic summarizer and Microsoft Summarizer.",Investigations in single document summarization by extraction method,"Due to the rapid growth of the Internet and online information services, huge information is readily made available online. There is no time for anybody to read everything but critical decisions are required to be made quickly. This has led to large-scale research efforts in text summarization. This paper combines contributions by Term Frequency weight, position weight and commendation weight to form an effective extraction based summary. We have analyzed the significance of each feature and we found that a combination of all three weights results in a better summary as close as human generated summary. We also show that our summarizer outperforms commercially available summarizers like Copernic summarizer and Microsoft Summarizer.", 
"Integration of all smart systems (such as smart home, smart parking, etc.) and the IoT devices in the city can play a vital role to develop the urban services by building their city digital and smarter. Interconnection of lots of IoT objects to collect urban data to launch a smart digital city effects vast volume of data generation, termed as Big Data. In this paper, we have established an IoT-based Smart City by using Big Data analytics while harvesting real-time data from the city.",Exploiting IoT and Big Data Analytics Defining Smart Digital City using Real-Time Urban Data,"Integration of all smart systems (such as smart home, smart parking, etc.) and the IoT devices (such as sensors, actuators, and smartphones) in the city can play a vital role to develop the urban services by building their city digital and smarter. However, interconnection of lots of IoT objects to collect urban data over the Internet to launch a smart digital city, effects vast volume of data generation, termed as Big Data. Thus, it is a challenging task to integrate IoT devices and smart systems in order to harvest and process such big amount of real-time city data in an effective manner aimed at creating a Smart Digital City. Therefore, in this paper, we have established an IoT-based Smart City by using Big Data analytics while harvesting real-time data from the city. We used sensors’ deployment including sensors at smart home, smart parking, vehicular networking, surveillance, weather and water monitoring system, etc., for real time data collection. The complete system is described by its proposed architecture and implementation prototype using Hadoop ecosystem in a real environment. In addition, the Smart Digital City services are extended by developing the intelligent Smart Transportation System by means of big graph processing to facilitate citizens while providing real-time traffic information and alerts. The proposed system consists of number of stages including data generation and collection, aggregation, filtration, classification, preprocessing, computing, and decision making. The efficiency of the system is extended by applying Big Data processing using Apache Spark over Hadoop. Whereas, the big city graph processing is achieved by using Giraph over Hadoop. The system is practically implemented by taken existing smart systems and IoT devices as city data sources to develop the Smart Digital City. The proposed system is evaluated with respect to efficiency in terms of scalability and real-time data processing.", 
"Each brain hemisphere is dominant for certain functions such as speech. The determination of speech laterality prior to surgery is of paramount importance for accurate risk prediction. In this study, we aimed to determine speech. laterality via EEG signals by using noninvasive machine learning techniques. The classification results obtained in the study are promising and lead the way for subsequent studies though not practically feasible.",Is it possible to detect cerebral dominance via EEG signals by using deep learning,"Each brain hemisphere is dominant for certain functions such as speech. The determination of speech laterality prior to surgery is of paramount importance for accurate risk prediction. In this study, we aimed to determine speech laterality via EEG signals by using noninvasive machine learning techniques. The retrospective study included 67 subjects aged 18–65 years who had no chronic diseases and were diagnosed as healthy based on EEG examination. The subjects comprised 35 right-hand dominant (speech center located in the left hemisphere) and 32 left-hand dominant individuals (speech center located in the right hemisphere). A spectrogram was created for each of the 18 EEG channels by using various Convolutional Neural Networks (CNN) architectures including VGG16, VGG19, ResNet, MobileNet, NasNet, and DenseNet. These architectures were used to extract features from the spectrograms. The extracted features were classified using Support Vector Machines (SVM) and the classification performances of the CNN models were evaluated using Area Under the Curve (AUC). Of all the CNN models used in the study, VGG16 had a higher AUC value (0.83 ± 0.05) in the determination of speech laterality compared to all other models. The present study is a pioneer investigation into the determination of speech laterality via EEG signals with machine learning techniques, which, to our knowledge, has never been reported in the literature. Moreover, the classification results obtained in the study are promising and lead the way for subsequent studies though not practically feasible.", 
"Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures. We show our methods benefit from richer training data, much more so than the current state-of-the-art method.",Is that you-Metric Learning Approaches for Face Identification,"Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures: (a) a logistic discriminant approach which learns the metric from a set of labelled image pairs (LDML) and (b) a nearest neighbour approach which computes the probability for two images to belong to the same class (MkNN). We evaluate our approaches on the Labeled Faces in the Wild data set, a large and very challenging data set of faces from Yahoo! News. The evaluation protocol for this data set defines a restricted setting, where a fixed set of positive and negative image pairs is given, as well as an unrestricted one, where faces are labelled by their identity. We are the first to present results for the unrestricted setting, and show that our methods benefit from this richer training data, much more so than the current state-of-the-art method. Our results of 79.3% and 87.5% correct for the restricted and unrestricted setting respectively, significantly improve over the current state-of-the-art result of 78.5%. Confidence scores obtained for face identification can be used for many applications e.g. clustering or recognition from a single training example. We show that our learned metrics also improve performance for these tasks.", 
"When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance can be obtained with language models.","It's Not Just Size That Matters, Small Language Models Are Also Few-Shot Learners","When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to GPT-3 can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models.", 
"JaTeCS is an open source Java library that supports research on automatic text categorization. It covers all the steps of an experimental activity, from reading the corpus to the evaluation of the experimental results. It provides data readers for many formats, including the most commonly used text corpora and lexical resources.",JaTeCS an open-source JAva TExt Categorization System,"JaTeCS is an open source Java library that supports research on automatic text categorization and other related problems, such as ordinal regression and quantification, which are of special interest in opinion mining applications. It covers all the steps of an experimental activity, from reading the corpus to the evaluation of the experimental results. As JaTeCS is focused on text as the main input data, it provides the user with many text-dedicated tools, e.g.: data readers for many formats, including the most commonly used text corpora and lexical resources, natural language processing tools, multi-language support, methods for feature selection and weighting, the implementation of many machine learning algorithms as well as wrappers for well-known external software (e.g., SVMlight) which enable their full control from code. JaTeCS support its expansion by abstracting through interfaces many of the typical tools and procedures used in text processing tasks. The library also provides a number of \template"" implementations of typical experimental setups (e.g., train-test, k-fold validation, grid-search optimization, randomized runs) which enable fast realization of experiments just by connecting the templates with data readers, learning algorithms and evaluation measures.", 
"Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. We formulate these four components into a joint deep learning framework and propose a new deep network architecture. ",Joint Deep Learning for Pedestrian Detection,"Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture1. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.", 
"In this research, we propose a particular version of KNN (K Nearest Neighbor) where the similarity between feature vectors is computed considering the similarity among attributes or features. We may expect the more compact representation of data items and the better performance.",K nearest neighbor for text summarization using feature similarity,"In this research, we propose a particular version of KNN (K Nearest Neighbor) where the similarity between feature vectors is computed considering the similarity among attributes or features as well as one among values. The task of text summarization is viewed into the binary classification task where each paragraph or sentence is classified into the essence or non-essence, and in previous works, improved results are obtained by the proposed version in the text classification and clustering. In this research, we define the similarity which considers both attributes and attribute values, modify the KNN into the version based on the similarity, and use the modified version as the approach to the text summarization task. As the benefits from this research, we may expect the more compact representation of data items and the better performance. Therefore, the goal of this research is to implement the text summarization algorithm which represents data items more compactly and provides the more reliability.", 
Summarization is a method of reducing the original text document into a short description. It is a difficult task for human beings to generate the summary for very large documents manually. The linguistic and statistical features of sentence can be used to find the importance of sentences.,Kannada text summarization using Latent Semantic Analysis,Text Summarization is a method of reducing the original text document into a short description. This short version retains the meaning and information content of the original text document. It is a difficult task for human beings to generate the summary for very large documents manually. The linguistic and statistical features of sentence can be used to find the importance of sentences. The Latent Semantic Analysis (LSA) captures automatically the semantic relationships between the sentences as a human being thinks. In this paper Singular Value Decomposition (SVD) is used to generate the summary. SVD finds the dimensions of the sentence vectors which are principal and mutually orthogonaI. These properties guaranty the relevance to original text document and non-redundancy respectively in machine generated summary., 
"Karc? Summarization is a novel methodology for extractive, generic summarization of text documents. An important feature of the proposed system is that it does not require any kind of information source or training data. The proposed summarizer outperformed all current state-of-the-art methods in terms of 200-word summaries. The method was assessed as quite insensitive to disorderly and missing texts due to its KUSH text processing module.","Karc? summarization, A simple and effective approach for automatic text summarization using Karc? entropy","Increases in the amount of text resources available via the Internet has amplified the need for automated document summarizing tools. However, further e?orts are needed in order to improve the quality of the existing summarization tools currently available. The current study proposes Karc? Summarization, a novel methodology for extractive, generic summarization of text documents. Karc? Entropy was used for the first time in a document summarization method within a unique approach. An important feature of the proposed system is that it does not require any kind of information source or training data. At the stage of presenting the input text, a tool for text processing was introduced; known as KUSH (named after its authors; Karc?, Uçkan, Seyyarer, and Hark), and is used to protect semantic consistency between sentences. The Karc? Entropy-based solution chooses the most e?ective, generic and most informational sentences within a paragraph or unit of text. Experimentation with the Karc? Summarization approach was tested using open access document text (Document Understanding Conference; DUC-2002, DUC-2004) datasets. Performance achievement of the Karc? Summarization approach was calculated using metrics known as Recall-Oriented Understudy for Gisting Evaluation (ROUGE). The experimental results showed that the proposed summarizer outperformed all current state-of-the-art methods in terms of 200-word summaries in the metrics of ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-W-1.2. In addition, the proposed summarizer outperformed the nearest competitive summarizers by a factor of 6.4% for ROUGE-1 Recall on the DUC-2002 dataset. These results demonstrate that Karc? Summarization is a promising technique and it is therefore expected to attract interest from researchers in the field. Our approach was shown to have a high potential for adoptability. Moreover, the method was assessed as quite insensitive to disorderly and missing texts due to its KUSH text processing module.", 
In this paper we present an extractive summarization method for the Kazakh language based on fuzzy logic. We aimed to extract and concatenate important sentences from the primary text to obtain its shorter form. We also applied our method on CNN/Daily Mail dataset.,Kazakh Text Summarization using Fuzzy Logic,"In this paper we present an extractive summarization method for the Kazakh language based on fuzzy logic. We aimed to extract and concatenate important sentences from the primary text to obtain its shorter form. With the rapid growth of information on the Internet there is a demand on its efficient and cost-effective summarization. Therefore the creation of automatic summarization methods is considered as a very important task of natural language processing. Our approach is based on the preprocessing of the sentences by applying morphological analysis and pronoun resolution techniques in order to avoid their early rejections. Afterwards, we determine the features of the processed sentences need for exploiting fuzzy logic methods. Additionally, since there is no available data for the given task, we collected and manually annotated our own dataset from the different Internet resources in the Kazakh language for the experimentation. We also applied our method on CNN/Daily Mail dataset. The ROUGE-N indicators were calculated to assess the quality of the proposed method. The ROUGE-L(f-score) score by the proposed method with pronoun resolution for the former dataset is 0.40, whereas for the latter one it is 0.38.", 
Keyphrase based Summary Evaluator (KpEval) for evaluating automatic summaries. System relies on keyphrases since they convey the most important concepts of a text. The system was applied to evaluate different summaries of Arabic multi-document data set presented at TAC2011.,Keyphrase based Evaluation of Automatic Text Summarization,"The development of methods to deal with the informative contents of the text units in the matching process is a major challenge in automatic summary evaluation systems that use fixed n-gram matching. The limitation causes inaccurate matching between units in a peer and reference summaries. The present study introduces a new Keyphrase based Summary Evaluator (KpEval) for evaluating automatic summaries. The KpEval relies on the keyphrases since they convey the most important concepts of a text. In the evaluation process, the keyphrases are used in their lemma form as the matching text unit. The system was applied to evaluate different summaries of Arabic multi-document data set presented at TAC2011. The results showed that the new evaluation technique correlates well with the known evaluation systems: Rouge-1, Rouge-2, Rouge-SU4, and AutoSummENG–MeMoG. KpEval has the strongest correlation with AutoSummENG–MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667 respectively.", 
Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text.,"Keyphrase Generation, A Text Summarization Struggle","Authors’ keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the simpler unsupervised methods, or the existing supervised ones.", 
Abstractive text summarization features the use of new phrases to obtain a condensed version of the source text. We propose to exploit keyphrases to guide the summary selection in a modified beam search process. We outperform the state-of-the-art systems on several datasets at no extra cost of training.,Keyphrase Guided Beam Search for Neural Abstractive Text Summarization,"As a recently proposed way of text summarization, abstractive text summarization features the use of new phrases to obtain a condensed version of the source text. Most approaches nowadays in this category are under the sequence-to-sequence framework, which is the bedrock of many text generation tasks. However, these approaches usually fail to get a sound representation of the source text, and they are liable to produce summaries with a low semantic relevance. In this work, we devise a novel structure of convolutional recurrent neural network-based encoder to get a better latent representation of the source text. Further, we propose to exploit keyphrases to guide the summary selection in a modified beam search process, thus contributing to a closer semantic relevance between the source text and the generated summary. With this new approach, we outperform the state-of-the-art systems on several datasets at no extra cost of training, particularly on the benchmark CNN/Daily Mail dataset where the source texts are lengthy.", 
"The objective of the study was to assess the knowledge and perception of COVID-19 and relevant universal safety measures among the Nepalese population. A web-based cross-sectional study was conducted among Nepalese from March 29 to April 07, 2020. The median knowledge score of the participants was 10.0 (± 3.0 IQR) Only about half knew about the concept of quarantine.",Knowledge and perception towards universal safety precautions during early phase of the COVID-19 outbreak in Nepal,"The objective of the study was to assess the knowledge and perception of COVID-19 and relevant universal safety measures among the Nepalese population. A web-based cross-sectional study was conducted among Nepalese adults from March 29 to April 07, 2020. A 13- and 15- items structured questionnaire assessed the COVID-19 related knowledge and perception of the universal safety measure. Kruskal–Wallis test and Mann–Whitney U test evaluated the differences in knowledge between the groups. Data analysis was performed using IBM SPSS Statistics for Windows Version 21.0 (IBM Corp. Armonk, NY, USA). Of the 884 surveys accessed, a total of 871 consented (electronically) and completed the online survey (response rate 98.52%). The median knowledge score of the participants was 10.0 (± 3.0 IQR). Although participants’ overall knowledge score was high, only about half of the participants knew about the concept of quarantine and the ideal distance to be maintained between individuals to prevent the transmission. Though the majority of the participants had positive perception towards universal safety measure of COVID-19, about 18% perceived that coronavirus infected only older people, 11% opined that the infection was highly fatal with no chances of survival and 70% considered that limiting consumptions of poultry and meat would prevent the spread of COVID-19. A statistically significant difference in knowledge was noted by participants’ age, educational status, occupational type, and household monthly income. This study found optimal knowledge and perception of universal safety measures of COVID-19 among the Nepalese population, but misinformation and misunderstanding prevailed.",  
"Pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge. In this paper we incorporate knowledge-awareness in language model pretraining. We simply signal existence of entities to the input of the transformer in pretraining, with an entity-extended tokenizer.",Knowledge-Aware Language Model Pretraining,"How much knowledge do pretrained language models hold? Recent research observed that pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge, or how to ensure they do so. In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entity-extended tokenizer; and at the output, with an additional entity prediction task. Our experiments show that solely by adding these entity signals in pretraining, significantly more knowledge is packed into the transformer parameters: we observe improved language modeling accuracy, factual correctness in LAMA knowledge probing tasks, and semantics in the hidden representations through edge probing. We also show that our knowledge-aware language model (KALM) can serve as a drop-in replacement for GPT-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.", 
"Automatic text summarization has recently achieved impressive performance thanks to advances in deep learning. There is still no guarantee that the generated summaries are grammatical, concise, and convey all salient information as the original documents have. This paper presents an unsupervised approach that combines rhetorical structure theory, deep neural model, and domain knowledge concern for ATS.",Knowledge-guided unsupervised rhetorical parsing for text summarization,"Automatic text summarization (ATS) has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale corpora. However, there is still no guarantee that the generated summaries are grammatical, concise, and convey all salient information as the original documents have. To make the summarization results more faithful, this paper presents an unsupervised approach that combines rhetorical structure theory, deep neural model, and domain knowledge concern for ATS. This architecture mainly contains three components: domain knowledge base construction based on representation learning, the attentional encoder–decoder model for rhetorical parsing, and subroutine-based model for text summarization. Domain knowledge can be effectively used for unsupervised rhetorical parsing thus rhetorical structure trees for each document can be derived. In the unsupervised rhetorical parsing module, the idea of translation was adopted to alleviate the problem of data scarcity. The subroutine-based summarization model purely depends on the derived rhetorical structure trees and can generate content-balanced results. To evaluate the summary results without golden standard, we proposed an unsupervised evaluation metric, whose hyper-parameters were tuned by supervised learning. Experimental results show that, on a large-scale Chinese dataset, our proposed approach can obtain comparable performances compared with existing methods.", 
"This paper introduces a new approach for unsupervised extractive summarization. It is based on the Minimum Description Length (MDL) principle using the Krimp dataset compression algorithm. Our approach represents a text as a transactional dataset, with sentences as transactions.",Krimping texts for better summarization,"Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words. In this paper, we introduce a new approach for unsupervised extractive summarization, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011). Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results.", 
"Labeled Faces in the Wild is an aid in studying the unconstrained face recognition problem. The database contains labeled face photographs spanning the range of conditions. It exhibits ""natural"" variability in factors such as pose, lighting, race, accessories, occlusions, and background.",Labeled Faces in the Wild A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.", 
Many software repositories collect and archive Android applications to facilitate the dissemination of Android applications. LACTA extensively employs Android domain knowledge in the process. LDA uses LDA to extract meaningful software topics for classification.,LACTA-An Enhanced Automatic Software Categorization on the Native Code of Android Applications,"Since Android has become a popular software platform for mobile devices recently, many software repositories collect and archive Android applications to facilitate the dissemination of Android applications. As the number of new Android applications tends to be rapidly increased in the near future, automatic software categorization will be in great demand. Although there are many approaches proposed for automatic software categorization, they do not consider the challenges specifically for Android applications. In this paper, we propose an enhancement called LACTA based on LACT to tackle this problem. LACTA extensively employs Android domain knowledge in the process and uses LDA to extract meaningful software topics for classification. We have conducted empirical experiments with 42 applications. The experimental results show that LACTA has promising improvements under the consideration of Android domain knowledge.", 
"In this article, I compare the classification performance of four non-parametric algorithms. The study area chosen is a complex mixed-use landscape in south-central Sweden with eight land-cover and land-use (LCLU) classes. The satellite imagery used for the classification were multi-temporal scenes from Sentinel-2 covering spring, summer, autumn and winter conditions. Highest overall accuracy was produced by support vector machines (SVM) closely followed by random forests (RF) and deep learning (DL).",Land cover and land use classification performance of machine learning algorithms in a boreal landscape using Sentinel 2 data,"In recent years, the data science and remote sensing communities have started to align due to userfriendly programming tools, access to high-end consumer computing power, and the availability of free satellite data. In particular, publicly available data from the European Space Agency’s Sentinel missions have been used in various remote sensing applications. However, there is a lack of studies that utilize these data to assess the performance of machine learning algorithms in complex boreal landscapes. In this article, I compare the classification performance of four non-parametric algorithms: support vector machines (SVM), random forests (RF), extreme gradient boosting (Xgboost), and deep learning (DL). The study area chosen is a complex mixed-use landscape in south-central Sweden with eight land-cover and land-use (LCLU) classes. The satellite imagery used for the classification were multi-temporal scenes from Sentinel-2 covering spring, summer, autumn and winter conditions. Using stratified random sampling, each LCLU class was allocated 1477 samples, which were divided into training (70%) and evaluation (30%) subsets. Accuracy was assessed through metrics derived from an error matrix, but primarily overall accuracy was used in allocating algorithm hierarchy. A two-proportion Z-test was used to compare the proportions of correctly classified pixels of the algorithms and a McNemar’s chi-square test was used to compare class-wise predictions. The results show that the highest overall accuracy was produced by support vector machines (0.758 ± 0.017), closely followed by extreme gradient boosting (0.751 ± 0.017), random forests (0.739 ± 0.018), and finally deep learning (0.733 ± 0.0023). The Z-test comparison of classifiers showed that a third of algorithm pairings were statistically different. On a class-wise basis, McNemar’s test results showed that 62% of class-wise predictions were significant from one another at the 5% level or less. Variable importance metrics show that nearly half of the top twenty Sentinel-2 bands belonged to the red edge (25%) and shortwave infrared (23%) portions of the electromagnetic spectrum, and were dominated by scenes from spring (38%) and summer (40%). The results are discussed within the scope of recent studies involving machine learning and Sentinel-2 data and key knowledge gaps identified. The article concludes with recommendations for future research.", 
TextRank is a system for unsupervised extractive summarization. It does not rely on any language-specific knowledge resources or any manually constructed.,Language Independent Extractive Summarization,"TextRank is a system for unsupervised extractive summarization that relies on an innovative application of iterative graph-based ranking algorithms to graphs encoding the cohesive structure of texts. An important characteristic of the system is that it does not rely on any language-specific knowledge resources or any manually constructed training data, and thus it is highly portable to new languages or domains.", 
"GPT-3 is an autoregressive language model with 175 billion parameters, 10x more than any previous non-s parse language model. It can achieve strong performance on translation, question-answering, and cloze tasks. However, it struggles with tasks that require on-the-fly reasoning or domain adaptation. The model can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans.",Language Models are Few-Shot Learners,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", 
"Task-oriented dialogue systems use four connected modules, namely Natural Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy (DP) and Natural Language Generation (NLG) A research challenge is to learn each module with the least amount of samples (i.e., few-shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning.",Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems,"Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy (DP) and Natural Language Generation (NLG). A research challenge is to learn each module with the least amount of samples (i.e., few-shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT- 2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication to future work.", 
This paper proposes a word association based method for generating summaries in a variety of languages. We show that a robust statistical method for finding associations which are specific to the given document(s) is applicable to many languages.,Language-independent multi-document text summarization with document-specific word associations,The goal of automatic text summarization is to generate an abstract of a document or a set of documents. In this paper we propose a word association based method for generating summaries in a variety of languages. We show that a robust statistical method for finding associations which are specific to the given document(s) is applicable to many languages. We introduce strategies that utilize the discovered associations to effectively select sentences from the document(s) to constitute the summary. Empirical results indicate that the method works reliably in a relatively large set of languages and outperforms methods reported in MultiLing 2013., 
"Smart city applications are formed by a network of distributed sensors. Mobile crowd-sensing (MCS) concept prescribes a drastically different platform for sensing. A network of smartphones, owned by a volunteer crowd, can capture, preprocess, and transmit data. Dedicated sensors imply higher deployment and maintenance costs, the MCS concept has known implementation challenges.",Large Scale Distributed Dedicated- and Non-Dedicated Smart City Sensing Systems,"The past decade has witnessed an explosion of interest in smart cities in which a set of applications such as smart healthcare, smart lighting, and smart transportation promise to drastically improve the quality and efficiency of these services. The skeleton of these applications is formed by a network of distributed sensors that captures data, pre-processes, and transmits it to a center for further processing. While these sensors are generally perceived to be a wireless network of sensing devices that are deployed permanently as part of an application, the emerging mobile crowd-sensing (MCS) concept prescribes a drastically different platform for sensing; a network of smartphones, owned by a volunteer crowd, can capture, preprocess, and transmit the data to the same center. We call these two forms of sensors dedicated and non-dedicated sensors in this paper. While dedicated sensors imply higher deployment and maintenance costs, the MCS concept also has known implementation challenges, such as incentivizing the crowd and ensuring the trustworthiness of the captured data, and covering a wide sensing area. Due to the pros/cons of each option, the decision as to which one is better becomes a non-trivial answer. In this paper, we conduct a thorough study of both types of sensors and draw conclusions about which one becomes a favorable option based on a given application platform.", 
Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial speech recognition service.,Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", 
"Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data.",Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning,"Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domainspecific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover’s Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.", 
An ensemble metric learning approach that consists of sparse block diagonal metric ensembling and joint metric learning. Its applications to face verification and retrieval outperform existing state-of-the-art methods in accuracy.,"Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval","Learning Mahanalobis distance metrics in a high-dimensional feature space is very difficult especially when structural sparsity and low rank are enforced to improve computational efficiency in testing phase. This paper addresses both aspects by an ensemble metric learning approach that consists of sparse block diagonal metric ensembling and joint metric learning as two consecutive steps. The former step pursues a highly sparse block diagonal metric by selecting effective feature groups while the latter one further exploits correlations between selected feature groups to obtain an accurate and low rank metric. Our algorithm considers all pairwise or triplet constraints generated from training samples with explicit class labels, and possesses good scalability with respect to increasing feature dimensionality and growing data volumes. Its applications to face verification and retrieval outperform existing state-of-the-art methods in accuracy while retaining high efficiency.", 
"This paper presents a new hierarchical representation of words, sentences and documents in a corpus. It infers the Dirichlet distributions for latent topics and latent themes in word level and sentence level. The proposed SLDA outperforms other methods for document summarization in terms of precision, recall and F-measure.",Latent Dirichlet learning for document summarization,"Automatic summarization is developed to extract the representative contents or sentences from a large corpus of documents. This paper presents a new hierarchical representation of words, sentences and documents in a corpus, and infers the Dirichlet distributions for latent topics and latent themes in word level and sentence level, respectively. The sentence-based latent Dirichlet allocation (SLDA) is accordingly established for document summarization. Different from the vector space summarization, SLDA is built to fit the fine structure of text documents, and is specifically designed for sentence selection. SLDA acts as a sentence mixture model with a mixture of Dirichlet themes, which are used to generate the latent topics in observed words. The theme model is inherent to distinguish sentences in a summarization system. In the experiments, the proposed SLDA outperforms other methods for document summarization in terms of precision, recall and F-measure.", 
"Latent semantic analysis (LSA) is a method for analyzing a piece of text with certain mathematical computation. Various application of intelligent information retrieval, search engines, internet news sites requires an accurate method of accessing document similarity. LSA shows that single value decomposition collapse multiple terms with same semantic.","Latent Semantic Analysis, An Approach to Understand Semantic of Text","Latent semantic analysis (LSA) is a method for analyzing a piece of text with certain mathematical computation and analyzing relationship between terms in the documents, between the documents in the corpus. Various application of intelligent information retrieval, search engines, internet news sites requires an accurate method of accessing document similarity in order to carry out classification, clustering, summarizing or search tasks. So in this paper we are studying latent semantic analysis based on single value decomposition. The aim of Latent semantic analysis is to exploit the global structure of documents. The emphasis of latent semantic analysis is to find hidden relationship in document for better understanding the relationship between terms and document in dataset. In this paper, we have conducting a study using Latent semantic analysis (LSA) to find correlation of terms in a dataset consisting of research papers of various natural language processing applications. LSA shows that single value decomposition collapse multiple terms with same semantic and can identify terms with multiple meaning and represent documents in lower dimensional conceptual space.", 
LSA is a theory of meaning as well as a method for extracting that meaning from passages of text. It defines a latent semantic space where documents and individual words are represented as vectors. LSA as a computational technique uses linear algebra to extract dimensions that represent that space.,Latent semantic analysis,"This article reviews latent semantic analysis (LSA), a theory of meaning as well as a method for extracting that meaning from passages of text, based on statistical computations over a collection of documents. LSA as a theory of meaning defines a latent semantic space where documents and individual words are represented as vectors. LSA as a computational technique uses linear algebra to extract dimensions that represent that space. This representation enables the computation of similarity among terms and documents, categorization of terms and documents, and summarization of large collections of documents using automated procedures that mimic the way humans perform similar cognitive tasks. We present some technical details, various illustrative examples, and discuss a number of applications from linguistics, psychology, cognitive science, education, information science, and analysis of textual data in general.", 
"Automatic summarization aims at shortening source documents while retaining main information. In this paper, we propose a novel approach for summarization based on a hierarchical Bayesian model of topic-semantic indexing.",Latent Topic-Semantic Indexing Based Automatic Text Summarization,"Automatic summarization, a difficult but pressing problem in natural language processing, aims at shortening source documents while retaining main information. In recent years, more statistical machine learning methods have been applied to automatic summarization. In this paper, we propose a novel approach for summarization, based on hierarchical Bayesian model of topic-semantic indexing (TSI) and extraction strategy of average log-likelihood. The new method is tested on Brown corpus, and its performance is analyzed by a well-designed blind experiment of one-way ANOVA on human reviews. The experimental results show that TSI model is promising on topic-driven summarization.", 
Layer-wise relevance propagation is a method to compute scores for image pixels and image regions denoting the impact of the particular image region on the prediction of the classifier.,Layer-wise Relevance Propagation for Deep Neural Network Architectures,We present the application of layer-wise relevance propagation to several deep neural networks such as the BVLC reference neural net and googlenet trained on ImageNet and MIT Places datasets. Layer-wise relevance propagation is a method to compute scores for image pixels and image regions denoting the impact of the particular image region on the prediction of the classifier for one particular test image. We demonstrate the impact of different parameter settings on the resulting explanation., 
This paper introduces a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We introduce recurrent neural network for the summary generation and achieve promising results.,"LCSTS, A Large Scale Chinese Short Text Summarization Dataset","Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.", 
Security bug reports (SBR) depict potential security vulnerabilities in software systems. Malicious attackers could exploit these SBRs. We studied the security bug reports of the Chromium project. We looked into three main aspects of these bug reports.,LDA Categorization of Security Bug Reports in Chromium Projects,"Security bug reports (SBR) depict potential security vulnerabilities in software systems. Bug tracking systems (BTS) usually contain huge numbers of bug reports including security-related ones. Malicious attackers could exploit these SBRs. Henceforth, it is very critical to pinpoint SBRs swiftly and correctly. In this work, we studied the security bug reports of the Chromium project. We looked into three main aspects of these bug reports, namely: frequencies of reporting them, how quickly they get fixed and is LDA effective in grouping these reports to known vulnerabilities types. We report our findings in these aspects.", 
LeafNATS is an open-source toolkit for training and evaluation sequence-to-sequence based models. The toolkit is modularized and extensible in addition to maintaining competitive performance in the NATS task.,"LeafNATS, An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization","Neural abstractive text summarization (NATS) has received a lot of attention in the past few years from both industry and academia. In this paper, we introduce an open-source toolkit, namely LeafNATS, for training and evaluation of different sequence-to-sequence based models for the NATS task, and for deploying the pre-trained models to real-world applications. The toolkit is modularized and extensible in addition to maintaining competitive performance in the NATS task. A live news blogging system has also been implemented to demonstrate how these models can aid blog/news editors by providing them suggestions of headlines and summaries of their articles.", 
Convolutional neural networks (CNN) have shown outstanding image classification performance in the largescale visual recognition challenge (ILSVRC2012) Success of CNNs is attributed to their ability to learn rich midlevel image representations. Learning CNNs amounts to estimating millions of parameters and requires a large number of annotated image samples. This property currently prevents application of CNN's to problems with limited training data.,Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks,"Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the largescale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich midlevel image representations as opposed to hand-designed low-level features used in other image classification methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be efficiently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred representation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.", 
Modern learning methods are playing an increasingly critical role in the field of personalised medicine. Personalised medicine refers to providing tailored medical treatment to individual patients through the identification of common features. This review paper provides an overview of the research progress in the application of learning methods with a focus on deep learning.,Learning for Personalized Medicine A Comprehensive Review From a Deep Learning Perspective,"With the recent advancements in analysing high volume, complex and unstructured data, modern learning methods are playing an increasingly critical role in the field of personalised medicine. Personalised medicine, which refers to providing tailored medical treatment to individual patients through the identification of common features, including their genetics, inheritance, and lifestyle, has attracted the attention of many researchers over the recent years. This review paper provides an overview of the research progress in the application of learning methods with a focus on deep learning in personalised medicine. In particular, three domains of applications are reviewed, including drug development, disease characteristics identification, and therapeutics effect prediction. The main objective of this survey is to consider the applied methods in detail and to offer insights into their pros and cons. Although having demonstrated advantages in coping with data complexity and nonlinearity, and in recognising features and associating structural data, the studied learning methods are not a panacea to all the medical problems. Hence, we discuss the existing research challenges and clarify the future study directions.", 
"Most modern face recognition systems rely on a feature representation given by a hand-crafted image descriptors. In this paper, we propose deep learning as a natural source for obtaining additional, complementary representations. We also present a novel application of deep learning to descriptors other than pixel intensity values, such as LBP.",Learning Hierarchical Representations for Face Verification with Convolutional Deep Belief Networks,"Most modern face recognition systems rely on a feature representation given by a hand-crafted image descriptor, such as Local Binary Patterns (LBP), and achieve improved performance by combining several such representations. In this paper, we propose deep learning as a natural source for obtaining additional, complementary representations. To learn features in high-resolution images, we make use of convolutional deep belief networks. Moreover, to take advantage of global structure in an object class, we develop local convolutional restricted Boltzmann machines, a novel convolutional learning model that exploits the global structure by not assuming stationarity of features across the image, while maintaining scalability and robustness to small misalignments. We also present a novel application of deep learning to descriptors other than pixel intensity values, such as LBP. In addition, we compare performance of networks trained using unsupervised learning against networks with random filters, and empirically show that learning weights not only is necessary for obtaining good multilayer representations, but also provides robustness to the choice of the network architecture parameters. Finally, we show that a recognition system using only representations obtained from deep learning can achieve comparable accuracy with a system using a combination of hand-crafted image descriptors. Moreover, by combining these representations, we achieve state-of-the-art results on a real-world face verification database.", 
Query-biased Web page summarization is the summarization of a Web page reflecting the relevance of it to a specific query. It plays an important role in search results representation of Web search engines. Most of existing learning-based summarization methods treat summarization as a sentence classification problem.,Learning query-biased web page summarization,"Query-biased Web page summarization is the summarization of a Web page reflecting the relevance of it to a specific query. It plays an important role in search results representation of Web search engines. In this paper, we propose a learning-based querybiased Web page summarization method. The summarization problem is solved within the typical sentence selection framework. Different from existing Web page summarization methods that use page content or link context alone, both of them are considered as the sources of sentences in this work. Most of existing learningbased summarization methods treat summarization as a sentence classification problem and train a classifier to discriminate between extracted sentences and non-extracted sentences of all training documents. The basic assumption of these methods is that sentences from different documents are comparable with respect to the class information. In contrast to the classification scheme, a ranking scheme is introduced to rank extracted sentences higher than non-extracted sentences of each training document. The underlying assumption that sentences within a document are comparable is weaker and more reasonable than the assumption of classification-based scheme. Extensive results using intrinsic evaluation metrics gauge many aspects of the proposed method.", 
"Natural Language Processing (NLP) methods allow us to understand and manipulate natural language text or speech. One of the most outstanding NLP task is the Automatic Text Summarization (ATS) task. In this paper, a method for describing the ideal behavior (gold standard) of an ATS system is proposed.",Learning Relevant Models using Symbolic Regression for Automatic Text Summarization,". Natural Language Processing (NLP) methods allow us to understand and manipulate natural language text or speech to do useful things. There are several specific techniques in this area, and although new approaches to solving the problems arise, its evaluation remains similar. NLP methods are regularly evaluated by a gold standard, which contains the correct results which must be obtained by a method. In this situation, it is desirable that NLP methods can close as possible to the results of the gold standard being evaluated. One of the most outstanding NLP task is the Automatic Text Summarization (ATS). ATS task consists in reducing the size of a text while preserving their information content. In this paper, a method for describing the ideal behavior (gold standard) of an ATS system, is proposed. The proposed method can obtain models that describe the ideal behavior which is described by the topline. In this work, eight models for ATS are obtained. These models generate better results than other models used in the state-of-the-art on ATS task.", 
"This is a challenging problem because seizure manifestations on EEG are extremely variable both inter- and intra-patient. By simultaneously capturing spectral, temporal and spatial information our recurrent convolutional neural network learns a general spatially invariant representation.",Learning Robust Features using Deep Learning for Automatic Seizure Detection,"We present and evaluate the capacity of a deep neural network to learn robust features from EEG to automatically detect seizures. This is a challenging problem because seizure manifestations on EEG are extremely variable both inter- and intra-patient. By simultaneously capturing spectral, temporal and spatial information our recurrent convolutional neural network learns a general spatially invariant representation of a seizure. The pro posed approach exceeds significantly previous results obtained on cross-patient classifiers both in terms of sensitivity and false positive rate. Furthermore, our model proves to be robust to missing channel and variable electrode montage.", 
We propose to extract semantic similarities based on topical representations of sentences. We propose a fuzzy hypergraph model in which nodes are sentences and fuzzy hyperedges are topics. We extract a set of sentences from the corpus by maximizing their relevance to a user-defined query.,"Learning with fuzzy hypergraphs, A topical approach to query-oriented text summarization","Existing graph-based methods for extractive document summarization represent sentences of a corpus as the nodes of a graph in which edges depict relationships of lexical similarity between sentences. This approach fails to capture semantic similarities between sentences when they express a similar information but have few words in common and are thus lexically dissimilar. To overcome this issue, we propose to extract semantic similarities based on topical representations of sentences. Inspired by the Hierarchical Dirichlet Process, we propose a topic model to infer topic distributions of sentences. As each topic defines a semantic connection among sentences with a certain degree of membership for each sentence, we propose a fuzzy hypergraph model in which nodes are sentences and fuzzy hyperedges are topics. To produce an informative summary, we extract a set of sentences from the corpus by simultaneously maximizing their relevance to a user-defined query, their centrality in the fuzzy hypergraph and their coverage of topics present in the corpus. We formulate an algorithm building on the theory of submodular functions to solve the associated optimization problem. A thorough comparative analysis with other graph-based summarizers demonstrates the superiority of our method in terms of content coverage of the summaries.", 
"This paper reports on the project called ""lecture summarization service"", a python-based RESTful service that utilizes the BERT model for text embeddings and K-Means clustering to identify sentences closest to the centroid for summary selection. The service also includes lecture and summary management, storing content on the cloud which can be used for collaboration.",Leveraging BERT for extractive text summarization on lectures,"In the last two decades, automatic extractive text summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content. However, many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. Recently, new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models. This paper reports on the project called “lecture summarization service”, a python-based RESTful service that utilizes the BERT model for text embeddings and K-Means clustering to identify sentences closest to the centroid for summary selection. The purpose of the service was to provide student’s a utility that could summarize lecture content, based on their desired number of sentences. On top of summary work, the service also includes lecture and summary management, storing content on the cloud which can be used for collaboration. While the results of utilizing BERT for extractive text summarization were promising, there were still areas where the model struggled, providing future research opportunities for further improvement.", 
"Sentence regression has achieved state-of-the-art performance in several widely-used practical systems. We propose a neural network model, Contextual Relation-based Summarization (CRSum), to take advantage of contextual relations among sentences. CRSum is able to pay a‰ attention to important content, i.e., words and sentences, in the surrounding context.",Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model,"As a framework for extractive summarization, sentence regression has achieved state-of-the-art performance in several widely-used practical systems. Œe most challenging task within the sentence regression framework is to identify discriminative features to encode a sentence into a feature vector. So far, sentence regression approaches have neglected to use features that capture contextual relations among sentences. We propose a neural network model, Contextual Relation-based Summarization (CRSum), to take advantage of contextual relations among sentences so as to improve the performance of sentence regression. Speci€cally, we €rst use sentence relations with a wordlevel aŠentive pooling convolutional neural network to construct sentence representations. Œen, we use contextual relations with a sentence-level aŠentive pooling recurrent neural network to construct context representations. Finally, CRSum automatically learns useful contextual features by jointly learning representations of sentences and similarity scores between a sentence and sentences in its context. Using a two-level aŠention mechanism, CRSum is able to pay aŠention to important content, i.e., words and sentences, in the surrounding context of a given sentence. We carry out extensive experiments on six benchmark datasets. CRSum alone can achieve comparable performance with state-ofthe-art approaches; when combined with a few basic surface features, it signi€cantly outperforms the state-of-the-art in terms of multiple ROUGE metrics.", 
5G enabled vehicular ad hoc network (5G-VANET) plays a promising role to support diverse intelligent transportation system (ITS) applications. The paper proposes a dynamic Stackelberg pricing game enabled multi-mode spectrum sharing solution. It says the proposed algorithm can improve the total transmission rate of VANET by at least 20% compared with the random selection method.,Leveraging Dynamic Stackelberg Pricing Game for Multi-Mode Spectrum Sharing in 5G-VANET,"5G enabled Vehicular ad hoc network (5G-VANET) plays a promising role to support diverse intelligent transportation system (ITS) applications. There are three types of communication modes in 5G-VANET: cellular mode, reuse mode and dedicated mode, i.e., vehicle users (VUEs) communicates with each other using the cellular network spectrum directly, in an underlay sharing way, and utilizing the allocated dedicated spectrum, respectively. However, how to dynamically share the multi-mode spectrum to optimize the network performance (i.e., network throughput) in 5G-VANET is a challenging task due to the high dynamic VANET environment and network resource heterogeneity. In this paper, we propose a dynamic Stackelberg pricing game enabled multi-mode spectrum sharing solution in 5G-VANET. In specific, we develop an access price strategy for different spectrum sharing modes considering the cellular BS’s revenue and whole network throughput, while VUEs can select communication modes in a distributed way and dynamically change the selections through an evolutionary game. Through testing different traffic scenarios generated by SUMO, we demonstrate the effectiveness of the proposed algorithm. Specifically, the proposed algorithm can improve the total transmission rate of VANET by at least 20% compared with the random selection method.", 
A machine learning based Distributed Denial of Service (DDoS) attack detection system has been presented in this paper. This system identifies whether any incoming traffic in a network is a DDoS type or not.,Leveraging Machine Learning Approach to Setup Software De ned Network(SDN) Controller Rules During DDoS Attack,"A machine learning based Distributed Denial of Service (DDoS) attack detection system, implemented in a virtual SDN environment testbed, has been presented in this paper. This system identifies whether any incoming traffic in a network is a DDoS type or not. To implement this approach, we applied AdaBoosting with decision stump as a weak classifier to train our model on a private network dataset in SDN environment. Our model showed up to 93% detection accuracy with a low false-positive rate. We have also tested and compared our model's accuracy with different machine learning algorithms and presented the result.", 
Text summarization is one of the most critical Natural Language Processing (NLP) tasks. This paper proposes two methods to address this task. The models employed in this paper are mT5 and an encoder-decoder version of the ParsBERT model.,Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization,"Text summarization is one of the most critical Natural Language Processing (NLP) tasks. More and more researches are conducted in this field every day. Pre-trained transformer-based encoder-decoder models have begun to gain popularity for these tasks. This paper proposes two methods to address this task and introduces a novel dataset named pn-summary for Persian abstractive text summarization. The models employed in this paper are mT5 and an encoder-decoder version of the ParsBERT model (i.e., a monolingual BERT model for Persian). These models are fine-tuned on the pn-summary dataset. The current work is the first of its kind and, by achieving promising results, can serve as a baseline for any future work.", 
"This paper tackles Arabic text summarization problem. The semantic redundancy and insignificance will be removed from the summarized text. It can be achieved by checking the text entailment relation, and lexical cohesion. LCEAS is a single document summarization, which is constructed using extraction technique.",Lexical cohesion and entailment based segmentation for arabic text summarization (lceas),"Text summarization is the process of creating a short description of a specified text while preserving its information context. This paper tackles Arabic text summarization problem. The semantic redundancy and insignificance will be removed from the summarized text. This can be achieved by checking the text entailment relation, and lexical cohesion. Accordingly, a text summarization approach (called LCEAS) based on lexical cohesion and text entailment relation is developed. In LCEAS, text entailment approach is enhanced to suit Arabic language. Roots and semantic-relations are used between the senses of the words to extract the common words. New threshold values are specified to suit entailment based segmentation for Arabic text. LCEAS is a single document summarization, which is constructed using extraction technique. To evaluate LCEAS, its performance is compared with previous Arabic text summarization systems. Each system output is compared against Essex Arabic Summaries Corpus (EASC) corpus (the model summaries), using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and Automatic Summarization Engineering (AutoSummEng) metrics. The outcome of LCEAS indicates that the developed approach outperforms the previous Arabic text summarization systems.", 
"The semantic redundancy and insignificance will be removed from the summarized text. It can be achieved by checking the text entailment relation, and lexical cohesion. LCEAS is a single document summarization, which is constructed using extraction technique.",Lexical Cohesion Based Topic Modeling for Summarization,"Text summarization is the process of creating a short description of a specified text while preserving its information context. This paper tackles Arabic text summarization problem. The semantic redundancy and insignificance will be removed from the summarized text. This can be achieved by checking the text entailment relation, and lexical cohesion. Accordingly, a text summarization approach (called LCEAS) based on lexical cohesion and text entailment relation is developed. In LCEAS, text entailment approach is enhanced to suit Arabic language. Roots and semantic-relations are used between the senses of the words to extract the common words. New threshold values are specified to suit entailment based segmentation for Arabic text. LCEAS is a single document summarization, which is constructed using extraction technique. To evaluate LCEAS, its performance is compared with previous Arabic text summarization systems. Each system output is compared against Essex Arabic Summaries Corpus (EASC) corpus (the model summaries), using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and Automatic Summarization Engineering (AutoSummEng) metrics. The outcome of LCEAS indicates that the developed approach outperforms the previous Arabic text summarization systems.", 
Research examines the use of such evidence through the review of appellate judgments. Photos/images evidence was used the most in State criminal cases and all Federal cases. Social media evidence was also used more in State civil cases.,LinkedLegal Investigating social media as evidence in courtrooms,"The pervasive nature of social media suggests it would increasingly appear as evidence in the courtroom as it has increasingly documented daily life. This research examines the use of such evidence through the review of appellate judgments. It has identified 5,189 appeal cases in federal and state jurisdictions for the period from 1 October 2000 to 30 September 2017. California was used for the state jurisdictional analysis and the Ninth Circuit Court of Appeals, which includes California, was used for the federal. In 2017, there was a 350% increase in Ninth Circuit cases using social media evidence as compared to the first cases in 2010. There was a 3933% increase in the California state cases from the first cases in 2007. Photos/images evidence were used the most in State criminal cases and all Federal cases, while posts evidence was used the most in State civil cases.", 
"In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from Liputan6.com, an online news portal. We leverage pre-trained language models to develop benchmark summarization methods.","Liputan6, A Large-scale Indonesian Dataset for Text Summarization","In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from Liputan6.com, an online news portal, and obtain 215,827 document– summary pairs. We leverage pre-trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT-based models. We include a thorough error analysis by examining machine-generated summaries that have low ROUGE scores, and expose both issues with ROUGE itself, as well as with extractive and abstractive summarization models.", 
Automatic text summarization is a system of summarizing text by computer where a text is given to the computer as input and the output is a shorter and less redundant form of the original text. Research was first started naively on single document abridgement but recently information is found from various sources about a single topic. Various techniques are compared here that have done for multi-document summarization.,Literature Review of Automatic Multiple Documents Text Summarization,"For the blessing of World Wide Web, the corpus of online information is gigantic in its volume. Search engines have been developed such as Google, AltaVista, Yahoo, etc., to retrieve specific information from this huge amount of data. But the outcome of search engine is unable to provide expected result as the quantity of information is increasing enormously day by day and the findings are abundant. So, the automatic text summarization is demanded for salient information retrieval. Automatic text summarization is a system of summarizing text by computer where a text is given to the computer as input and the output is a shorter and less redundant form of the original text. An informative précis is very much helpful in our daily life to save valuable time. Research was first started naively on single document abridgement but recently information is found from various sources about a single topic in different website, journal, newspaper, text book, etc., for which multi-document summarization is required. In this paper, automatic multiple documents text summarization task is addressed and different procedure of various researchers are discussed. Various techniques are compared here that have done for multi-document summarization. Some promising approaches are indicated here and particular concentration is dedicated to describe different methods from raw level to similar like human experts, so that in future one can get significant instruction for further analysis.", 
"In the time of overloaded online information, automatic text summarization is especially demanded for salient information retrieval from huge amount electronic text. A significant précis is very much helpful in our day to day life which can save valuable time. This literature review intends to observe the trends of abstraction procedure using natural language processing.",Literature Review of Automatic Single Document Text Summarization Using NLP,"In the time of overloaded online information, automatic text summarization is especially demanded for salient information retrieval from huge amount electronic text. For the blessing of World Wide Web, the mass of data is now enormous in its volume. Researchers realized this fact from various aspects and tried to generate an automatic abstract of the gigantic body of data from the commencement of the last half century. Numerous ways are there for characterizing different approaches to passage recapitulation: extractive and abstractive from single or compound document, objective of content abridgement, characteristic of text summarization, level of processing from superficial to profound and sort of article’s content. A significant précis is very much helpful in our day to day life which can save valuable time. The investigation was at first commenced naively on single document abstraction. In this paper, automatic single document text summarization task is addressed and different methodologies of various researchers are discussed from the very beginning of this research to this modern age. This literature review intends to observe the trends of abstraction procedure using natural language processing. Also some promising approaches are indicated and particular concentration is dedicated for the categorization of diversified methods from raw level to similar like human professionals, so that in future one can get precious direction for further analysis.", 
Research in the field of text summarization began in the 1950s and until now there is no system that can produce summaries such as professionals or humans. The extractive approach is still in demand in the past three years because the extractive is easier than abstractive.,"Literature Review of Automatic Text Summarization, Research Trend, Dataset and Method","Automatic text summarization can be defined as the process of presenting one or more text documents while maintaining the main information content using an automatic machine with no more than half the original text or less than the original text. Research in the field of text summarization began in the 1950s and until now there is no system that can produce summaries such as professionals or humans. This paper aims to identify and analyze methods, datasets and trends in automatic text summarization research from 2015 to 2019. The method used a systematic literature review (SLR) about automatic text summarization. The results obtained are that research on automatic text summarization is still relevant to date. The extractive approach is still in demand in the past three years because the extractive is easier than abstractive and the opportunity to combine methods is still open, for example using a neuro computing approach, namely the emergence of a new DQN method (Deep Q-Network) which shows comparable results and even better. The text summarization research trend has also undergone a slight change in the past three years where new things have emerged that are trends that are leading to optimization, how to optimize text summarization performance in order to get high accuracy.", 
Research in the field of text summarization began in the 1950s and until now there is no system that can produce summaries such as professionals or humans. The extractive approach is still in demand in the past three years because the extractive is easier than abstractive.,"Literature review on automatic text summarization, Single and multiple summarizations","The online information available on world wide web is in enormous amount. Search engines like Google, Yahoo were developed to retrieve information from the databases. But actual results were not obtained as the electronic information is increasing day by day. Thus automatic summarization came into demand. Automatic summarization gathers several documents as input and provides the shorter summarized version as output which is informative, unambiguous, save valuable time. Research was done on a single document and moved towards multiple documents. This review categorizes single and multiple summarization methods.", 
"Text summarization is a method which generates a shorter and a precise form of one or more text documents. There are a number of approaches to multi-document summarization such as Graph, Cluster, Term-Frequency and Latent Semantic Analysis (LSA) based.",Literature Study on Multi-document Text Summarization Techniques,"Text summarization is a method which generates a shorter and a precise form of one or more text documents. Automatic text summarization plays an essential role in finding information from large text corpus or an internet. What had actually started as a single document Text Summarization has now evolved and developed into generating multi-document summarization. There are a number of approaches to multi-document summarization such as Graph, Cluster, Term-Frequency, Latent Semantic Analysis (LSA) based etc. In this paper we have started with introduction of multi-document summarization and then have further discussed comparison and analysis of various approaches which comes under the multi-document summarization. The paper also contains details about the benefits and problems in the existing methods. This would especially be helpful for researchers working in this field of text data mining. By using this data, researchers can build new or mixed based approaches for multi-document summarization.", 
"Each vehicle is acting as a smart device which could establish a smart communication among the vehicles. The aim of VANET architecture is to provide an efficient and effective emergency and warning alerts to the vehicles so that the vehicle can take appropriate decisions. In this paper, we have developed a live emergency alerts to be shown to the cars through an android application.",Live Emergency and Warning Alerts Through Android Application for Vehicular Ad Hoc Network Communication (Android VANET),"The technology is growing towards smart communication through smart devices. This smart communication leads to the development of Vehicular Ad Hoc Network (VANET). These days each vehicle is acting as a smart device which could establish a smart communication among the vehicles. The growth of VANET communication passes through various stages such as RoadSide Unit (RSU), Vehicle to Vehicle (V2V), Cluster based, Internet of Vehicles (IoV), and Web VANET (WVANET) Communication models. All these advancements in VANET architecture provide a smart communication among the vehicles. The vital aim of VANET architecture is to provide an efficient and effective emergency and warning alerts to the vehicles so that the vehicle can take appropriate decisions without any delay to safeguard the safety of the passengers. However, it will be more reliable to the vehicles if the VANET architecture could provide a live emergency and warning alerts to the vehicles in well ahead of time. In order to provide live emergency and warning alerts the communication as well as the device should be smart. In this paper, we have developed a live emergency and warning alerts to the vehicles though android application. Each vehicle will have the android application installed on it, in which the entire live driving scenario is provided. Once, the live driving scenario is provided, the live emergency and warning alerts can be shown to the vehicles in well ahead of time. As live emergency alerts are shown to the vehicles, it will help the vehicles to take the right decision more effectively.", 
This research work explores the early prediction of liver disease using various decision tree techniques. Analysis proves that Decision Stump provides the highest accuracy than other techniques. The main purpose of this work is to calculate the performance of various decision Tree techniques.,LIVER DISEASE PREDICTION BY USING DIFFERENT DECISION TREE TECHNIQUES,"Early prediction of liver disease is very important to save human life and take proper steps to control the disease. Decision Tree algorithms have been successfully applied in various fields especially in medical science. This research work explores the early prediction of liver disease using various decision tree techniques. The liver disease dataset which is select for this study is consisting of attributes like total bilirubin, direct bilirubin, age, gender, total proteins, albumin and globulin ratio. The main purpose of this work is to calculate the performance of various decision tree techniques and compare their performance. The decision tree techniques used in this study are J48, LMT, Random Forest, Random tree, REPTree, Decision Stump, and Hoeffding Tree. The analysis proves that Decision Stump provides the highest accuracy than other techniques.", 
"Fine-grained image recognition is a challenging computer vision problem. In this paper, we prove that selecting useful deep descriptors contributes well to image recognition. A novel Mask-CNN model with out the fully connected layers is proposed. The model has a small feature dimensionality and efficient inference speed.",Mask-CNN: Localizing parts and selecting descriptors for fine-grained bird species categorization,"TED MANUSC Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate cate gories, and the large intra-class variations in poses, scales and rotations. In this paper, we prove that selecting useful deep descriptors contributes well to fine-grained image recognition. Specifically, a novel Mask-CNN model with out the fully connected layers is proposed. Based on the part annotations, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate weighted object/part masks for selecting useful and meaningful convolutional descriptors. After that, a three-stream Mask-CNN model is built for aggregat ing the selected object- and part-level descriptors simultaneously. Thanks to discarding the parameter redundant fully connected layers, our Mask-CNN has a small feature dimensionality and efficient inference speed by comparing with other fine-grained approaches. Furthermore, we obtain a new state-of-the-art accuracy on two challenging fine-grained bird species categorization datasets, which validates the effectiveness of both the descriptor selection scheme and the proposed Mask-CNN model.", 
"The authors perform an in-depth, focused, and large-scale analysis of logging code constructs at two levels: the file level and catch-blocks level. They answer several research questions related to statistical and content analysis. The machine-learning-based model is found to be e?ective in catch-block logging prediction.",Logging Analysis and Prediction in Open Source Java Project,"Log statements present in source code provide important information to the software developers because they are useful in various software development activities such as debugging, anomaly detection, and remote issue resolution. Most of the previous studies on logging analysis and prediction provide insights and results after analyzing only a few code constructs. In this chapter, the authors perform an in-depth, focused, and large-scale analysis of logging code constructs at two levels: the file level and catch-blocks level. They answer several research questions related to statistical and content analysis. Statistical and content analysis reveals the presence of di?erentiating properties among logged and nonlogged code constructs. Based on these findings, the authors propose a machine-learning-based model for catch-blocks logging prediction. The machine-learning-based model is found to be e?ective in catch-blocks logging prediction.", 
"Low-Power Wide Area Networks (LPWANs) are the ideal technologies to send small data occasionally. With their long range capabilities and low energy consumption, they are ideal technologies for the Internet of Things.",Long Range Communications in Urban and Rural Environments,"ow-Power Wide Area Networks (LPWANs) are prominent technologies in the field of the Internet of Things (loT). Due to the long range capabilities and low energy consumption, Low-Power Wide Area Networks (LPWANs) are the ideal technologies to send small data occasionally. With their unique characteristics, LPWANs can be used in many applications and in different environments such as urban, rural and even indoor. The work in this paper presents a study on the LPWAN LoRa technology, by testing and evaluating its range, signal quality properties and its performance in delivering data. Three distinct scenarios are proposed and tested, one rural and two urban scenarios in Portugal. The maximum communication range achieved was 5660 meters for the rural scenario and around 2000 meters for the urban ones, and we concluded that there are three essential scenario characteristics that influence the achieved results: distance, elevation difference and obstacles in the signal path.", 
Unsupervised query-focused extractive summarization systems use query-specific features along with short-range language models. We hypothesize that applying long-span n-gram-based and neural LMs that better capture larger context can help improve these subtasks.,Long-Span Language Models for Query-Focused Unsupervised Extractive Text Summarization,"Effective unsupervised query-focused extractive summarization systems use query-specific features along with short-range language models (LMs) in sentence ranking and selection summarization subtasks. We hypothesize that applying long-span n-gram-based and neural LMs that better capture larger context can help improve these subtasks. Hence, we outline the first attempt to apply long-span models to a query-focused summarization task in an unsupervised setting. We also propose the Across Sentence Boundary LSTM-based LMs, ASBLSTM and bi ASBLSTM , that is geared towards the query-focused summarization subtasks. Intrinsic and extrinsic experiments on a real word corpus with 100 Wikipedia event descriptions as queries show that using the long-span models applied in an integer linear programming (ILP) formulation of MMR criterion are the most effective against several state-of-the-art baseline methods from the literature.", 
"Connected vehicles (CV) technology has recently drawn an increasing attention from governments, vehicle manufacturers, and researchers. This study applied the CV concept on a congested expressway (SR408) in Florida to improve traffic safety. The study of CV MPR is worthwhile in the CV transition period, authors say. Results of this study provide useful insight for the management ofCV MPR as managed-lane CV platoons, they say.",Longitudinal safety evaluation of connected vehiclesâ€™ platooning on expressways,"Connected vehicles (CV) technology has recently drawn an increasing attention from governments, vehicle manufacturers, and researchers. One of the biggest issues facing CVs popularization associates it with the market penetration rate (MPR). The full market penetration of CVs might not be accomplished recently. Therefore, traffic flow will likely be composed of a mixture of conventional vehicles and CVs. In this context, the study of CV MPR is worthwhile in the CV transition period. The overarching goal of this study was to evaluate longitudinal safety of CV platoons by comparing the implementation of managed-lane CV platoons and all lanes CV platoons (with same MPR) over non-CV scenario. This study applied the CV concept on a congested expressway (SR408) in Florida to improve traffic safety. The Intelligent Driver Model (IDM) along with the platooning concept were used to regulate the driving behavior of CV platoons with an assumption that the CVs would follow this behavior in real-world. A high-level control algorithm of CVs in a managed-lane was proposed in order to form platoons with three joining strategies: rear join, front join, and cut-in joint. Five surrogate safety measures, standard deviation of speed, time exposed time-to-collision (TET), time integrated time-to-collision (TIT), time exposed rear-end crash risk index (TERCRI), and sideswipe crash risk (SSCR) were utilized as indicators for safety evaluation. The results showed that both CV approaches (i.e., managed-lane CV platoons, and all lanes CV platoons) significantly improved the longitudinal safety in the studied expressway compared to the non-CV scenario. In terms of surrogate safety measures, the managed-lane CV platoons significantly outperformed all lanes CV platoons with the same MPR. Different time-to-collision (TTC) thresholds were also tested and showed similar results on traffic safety. Results of this study provide useful insight for the management of CV MPR as managed-lane CV platoons.", 
State of the art in visual object retrieval from large databases is achieved by systems that are inspired by text retrieval. This paper explores techniques to map each visual region to a weighted set of words. We show that soft-assignment is always beneficial for retrieval with large vocabularies.,Lost in Quantization Improving Particular Object Retrieval in Large Scale Image Databases,"The state of the art in visual object retrieval from large databases is achieved by systems that are inspired by text retrieval. A key component of these approaches is that local regions of images are characterized using high-dimensional descriptors which are then mapped to “visual words” selected from a discrete vocabulary. This paper explores techniques to map each visual region to a weighted set of words, allowing the inclusion of features which were lost in the quantization stage of previous systems. The set of visual words is obtained by selecting words based on proximity in descriptor space. We describe how this representation may be incorporated into a standard tf-idf architecture, and how spatial verification is modified in the case of this soft-assignment. We evaluate our method on the standard Oxford Buildings dataset, and introduce a new dataset for evaluation. Our results exceed the current state of the art retrieval performance on these datasets, particularly on queries with poor initial recall where techniques like query expansion suffer. Overall we show that soft-assignment is always beneficial for retrieval with large vocabularies, at a cost of increased storage requirements for the index.", 
"Electroencephalographms (EEGs) are records of brain electrical activity. It is an indispensable tool for diagnosing neurological diseases, such as epilepsy. Wavelet transform (WT) is an effective tool for analysis of nonstationary signal. The research demonstrated that the Lyapunov exponents and Wavelet Coefficient are the features which well represent the EEG signals.",Lyapunov Features based EEG Signal Classification By Multi-Class SVM,"Electroencephalographms (EEGs) are records of brain electrical activity. It is an indispensable tool for diagnosing neurological diseases, such as epilepsy. Wavelet transform (WT) is an effective tool for analysis of nonstationary signal, such as EEGs. Wavelet analysis is used to decompose the EEG into delta, theta, alpha, beta, and gamma sub-bands. Lyapunov exponent is used to quantify the nonlinear chaotic dynamics of the signal.. Furthermore, the distinct states of brain activity had different chaotic dynamics quantified by nonlinear invariant measures such as Lyapunov exponents. The probabilistic neural network (PNN) and radial basis function neural network were tested and also their performance of classification rate was evaluated using benchmark dataset. Decision making was performed in two stages: feature extraction by computing the Lyapunov exponents, Wavelet Coefficients and classification using the classifiers trained on the extracted features. Our research demonstrated that the Lyapunov exponents and Wavelet Coefficients are the features which well represent the EEG signals and the multi-class SVM and PNN trained on these features achieved high classification accuracies such as 96% and 94%.", 
"This study outlines the state-of-the-art architecture, recent advances, and open research challenges in communication technologies for M2M. An overview of considerable architectural enhancements and novel techniques expected in 5G networks is presented.","M2M Communications in 5G State-of-the-Art Architecture, Recent Advances, and Research Challenges","M2M communication o?ers ubiquitous applications and is one of the leading facilitators of the Internet of Things paradigm. Unlike human-to-human communication, distinct features of M2M traffic necessitates specialized and interoperable communication technologies. However, most existing solutions o?ering wired or wireless connectivity have limitations that hinder widespread horizons of M2M applications. To cope with the peculiar nature of M2M traffic, the evolving 5G system considers the integration of key enabling networking technologies for ubiquitous connectivity and guaranteed QoS. This study outlines the state-of-the-art architecture, recent advances, and open research challenges in communication technologies for M2M. Also, an overview of considerable architectural enhancements and novel techniques expected in 5G networks is presented, followed by the resultant services and benefits for M2M communications.", 
"The widespread adoption of Android devices and their capability to access significant private and confidential information have resulted in these devices being targeted by malware developers. In this paper, we present two machine learning aided approaches for static analysis of Android malware.",Machine learning aided Android malware classification,"The widespread adoption of Android devices and their capability to access significant private and confidential information have resulted in these devices being targeted by malware developers. Existing Android malware analysis techniques can be broadly categorized into static and dynamic analysis. In this paper, we present two machine learning aided approaches for static analysis of Android malware. The first approach is based on permissions and the other is based on source code analysis utilizing a bag-of-words representation model. Our permission-based model is computationally inexpensive, and is implemented as the feature of OWASP Seraphimdroid Android app that can be obtained from Google Play Store. Our evaluations of both approaches indicate an F-score of 95.1% and F-measure of 89% for the source code-based classification and permission-based classification models, respectively.", 
Analysts can now apply machine learning algorithms to large baseball data sets to derive meaningful insights into player and team performance. We find two algorithms dominate the literature: Support Vector Machines for classification problems and k-nearest neighbors for both classification and Regression problems. We postulate that recent proliferation of neural networks in general machine learning research will soon carry over into baseball analytics.,Machine Learning Applications in Baseball A Systematic Literature Review,"Statistical analysis of baseball has long been popular, albeit only in limited capacity until relatively recently. In particular, analysts can now apply machine learning algorithms to large baseball data sets to derive meaningful insights into player and team performance. In the interest of stimulating new research and serving as a go-to resource for academic and industrial analysts, we perform a systematic literature review of machine learning applications in baseball analytics. The approaches employed in literature fall mainly under three problem class umbrellas: Regression, Binary Classification, and Multiclass Classification. We categorize these approaches, provide our insights on possible future applications, and conclude with a summary of our findings. We find two algorithms dominate the literature: (1) Support Vector Machines for classification problems and (2) k-nearest neighbors for both classification and Regression problems. We postulate that recent proliferation of neural networks in general machine learning research will soon carry over into baseball analytics.", 
"Machine learning and deep learning, as we know, have started ruling over almost every field in the computing industry and so, has revolutionized the process of text summarization too. This paper presents the modus operandi of an Architecture called Encoder-Decoder, under the machine learning approach.",Machine Learning Approach for Automatic Text Summarization Using Neural Networks,"Machine learning and deep learning, as we know, have started ruling over almost every field in the computing industry and so, has revolutionized the process of text summarization too. Automatic text summarization is an advancing realm of the natural language processing research in which concise textual summaries are generated from lengthy input documents. Extensive research has been carried out on how automatic summarization can be prosecuted through various extractive and abstractive techniques. In this paper, we address all the approaches to text summarization and present the modus operandi of an Architecture called Encoder-Decoder, under the machine learning approach. Moreover, we propose several novel implementation models for this architecture, in Keras and TensorFlow that consists of various machine learning and deep learning neural network libraries.", 
"In this study, we ranked the Multimodal Features extracted from Congestive Heart Failure (CHF) and Normal Sinus Rhythm (NSR) subjects. We categorized the ranked features into 1 to 5 categories based on Empirical Receiver Operating Characteristics (EROC) values. Highest detection performance in terms of accuracy and AUC was obtained with all multimodal features using SVM Gaussian. The findings indicate that proposed approach with feature ranking can be very useful for automatic detection of heart failure patients.",Machine learning based congestive heart failure detection using feature importance ranking of multimodal features,"In this study, we ranked the Multimodal Features extracted from Congestive Heart Failure (CHF) and Normal Sinus Rhythm (NSR) subjects. We categorized the ranked features into 1 to 5 categories based on Empirical Receiver Operating Characteristics (EROC) values. Instead of using all multimodal features, we use high ranking features for detection of CHF and normal subjects. We employed powerful machine learning techniques such as Decision Tree (DT), Naïve Bayes (NB), SVM Gaussian, SVM RBF and SVM Polynomial. The performance was measured in terms of Sensitivity, Specificity, Positive Predictive Value (PPV), Negative Predictive Value (NPV), Accuracy, False Positive Rate (FPR), and area under the Receiver Operating characteristic Curve (AUC). The highest detection performance in terms of accuracy and AUC was obtained with all multimodal features using SVM Gaussian with Sensitivity (93.06%), Specificity (81.82%), Accuracy (88.79%) and AUC (0.95). Using the top five ranked features, the highest performance was obtained with SVM Gaussian yields accuracy (84.48%), AUC (0.86); top nine ranked features using Decision Tree and Naïve Bayes got accuracy (84.48%), AUC (0.88); last thirteen ranked features using SVM polynomial obtained accuracy (80.17%), AUC (0.84). The findings indicate that proposed approach with feature ranking can be very useful for automatic detection of congestive heart failure patients and can be very helpful for further decision making by the clinicians and physicians in order to decrease the mortality rate.", 
"Software-Defined Networks (SDN) promises to change the way we design, build, and operate network architecture. The focus is now shifted to a single point of failure where the central controller is a prime target. Integration of intrusion detection system (IDS) into the SDN architecture is essential to provide a network with attack countermeasure.",Machine learning based intrusion detection system for software defined networks,"Software-Defined Networks (SDN) is an emerging area that promises to change the way we design, build, and operate network architecture. It tends to shift from traditional network architecture of proprietary based to open and programmable network architecture. However, this new innovative and improved technology also brings another security burden into the network architecture, with existing and emerging security threats. The network vulnerability has become more open to intruders: the focus is now shifted to a single point of failure where the central controller is a prime target. Therefore, integration of intrusion detection system (IDS) into the SDN architecture is essential to provide a network with attack countermeasure. The work designed and developed a virtual testbed that simulates the processes of the real network environment, where a star topology is created with hosts and servers connected to the OpenFlow OVS-switch. Signature-based Snort IDS is deployed for traffic monitoring and attack detection, by mirroring the traffic destine to the servers. The vulnerability assessment shows possible attacks threat exist in the network architecture and effectively contain by Snort IDS except for the few which the suggestion is made for possible mitigation. In order to provide scalable threat detection in the architecture, a flow-based IDS model is developed. A flow-based anomaly detection is implemented with machine learning to overcome the limitation of signature-based IDS. The results show positive improvement for detection of almost all the possible attacks in SDN environment with our pattern recognition of neural network for machine learning using our trained model with over 97% accuracy.", 
Cloud computing has been widely adopted by application service providers (ASPs) and enterprises. Firewalls and traditional rule-based security protection techniques are not sufficient to protect user-data in multi-cloud scenarios.,Machine Learning for Anomaly Detection and Categorization in Multi-cloud Environments,"Cloud computing has been widely adopted by application service providers (ASPs) and enterprises to reduce both capital expenditures (CAPEX) and operational expenditures (OPEX). Applications and services previously running on private data centers are now being migrated to private or public clouds. Since most of the ASPs and enterprises have globally distributed user bases, their services need to be distributed across multiple clouds, spread across the globe which can achieve better performance in terms of latency, scalability and load balancing. The shift has eventually led the research community to study multi-cloud environments. However, the widespread acceptance of such environments has been hampered by major security concerns. Firewalls and traditional rule-based security protection techniques are not sufficient to protect user-data in multi-cloud scenarios. Recently, advances in machine learning techniques have attracted the attention of the research community to build intrusion detection systems (IDS) that can detect anomalies in the network traffic. Most of the research works, however, do not differentiate among different types of attacks. This is, in fact, necessary for appropriate countermeasures and defense against attacks. In this paper, we investigate both detecting and categorizing anomalies rather than just detecting, which is a common trend in the contemporary research works. We have used a popular publicly available dataset to build and test learning models for both detection and categorization of different attacks. To be precise, we have used two supervised machine learning techniques, namely linear regression (LR) and random forest (RF). We show that even if detection is perfect, categorization can be less accurate due to similarities between attacks. Our results demonstrate more than 99% detection accuracy and categorization accuracy of 93.6%, with the inability to categorize some attacks. Further, we argue that such categorization can be applied to multi-cloud environments using the same machine learning techniques.", 
"Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice. In this work, we evaluate ML models for RCT classification using an external dataset.",Machine learning for identifying Randomized Controlled Trials An evaluation and practitioner's guide,"Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice, in part because the best way to make use of the technology in a typical workflow is unclear. In this work, we evaluate ML models for RCT classification (support vector machines, convolutional neural networks, and ensemble approaches). We trained and optimized support vector machine and convolutional neural network models on the titles and abstracts of the Cochrane Crowd RCT set. We evaluated the models on an external dataset (Clinical Hedges), allowing direct comparison with traditional database search filters. We estimated area under receiver operating characteristics (AUROC) using the Clinical Hedges dataset. We demonstrate that ML approaches better discriminate between RCTs and non?RCTs than widely used traditional database search filters at all sensitivity levels; our best?performing model also achieved the best results to date for ML in this task (AUROC 0.987, 95% CI, 0.984?0.989). We provide practical guidance on the role of ML in (1) systematic reviews (high?sensitivity strategies) and (2) rapid reviews and clinical question answering (high?precision strategies) together with recommended probability cutoffs for each use case. Finally, we provide open?source software to enable these approaches to be used in practice.", 
"The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care. The utility of a machine learning-based predictive model will depend on factors including data heterogeneous, data depth, data breadth and nature of modelling task.",Machine learning in cardiovascular medicine are we there yet,"Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.", 
Machine Learning is one of the most promising application areas in the field of Information Technology. The application of machine learning in an education area is currently very interesting to researchers and scientists.,MACHINE LEARNING IN EDUCATION - A SURVEY OF CURRENT RESEARCH TRENDS,"Nowadays, Machine Learning (ML) is one of the most promising application areas in a field of Information Technology where its application scope is almost unlimited. The application of machine learning in an education area is currently very interesting to researchers and scientists, and it is the main focus of our study. The aim of this paper is to evaluate the possibilities of applying and using machine learning in the education area. This paper identifies and analyses suitable literature, research papers and articles in order to determine their categorization in the field of education, to determine the current trends of using machine learning in education, and to determine its current and future applications.", 
The detection of epileptogenic region in brain is important for the detection of epilepsy disease. This paper proposes an automatic detection and diagnosis of EEG signals for epilepsy disease using soft computing approaches. The proposed method is used in many clinical diagnosis.,Machine learning method based detection and diagnosis for epilepsy in EEG signal,"The epileptic seizure can be detected using electroencephalogram (EEG) signals. The detection of epileptogenic region in brain is important for the detection of epilepsy disease. The signals from epileptogenic region in brain are focal signal and the signal from normal regions in brain is non-focal signal. Hence, the detection of focal signal is important for epilepsy disease detection. This paper proposes an automatic detection and diagnosis of EEG signals for epilepsy disease using soft computing approaches as adaptive neuro fuzzy inference system (ANFIS) and neural networks (NN). In this paper, the features from decomposed coefficients as bias (B), weight feature (W), entropy(E), activity feature (AF), mobility feature (MF), complexity feature (CF), skewness (S) and kurtosis (K) are extracted for the classification of EEG signals into either focal or non-focal signals for epilepsy disease detection and diagnosis. The detection of focal signal is achieved by ANFIS classifier and the diagnosis of the severity levels in focal signal is achieved by NN classification approach. The proposed method is used in many clinical diagnosis.", 
"In the present article, we address the problem of automatic text classification according to the author's gender. We used a preexisting corpus of Russian-language texts RusPersonality labeled with information on their authors.",Machine Learning Models of Text Categorization by Author Gender Using Topic-Independent Features,"In the present article, we address the problem of automatic text classification according to the author’s gender. We used a preexisting corpus of Russian-language texts RusPersonality labeled with information on their authors (gender, age, psychological testing and so on). We performed the comparative study of machine learning techniques for gender attribution in Russian-language texts after deliberately removing gender bias in topics and genre. The obtained models of classifying Russian texts by their authors’ gender demonstrate accuracy close to the state-of-the-art and even higher (up to 0.86 +/-0.03 in Accuracy, 86% in F1-score).", 
"Heart diseases are the reason for 12 million deaths every year. In most of the countries, half of the deaths are due to cardiovascular diseases. Early diagnosis of cardiovascular sicknesses can help in settling on choices on the way of life changes.",Machine Learning Techniques For Heart Disease Prediction,"According to WHO (World Health Organization), Heart diseases are the reason for 12 million deaths every year. In most of the countries, half of the deaths are due to cardiovascular diseases. The early diagnosis of cardiovascular sicknesses can help in settling on choices on the way of life changes in high hazard patients and thusly diminish the difficulties. In this paper, machine learning techniques are used for the detection of heart disease. We also applied sampling techniques for handling unbalanced datasets. Various machine learning methods are used to predict the overall risk. The framingham_heart_disease dataset is publically available on the Kaggle. This dataset is used in our experiments. The end goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD). The dataset contains 15 features that give patient information. By applying machine learning techniques, we achieved 99% accuracy in heart disease detection.", 
Alert-brokers will be critical tools for managing alert streams in the LSST era. We develop a machine learning pipeline to characterize and classify variable and transient sources only using multiband optical photometry.,Machine-learning-based Brokers for Real-time Classification of the LSST Alert Stream,"The unprecedented volume and rate of transient events that will be discovered by the Large Synoptic Survey Telescope (LSST) demand that the astronomical community update its follow-up paradigm. Alert-brokers— automated software system to sift through, characterize, annotate, and prioritize events for follow-up—will be critical tools for managing alert streams in the LSST era. The Arizona-NOAO Temporal Analysis and Response to Events System (ANTARES) is one such broker. In this work, we develop a machine learning pipeline to characterize and classify variable and transient sources only using the available multiband optical photometry. We describe three illustrative stages of the pipeline, serving the three goals of early, intermediate, and retrospective classification of alerts. The first takes the form of variable versus transient categorization, the second a multiclass typing of the combined variable and transient data set, and the third a purity-driven subtyping of a transient class. Although several similar algorithms have proven themselves in simulations, we validate their performance on real observations for the first time. We quantitatively evaluate our pipeline on sparse, unevenly sampled, heteroskedastic data from various existing observational campaigns, and demonstrate very competitive classification performance. We describe our progress toward adapting the pipeline developed in this work into a real-time broker working on live alert streams from time-domain surveys.", 
Heart disease is the number one killer in the world today. Machine learning classification techniques can significantly benefit the medical field by providing an accurate diagnosis.,Machine learning classification techniques for heart disease prediction a review,"The most crucial task in the healthcare field is disease diagnosis. If a disease is diagnosed early, many lives can be saved. Machine learning classification techniques can significantly benefit the medical field by providing an accurate and quick diagnosis of diseases. Hence, save time for both doctors and patients. As heart disease is the number one killer in the world today, it becomes one of the most difficult diseases to diagnose. In this paper, we provide a survey of the machine learning classification techniques that have been proposed to help healthcare professionals in diagnosing heart disease. We start by overviewing the machine learning and de-scribing brief definitions of the most commonly used classification techniques to diagnose heart disease. Then, we review represent-able research works on using machine learning classification techniques in this field. Also, a detailed tabular comparison of the surveyed papers is presented.", 
Proposed method utilizes the sentence extraction in a single document and produces a generic summary for a given Malayalam document. Sentences in the document are ranked based on the word score of each word present in it.,"Malayalam text summarization, An extractive approach","Automatic summarization of text is on of the area of interest in the field of natural language processing. The proposed method utilizes the sentence extraction in a single document and produces a generic summary for a given Malayalam document (Extractive summarization). Sentences in the document are ranked based on the word score of each word present in it. Top N ranked sentences are extracted and arrange them in their chronological order for summary generation, where N represents the size of summary with respect to the percentage of original document size (condensation rate). The standard metric ROUGE is used for performance evaluation. ROUGE calculates the n-gram overlap between a generated summary and reference summaries. Reference summaries were constructed manually. Experiments show that the results are promising.", 
"The security against various malicious attacks are achieved by using malicious vehicles identification and trust management (MAT) algorithm. This key exchanging algorithm is useful to prevent the various attacks, like impersonate attack, man in middle attack, etc. The proposed MAT algorithm accurately evaluates the trustworthiness of each node as well as information to control different attacks.",Malicious Software Classification using Transfer Learning of ResNet-50 Deep Neural Network,"In this fifth generation of vehicular communication, the security against various malicious attacks are achieved by using malicious vehicles identification and trust management (MAT) algorithm. Basically, the proposed MAT algorithm performs in two dimensions, they are (i) Node trust and (ii) information trust accompanied with a digital signature and hash chain concept. In node trust, the MAT algorithm introduces the special form of key exchanging algorithm to every members of public group key, and later the vehicles with same target location are formed into cluster. The public group key is common for each participant but everyone maintain their own private key to produce the secret key. The proposed MAT algorithm, convert the secrete key into some unique form that allows the CMs (cluster members) to decipher that secrete key by utilizing their own private key. This key exchanging algorithm is useful to prevent the various attacks, like impersonate attack, man in middle attack, etc. In information trust, the MAT algorithm assigns some special nodes (it has common distance from both vehicles) for monitoring the message forwarding activities as well as routing behavior at particular time. This scheme is useful to predict an exact intruder and after time out the special node has dropped all the information. The proposed MAT algorithm accurately evaluates the trustworthiness of each node as well as information to control different attacks and become efficient for improving a group lifetime, stability of cluster, and vehicles that are located on their target place at correct time.", 
"The security against various malicious attacks are achieved by using malicious vehicles identification and trust management (MAT) algorithm. This key exchanging algorithm is useful to prevent the various attacks, like impersonate attack, man in middle attack, etc. The proposed MAT algorithm accurately evaluates the trustworthiness of each node as well as information to control different attacks.",MALICIOUS VEHICLES IDENTIFYING AND TRUST MANAGEMENT ALGORITHM FOR ENHANCE THE SECURITY IN 5G -VANET,"In this fifth generation of vehicular communication, the security against various malicious attacks are achieved by using malicious vehicles identification and trust management (MAT) algorithm. Basically, the proposed MAT algorithm performs in two dimensions, they are (i) Node trust and (ii) information trust accompanied with a digital signature and hash chain concept. In node trust, the MAT algorithm introduces the special form of key exchanging algorithm to every members of public group key, and later the vehicles with same target location are formed into cluster. The public group key is common for each participant but everyone maintain their own private key to produce the secret key. The proposed MAT algorithm, convert the secrete key into some unique form that allows the CMs (cluster members) to decipher that secrete key by utilizing their own private key. This key exchanging algorithm is useful to prevent the various attacks, like impersonate attack, man in middle attack, etc. In information trust, the MAT algorithm assigns some special nodes (it has common distance from both vehicles) for monitoring the message forwarding activities as well as routing behavior at particular time. This scheme is useful to predict an exact intruder and after time out the special node has dropped all the information. The proposed MAT algorithm accurately evaluates the trustworthiness of each node as well as information to control different attacks and become efficient for improving a group lifetime, stability of cluster, and vehicles that are located on their target place at correct time.", 
Research article claims to be first in accessing the asymmetrical effect of multiple categories of terroristic activities on stock indexes. Results indicated that such disruption causes a temporary negative effect on stock index only in the short run.,Market miracles Resilience of Karachi stock exchange index against terrorism in Pakistan,"Terrorism plays a pivotal role in influencing the stock indexes of many countries. This research article claims to be first in accessing the asymmetrical effect of multiple categories of terroristic activities on stock indexes in the presence of macroeconomic volatility. This research utilized a Non-linear autoregressive distributive lag model (NARDL) to find out the asymmetrical relationship between terroristic disruptions and stock indexes. Data on terroristic attacks have been incorporated from 2002 to 2015, and more than 4600 observations taken into account. Positive shocks to attacks on mosques, killed in mosque attacks, killed in drone attacks, and army personal fatalities are negatively affecting the stock prices in the short run. Results indicated that such disruption causes a temporary negative effect on stock indexes only in the short run but in the long-run stock exchange remains resilient against such disruptions.", 
"VANET is a next generation communication technology where vehicles create an autonomous network with assistance of RSU (Road Side Units) VANET provides legitimate information to the users on the road, in order to increase the road and user's safety. The fast propagation of emergency and local warning messages to the approaching vehicles will be helpful for preventing secondary accidents. If security of the network is not guaranteed, several attacks may occur, thereby alert messages may not reach to the RSUs on time.",MAR_Sybil Cooperative RSU Based Detection and Prevention of Sybil Attacks in Routing Process of VANET,"VANET is a next generation communication technology where vehicles create an autonomous network with assistance of RSU (Road Side Units). VANET provides legitimate information to the users on the road, in order to increase the road and user’s safety. It provides useful information to the vehicles about directions, location mapping, premises, etc. The fast propagation of emergency and local warning messages to the approaching vehicles will be helpful for preventing secondary accidents. If security of the network is not guaranteed, several attacks may occur, thereby alert messages may not reach to the RSUs on time or message may get spoof due to attack on it. Major attacks on emergency messages such as timing attack, spoofing attack, DoS attack, Sybil attack, etc. would be considered to mitigate and make the communication and infrastructure more secure. One of the major emergency message attack is Sybil, which is used by selfish message propagators to claim authenticity of their message by propagating with multiple identities. By deceiving other nodes with their false support using multiple identifies, they can achieve various purpose in network like vehicle rerouting, speed shift etc. This work proposes a RSU cooperative detection mechanism involving triangulation and fake propagation by RSU to identify the Sybil attacks in the VANET. The proposed methodology secures fast communication emergency messages to prevent secondary accidents by validating the message sources, nodes will accept only the messages from RSU which are digitally signed and source validated against Sybil nodes. The performance evaluation is made with existing methodology and effectiveness of the proposed methodology results demonstrated in article. The spatial location and trajectory of attacker is learnt effectively with RSU cooperation in the proposed work.", 
"Small metal-oxide-metal (MOM) capacitors are essential to energy-efficient mixed-signal integrated circuit design. Despite conventional wisdom, extensive measurements show that vertical-field and lateral-field MOM capacitors have the same matching properties when the actual capacitor area is considered. The capacitance can be increased by increasing the spacing between the fingers, at the expense of increased chip area.",Matching Properties of Femtofarad and Sub-Femtofarad MOM Capacitors,"Small metal-oxide-metal (MOM) capacitors are essential to energy-efficient mixed-signal integrated circuit design. However, only few reports discuss their matching properties based on large sets of measured data. In this paper, we report matching properties of femtofarad and sub-femtofarad MOM vertical-field parallel-plate capacitors and lateral-field fringing capacitors. We study the effect of both the finger-length and finger-spacing on the mismatch of lateral-field capacitors. In addition, we compare the matching properties and the area efficiency of vertical-field and lateral-field capacitors. We use direct mismatch measurement technique, and we illustrate its feasibility using experimental measurements and Monte Carlo simulations. The test-chips are fabricated in a 0.18 ?m CMOS process. A large number of test structures is characterized (4800 test structures), which improves the statistical reliability of the extracted mismatch information. Despite conventional wisdom, extensive measurements show that vertical-field and lateral-field MOM capacitors have the same matching properties when the actual capacitor area is considered. Measurements show that the mismatch depends on the capacitor area but not on the spacing; thus, for a given mismatch specification, the lateral-field MOM capacitor can have arbitrarily small capacitance by increasing the spacing between the capacitor fingers, at the expense of increased chip area.", 
Learn the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining. It is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions.,Matching Words and Pictures,"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann’s hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.", 
"Material recognition in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter. We introduce a new, large-scale, open dataset of materials in the wild. We combine this dataset with deep learning to achieve material recognition and segmentation.",Material Recognition in the Wild with the Materials in Context Database,"Recognizing materials in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter, which combine to make the problem particularly difficult. In this paper, we introduce a new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC), and combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild. MINC is an order of magnitude larger than previous material databases, while being more diverse and well-sampled across its 23 categories. Using MINC, we train convolutional neural networks (CNNs) for two tasks: classifying materials from patches, and simultaneous material recognition and segmentation in full images. For patch-based classification on MINC we found that the best performing CNN architectures can achieve 85.2% mean class accuracy. We convert these trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field (CRF) to predict the material at every pixel in an image, achieving 73.1% mean class accuracy. Our experiments demonstrate that having a large, well-sampled dataset such as MINC is crucial for real-world material recognition and segmentation.", 
Unsupervised text summarization model generates a summary by extracting salient sentences in given document(s) One of the advantages of this model is that it can directly discover key sentences. ,"MCMR, Maximum coverage and minimum redundant text summarization model","In paper, we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(s). In particular, we model text summarization as an integer linear programming problem. One of the advantages of this model is that it can directly discover key sentences in the given document(s) and cover the main content of the original document(s). This model also guarantees that in the summary can not be multiple sentences that convey the same information. The proposed model is quite general and can also be used for single- and multi-document summarization. We implemented our model on multi-document summarization task. Experimental results on DUC2005 and DUC2007 datasets showed that our proposed approach outperforms the baseline systems.", 
"Cybercrime takes many forms and has garnered much attention in the media, making information security a more urgent priority. In order to fight cyber-crime, criminal evidence must be gathered from computer-based systems. This is quite different from the collection of conventional criminal evidence and can confuse investigators.",Measures of retaining digital evidence to prosecute computer-based cyber crimes,"With the rapid growth of computer and network systems in recent years, there has also been a corresponding increase in cyber-crime. Cybercrime takes many forms and has garnered much attention in the media, making information security a more urgent and important priority. In order to fight cyber-crime, criminal evidence must be gathered from these computer-based systems. This is quite different from the collection of conventional criminal evidence and can confuse investigators attempting to deal with the forensics of cyber-crime, highlighting the importance of computer forensics. In this paper, we offer solutions to guard against cyber-crime through the implementation of software toolkits for computer-based systems. In this way, those who engage in criminal acts in cyber-space can be more easily apprehended.", 
"A new test measures a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. Models must possess extensive world knowledge and problem solving ability to attain high accuracy.",Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model’s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.", 
"Disease diagnosis is the identification of an health issue, disease, disorder, or other condition that a person may have. Traditional methods which are used to diagnose a disease are manual and error-prone. Usage of Artificial Intelligence (AI) predictive techniques enables auto diagnosis and reduces detection errors.",Medical Diagnostic Systems Using Artificial Intelligence (AI) Algorithms Principles and Perspectives,"Disease diagnosis is the identification of an health issue, disease, disorder, or other condition that a person may have. Disease diagnoses could be sometimes very easy tasks, while others may be a bit trickier. There are large data sets available; however, there is a limitation of tools that can accurately determine the patterns and make predictions. The traditional methods which are used to diagnose a disease are manual and error-prone. Usage of Artificial Intelligence (AI) predictive techniques enables auto diagnosis and reduces detection errors compared to exclusive human expertise. In this paper, we have reviewed the current literature for the last 10 years, from January 2009 to December 2019. The study considered eight most frequently used databases, in which a total of 105 articles were found. A detailed analysis of those articles was conducted in order to classify most used AI techniques for medical diagnostic systems. We further discuss various diseases along with corresponding techniques of AI, including Fuzzy Logic, Machine Learning, and Deep Learning. This research paper aims to reveal some important insights into current and previous different AI techniques in the medical field used in today’s medical research, particularly in heart disease prediction, brain disease, prostate, liver disease, and kidney disease. Finally, the paper also provides some avenues for future research on AI-based diagnostics systems based on a set of open problems and challenges.", 
Emotion analysis using EEG signals is one such problem that has been studied extensively in recent times. A merged LSTM model has been proposed for binary classification of emotions.,Merged LSTM Model for emotion classification using EEG signals,The applicability of contemporary deep learning techniques have seen considerable improvements in the field of biomedical signal analysis. Emotion analysis using EEG signals is one such problem that has been studied and worked upon extensively in recent times. In this paper we have proposed a novel methodology to classify emotions using signal processing techniques such as wavelet transform and statistical measures for feature extraction and dimensionality reduction followed by developing state of the art neural architecture for the classification task. A merged LSTM model has been proposed for binary classification of emotions. The model’s applicability and accuracy has been validated using DEAP dataset which is the benchmark dataset for emotion recognition., 
"Previous biomedical text-mining efforts have focused on abstracts and full texts. Full texts are more expensive to process and more expensive in terms of accuracy. In this study, we create reduced versions of full text documents that contain only important portions. We explore the use of MeSH terms, manually assigned to documents by trained annotators, as clues to select important text segments from the full text.","MeSH, a window into full text for document summarization","Previous research in the biomedical text-mining domain has historically been limited to titles, abstracts and metadata available in MEDLINE records. Recent research initiatives such as TREC Genomics and BioCreAtIvE strongly point to the merits of moving beyond abstracts and into the realm of full texts. Full texts are, however, more expensive to process not only in terms of resources needed but also in terms of accuracy. Since full texts contain embellishments that elaborate, contextualize, contrast, supplement, etc., there is greater risk for false positives. Motivated by this, we explore an approach that offers a compromise between the extremes of abstracts and full texts. Specifically, we create reduced versions of full text documents that contain only important portions. In the long-term, our goal is to explore the use of such summaries for functions such as document retrieval and information extraction. Here, we focus on designing summarization strategies. In particular, we explore the use of MeSH terms, manually assigned to documents by trained annotators, as clues to select important text segments from the full text documents. Our experiments confirm the ability of our approach to pick the important text portions. Using the ROUGE measures for evaluation, we were able to achieve maximum ROUGE-1, ROUGE-2 and ROUGE-SU4 F-scores of 0.4150, 0.1435 and 0.1782, respectively, for our MeSH term-based method versus the maximum baseline scores of 0.3815, 0.1353 and 0.1428, respectively. Using a MeSH profile-based strategy, we were able to achieve maximum ROUGE F-scores of 0.4320, 0.1497 and 0.1887, respectively. Human evaluation of the baselines and our proposed strategies further corroborates the ability of our method to select important sentences from the full texts.", 
A paper proposes a text summarization approach for Java code that makes use of code level nano-patterns to obtain text summary. The approach also looks for associations between these nano- patterns in a Java method code and then use a template based text generation to obtain the final text summary of the Java method.,Method Level Text Summarization for Java Code Using Nano-Patterns,"Rapid growth in providing automated solutions resulted in large code bases to get quickly developed and consumed. However, maintaining code and its subsequent reuse pose some challenges here. One of the best practices used to handle such issues is also to provide suitable text summary of the code to allow the human developers to comprehend the code easily, but this can be quite time-consuming and costly affair. A few efforts have been made in this direction where the text summary of the code either generated from the method signature or its body. In this paper, we propose a text summarization approach for Java code that makes use of identification of code level nano-patterns to obtain text summary. The approach also looks for associations between these nano-patterns in a Java method code and then use a template based text generation to obtain the final text summary of the Java method. We evaluated the summary generated by the proposed approach using a controlled experiment with other three existing approaches. Our results suggested that the summary generated by our approach was better on the part of completeness and correctness criteria. The feedback obtained during the experimental validation suggested additional inputs to improve the generated text summary on the other two accounts as well.", 
The paper presents a framework for developing optimum crop plan using linear programming approach. Data set related to the state of Punjab will be used for demonstrating the methodology. Objective function is to maximize net returns for the region under the constraints of land availability and water sustainability.,Methodology for region level optimum crop plan,"The paper presents a framework for developing optimum crop plan using linear programming approach. The methodology aims for optimizing the level of each activity of different crops, level of input use, and output produced under different resource endowments and price scenarios. Data for this study relates to farm house hold data regarding land use, rainfall, water status, cropping pattern etc. The data has been taken from the ‘‘cost of cultivation scheme’’ run by the Directorate of Economics and Statistics, Ministry of Agriculture. Objective function is to maximize the net returns for the region under the constraints of land availability and water sustainability. General algebraic modelling system has been used for developing the optimum crop plan. Data set related to the state of Punjab will be used for demonstrating the methodology.", 
"Method, algorithm and means for extracting knowledge from natural text are proposed. Algorithm should include a hierarchical multi-level procedure for recognizing concepts, relationships, predicates and rules.",Methods and Models of Intellectual Processing of Texts for Building Ontologies of Software for Medical Terms Identification in Content Classification,"The article investigates the problem of automated development of basic ontology. A method, algorithm and means for extracting knowledge from natural text are proposed. It is shown that such an algorithm should be multistage and include a hierarchical multi-level procedure for recognizing concepts, relationships, predicates and rules, which are introduced as a result of ontology. The analysis of the subject area is the search and analysis of various information systems analogues. The analysis of methods and criteria of information systems is carried out. The analysis of the system and its functionality is presented. This paper examines methods and models of intellectual text processing, the results of which are intended to build software ontologies, and are used during Ontology Learning, when it is necessary to improve, extend, modify an existing ontology model, or build ontology from basic ontology, having only text-collection collections as sources of knowledge. In the latter case, the task is particularly complex and requires the use of the full range of text mining methods (Text Mining as TM). The paper deals with the solution of TM problems at different stages of the PMT processing: obtaining information (identifying entities - concepts and terms, their properties, facts, events, establishing relationships between entities, in particular associative ones), categorization, and clustering, semantic annotation. Below we will consider the tools of automated analysis of natural language and software products implemented on their basis for filling the system.", 
"Microexpressions are fleeting, lasting only a few frames within a video sequence. They are difficult to perceive and interpret correctly, and they are challenging to identify and categorize automatically. Existing recognition methods are often ineffective at handling subtle face displacements. Facial Dynamics Map is proposed to characterize the movements of a microexpression.",Microexpression Identification and Categorization using a Facial Dynamics Map,"Unlike conventional facial expressions, microexpressions are instantaneous and involuntary reflections of human emotion. Because microexpressions are fleeting, lasting only a few frames within a video sequence, they are difficult to perceive and interpret correctly, and they are highly challenging to identify and categorize automatically. Existing recognition methods are often ineffective at handling subtle face displacements, which can be prevalent in typical microexpression applications due to the constant movements of the individuals being observed. To address this problem, a novel method called the Facial Dynamics Map is proposed to characterize the movements of a microexpression in different granularity. Specifically, an algorithm based on optical flow estimation is used to perform pixel-level alignment for microexpression sequences. Each expression sequence is then divided into spatiotemporal cuboids in the chosen granularity. We also present an iterative optimal strategy to calculate the principal optical flow direction of each cuboid for better representation of the local facial dynamics. With these principal directions, the resulting Facial Dynamics Map can characterize a microexpression sequence. Finally, a classifier is developed to identify the presence of microexpressions and to categorize different types. Experimental results on four benchmark datasets demonstrate higher recognition performance and improved interpretability.", 
"MicroRNAs (miRNAs) play a key role in the modulation of translation efficiency. Known pre-mi RNAs are listed in miRBase, and they have been discovered in a variety of organisms. To aid that important decision-making process, we aimed to establish a means to differentiate pre- miRNAs from different species.",MicroRNA categorization using sequence motifs and k-mers,"Post-transcriptional gene dysregulation can be a hallmark of diseases like cancer and microRNAs (miRNAs) play a key role in the modulation of translation efficiency. Known pre-miRNAs are listed in miRBase, and they have been discovered in a variety of organisms ranging from viruses and microbes to eukaryotic organisms. The computational detection of pre-miRNAs is of great interest, and such approaches usually employ machine learning to discriminate between miRNAs and other sequences. Many features have been proposed describing pre-miRNAs, and we have previously introduced the use of sequence motifs and k-mers as useful ones. There have been reports of xeno-miRNAs detected via next generation sequencing. However, they may be contaminations and to aid that important decision-making process, we aimed to establish a means to differentiate pre-miRNAs from different species. To achieve distinction into species, we used one species’ pre-miRNAs as the positive and another species’ premiRNAs as the negative training and test data for the establishment of machine learned models based on sequence motifs and k-mers as features. This approach resulted in higher accuracy values between distantly related species while species with closer relation produced lower accuracy values. We were able to differentiate among species with increasing success when the evolutionary distance increases. This conclusion is supported by previous reports of fast evolutionary changes in miRNAs since even in relatively closely related species a fairly good discrimination was possible.", 
Object recognition in the context of the broader question of scene understanding. Objects are labeled using per- instance segmentations to aid in precise object localization. The dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old.,Microsoft COCO Common Objects in Context,"We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", 
"Recent work on mid-level visual representations aims to capture information at a level of complexity higher than typical ""visual words"" Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels.",Mid-level Visual Element Discovery as Discriminative Mode Seeking,"Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical “visual words”, but lower than full-blown semantic objects. Several approaches have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset.", 
"Millimeter wave (mmWave) communication has raised increasing attentions from both academia and industry due to its exceptional advantages. Compared with existing wireless communication techniques, such as WiFi and 4G, mmWave communications adopt much higher carrier frequencies. The major challenge is how to overcome its shortcomings while fully utilizing its advantages, authors say. They present a taxonomy based on the layered model and give an extensive review on mm Wave communications.",Millimeter Wave Communication A Comprehensive Survey,"Millimeter wave (mmWave) communication has raised increasing attentions from both academia and industry due to its exceptional advantages. Compared with existing wireless communication techniques, such as WiFi and 4G, mmWave communications adopt much higher carrier frequencies and thus come with advantages including huge bandwidth, narrow beam, high transmission quality, and strong detection ability. These advantages can well address difficult situations caused by recent popular applications using wireless technologies. For example, mmWave communications can significantly alleviate the skyrocketing traffic demand of wireless communication from video streaming. Meanwhile, mmWave communications have several natural disadvantages, e.g., severe signal attenuation, easily blocked by obstacles, and small coverage, due to its short wavelengths. Hence, the major challenge is how to overcome its shortcomings while fully utilizing its advantages. In this paper, we present a taxonomy based on the layered model and give an extensive review on mmWave communications. Specially, we divide existing efforts into four categories that investigate physical layer, MAC layer, network layer, and cross layer optimization, respectively. First, we present an overview of some technical details in physical layer. Second, we summarize available literature in MAC layer that pertains to protocols and scheduling schemes. Third, we make an in-depth survey of related research work in network layer, providing brain storming and methodology for enhancing the capacity and coverage of mmWave networks. Fourth, we analyze available research work related to cross layer allocation/optimization for mmWave communications. Fifth, we make a review of mmWave applications to illustrate how mmWave technology can be employed to satisfy other services. At the end of each section described above, we point out the inadequacy of existing work and identify the future work. Sixth, we present some available resources for mmWave communications, including related books about mmWave, commonly used mmWave frequencies, existing protocols based on mmWave, and experimental platforms. Finally, we have a simple summary and point out several promising future research directions", 
Neural models have become successful at producing abstractive summaries that are human-readable and fluent. Injecting structural world knowledge from Wikidata helps our abstractive summarization model to be more fact-aware. We test our model on CNN/Daily Mail summarization dataset and show improvements on ROUGE scores over the baseline Transformer model.,"Mind The Facts, Knowledge-Boosted Coherent Abstractive Text Summarization","Neural models have become successful at producing abstractive summaries that are human-readable and fluent. However, these models have two critical shortcomings: they often don’t respect the facts that are either included in the source article or are known to humans as commonsense knowledge, and they don’t produce coherent summaries when the source article is long. In this work, we propose a novel architecture that extends Transformer encoder-decoder architecture in order to improve on these shortcomings. First, we incorporate entity-level knowledge from the Wikidata knowledge graph into the encoder-decoder architecture. Injecting structural world knowledge from Wikidata helps our abstractive summarization model to be more fact-aware. Second, we utilize the ideas used in Transformer-XL language model in our proposed encoder-decoder architecture. This helps our model with producing coherent summaries even when the source article is long. We test our model on CNN/Daily Mail summarization dataset and show improvements on ROUGE scores over the baseline Transformer model. We also include model predictions for which our model accurately conveys the facts, while the baseline Transformer model doesn’t.", 
"Most cities in sub-Saharan Africa rely on minibus taxis for public transport. The system is often seen as disorganized, unregulated and inefficient. Kampala's minibus taxi system needs to be reformed to make it safer, cleaner and more efficient.","Minibus taxis in Kampala's paratransit system Operations, economics and efficiency","Most cities in sub-Saharan Africa rely for their public transport on paratransit in the form of fourteen- to twenty-seater privately owned and mostly old minibus taxis. The system is often seen as disorganized, unregulated and inefficient. To assess the accuracy of this picture, we analyzed the operations and economics of Kampala's minibus taxi system and its efficiency from the passengers' and the drivers' perspectives, using ‘?oating car data’. We found that the picture is largely accurate. Our findings suggest the need for moderate transformation: adequate enforcement of regulations, reorganization of ownership, renewal of ?eets, and integration of ICT systems to facilitate scheduling, booking and fare collection. This will help to make the system safer, cleaner and more efficient for Kampalan commuters and more stable, secure and profitable for the minibus taxi drivers and the mini industries that depend on them.", 
"Text categorization is an essential task in Web content analysis. In this paper, we focus on the minimally-supervised setting that aims to categorize documents e?ectively. We propose a novel framework for minimally supervised categorization by learning from a text-rich network. ",Minimally-Supervised Structure-Rich Text Categorization via Learning on Text-Rich Networks,"Text categorization is an essential task in Web content analysis. Considering the ever-evolving Web data and new emerging categories, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting that aims to categorize documents e?ectively, with a couple of seed documents annotated per category. We recognize that texts collected from the Web are often structure-rich, i.e., accompanied by various metadata. One can easily organize the corpus into a text-rich network, joining raw text documents with document attributes, high-quality phrases, label surface names as nodes, and their associations as edges. Such a network provides a holistic view of the corpus’ heterogeneous data sources and enables a joint optimization for network-based analysis and deep textual model training. We therefore propose a novel framework for minimally supervised categorization by learning from the text-rich network. Specifically, we jointly train two modules with di?erent inductive biases – a text analysis module for text understanding and a network learning module for class-discriminative, scalable network learning. Each module generates pseudo training labels from the unlabeled document set, and both modules mutually enhance each other by co-training using pooled pseudo labels. We test our model on two real-world datasets. On the challenging e-commerce product categorization dataset with 683 categories, our experiments show that given only three seed documents per category, our framework can achieve an accuracy of about 92%, significantly outperforming all compared methods; our accuracy is only less than 2% away from the supervised BERT model trained on about 50K labeled documents.", 
Automatic text summarization aims to produce summaries for one or more texts using machine techniques. Our system uses a clustering algorithm and an adapted discriminant analysis method: mRMR (minimum redundancy and maximum relevance).,Minimum redundancy and maximum relevance for single and multi-document Arabic text summarization,"Automatic text summarization aims to produce summaries for one or more texts using machine techniques. In this paper, we propose a novel statistical summarization system for Arabic texts. Our system uses a clustering algorithm and an adapted discriminant analysis method: mRMR (minimum redundancy and maximum relevance) to score terms. Through mRMR analysis, terms are ranked according to their discriminant and coverage power. Second, we propose a novel sentence extraction algorithm which selects sentences with top ranked terms and maximum diversity. Our system uses minimal language-dependant processing: sentence splitting, tokenization and root extraction. Experimental results on EASC and TAC 2011 MultiLingual datasets showed that our proposed approach is competitive to the state of the art systems.", 
"Mid-level or semi-local features learnt using class-level information are potentially more distinctive. They preserve some of the robustness properties with respect to occlusions and image clutter. We propose a new scheme for extracting mid-level features for image classification, based on relevant pattern mining.",Mining Mid-level Features for Image Classifcation,"Mid-level or semi-local features learnt using class-level information are potentially more distinctive than the traditional low-level local features constructed in a purely bottom-up fashion. At the same time they preserve some of the robustness properties with respect to occlusions and image clutter. In this paper we propose a new and effective scheme for extracting mid-level features for image classification, based on relevant pattern mining. In particular, we mine relevant patterns of local compositions of densely sampled low-level features. We refer to the new set of obtained patterns as Frequent Local Histograms or FLHs. During this process, we pay special attention to keeping all the local histogram information and to selecting the most relevant reduced set of FLH patterns for classification. The careful choice of the visual primitives and an extension to exploit both local and global spatial information allow us to build powerful bag-of-FLH-based image representations. We show that these bag-of-FLHs are more discriminative than traditional bag-of-words and yield state-of-the-art results on various image classification benchmarks, including Pascal VOC.", 
"Speech-to-text summarization systems usually take as input the output of an automatic speech recognition system. We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language. We explore the possibilities offered by phonetic information to select the background information.",Mixed-Source Multi-Document Speech-to-Text Summarization,"Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries. We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech-to-text summarization. In this work, we explore the possibilities offered by phonetic information to select the background information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information. Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source.", 
"ML-Net is a novel end-to-end deep learning framework for multi-label classification of biomedical texts. It combines a label prediction network with an automated label count prediction mechanism to provide an optimal set of labels. Unlike traditional machine learning methods, ML-Net does not require human effort for feature engineering. It is a highly efficient and scalable approach to tasks with a large number of labels, so there is no need to build individual classifiers.",ML-Net multi-label classification of biomedical texts with deep neural networks,"In multi-label text classification, each textual document is assigned 1 or more labels. As an important task that has broad applications in biomedicine, a number of different computational methods have been proposed. Many of these methods, however, have only modest accuracy or efficiency and limited success in practical use. We propose ML-Net, a novel end-to-end deep learning framework, for multi-label classification of biomedical texts. Materials and ML-Net combines a label prediction network with an automated label count prediction mechanism to provide an optimal set of labels. This is accomplished by leveraging both the predicted confidence score of each label and the deep contextual information (modeled by ELMo) in the target document. We evaluate ML-Net on 3 independent corpora in 2 text genres: biomedical literature and clinical notes. For evaluation, we use example-based measures, such as precision, recall, and the F measure. We also compare ML-Net with several competitive machine learning and deep learning baseline models. Our benchmarking results show that ML-Net compares favorably to state-of-the-art methods in multilabel classification of biomedical text. ML-Net is also shown to be robust when evaluated on different text genres in biomedicine. ML-Net is able to accurately represent biomedical document context and dynamically estimate the label count in a more systematic and accurate manner. Unlike traditional machine learning methods, MLNet does not require human effort for feature engineering and is a highly efficient and scalable approach to tasks with a large set of labels, so there is no need to build individual classifiers for each separate label.", 
MLUM is the first large-scale MultiLingual SUMmarization dataset. It contains 1.5M+ article/ summary pairs in five different languages. It was obtained from online newspapers.,"MLSUM, The Multilingual Summarization Corpus","We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages – namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.", 
"Latest and upcoming mobile application processors are typically implemented as heterogeneous multiprocessor system-on chips. Heterogeneous computing is an attractive proposition in theory, but software support at all levels is essential to fully realize its promises. In this article, we present compilation time and runtime techniques to unleash the full potential of heterogeneous multicore processors.",Mobile application processors Techniques for software power-performance optimization,"The latest and upcoming mobile application processors, embedded in a myriad of consumer devices, are typically implemented as heterogeneous multiprocessor system-on chips, comprised of various processing engines such as general-purpose cores with differing characteristics, GPUs, DSPs, nonprogrammable accelerators, and reconfigurable computing devices. The heterogeneity allows application kernels with specific requirements to be executed on processing core(s) ideally suited to perform that computation, resulting in a significantly improved performance and energy-efficiency. However, while heterogeneous computing is an attractive proposition in theory, considerable software support at all levels is essential to fully realize its promises. The system software needs to orchestrate the different on-chip compute resources in a synergistic manner with minimal engagement from the application developers, as well as fulfill the stringent power-thermal constraint in mobile computing. In this article, we present compiler time and runtime techniques to unleash the full potential of heterogeneous multiprocessors toward high-performance energy-efficient computing on consumer devices.",  
"Digital forensics has grown out of the mainstream practice of computer forensics. Practitioners are faced with various types of cellular phone generation technologies. This paper will outline the common cell phone technologies, their characteristics, and device handling procedures.",Mobile device analysis,"The increased usage and proliferation of small scale digital devices, like celluar (mobile) phones has led to the emergence of mobile device analysis tools and techniques. This field of digital forensics has grown out of the mainstream practice of computer forensics. Practitioners are faced with various types of cellular phone generation technologies, proprietary embedded firmware systems, along with a staggering amount of unique cable connectors for different models of phones within the same manufacturer brand. This purpose of this paper is to provide foundational concepts for the data forensic practitioner. It will outline the common cell phone technologies, their characteristics, and device handling procedures. Further data evidence storage areas are also explained along with data types found in the various storage areas. Specific information is also noted about BlackBerry and iPhone devices. Detailed procedures for data analysis/extraction for mobile devices and how to use the various toolkits that are available is beyond the scope of this paper; the staggering numbers of cell phones and the intricacies of the toolkits makes this impossible. However, resources for the reader to further investigate the topic are attached in the appendix.", 
Instant Messaging is a popular smartphone's application. WhatsApp is widely used judging from its users that reach more than 1 Billion users. The number of users or possible crime target and security features in WhatsApp can lead to crime. Investigators need to use mobile forensic methodologies and tools for investigating smartphone and finding out the crime evidence.,Mobile Forensic Tools Evaluation for Digital Crime Investigation,"Instant Messaging is a popular smartphone’s application. One example of Instant Messaging application is WhatsApp. WhatsApp is widely used judging from its users that reach more than 1 Billion users in January 2017. WhatsApp’s security recently has been updated with latest encryption type and technology by implementing end-to-end encryption. The number of users or possible crime target and security features in WhatsApp can lead to crime by people that have criminal intentions. Investigators need to use mobile forensic methodologies and tools for investigating smartphone and finding out the crime evidence. However, investigators are often facing challenges during the investigation because of incompatibility between forensic tools and mobile technology. This research will experiment using available forensic tools with NIST forensic method for extracting latest WhatsApp’s artifacts. Forensics tools capabilities will be evaluated and compared to find its strengths and weaknesses.", 
Advances in semiconductor technologies related to mobile phones have increased the potential for data stored on mobile phones to be used as evidence in civil or criminal cases. This paper examines the nature of some of the newer pieces of information that can become potential evidence. It will also cover some the inherent differences between mobile phone forensics and computer forensics.,"Mobile forensics an overview, tools, future trends and challenges from law enforcement perspective","Mobile phone proliferation in our societies is on the increase. Advances in semiconductor technologies related to mobile phones and the increase of computing power of mobile phones led to an increase of functionality of mobile phones while keeping the size of such devices small enough to fit in a pocket. This led mobile phones to become portable data carriers. This in turn increased the potential for data stored on mobile phone handsets to be used as evidence in civil or criminal cases. This paper examines the nature of some of the newer pieces of information that can become potential evidence on mobile phones. It also discusses some of the emerging technologies and their potential impact on mobile phone based evidence. The paper will also cover some of the inherent differences between mobile phone forensics and computer forensics. It also highlights some of the weaknesses of mobile forensic toolkits and procedures. Finally, the paper shows the need for more in depth examination of mobile phone evidence.", 
Mobile devices can also be used to perform activities that may be of malicious intent or criminal in nature. This makes mobile devices a valuable source of digital evidence. This paper aims at testing the harmonised digital forensic investigation process through a case study of a mobile forensic investigation.,Mobile Forensics using the Harmonised Digital Forensic Investigation Process,"Mobile technology is among the fastest developing technologies that have changed the way we live our daily lives. Over the past few years, mobile devices have become the most popular form of communication around the world. However, bundled together with the good and advanced capabilities of the mobile technology, mobile devices can also be used to perform various activities that may be of malicious intent or criminal in nature. This makes mobile devices a valuable source of digital evidence. For this reason, the technological evolution of mobile devices has raised the need to develop standardised investigation process models and procedures within the field of digital forensics. This need further supports the fact that forensic examiners and investigators face challenges when performing data acquisition in a forensically sound manner from mobile devices. This paper, therefore, aims at testing the harmonised digital forensic investigation process through a case study of a mobile forensic investigation. More specifically, an experiment was conducted that aims at testing the performance of the harmonised digital forensic investigation process (HDFIP) as stipulated in the ISO/lEe 27043 draft international standard through the extraction of potential digital evidence from mobile devices.", 
"Cloud-based vehicular networks are a promising paradigm to improve vehicular services. We propose a cloud-based mobile-edge computing (MEC) off-loading framework. We present an efficient predictive combination-mode relegation scheme, where the tasks are adaptively off-loaded to the servers.",Mobile-edge Computing for Vehicular networks,"Cloud-based vehicular networks are a promising paradigm to improve vehicular services through distributing computation tasks between remote clouds and local vehicular terminals. To further reduce the latency and the transmission cost of the computation off-loading, we propose a cloud-based mobile-edge computing (MEC) off-loading framework in vehicular networks. In this framework, we study the effectiveness of the computation transfer strategies with vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication modes. Considering the time consumption of the computation task execution and the mobility of the vehicles, we present an efficient predictive combination-mode relegation scheme, where the tasks are adaptively off-loaded to the MEC servers through direct uploading or predictive relay transmissions. Illustrative results indicate that our proposed scheme greatly reduces the cost of computation and improves task transmission efficiency.", 
"Connected and autonomous vehicles will play a pivotal role in future Intelligent Transportation Systems (ITSs) and smart cities, in general. High-speed and low-latency wireless communication links will allow municipalities to warn vehicles against safety hazards and support cloud-driving solutions. Millimeter wave techniques have been introduced as a means of fulfilling such high data rate requirements. ",Modeling and Design of Millimeter-Wave Networks for Highway Vehicular Communication,"Connected and autonomous vehicles will play a pivotal role in future Intelligent Transportation Systems (ITSs) and smart cities, in general. High-speed and low-latency wireless communication links will allow municipalities to warn vehicles against safety hazards, as well as support cloud-driving solutions to drastically reduce traffic jams and air pollution. To achieve these goals, vehicles need to be equipped with a wide range of sensors generating and exchanging high rate data streams. Recently, millimeter wave (mmWave) techniques have been introduced as a means of fulfilling such high data rate requirements. In this paper, we model a highway communication network and characterize its fundamental link budget metrics. In particular, we specifically consider a network where vehicles are served by mmWave Base Stations (BSs) deployed alongside the road. To evaluate our highway network, we develop a new theoretical model that accounts for a typical scenario where heavy vehicles (such as buses and lorries) in slow lanes obstruct Line-of-Sight (LOS) paths of vehicles in fast lanes and, hence, act as blockages. Using tools from stochastic geometry, we derive approximations for the Signal-to-Interference-plus-Noise Ratio (SINR) outage probability, as well as the probability that a user achieves a target communication rate (rate coverage probability). Our analysis provides new design insights for mmWave highway communication networks. In considered highway scenarios, we show that reducing the horizontal beam-width from 90? to 30? determines a minimal reduction in the SINR outage probability (namely, 4 · 10?2 at maximum). Also, unlike bi-dimensional mmWave cellular networks, for small BS densities (namely, one BS every 500 m) it is still possible to achieve an SINR outage probability smaller than 0.2.", 
We consider the problem of modeling annotated data with multiple types. We describe three hierarchical probabilistic mixture models which aim to describe such data. We conduct experiments on the Corel database of images and captions.,Modeling Annotated Data,"We consider the problem of modeling annotated data—data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval.", 
"Since Apache Spark's first release in 2014, it has become one of the most popular technologies in the big data domain. Fundamental questions regarding Spark usage remain unknown. In this paper, our goal is to learn the Spark usage domains by mining a large number of open source projects.",Modeling Topics on Open Source Apache Spark Repositories,"Since Apache Spark’s first release in 2014, it has burst onto the big data scene and became one of the most popular technologies in the domain. Despite the popularity of the Spark project, there have not been research studies towards learning the current state of Spark usage based on a large dataset of open source repositories. Fundamental questions regarding Spark usage remain unknown. In this paper, our goal is to learn the Spark usage domains by mining a large number of open source projects. We train a topic model to get topics of Spark applications and show that we can obtain the Spark applications’ business domains, techniques, and algorithms. The domains can be used to provide finer granularity of categories than GitHub’s current collection service. Since Spark supports multiple programming languages, we also discuss the topic differences across the three languages.", 
"BERT and GPT-2 have shown a great performance in tasks such as text classification, translation and NLI tasks. In this article, we analyse both algorithms.",Modern Methods for Text Generation,"Synthetic text generation is challenging and has limited success. Recently, a new architecture, called Transformers, allow machine learning models to understand better sequential data, such as translation or summarization. BERT and GPT-2, using Transformers in their cores, have shown a great performance in tasks such as text classification, translation and NLI tasks. In this article, we analyse both algorithms and compare their output quality in text generation tasks.", 
"We aim to use Deep Learning Techniques (DLTs), to analysis stable CVD that would give valuable awareness to decrease misdiagnosis in the Robust Healthcare Industry. We are using Artificial Neural Network (ANN), model characterize to predict CVD patients and configuration.",Molecular Diagnostic and Using Deep Learning Techniques for Predict Functional Recovery of Patients Treated of Cardiovascular Disease,"Today, with the development of industry and mechanized life style, the prevalence of the disease is rising steadily as well. Observing at the trend and lifecycle style, its predict that after ten years around 23.6 million people die because of Cardiovascular Disease (CVD). For that reason, aim to use Deep Learning Techniques (DLTs), to analysis stable CVD that would give valuable awareness to decrease misdiagnosis in the Robust Healthcare Industry (RHI). An objective of this paper is first, Molecular diagnosis (MD), and second using Deep Learning Techniques DLTs, to synthesis and characterize to accumulate (raw information) from CVD patients, those who admitted the emergency section between January (2018 to December 2019). We are using Artificial Neural Network (ANN), model characterize to predict CVD patients and configuration, Feature selection (FS), Mean Square Error (MSE), accuracy, sensitivity. The ANN accuracy is 98.4, K-nearest neighbor (KNN) accuracy is 98.01%, Naïve Bayes (NB), accuracy is 96.99%. Decision tree (DT), accuracy is 87.81%. Our robust data driven model explore the efficient accuracy rate to predict CVD patients. The ANN model in term of their efficient in disease analysis, and prognosis of the RHI.", 
"Little is known about the cryptographic mechanisms and security guarantees of secure group communication in instant messaging. We first provide a comprehensive and realistic security model. We analyze three widely used real-world protocols: Signal, WhatsApp, and Threema. We propose generic countermeasures to enhance the protocols.","More is Less On the End-to-End Security of Group Chats in Signal, WhatsApp, and Threema","Secure instant messaging is utilized in two variants: one-to-one communication and group communication. While the first variant has received much attention lately, little is known about the cryptographic mechanisms and security guarantees of secure group communication in instant messaging. To approach an investigation of group instant messaging protocols, we first provide a comprehensive and realistic security model. This model combines security and reliability goals from various related literature to capture relevant properties for communication in dynamic groups. Thereby the definitions consider their satisfiability with respect to the instant delivery of messages. To show its applicability, we analyze three widely used real-world protocols: Signal, WhatsApp, and Threema. By applying our model, we reveal several shortcomings with respect to the security definition. Therefore we propose generic countermeasures to enhance the protocols regarding the required security and reliability goals. Our systematic analysis reveals that (1) the communications’ integrity – represented by the integrity of all exchanged messages – and (2) the groups’ closeness – represented by the members’ ability of managing the group – are not end-to-end protected. We additionally show that strong security properties, such as Future Secrecy which is a core part of the one-to-one communication in the Signal protocol, do not hold for its group communication.", 
"CapsNet-based framework classifies the two-class motor imagery, namely right-hand and left-hand movements. The proposed framework outperformed state-of-the-art CNN-based methods and various conventional machine learning approaches. ",Motor Imagery EEG Classification Using Capsule Networks,"Various convolutional neural network (CNN)-based approaches have been recently proposed to improve the performance of motor imagery based-brain-computer interfaces (BCIs). However, the classification accuracy of CNNs is compromised when target data are distorted. Specifically for motor imagery electroencephalogram (EEG), the measured signals, even from the same person, are not consistent and can be significantly distorted. To overcome these limitations, we propose to apply a capsule network (CapsNet) for learning various properties of EEG signals, thereby achieving better and more robust performance than previous CNN methods. The proposed CapsNet-based framework classifies the two-class motor imagery, namely right-hand and left-hand movements. The motor imagery EEG signals are first transformed into 2D images using the short-time Fourier transform (STFT) algorithm and then used for training and testing the capsule network. The performance of the proposed framework was evaluated on the BCI competition IV 2b dataset. The proposed framework outperformed state-of-the-art CNN-based methods and various conventional machine learning approaches. The experimental results demonstrate the feasibility of the proposed approach for classification of motor imagery EEG signals.", 
"Motor imagery (MI) based electroencephalogram (EEG) signals are a widely used form of input in brain computer interface systems (BCIs). There are a number of ways to classify data, a question still persists as to which technique should be employed in the domain of MI based EEG signals.",Motor Imagery EEG signal Classification on DWT and Crosscorrelated signal features,"Motor imagery (MI) based electroencephalogram (EEG) signals are a widely used form of input in brain computer interface systems (BCIs). Although there are a number of ways to classify data, a question still persists as to which technique should be employed in the domain of MI based EEG signals. In this paper, an attempt is made to find the best classification algorithm and feature extraction technique by comparing some of the prominently used algorithms on a same base dataset. Feature extraction techniques like discrete wavelet transform (DWT) and cross-correlation have been studied and compared. Five classification algorithms have been implemented which are logistic regression (LR), kernalised logistic regression (KLR), multilayer perceptron neural network (MLP), probabilistic neural network (PNN) and Least-square support vector machine (LS-SVM). Dataset IVa of BCI competition III has been used as a base dataset to test the algorithms. Evaluation of the algorithms has been done using a 10-fold cross-validation procedure. Experimental results show that a combination of DWT and LSSVM classifier outperforms the other procedures.", 
"Classification of electroencephalogram (EEG) data for different motor imagery (MI) tasks is a major concern in the brain-computer interface (BCI) applications. In this paper, an efficient feature extraction scheme is proposed based on the discrete wavelet transform (DWT) of the EEG signal.",Motor Imagery EEG Signal Classification Scheme Based on Wavelet Domain Statistical Features,"Classification of electroencephalogram (EEG) data for different motor imagery (MI) tasks is a major concern in the brain-computer interface (BCI) applications. In this paper, an efficient feature extraction scheme is proposed based on the discrete wavelet transform (DWT) of the EEG signal. The EEG data of each channel is windowed into several frames and DWT is performed on each frame of data. Considering only the approximate DWT coefficients, a set of statistical features are extracted, namely wavelet domain energy, entropy, variance, and maximum. In order to reduce the dimension of the proposed feature vector, which is composed of average statistical feature values of all channels, principal component analysis (PCA) is employed. For the purpose of classification, k nearest neighbor (KNN) classifier is employed. Proposed classification scheme not only offers significant reduction in feature dimensionality but also provides satisfactory classification accuracy. For the purpose of performance analysis, publicly available MI dataset IVa of BCI Competition-III is used and a very satisfactory performance is obtained in classifying the MI data in two classes, namely right hand and right foot MI tasks.", 
Proposed method selects 18 electrodes out of 118 to analyze the non-stationary and nonlinear EEG signal behaviors. EWT method achieves an average classification accuracy of 95.2% and 94.6% respectively. The better performances of EWT over those of the existing methods demonstrate the effectiveness and great potential of EWT for brain-computer interface (BCI) systems.,Motor Imagery EEG Signals Classification Based on Mode Amplitude and Frequency Components Using Empirical Wavelet Transform,"As one of the key techniques determining the overall system performances, efficient and reliable algorithms for improving the classification accuracy of motor imagery (MI) based electroencephalography (EEG) signals are highly desired for the development of brain-computer interface (BCI) systems. In this study, we propose, for the first time to the best of our knowledge, a novel data adaptive empirical wavelet transform (EWT) based signal decomposition method for improving the classification accuracy of MI based EEG signals. Specifically, to reduce the system complexity and execution time, the proposed method selects 18 electrodes out of 118 to analyze the non-stationary and nonlinear EEG signal behaviors. Meanwhile, the method adopts the Welch power spectral density (PSD) analysis method for single mode selection out of the total 10 for each channel, and the Hilbert transform (HT) method for both instantaneous amplitude (IA) and instantaneous frequency (IF) signal components extraction for each selected mode. With seven commonly used machine-learning classifiers adopted, extensive experiments were conducted with the benchmark dataset IVa from BCI competition III to evaluate the performance of the proposed method. Results show that with the IA and IF component features being tested using the leastsquare support vector machine (LS-SVM) classifier, the EWT method achieves an average classification accuracy of 95.2% and 94.6% respectively, which is higher as compared with the existing methods. While for every participant, a classification accuracy of at least 80% could be achieved by employing a single feature only. Results also show that a combination of EWT and higher order statistics features, which contain both kurtosis and skewness of the extracted instantaneous components, help achieve a higher success rate. The better performances of EWT over those of the existing methods demonstrate the effectiveness and great potential of EWT for BCI system applications.", 
Motor imagery (MI) tasks-based brain–computer interface (BCI) system finds applications for disabled people to communicate with surrounding. Electroencephalogram (EEG) recordings provide noninvasive way for imaging of MI tasks in BCI system.,Motor imagery tasks-based EEG signals classification using tunable-Q wavelet transform,"Motor imagery (MI) tasks-based brain–computer interface (BCI) system finds applications for disabled people to communicate with surrounding. The BCI system reliability is relied on how well the different MI tasks are assessed and identified. Electroencephalogram (EEG) recordings provide a noninvasive way for imaging of MI tasks in BCI system. In this framework, tunable-Q wavelet transform (TQWT)-based feature extraction method is proposed for the classification of different MI tasks EEG signals. The TQWT parameters are tuned for the decomposition of EEG signal into sub-bands. Time domain measures of sub-bands are considered as features for MI tasks EEG signals. The TQWT-based features are tested on least-squares support vector machine classifier for the classification of right-hand and right-foot MI tasks. The proposed method provides 96.89% MI tasks classification accuracy, which is the highest as compared to other existing same data set methods. The suggested method can be used for identification of MI tasks in a BCI system designed for controlling robotic arm and wheel chairs, etc.", 
This paper proposes a method using which one can analyze different languages to find sentiments in them and perform sentiment analysis. The method leverages different techniques of machine learning to analyze the text.,"MSATS, Multilingual sentiment analysis via text summarization","Sentiment Analysis has been a keen research area for past few years. Though much of the exploration that has been done supports English language only. This paper proposes a method using which one can analyze different languages to find sentiments in them and perform sentiment analysis. The method leverages different techniques of machine learning to analyze the text. Machine translation is used in the system to provide with the feature of dealing with different languages. After the machine translation, text is processed for finding the sentiments in the text. With the advent of blogs, forums and online reviews there is substantial text present on internet that can be used to analyze the sentiment about a particular subject or an object. Hence to reduce the processing it is beneficial to extract the important text present in it. So the system proposed uses text summarization process to extract important parts of text and then uses it to analyze the sentiments about the particular subject and its aspects.", 
"Open Source communities typically use a software repository to archive various software projects. With thousands of projects in typical repositories, manually locating related projects can be difficult. MUDABlue is a tool that automatically categorizes software systems. It allows a software system to be a member of multiple categories.",MUDABlue An automatic categorization system for Open Source repositories,"Open Source communities typically use a software repository to archive various software projects with their source code, mailing list discussions, documentation, bug reports, and so forth. For example, SourceForge currently hosts over seventy thousand Open Source software systems. Because of the size of the rich information content, such repositories offer numerous opportunities for sharing information among projects. For example, one would like to know a set of projects that are related or similar to each other, so that the project groups can collaborate and share their work. With thousands of projects in typical repositories, however, manually locating related projects can be difficult. Hence, we propose MUDABlue, a tool that automatically categorizes software systems. MUDABlue has three major aspects: (1) it relies on no other information than the source code, (2) it determines category sets automatically, and (3) it allows a software system to be a member of multiple categories. MUDABlue has a Web interface to visualize determined categories, which eases browsing a software repository. We show the effectiveness of MUDABlue’s categorization capability by comparing its generated categories with that of some other existing research tools.", 
Skin diseases remain a major cause of disability worldwide and contribute approximately 1.79% of the global burden of disease measured in disability-adjusted life years. We propose an intelligent digital diagnosis scheme to improve the classification accuracy of multiple diseases.,Multi-class multi-level classification algorithm for skin lesions classification using machine learning techniques,"Skin diseases remain a major cause of disability worldwide and contribute approximately 1.79% of the global burden of disease measured in disability-adjusted life years. In the United Kingdom alone, 60% of the population suffer from skin diseases during their lifetime. In this paper, we propose an intelligent digital diagnosis scheme to improve the classification accuracy of multiple diseases. A Multi-Class MultiLevel (MCML) classification algorithm inspired by the “divide and conquer” rule is explored to address the research challenges. The MCML classification algorithm is implemented using traditional machine learning and advanced deep learning approaches. Improved techniques are proposed for noise removal in the traditional machine learning approach. The proposed algorithm is evaluated on 3672 classified images, collected from different sources and the diagnostic accuracy of 96.47% is achieved. To verify the performance of the proposed algorithm, its metrics are compared with the Multi-Class Single-Level classification algorithm which is the main algorithm used in most of the existing literature. The results also indicate that the MCML classification algorithm is capable of enhancing the classification performance of multiple skin lesions.", 
"Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance.",Multi-column Deep Neural Networks for Image Classification,"Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", 
Cellular systems are facing the ever-increasing demand for vehicular communication aimed at applications such as advanced driving assistance and ultimately fully autonomous driving. This study aims to present a sophisticated clustering mechanism that enables cellular systems to accommodate a massive number of moving Machine Type Communication (MTC) objects. The study achieved a sufficient level of prediction accuracy with fewer training data through a learned prediction function.,Multi-Dimensional Affinity Propagation Clustering Applying a Machine Learning in 5G-Cellular V2X,"Cellular systems are facing the ever-increasing demand for vehicular communication aimed at applications such as advanced driving assistance and ultimately fully autonomous driving. Cellular Vehicle to Anything (C-V2X) has become more applicable with the release of the first sets of 5G (5th Generation) system specifications. The highly capable 5G systems will therefore support even a larger number of moving objects. This study aims to present a sophisticated clustering mechanism that enables cellular systems to accommodate a massive number of moving Machine Type Communication (MTC) objects with a minimum set of connections while maintaining system scalability. Specifically, we proposed Normalized Multi Dimension-Affinity Propagation Clustering (NMDP-APC) scheme and applied it for Vehicular Ad hoc Network (VANET) clustering. For VANET clustering formation, our study employed Machine Learning (ML) to determine the granularity, i.e., the size and span of clusters desirable for use in dynamic motion environments. The study achieved a sufficient level of prediction accuracy with fewer training data through a learned prediction function based on the selected key criteria. This paper also proposes a system sequence designed with a series of procedures fully compliant with C-V2X systems. We demonstrated substantial simulations and numerical experiments with theoretical analysis, specifically applying soft-margin-based Support Vector Machine (SVM) algorithm. The simulation results confirmed that the granularity parameter we applied fairly controls the size of VANET clusters although vehicles are in motion and that the prediction performance has been adjusted through controlling of key SVM parameters.", 
"To analyze large numbers of textual documents, it is desirable to manage documents in a multi-dimensional text database. We propose a new phrase ranking measure to leverage the relation between document subsets induced by multi- dimensional context. We develop a cube-based analytical platform that implements an efficient solution.","Multi-Dimensional, Phrase-Based Summarization in Text Cubes","To systematically analyze large numbers of textual documents, it is often desirable to manage documents (and their metadata) in a multi-dimensional text database (Text Cube). Such structure provides flexibility of understanding local information with different granularities. Moreover, the contextualized analysis derived from cube structure often yields comparative insights. To quickly digest the content of subsets of documents in the multi-dimensional context, we study the problem of phrase-based summarization of a subset of documents of interest. We propose a new phrase ranking measure to leverage the relation between document subsets induced by multi-dimensional context and identify phrases that truly distinguish the queried subset of documents from neighboring subsets (i.e., background). Our quality evaluation suggests the new measure involving dynamic, query-dependent background generation is more effective than previous measures using the whole corpus as a static background for finding representative phrases. Computing this measure is more expensive due to the need of access to many subsets of documents to answer one query. We develop a cube-based analytical platform that implements an efficient solution by materializing a deliberately selected part of statistics, and using these statistics to perform online query processing within a constant latency constraint. Our experiments in a large news dataset demonstrate the efficiency in both query processing time and storage cost.", 
Proposed work is based on abstractive summarization which is the division of text summarization. It developed a summary of the multi-document using the semantic relationship between the input documents. The system generates a summary based on the predicates argument structure of the sentences.,Multi-document abstractive summarization based on predicate argument structure,"The proposed work is based on abstractive summarization which is the division of text summarization. It developed a summary of the multi-document using the semantic relationship between the input documents rather than what we get exactly from the input document. It is very necessary because of the difficulty of generating abstract manually and also a challenging task. In our system, summary is generated based on the predicate argument structure of the sentences. Semantic role labeling is utilized to obtain the predicate argument structure. Main steps involved in the proposed system: Predicate argument structure of the sentences is extracted to represent text semantically as the first step. Next, it group semantically similar predicate argument structure using hybrid approach of K-mean and agglomerative hierarchical clustering by utilizing semantic similarity measures. K-mean is selected due to its run time efficiency and agglomerative hierarchical clustering is selected due to its quality. We extract twelve features of the predicate argument and feature selection is made randomly in the optimization stage. Then top ranked predicate argument structures taken from the optimization phase. Sentences for summary is selected from top ranked predicate argument structure by utilizing language generation. The Proposed study is evaluated by Document Understanding Conference 2002 (DUC 2002). We observed that the proposed work saved the computation time and provides better result than existing systems.", 
Our proposed approach identifies the most important document in the multi-document set. The sentences in this document are aligned to sentences in other documents to generate clusters of similar sentences. We generate K-shortest paths from the sentences in each cluster using a word-graph structure.,Multi-document abstractive summarization using ILP based multi-sentence compression,"Abstractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. In this work, we aim at developing an abstractive summarizer. First, our proposed approach identifies the most important document in the multi-document set. The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences. Second, we generate K-shortest paths from the sentences in each cluster using a word-graph structure. Finally, we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming (ILP) model with the objective of maximizing information content and readability of the final summary. Our ILP model represents the shortest paths as binary variables and considers the length of the path, information score and linguistic quality score in the objective function. Experimental results on the DUC 2004 and 2005 multi-document summarization datasets show that our proposed approach outperforms all the baselines and state-of-the-art extractive summarizers as measured by the ROUGE scores. Our method also outperforms a recent abstractive summarization technique. In manual evaluation, our approach also achieves promising results on informativeness and readability.", 
"Arabic is one of the most semantically and syntactically complex languages in the world. A key challenging issue in text mining is text summarization. We propose an unsupervised score-based method which combines the vector space model, continuous bag of words, clustering, and a statistically-based method. An experiment on the Essex Arabic Summaries Corpus showed promising results in comparison with existing methods.",Multidocument Arabic Text Summarization Based on Clustering and Word2Vec to Reduce Redundancy,"Arabic is one of the most semantically and syntactically complex languages in the world. A key challenging issue in text mining is text summarization, so we propose an unsupervised scorebased method which combines the vector space model, continuous bag of words (CBOW), clustering, and a statistically-based method. The problems with multidocument text summarization are the noisy data, redundancy, diminished readability, and sentence incoherency. In this study, we adopt a preprocessing strategy to solve the noise problem and use the word2vec model for two purposes, first, to map the words to fixed-length vectors and, second, to obtain the semantic relationship between each vector based on the dimensions. Similarly, we use a k-means algorithm for two purposes: (1) Selecting the distinctive documents and tokenizing these documents to sentences, and (2) using another iteration of the k-means algorithm to select the key sentences based on the similarity metric to overcome the redundancy problem and generate the initial summary. Lastly, we use weighted principal component analysis (W-PCA) to map the sentences’ encoded weights based on a list of features. This selects the highest set of weights, which relates to important sentences for solving incoherency and readability problems. We adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as an evaluation measure to examine our proposed technique and compare it with state-of-the-art methods. Finally, an experiment on the Essex Arabic Summaries Corpus (EASC) using the ROUGE-1 and ROUGE-2 metrics showed promising results in comparison with existing methods.", 
Summarization has become one of the most applicable topics in data mining. Autoencoder neural network and deep belief network were tested on DUC 2007 dataset. The results show a better performance of autoencoding network versus deep belief Network.,Multi-Document Extractive Text Summarization via Deep Learning Approach,"Today, given the huge amount of information, summarization has become one of the most applicable topics in data mining that can help users gain access to useful data over a short period of time. In this study, two multi-document extractive text Summarization systems are introduced. The major objective of this research is to use autoencoder neural network and deep belief network separately for scoring sentences in a document to compare their performances. Deep neural networks can improve the results by generating new features. The abovementioned systems were tested on DUC 2007 dataset and evaluated using ROUGE-1 and ROUGE-2 criteria. The results show a better performance of autoencoder network versus deep belief network. It is also possible to compare these values with results of other systems to realize the effectiveness of the proposed methods.", 
Text summarization is the process of generating a brief version of a text that preserves the salient information of the text. This study focused on extracting informative summaries from multiple documents using commonly used hand-crafted features from the literature. The study recommended the use of fuzzy systems based on a feature vector and a fuzzy rule set.,"Multi-document extractive text summarization, A comparative assessment on features","Text summarization is the process of generating a brief version of a text that preserves the salient information of the text. For information retrieval, it is a good dimension reduction solution. In addition, it reduces the required reading time. This study focused on extracting informative summaries from multiple documents using commonly used hand-crafted features from the literature. The first investigation focused on the generation of a feature vector. The features were the number of sentences, term frequency, similarity with the title, term frequency-inverse sentence frequency, sentence position, sentence length, sentence–sentence similarity, bushy-path results, phrases of the sentence, proper nouns, n-gram co-occurrence, and length of the document. Secondly, several combinations of these features were examined and a shallow multi-layer perceptron and two differently modeled fuzzy inference systems were used to extract salient sentences from texts in the Document Understanding Conference (DUC) dataset. The summarization performances of these models were evaluated using original classification performance metrics, and recall-oriented understudy for gisting evaluation (ROUGE)-n. This study recommended the use of fuzzy systems based on a feature vector and a fuzzy rule set for extractive text summarization. The extraction methods were evaluated against a changing compression ratio. Results of experiments showed that the implemented neural model tended to incorrectly infer sentences that were not considered salient by human annotators. However, for distinguishing between summary-worthy and summary-unworthy sentences, the fuzzy inference systems performed better than the utilized neural network, as well as better than the existing fuzzy inference-based text summarization approaches in the literature.", 
We propose to model sentences as hyperedges and words as vertices using a hypergraph. We combine it with topic signatures to differentiate between descriptive sentences and non-descriptive sentences. We outperform a number of baseline in the DUC 2001 dataset using the ROUGE metric.,Multi-document Hyperedge-based Ranking for Text Summarization,"In a multi-document settings, graph-based extractive summarization approaches build a similarity graph out of sentences in each cluster of documents then use graph centrality approaches to measure the importance of sentences. The similarity is computed between each pair of sentences. However, it is not clear if such approach captures high-order relations among more than two sentences or can differentiate between descriptive sentences of the cluster in comparison with other clusters. In this paper, we propose to model sentences as hyperedges and words as vertices using a hypergraph and combine it with topic signatures to differentiate between descriptive sentences and non-descriptive sentences. To rank sentences, we propose a new random walk over hyperedges that will prefer descriptive sentences of the cluster when measuring their centrality scores. Our approach outperform a number of baseline in the DUC 2001 dataset using the ROUGE metric.", 
"Multi-document summarization is of great value to many real world applications since it can help people get the main ideas within a short time. In this paper, we tackle the problem of extracting summary sentences from multi-document sets by applying sparse coding techniques.",Multi-Document Summarization Based on Two-Level Sparse Representation Model,"Multi-document summarization is of great value to many real world applications since it can help people get the main ideas within a short time. In this paper, we tackle the problem of extracting summary sentences from multi-document sets by applying sparse coding techniques and present a novel framework to this challenging problem. Based on the data reconstruction and sentence denoising assumption, we present a two-level sparse representation model to depict the process of multi-document summarization. Three requisite properties is proposed to form an ideal reconstruct able summary: Coverage, Sparsity and Diversity. We then formalize the task of multi-document summarization as an optimization problem according to the above properties, and use simulated annealing algorithm to solve it. Extensive experiments on summarization benchmark data sets DUC2006 and DUC2007 show that our proposed model is effective and outperforms the state-of-the-art algorithms.", 
There are two main categories of multi-document summarization: term-based and ontology-based methods. This paper presents a pattern-based model for generic multi- document summarization. It exploits closed patterns to extract the most salient sentences from a document collection. The method significantly outperforms the state-of-the-art methods.,Multi-document Summarization using Closed Patterns,"There are two main categories of multi-document summarization: term-based and ontology-based methods. A term-based method cannot deal with the problems of polysemy and synonymy. An ontology-based approach addresses such problems by taking into account of the semantic information of document content, but the construction of ontology requires lots of manpower. To overcome these open problems, this paper presents a pattern-based model for generic multi-document summarization, which exploits closed patterns to extract the most salient sentences from a document collection and reduce redundancy in the summary. Our method calculates the weight of each sentence of a document collection by accumulating the weights of its covering closed patterns with respect to this sentence, and iteratively selects one sentence that owns the highest weight and less similarity to the previously selected sentences, until reaching the length limitation. The sentence weight calculation by patterns reduces the dimension and captures more relevant information. Our method combines the advantages of the term-based and ontology-based models while avoiding their weaknesses. Empirical studies on the benchmark DUC2004 datasets demonstrate that our pattern-based method significantly outperforms the state-of-the-art methods. Multi-document summarization can be used to extract a particular individual’s opinions in the form of closed patterns, from this individual’s documents shared in social networks, hence provides a useful tool for further analyzing the individual’s behavior and influence in group activities.", 
The Markov Random Walk model has been recently exploited for multi-document summarization by making use of link relationships between sentences. This paper proposes two models to fully leverage the cluster-level information. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models.,Multi-document summarization using cluster-based link analysis,"The Markov Random Walk model has been recently exploited for multi-document summarization by making use of the link relationships between sentences in the document set, under the assumption that all the sentences are indistinguishable from each other. However, a given document set usually covers a few topic themes with each theme represented by a cluster of sentences. The topic themes are usually not equally important and the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cluster. This paper proposes the Cluster-based Conditional Markov Random Walk Model (ClusterCMRW) and the Cluster-based HITS Model (ClusterHITS) to fully leverage the cluster-level information. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models. The results also demonstrate that the ClusterCMRW model is more robust than the ClusterHITS model, with respect to different cluster numbers.", 
Text summarization aims to generate condensed summary from a large set of documents on the same topic. The proposed method generates high ROUGE score summaries and is comparable to the state-of-the-art summarization methods.,Multi-document summarization using evolutionary multi-objective optimization,Text summarization aims to generate condensed summary from a large set of documents on the same topic. We formulate text summarization task as a multi-objective optimization problem by defining information coverage and diversity as two conflicting objective functions. Œe result solutions represent summaries that ensure the maximum coverage of the original document and the diversity of the sentences in the summary among each other. Œe initial experiment using DUC2002 multi-document summarization task dataset and ROUGE evaluation metric shows that the proposed method generates high ROUGE score summaries and is comparable to the state-of-the-art summarization methods., 
Most multi-document summarization systems follow the extractive framework based on various features. Support Vector Regression (SVR) model is used for automatically combining the features and scoring the sentences. ,Multi-document Summarization Using Support Vector Regression,"Most multi-document summarization systems follow the extractive framework based on various features. While more and more sophisticated features are designed, the reasonable combination of features becomes a challenge. Usually the features are combined by a linear function whose weights are tuned manually. In this task, Support Vector Regression (SVR) model is used for automatically combining the features and scoring the sentences. Two important problems are inevitably involved. The first one is how to acquire the training data. Several automatic generation methods are introduced based on the standard reference summaries generated by human. Another indispensable problem in SVR application is feature selection, where various features will be picked out and combined into different feature sets to be tested. With the aid of DUC 2005 and 2006 data sets, comprehensive experiments are conducted with consideration of various SVR kernels and feature sets. Then the trained SVR model is used in the main task of DUC 2007 to get the extractive summaries.", 
A modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally. ,Multi-document summarization via budgeted maximization of submodular functions,"We treat the text summarization problem as maximizing a submodular function under a budget constraint. We show, both theoretically and empirically, a modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally, and we derive new approximation bounds in doing so. Experiments on DUC’04 task show that our approach is superior to the best performing method from the DUC’04 evaluation on ROUGE-1 scores.", 
"Multi-document summarization aims to create a compressed summary while retaining the main characteristics of the original set of documents. In this paper, we propose a new framework based on sentence-level semantic analysis and symmetric non-negative matrix factorization. ",Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization,"Multi-document summarization aims to create a compressed summary while retaining the main characteristics of the original set of documents. Many approaches use statistics and machine learning techniques to extract sentences from documents. In this paper, we propose a new multi-document summarization framework based on sentence-level semantic analysis and symmetric non-negative matrix factorization. We first calculate sentence-sentence similarities using semantic analysis and construct the similarity matrix. Then symmetric matrix factorization, which has been shown to be equivalent to normalized spectral clustering, is used to group sentences into clusters. Finally, the most informative sentences are selected from each group to form the summary. Experimental results on DUC2005 and DUC2006 data sets demonstrate the improvement of our proposed framework over the implemented existing summarization systems. A further study on the factors that benefit the high performance is also conducted.", 
Web has become rich source of information about many aspects of organizational activities. One input towards gathering competitive intelligence is to mine the text sources available on the web for any particular company.,"Multi-document Text Summarization for Competitor Intelligence, A Methodology","With increasing adoption of Internet and various social media technologies by companies, the web has become a rich source of information about many aspects of organizational activities. Hence one input towards gathering competitive intelligence is to mine the text sources available on the web for any particular company and use the summarized version of that information for strategic decision making. This paper discusses a methodological approach and an architecture for such summarization system which can be used for gathering competitor intelligence by the companies.", 
Proposed system is a complete E-learning system for the domain Operating systems. A similarity check followed by multi-document summarization leads to a non-redundant answer. The system avoids the need to maintain the knowledge base thus reducing the space complexity.,Multi-document Text Summarization in E-learning System for Operating System Domain,The query answering in E-learning systems generally mean retrieving relevant answer for the user query. In general the conventional E-learning systems retrieve answers from their inbuilt knowledge base. This leads to the limitation that the system cannot work out of its bound i.e. it does not answer for a query whose contents are not in the knowledge base. The proposed system overcomes this limitation by passing the query online and carrying out multidocument summarization on online documents. The proposed system is a complete E-learning system for the domain Operating systems. The system avoids the need to maintain the knowledge base thus reducing the space complexity. A similarity check followed by multi-document summarization leads to a non-redundant answer. The queries are classified into simple and complex types. Brief answers are retrieved for simple queries whereas detailed answers are retrieved for complex queries., 
"This paper presents a method for generating multi-document text summary building on single document text summaries. The average F-measure of 0.30493 on DUC 2002 dataset has been observed, which is comparable to two of five top performing multi- document text summarization systems.",Multi-document Text Summarization Using Sentence Extraction,"This paper presents a method for generating multi-document text summary building on single document text summaries and by combining those single document text summaries using cosine similarity. For the generation of single document text summaries features like document feature, sentence position feature, normalized sentence length feature, numerical data feature, and proper noun feature are used. Single document text summaries are combined after calculating cosine similarity between the different single document text summaries generated and from each combination, sentences with high total sentence weight are extracted to generate multi-document text summary. The average F-measure of 0.30493 on DUC 2002 dataset has been observed, which is comparable to two of five top performing multi-document text summarization systems reported on the DUC 2002 dataset.", 
This paper proposes a new multi-document summarization method that combines topic model and fuzzy logic model. The proposed method extracts some relevant topic words from source documents. The extracted words are used as elements of fuzzy sets.,Multi-document Text Summarization Using Topic Model and Fuzzy Logic,"The automation of the process of summarizing documents plays a major rule in many applications. Automatic Text Summarization has been focused on retaining the essential information without affecting the document quality. This paper proposes a new multi-document summarization method that combines topic model and fuzzy logic model. The proposed method extracts some relevant topic words from source documents. The extracted words are used as elements of fuzzy sets. Meanwhile, each sentence on the source document is used to generate a fuzzy relevance rule that measures the importance of each sentence. A fuzzy inference system is used to generate the final summarization. Our summarization results are evaluated against some well-known summary systems and performed well in divergences and similarities.", 
"Document summarization is an emerging technique for understanding the main purpose of any kind of documents. To visualize a large text document within a short duration and small visible area like PDA screen, summarization provides a greater flexibility and convenience. In this paper we study various text summarization techniques e.g. RANDOM, LEAD and MEAD.","Multi-document Text Summarization, SimWithFirst Based Features and Sentence Co-selection Based Evaluation","Document summarization is an emerging technique for understanding the main purpose of any kind of documents. To visualize a large text document within a short duration and small visible area like PDA screen, summarization provides a greater flexibility and convenience. In this paper we study various text summarization techniques e.g. RANDOM, LEAD and MEAD. Then, we propose two techniques for both single and multi document text summarization. One is adding a new feature SimWithFirst (Similarity With First Sentence) with MEAD (Combination of Centroid, Position, and Length Features) called CPSL and another is the combination of LEAD and CPSL called LESM. Finally we simulate and compare the results of new techniques with conventional ones called MEAD with respect to some evaluation techniques. Simulation results demonstrate that CPSL shows better performance for short summarization than MEAD and for remaining cases it is almost similar to MEAD. Furthermore, simulation results demonstrate that LESM also shows better performance for short summarization than MEAD but for remaining cases it does not show better performance than MEAD.", 
System-generated abstractive summaries often face the pitfall of factual inconsistency. Span-Fact is a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection.,Multi-Fact Correction in Abstractive Text Summarization,"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or autoregressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.", 
"Electroencephalogram (EEG) analysis has been widely used in the diagnosis of stroke diseases for its low cost and noninvasive characteristics. This paper proposes a novel EEG stroke signal classification method to classify the EEG signals of stroke patients with cerebral infarction and cerebral hemorrhage. By combining hierarchical theory, wavelet packet energy and fuzzy entropy, a multi-feature fusion method is proposed.",Multi-Feature Fusion Method Based on EEG Signal and its Application in Stroke Classification,"Electroencephalogram (EEG) analysis has been widely used in the diagnosis of stroke diseases for its low cost and noninvasive characteristics. In order to classify the EEG signals of stroke patients with cerebral infarction and cerebral hemorrhage, this paper proposes a novel EEG stroke signal classification method. This method has two highlights. The first is that a multi-feature fusion method is given by combining wavelet packet energy, fuzzy entropy and hierarchical theory. The second highlight is that a suitable classification model based on ensemble classifier is constructed for perfectly classification stroke signals. Entropy is an accessible way to measure information and uncertainty of time series. Many entropy-based methods have been developed these years. By comparing with the performances of permutation entropy, sample entropy, approximate entropy in measuring the characteristic of stroke patient’s EEG signals, it can be found that fuzzy entropy has best performance in characterization stroke EEG signal. By combining hierarchical theory, wavelet packet energy and fuzzy entropy, a multi-feature fusion method is proposed. The method first calculates wavelet packet energy of EEG stroke signal, then extracts hierarchical fuzzy entropy feature by combining hierarchical theory and fuzzy entropy. The experimental results show that, compared with the fuzzy entropy feature, the classification accuracy based on the fusion feature of wavelet packet energy and hierarchical fuzzy entropy is much higher than benchmark methods. It means that the proposed multi-feature fusion method based on stroke EEG signal is an efficient measure in classifying ischemic and hemorrhagic stroke. Support vector machine (SVM), decision tree and random forest are further used as the stroke signal classification models for classifying ischemic stroke and hemorrhagic stroke. Experimental results show that, based on the proposed multi-feature fusion method, the ensemble method of random forest can get the best classification performance in accuracy among three models.", 
"Industrial Wireless Sensor Networks (IWSN) are used to acquire sensor data that need real-time processing. To be cost-effective, IWSN are also expected to be low-cost and low-power. Bluetooth Low Energy (BLE) is a promising technology, as it allows to implement low- cost industrial networks.",Multi-hop Real-time Communications over Bluetooth Low Energy Industrial Wireless Mesh Networks,"Industrial Wireless Sensor Networks (IWSN) are used to acquire sensor data that need real-time processing, therefore they require predictable behavior and real-time guarantees. To be cost-effective, IWSN are also expected to be low-cost and low-power. In this context, Bluetooth Low Energy (BLE) is a promising technology, as it allows to implement low-cost industrial networks. As BLE is a shortrange technology, a multi-hop mesh network is needed to cover a large area. Nevertheless the recently published Bluetooth mesh networking specifications do not provide support for real-time communications over multi-hop mesh networks. To overcome this limitation, this paper proposes the Multi-hop Real-time BLE (MRT-BLE) protocol, a real-time protocol developed on top of BLE, that allows for bounded packet delays over mesh networks. MRT-BLE also provides priority support. The paper describes in detail the MRT-BLE protocol and how to implement it on Commercial-Off-The-Shelf devices. Two kinds of performance evaluation for the MRT-BLE protocol are provided. The first one is a worst-case end-to-end delay analysis, while the second one is based on experimental results obtained through measurements on a real testbed.", 
This study proposes an improved hybrid model built with empirical mode decomposition (EMD) features combined with weighted multi- kernel random vector functional link network (WMKRVFLN) The model is optimised with an efficient optimisation algorithm known as water cycle algorithm (WCA) The approach is superior to several state-of-the-art techniques and is highly comparable to many such techniques.,Multi-kernel-based random vector functional link network with decomposed features for epileptic EEG signal classification,"This study proposes an improved hybrid model built with empirical mode decomposition (EMD) features combined with weighted multi-kernel random vector functional link network (WMKRVFLN) where the kernel parameters are optimised with an efficient optimisation algorithm known as water cycle algorithm (WCA) for diagnosis and classification of epileptic electroencephalogram (EEG) signals. The proposed model with optimisation is known as WCA–EMD–WMKRVFLN. The tanh and wavelet kernel functions are contributing together to the effectiveness of the proposed model. The features generated from EMD in terms of intrinsic mode functions (IMFs) are modulated to find important statistical and entropy based features and these features in a reduced form are employed as inputs to the model to classify epileptic EEG signals. The presented approach is evaluated in terms of percentage correct classification accuracy (ACC), specificity and sensitivity using two datasets and is compared with different classifiers and state-of-the-art techniques. The highest accuracies of 99.69% (five-class) and 100% (three-class) achieved using the Bonn-University dataset and 99.0% ACC (two-class) achieved using the Bern-Barcelona dataset. The achieved results report that the presented approach is a promising approach for EEG signal classification and is superior to several state-of-the-art techniques and is highly comparable to many such techniques.", 
Sexism has the potential to assist social scientists and policy makers in studying and countering sexism better. The best proposed method outperforms several deep learning as well as traditional machine learning baselines by an appreciable margin. We develop a neural solution for this multi-label classification that can combine sentence representations.,Multi-label Categorization of Accounts of Sexism using a Neural Framework,"Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in studying and countering sexism better. The existing work on sexism classification, which is different from sexism detection, has certain limitations in terms of the categories of sexism used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. The best proposed method outperforms several deep learning as well as traditional machine learning baselines by an appreciable margin.", 
"This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to the ICD-10 (International Classification of Diseases) codes. This approach is based on Machine learning techniques, which is evaluated over CépiDC corpus.",Multi-label Categorization of French Death Certificates using NLP and Machine Learning,"The medical information represents an invaluable source of knowledge concerning the medical history of the patient, but the manner of their presentation make it badly exploited. The idea of this paper is based on the analysis of the death reports written in natural language, which are rich of information, and can be exploited in the calculation of mortality statistics, giving preventive solutions, as well as, help medical professional in their research. This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to ICD-10 (International Classification of Diseases) codes. This approach is based on Machine learning techniques, which is evaluated over CépiDC corpus. The experiment showed that our approach gives interesting results, with an average F1-measue of 79.02%.", 
A multi-modal sentence summarization task that produces a short summary from a pair of sentence and image is more challenging than sentence summarizing. We propose a modality-based attention mechanism to pay different attention to image patches and text units. We design image filters to selectively use visual information to enhance the semantics.,Multi-modal Sentence Summarization with Modality Attention and Image Filtering.,"In this paper, we introduce a multi-modal sentence summarization task that produces a short summary from a pair of sentence and image. This task is more challenging than sentence summarization. It not only needs to effectively incorporate visual features into standard text summarization framework, but also requires to avoid noise of image. To this end, we propose a modality-based attention mechanism to pay different attention to image patches and text units, and we design image filters to selectively use visual information to enhance the semantics of the input sentence. We construct a multimodal sentence summarization dataset and extensive experiments on this dataset demonstrate that our models significantly outperform conventional models which only employ text as input. Further analyses suggest that sentence summarization task can benefit from visually grounded representations from a variety of aspects.", 
"The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video. In this work, we propose an extractive multi- modal summarizing method that can automatically generate a textual summary. We further introduce an MMS corpus in English and Chinese, which is released to the public.","Multi-modal summarization for asynchronous collection of text, image, audio and video","The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video. In this work, we propose an extractive multi-modal summarization method that can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal content. For audio information, we design an approach to selectively use its transcription. For visual information, we learn the joint representations of text and images using a neural network. Finally, all of the multimodal aspects are considered to generate the textual summary by maximizing the salience, non-redundancy, readability and coverage through the budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese, which is released to the public. The experimental results obtained on this dataset demonstrate that our method outperforms other competitive baseline methods.", 
"Text summarization is the task of compressing and rewriting a long document into a short summary. We address these three important aspects of a good summary via a reinforcement learning approach. We use two novel reward functions: ROUGESal and Entail, on top of a coverage-based baseline.",Multi-Reward Reinforced Summarization with Saliency and Entailment,"Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, directed logical entailment, and non-redundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words detected via a keyphrase classifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these rewards are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results (including human evaluation) on the CNN/Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.", 
"Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness. This paper presents a simple but effective scheme called multiscale orderless pooling (MOP-CNN).",Multi-scale Orderless Pooling of Deep Convolutional Activation Features,"Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multiscale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.", 
"Query-oriented summarization aims at extracting an informative summary from a document collection for a given query. Most of existing work assumes that documents related to the query only talk about one topic. However, statistics show that a large portion of summarization tasks talk about multiple topics.",Multi-topic based Query-oriented Summarization,"Query-oriented summarization aims at extracting an informative summary from a document collection for a given query. It is very useful to help users grasp the main information related to a query. Existing work can be mainly classified into two categories: supervised method and unsupervised method. The former requires training examples, which makes the method limited to predefined domains. While the latter usually utilizes clustering algorithms to find ‘centered’ sentences as the summary. However, the method does not consider the query information, thus the summarization is general about the document collection itself. Moreover, most of existing work assumes that documents related to the query only talks about one topic. Unfortunately, statistics show that a large portion of summarization tasks talk about multiple topics. In this paper, we try to break limitations of the existing methods and study a new setup of the problem of multi-topic based query-oriented summarization. We propose using a probabilistic approach to solve this problem. More specifically, we propose two strategies to incorporate the query information into a probabilistic model. Experimental results on two different genres of data show that our proposed approach can effectively extract a multi-topic summary from a document collection and the summarization performance is better than baseline methods. The approach is quite general and can be applied to many other mining tasks, for example product opinion analysis and question answering.", 
"A key challenging issue in text mining is text summarization. Problems include noisy data, redundancy, diminished readability, and sentence incoherency. We propose an unsupervised score-based method which combines the vector space model, continuous bag of words and clustering.",Multidocument Arabic Text Summarization Based on Clustering and Word2Vec to Reduce Redundancy,"Arabic is one of the most semantically and syntactically complex languages in the world. A key challenging issue in text mining is text summarization, so we propose an unsupervised score-based method which combines the vector space model, continuous bag of words (CBOW), clustering, and a statistically-based method. The problems with multidocument text summarization are the noisy data, redundancy, diminished readability, and sentence incoherency. In this study, we adopt a preprocessing strategy to solve the noise problem and use the word2vec model for two purposes, first, to map the words to fixed-length vectors and, second, to obtain the semantic relationship between each vector based on the dimensions. Similarly, we use a k-means algorithm for two purposes: (1) Selecting the distinctive documents and tokenizing these documents to sentences, and (2) using another iteration of the k-means algorithm to select the key sentences based on the similarity metric to overcome the redundancy problem and generate the initial summary. Lastly, we use weighted principal component analysis (W-PCA) to map the sentences’ encoded weights based on a list of features. This selects the highest set of weights, which relates to important sentences for solving incoherency and readability problems. We adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as an evaluation measure to examine our proposed technique and compare it with state-of-the-art methods. Finally, an experiment on the Essex Arabic Summaries Corpus (EASC) using the ROUGE-1 and ROUGE-2 metrics showed promising results in comparison with existing methods.", 
We report the experimental results on production of multilayer soft x-ray and EUV mirrors. They are used in x-rays spectroscopy and fluorescence analysis.,Multilayer optics for x-ray and y -radiation,"We report the experimental results on production of multilayer soft x-ray and EUV mirrors and their application in x-ray spectroscopy and fluorescence analysis, as well as for development of EUV lithographic and x-ray microscopic devices and soft x-ray point sources. The problem of the production and the investigation of short-period x-ray multilayers and multilayer y-filters is discussed.", 
"An increasing trend in using deep learning for electroencephalograph (EEG) analysis is evident. Cognitive process is based on understanding human brain cognition through signals, such as EEG. Deep learning can aid in developing cognitive systems and related applications by improving EEG decoding. The classification and recognition of EEG have consistently been challenging due to its characteristics.",Multilevel Weighted Feature Fusion Using Convolutional Neural Networks for EEG Motor Imagery Classification,"Deep learning methods, such as convolution neural networks (CNNs), have achieved remarkable success in computer vision tasks. Hence, an increasing trend in using deep learning for electroencephalograph (EEG) analysis is evident. Extracting relevant information from CNN features is one of the key reasons behind the success of the CNN-based deep learning models. Some CNN models use convolutional features from different CNN layers with good effect. However, extraction and fusion of multilevel convolutional features remain unexplored for EEG applications. Moreover, cognitive computing and artificial intelligence experience increasing applications in all fields. Cognitive process is based on understanding human brain cognition through signals, such as EEG. Hence, deep learning can aid in developing cognitive systems and related applications by improving EEG decoding. The classification and recognition of EEG have consistently been challenging due to its characteristics of dynamic time series data and low signal-to-noise ratio. However, the information hidden in different convolution layers can aid in improving feature discrimination capability. In this paper, we use the EEG motor imagery data to uncover the benefits of extracting and fusing multilevel convolutional features from different CNN layers, which are abstract representations of the input at various levels. Our proposed CNN model can learn robust spectral and temporal features from the raw EEG data. We demonstrate that such multilevel feature fusion outperforms the models that use features only from the last layer. Our results are better than the state of the art for EEG decoding and classification.", 
This paper describes a language-independent model for multi-class sentiment analysis using a simple neural network architecture of five layers. The advantage of the proposed model is that it does not rely on language-specific features such as dictionaries. ,Multilingual Multi-class Sentiment Classification Using Convolutional Neural Networks,"This paper describes a language-independent model for multi-class sentiment analysis using a simple neural network architecture of five layers (Embedding, Conv1D, GlobalMaxPooling and two Fully-Connected). The advantage of the proposed model is that it does not rely on language-specific features such as ontologies, dictionaries, or morphological or syntactic pre-processing. Equally important, our system does not use pre-trained word2vec embeddings which can be costly to obtain and train for some languages. In this research, we also demonstrate that oversampling can be an effective approach for correcting class imbalance in the data. We evaluate our methods on three publicly available datasets for English, German and Arabic, and the results show that our system’s performance is comparable to, or even better than, the state of the art for these datasets. We make our source-code publicly available.", 
We study correlation of rankings of text summarization systems using evaluation methods with and without human models. Research is carried out using a new content-based evaluation framework called FRESA to compute divergences among probability distributions.,Multilingual Summarization Evaluation without Human Models,"We study correlation of rankings of text summarization systems using evaluation methods with and without human models. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as coverage, Responsiveness, Pyramids and ROUGE studying their associations in various text summarization tasks including generic and focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions.", 
Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. Traditional methods can not fully exploit multimodal information. We propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem.,Multimodal Classification with Deep Convolutional-Recurrent Neural Networks for Electroencephalography,"Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. However, it is very difficult to obtain satisfactory classification accuracy due to traditional methods can not fully exploit multimodal information. Herein, we propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem, which is designed to preserve the multimodal information of EEG. In addition, optical flow is introduced to represent the variant information of EEG. We train a deep neural network (DNN) with convolutional neural network (CNN) and recurrent neural network (RNN) for the EEG classification task by using EEG video and optical flow. The experiments demonstrate that our approach has many advantages, such as more robustness and more accuracy in EEG classification tasks. According to our approach, we designed a mixed BCI-based rehabilitation support system to help stroke patients perform some basic operations.", 
"A system based on the deep autoencoder architecture is designed to extract discriminant features in the multimodal data representation. Autoencoder can be seen as a compression approach, we extend it to handle multimodals at the encoder layer, reconstructed and retrieved at the decoder layer.",Multimodal deep learning approach for joint EEG-EMG data compression and classification,"In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer. We show through experimental results, that exploiting both multimodal data intercorellation and intracorellation 1) Significantly reduces signal distortion particularly for high compression levels 2) Achieves better accuracy in classifying EEG and EMG signals recorded and labeled according to the sentiments of the volunteer.", 
The proposed emotion recognition model is based on the hierarchical long-short term memory neural network (LSTM) for video-electroencephalogram (Video-EEG) signal interaction. The inputs are facial-video and EEG signals from the subjects when they are watching the emotion-stimulated video. ,Multimodal Fused Emotion Recognition About Expression-EEG Interaction and Collaboration Using Deep Learning,"The proposed emotion recognition model is based on the hierarchical long-short term memory neural network (LSTM) for video-electroencephalogram (Video-EEG) signal interaction. The inputs are facial-video and EEG signals from the subjects when they are watching the emotion-stimulated video. The outputs are the corresponding emotion recognition results. Facial-video features and corresponding EEG features are extracted based on a fully connected neural network (FC) at each time point. These features are fused through hierarchical LSTM to predict the key emotional signal frames at the next time point until the emotion recognition result is calculated at the last time point. Specially, a self-attention mechanism is applied to show the correlation of the stacked LSTM at different hierarchies. In this process, the ‘‘selective focus’’ is used to analyze the human-emotional temporal sequences in each model, which improves the utilization of the key spatial EEG signals. Moreover, the process includes the temporal attention mechanism to predict the key signal frame at next time point, which utilizes the key emotion data in temporal domain. The experimental results prove that the classification rate (CR) and F1-score of the proposed emotion recognition model are significantly increased by at least 2% and 0.015, respectively, compared to other methods.", 
We perform multi-document summarization by generating compressed versions of source sentences as summary candidates. We combine a parse-and-trim approach with a novel technique for producing multiple alternative compressions.,Multiple Alternative Sentence Compressions for Automatic Text Summarization,"We perform multi-document summarization by generating compressed versions of source sentences as summary candidates and using weighted features of these candidates to construct summaries. We combine a parse-and-trim approach with a novel technique for producing multiple alternative compressions for source sentences. In addition, we use a novel method for tuning the feature weights that maximizes the change in the ROUGE-2 score (?ROUGE) between the already existing summary state and the new state that results from the addition of the candidate under consideration. We also describe experiments using a new paraphrase-based feature for redundancy checking. Finally, we present the results of our DUC2007 submissions and some ideas for future work.", 
"The main purpose of this paper is to apply several approaches to classify motor imageries originating from the brain in a more robust manner. For this study, dataset II from BCI competition III was used. Several classic classifiers were implemented to be utilized in the multiple classifier system.",Multiple classifier system for EEG signal classification with application to brain–computer interfaces,"In this paper, we demonstrate the use of a multiple classifier system for classification of electroencephalogram (EEG) signals. The main purpose of this paper is to apply several approaches to classify motor imageries originating from the brain in a more robust manner. For this study, dataset II from BCI competition III was used. To extract features from the brain signal, discrete wavelet transform decomposition was used. Then, several classic classifiers were implemented to be utilized in the multiple classifier system, which outperforms the reported results of other proposed methods on the dataset. Also, a variety of classifier combination methods along with genetic algorithm feature selection were evaluated and compared in order to diminish classification error. Our results suggest that an ensemble system can be employed to boost EEG classification accuracy.", 
"Method is developed for the 2013 Emotion Recognition in the Wild Challenge. The proposed method achieves competitive results, with an accuracy gain of approximately 10% above the challenge baseline.",Multiple Kernel Learning for Emotion Recognition in the Wild,"We propose a method to automatically detect emotions in unconstrained settings as part of the 2013 Emotion Recognition in the Wild Challenge [16], organized in conjunction with the ACM International Conference on Multimodal Interaction (ICMI 2013). Our method combines multiple visual descriptors with paralinguistic audio features for multimodal classification of video clips. Extracted features are combined using Multiple Kernel Learning and the clips are classified using an SVM into one of the seven emotion categories: Anger, Disgust, Fear, Happiness, Neutral, Sadness and Surprise. The proposed method achieves competitive results, with an accuracy gain of approximately 10% above the challenge baseline.", 
"Text Summarization plays an important role in the area of text mining and natural language processing. This paper presents a novel approach to generate abstractive summary from extractive summary using WordNet ontology. An experimental result shows the generated summary in well-compressed, grammatically correct and human readable format.",Multiple Text Document Summarization System using hybrid Summarization technique,"Text Summarization plays an important role in the area of text mining and natural language processing. As the information resources are increasing tremendously, readers are overloaded with loads of information. Finding out the relevant data and manually summarizing it in short time is much more difficult, challenging and tedious task for a human being. Text Summarization aims to compress the source text into a shorter and concise form with preserving its information content and overall meaning. Summarization can be classified into two main categories i.e. extractive summarization and abstractive summarization. This paper presents a novel approach to generate abstractive summary from extractive summary using WordNet ontology. An experimental result shows the generated summary in well-compressed, grammatically correct and human readable format.", 
"The MUSEEC tool implements several extractive summarization techniques. It can be applied, with some minor adaptations, to documents in multiple languages. It provides three summarization methods: MUSE, POLY and WECOM.","MUSEEC, A Multilingual Text Summarization Tool","The MUSEEC (MUltilingual SEntence Extraction and Compression) summarization tool implements several extractive summarization techniques – at the level of complete and compressed sentences – that can be applied, with some minor adaptations, to documents in multiple languages. The current version of MUSEEC provides the following summarization methods: (1) MUSE – a supervised summarizer, based on a genetic algorithm (GA), that ranks document sentences and extracts top–ranking sentences into a summary, (2) POLY – an unsupervised summarizer, based on linear programming (LP), that selects the best extract of document sentences, and (3) WECOM – an unsupervised extension of POLY that compiles a document summary from compressed sentences. In this paper, we provide an overview of MUSEEC methods and its architecture in general.", 
"Named data networking and software defined networking share mutual courage in changing legacy networking architecture. In this article, we first see both SDN and NDN enabled VNs from a bird's eye view. We present an architecture that combines SDN functionalities within VNs to retrieve the required content using NDN.",Named Data Networking for Software Defned Vehicular Networks,"Named data networking and software defined networking share mutual courage in changing legacy networking architectures. In the case of NDN, IP-based communication has been tackled by naming the data or content itself, while SDN proposes to decouple the control and data planes to make various services manageable without physical interference with switches and routers. Both NDN and SDN also support communication via heterogeneous interfaces and have been recently investigated for vehicular networks. Naïve VNs are based on the IP-based legacy, which is prone to several issues due to the dynamic network topology among other factors. In this article, we first see both SDN and NDN enabled VNs from a bird’s eye view, and for the very first time, we present an architecture that combines SDN functionalities within VNs to retrieve the required content using NDN. Moreover, we discuss a number of current research challenges and provide a precise roadmap that can be considered for the research community to jointly address such challenges.", 
"NER is used to locate and classify atomic elements in text into predetermined classes. NER is used in many applications like text summarization, text classification, question answering and machine translation systems. A Condition based approach has been used for developing NER system for Punjabi language.",Named entity recognition for punjabi language text summarization,"Named Entity Recognition (NER) is used to locate and classify atomic elements in text into predetermined classes such as the names of persons, organizations, locations, concepts etc. NER is used in many applications like text summarization, text classification, question answering and machine translation systems etc. For English a lot of work has already done in field of NER, where capitalization is a major clue for rules, whereas Indian Languages do not have such feature. This makes the task difficult for Indian languages. This paper explains the Named Entity Recognition System for Punjabi language text summarization. A Condition based approach has been used for developing NER system for Punjabi language. Various rules have been developed like prefix rule, suffix rule, propername rule, middlename rule and lastname rule. For implementing NER, various resources in Punjabi, have been developed like a list of prefix names, a list of suffix names, a list of proper names, middle names and last names. The Precision, Recall and F-Score for condition based NER approach are 89.32%, 83.4% and 86.25% respectively.", 
Sleep state identification is a time consuming procedure where the sleep state is determined in epochs of 60 second. The proposed system comprises two major step; feature extraction and classification. The performance of the proposed system was evaluated for both preterm and fullterm records assembled in one group of data using 10 fold cross validation.,Neonatal Sleep State Identification Using Deep Learning Autoencoders,"Neonatal sleep state analysis provides a tool for diagnosis of several possible physiological disorders in newborns. The sleep state identification is a time consuming procedure where the sleep state is determined in epochs of 60 second for an entire sleep recording. A new technique for automated sleep state identification in neonates is proposed. The proposed system comprises two major step; feature extraction and classification. Twelve features were extracted from a single EEG recording. The features extracted were based on statistical parameters extracted from both temporal and spectral domains of the EEG signal. The total number of recordings used was 29 EEG recordings acquired from newborns (14 preterm infants and 15 fullterm). The classification was done based on deep autoencoder neural networks. The structure of the network used was two autoencoder layers and one softnet output layer. The performance of the proposed system was evaluated for both preterm and fullterm records assembled in one group of data using 10 fold cross validation. Also, the performance was tested for the two groups separately. The reported accuracy was 0.804 for the entire data sets.", 
New convolutional neural network (CNN) architecture developed for place recognition task. Architecture is trainable in an end-to-end manner directly for the place recognition Task. Architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks.,NetVLAD CNN architecture for weakly supervised place recognition,"We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, We develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. the main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the “Vector of Locally Aggregated Descriptors” image representation commonly used in image retrieval. the layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, We develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View time Machine. Finally, We show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.",  
"We propose a novel deep network structure called ""Network In Network""(NIN) to enhance model discriminability for local patches within the receptive field. NIN can be implemented by stacking multiple of the above described structure. ",Network In Network,"We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking multiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", 
The development of light detection and ranging inaugurated a new era in autonomous driving. Autonomous vehicles are prone to making erroneous decisions and causing serious disasters. Networking and communication technologies can greatly make up for sensor deficiencies.,Networking and Communications in Autonomous Driving A Survey,"The development of light detection and ranging, Radar, camera, and other advanced sensor technologies inaugurated a new era in autonomous driving. However, due to the intrinsic limitations of these sensors, autonomous vehicles are prone to making erroneous decisions and causing serious disasters. At this point, networking and communication technologies can greatly make up for sensor deficiencies, and are more reliable, feasible and efficient to promote the information interaction, thereby improving autonomous vehicle’s perception and planning capabilities as well as realizing better vehicle control. This paper surveys the networking and communication technologies in autonomous driving from two aspects: intra- and inter-vehicle. The intra-vehicle network as the basis of realizing autonomous driving connects the on-board electronic parts. The inter-vehicle network is the medium for interaction between vehicles and outside information. In addition, we present the new trends of communication technologies in autonomous driving, as well as investigate the current mainstream verification methods and emphasize the challenges and open issues of networking and communications in autonomous driving.", 
"In this work, we study abstractive text summarization by exploring different models such as LSTM-encoder-decoder with attention, pointer-generator networks, coverage mechanisms, and transformers.",Neural Abstractive Text Summarization and Fake News Detection,"In this work, we study abstractive text summarization by exploring different models such as LSTM-encoder-decoder with attention, pointer-generator networks, coverage mechanisms, and transformers. Upon extensive and careful hyperparameter tuning we compare the proposed architectures against each other for the abstractive text summarization task. Finally, as an extension of our work, we apply our text summarization model as a feature extractor for a fake news detection task where the news articles prior to classification will be summarized and the results are compared against the classification using only the original news text.", 
"In the past few years, neural abstractive text summarization with sequence-to-sequence ( sequencing) models have gained a lot of popularity. Many interesting techniques have been proposed to improve these models. This paper provides a comprehensive literature survey on di?erent sequences2 sequencing models. We also develop an open source library, namely, Neural Abstractive Text Summarizer (NATS) toolkit.",Neural Abstractive Text Summarization with Sequence-to-Sequence Models,"In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve seq2seq models, making them capable of handling di?erent challenges, such as saliency, ?uency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques di?er in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efciency and parallelism for training a model. In this paper, we provide a comprehensive literature survey on di?erent seq2seq models for abstractive text summarization from the viewpoint of network structures, training strategies, and summary generation algorithms. Several models were frst proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Hence, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely, Neural Abstractive Text Summarizer (NATS) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used CNN/Daily Mail dataset to examine the e?ectiveness of several di?erent neural network components. Finally, we benchmark two models implemented in NATS on the two recently released datasets, namely, Newsroom and Bytecup.", 
This work is a discussion about our ongoing research on abstractive text summarization. We aim to investigate methods to infuse prior knowledge into deep neural networks. We believe that these approaches can obtain better performance than the state-of-the-art models.,Neural Abstractive Text Summarization,"Abstractive text summarization is a complex task whose goal is to generate a concise version of a text without necessarily reusing the sentences from the original source, but still preserving the meaning and the key contents. We address this issue by modeling the problem as a sequence to sequence learning and exploiting Recurrent Neural Networks (RNNs). This work is a discussion about our ongoing research on abstractive text summarization, where our aim is to investigate methods to infuse prior knowledge into deep neural networks. We believe that these approaches can obtain better performance than the state-of-the-art models for generating well-formed and meaningful summaries.", 
Neural codes provide a high-level descriptor of the visual content of the image. They perform competitively even when the convolutional neural network has been trained for an unrelated classification task. A simple PCA compression provides very good short codes that give state-of-the-art accuracy.,Neural Codes for Image Retrieval,"It has been shown that the activations invoked by an image within the top layers of a large convolutional neural network provide a high-level descriptor of the visual content of the image. In this paper, we investigate the use of such descriptors (neural codes) within the image retrieval application. In the experiments with several standard retrieval benchmarks, we establish that neural codes perform competitively even when the convolutional neural network has been trained for an unrelated classification task (e.g. Image-Net). We also evaluate the improvement in the retrieval performance of neural codes, when the network is retrained on a dataset of images that are similar to images encountered at test time. We further evaluate the performance of the compressed neural codes and show that a simple PCA compression provides very good short codes that give state-of-the-art accuracy on a number of datasets. In general, neural codes turn out to be much more resilient to such compression in comparison other state-of-the-art descriptors. Finally, we show that discriminative dimensionality reduction trained on a dataset of pairs of matched photographs improves the performance of PCA-compressed neural codes even further. Overall, our quantitative experiments demonstrate the promise of neural codes as visual descriptors for image retrieval.", 
"We show that discourse structure, as defined by Rhetorical Structure Theory, benefits text categorization. We use a neural network and a newly proposed attention mechanism.",Neural Discourse Structure for Text Categorization,"We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.", 
"Sentence-level extractive text summarization is substantially a node classification task of network mining. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly. In this paper, we propose HAHSum, which well models different levels of information, including words and sentences.",Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.", 
"Most extractive summarization methods focus on the main body of the document. But the gist of a document may lie in side information, such as the title and image captions. We propose to explore side information in the context of single-document extractive summation.",Neural Extractive Summarization with Side Information,"Most extractive summarization methods focus on the main body of the document from which sentences need to be extracted. However, the gist of the document may lie in side information, such as the title and image captions which are often available for newswire articles. We propose to explore side information in the context of single-document extractive summarization. We develop a framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor with attention over side information. We evaluate our model on a large scale news dataset. We show that extractive summarization with side information consistently outperforms its counterpart that does not use any side information, in terms of both informativeness and fluency.", 
"This work presents a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. ",Neural Extractive Text Summarization with Syntactic Compression,"Recent neural network approaches to summarization are largely either selection-based extraction or generation-based abstraction. In this work, we present a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. For learning, we construct oracle extractive-compressive summaries, then learn both of our components jointly with this supervision. Experimental results on the CNN/Daily Mail and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-the-shelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical.", 
Heart disease diagnosis has become a difficult task in the field of medicine. Neural Networks has emerged as an important method of classification. Multi-layer Perceptron Neural Network with Back-propagation has been employed as the training algorithm in this work.,Neural network based intelligent system for predicting heart disease,Heart disease diagnosis has become a difficult task in the field of medicine. This diagnosis depends on a thorough and accurate study of the patient’s clinical tests data on the health history of an individual. The tremendous improvement in the field of machine learning aim at developing intelligent automated systems which helps the medical practitioners in predicting as well as making decisions about the disease. Such an automated system for medical diagnosis would enhance timely medical care followed by proper subsequent treatment thereby resulting in significant life saving. Incorporating the techniques of classification in these intelligent systems achieve at accurate diagnosis. Neural Networks has emerged as an important method of classification. Multi-layer Perceptron Neural Network with Back-propagation has been employed as the training algorithm in this work. This paper proposes a diagnostic system for predicting heart disease. For diagnosis of heart disease 14 significant attributes are used in proposed system as per the medical literature. The results tabulated evidently prove that the designed diagnostic system is capable of predicting the risk level of heart disease effectively when compared to other approaches., 
We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. We train our models on large scale corpora containing hundreds of thousands of document- summary pairs.,Neural Summarization by Extracting Sentences and Words,Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation., 
"Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models. We highlight three primary shortcomings.","Neural Text Summarization, A Critical Evaluation","Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.", 
"In this paper, we approach this problem with a similar class of models, in a relatively smaller data setting.",Neural Text Summarization,"Generation based text summarization is a hard task and recent deep learning attempts show that sequence to sequence models hold promise. In this paper, we approach this problem with a similar class of models, in a relatively smaller data setting and attempt to alleviate the challenges faced with imitation learning.", 
A new multigraph-based text summarizer method is proposed to reduce the size of text data. The method produces a multi-edge-irregular-graph that represents words occurrence in the sentences of the target text. The proposed method is fast and can be implement for real time summarization.,New graph-based text summarization method,"The exponential growth of text data on the World Wide Web as well as on databases off line created a critical need for efficient text summarizers that significantly reduce its size while maintaining its integrity. In this paper, we present a new multigraph-based text summarizer method. This method is unique in that it produces a multi-edge-irregular-graph that represents words occurrence in the sentences of the target text. This graph is then converted into a symmetric matrix from which we can produce the ranking of sentences and hence obtain the summarized text using a threshold. To test our method performance, we compared our results with those from the most popular publicly available text summarization software using a corpus of 1000 samples from 6 different applications: health, literature, politics, religion, science and sports. The simulation results show that the proposed method produced better or comparable summaries in all cases. The proposed method is fast and can be implement for real time summarization.", 
"News categorization scheme proposed to filter out and categorize news headlines related to Pakistan Stock Exchange (PSX) using negligible manual effort. By using domain knowledge, category names are selected manually. Natural Language Processing (NLP) based technique is used to extract context of seed keyword.",News Headlines Categorization Scheme for Unlabelled Data,"Text categorization without training data is a difficult task and requires enough amount of hand labelled data to apply supervised methods, while manual labelling is a tedious job. In this paper a news categorization scheme is proposed to filter out and categorize news headlines related to Pakistan Stock Exchange (PSX) using negligible manual effort. By using domain knowledge, category names are selected manually then these category names are used as seed keyword to filter out news headlines. Natural Language Processing (NLP) based technique is used to extract context of seed keyword from initially filtered news headlines. These context terms are added in keyword list for string matching that further refines news filtration. Each news headline in a filtered news group is labelled and assigned a seed keyword term as a category label. Finally, a supervised classification technique is used to ensure the segregation of news categories as well as validates the performance of multiclass classification. Prepared dataset will be published in near future for potential uses explored by research community.", 
"News image captions contain more detailed information such as entity names and events. Detailed information is usually contained in news text but not in news images. The proposed model is trained on the DailyMail news image captioning corpora which are created by collecting images, caption, news texts.",News Image Captioning Based On Text Summarization Using Image As Query,"News image captioning aims to generate captions or descriptions for news images automatically, serving as draft captions for creating news image captions manually. News image captions contain more detailed information such as entity names and events than generic image captions do. Detailed information is usually contained in news text but not in news images. Generic image captioning does not make full use of the accompanying news text to generate image captions. This paper proposes a news image captioning method based on the attentional encoder-decoder model through summarizing the news text according to query image. The multi-modal attentional mechanism is proposed to compute the context vector. The proposed model is trained on the DailyMail news image captioning corpora which are created by collecting images, caption, news texts through parsing the html-formatted documents. Experiments on the DailyMail test dataset show that the proposed method outperforms the generic image captioning and the text summarization method.", 
"Method allows users to view summaries of Web documents from small, mobile devices. Unlike previous approaches, ours does not require the documents to be in HTML. Currently, the method is used to summarize news articles sent to a Web mail account in plain text format.","News to go, hierarchical text summarization for mobile devices","We present an evaluation of a novel hierarchical text summarization method that allows users to view summaries of Web documents from small, mobile devices. Unlike previous approaches, ours does not require the documents to be in HTML since it infers a hierarchical structure automatically. Currently, the method is used to summarize news articles sent to a Web mail account in plain text format. Subjects used a Web-enabled mobile phone emulator to access the account’s inbox and view the summarized news articles. They then used the summaries to complete several information-seeking tasks, which involved answering factual questions about the stories. In comparing the hierarchical text summary setting to that in which subjects were given the full text articles, there was no significant difference in task accuracy or the time taken to complete the task. However, in the hierarchical summarization setting, the number of bytes transferred per user request is less than half that of the full text case. Finally, in comparing the new method to three other summarization methods, subjects achieved significantly better accuracy on the tasks when using hierarchical summaries.", 
"Traditionally, the most general mechanism of personal authentication was using alphanumeric passwords. Graphical passwords can be an alternative, but it is vulnerable to shoulder-surfing attacks. This paper introduces a personal authentication system using a machine learning technique with electroencephalography (EEG) signals.",Next-Generation Personal Authentication Scheme Based on EEG Signal and Deep Learning,"The personal authentication technique is an essential tool in this complex and modern digital information society. Traditionally, the most general mechanism of personal authentication was using alphanumeric passwords. However, passwords that are hard to guess or to break, are often hard to remember. There are demands for a technology capable of replacing the text-based password system. Graphical passwords can be an alternative, but it is vulnerable to shoulder-surfing attacks. This paper looks through a number of recently developed graphical password systems and introduces a personal authentication system using a machine learning technique with electroencephalography (EEG) signals as a new type of personal authentication system which is easier for a person to use and more difficult for others to steal than other preexisting authentication systems.", 
Long documents often hamper trivial work. Text summarization is a major research topic in Natural Language Processing. We present a novel technique for generating the summarization of domain specific text.,NLP based text summarization using semantic analysis,"Due to an exponential growth in the generation of textual data, the need for tools and mechanisms for automatic summarization of documents has become very critical. Text documents are vital to any organization's day-to-day working and as such, long documents often hamper trivial work. Therefore, an automatic summarizer is vital towards reducing human effort. Text summarization is an important activity in the analysis of a high volume text documents and is currently a major research topic in Natural Language Processing. It is the process of generation of the summary of input text by extracting the representative sentences from it. In this project, we present a novel technique for generating the summarization of domain specific text by using Semantic Analysis for text summarization, which is a subset of Natural Language Processing.", 
Web-page classification is much more difficult than pure-text classification. We propose to improve the performance by removing the noise through summarization techniques. We give empirical evidence that ideal Web-page summaries generated by human editors can indeed improve performance. ,Noise reduction through summarization for Web-page classification,"Due to a large variety of noisy information embedded in Web pages, Web-page classification is much more difficult than pure-text classification. In this paper, we propose to improve the Web-page classification performance by removing the noise through summarization techniques. We first give empirical evidence that ideal Web-page summaries generated by human editors can indeed improve the performance of Web-page classification algorithms. We then put forward a new Web-page summarization algorithm based on Web-page layout and evaluate it along with several other state-of-the-art text summarization algorithms on the LookSmart Web directory. Experimental results show that the classification algorithms (NB or SVM) augmented by any summarization approach can achieve an improvement by more than 5.0% as compared to pure-text-based classification algorithms. We further introduce an ensemble method to combine the different summarization algorithms. The ensemble summarization method achieves more than 12.0% improvement over pure-text based methods.", 
"In brain–computer interface (BCI) systems, classification is an important signal processing step. Recently, the sparse representation based classification (SRC) method has shown a robust classification performance in many pattern recognition fields including BCI. SRC method provided better classification accuracy and noise robustness compared with the SVM method.",Noise robustness analysis of sparse representation based classification method for non-stationary EEG signal classification,"In the electroencephalogram (EEG)-based brain–computer interface (BCI) systems, classification is an important signal processing step to control external devices using brain activity. However, scalp-recorded EEG signals have inherent non-stationary characteristics; thus, the classification performance is deteriorated by changing the background activity of the EEG during the BCI experiment. Recently, the sparse representation based classification (SRC) method has shown a robust classification performance in many pattern recognition fields including BCI. In this study, we aim to analyze noise robustness of the SRC method to evaluate the capability of the SRC for non-stationary EEG signal classification. For this purpose, we generate noisy test signals by adding a noise source such as random Gaussian and scalp-recorded background noise into the original motor imagery based EEG signals. Using the noisy test signals and real online-experimental dataset, we compare the classification performance of the SRC and support vector machine (SVM). Furthermore, we analyze the unique classification mechanism of the SRC. We observed that the SRC method provided better classification accuracy and noise robustness compared with the SVM method. In addition, the SRC has an inherent adaptive classification mechanism that makes it suitable for time-varying EEG signal classification for online BCI systems.", 
In this paper we apply self-knowledge distillation to text summarization. We argue this can alleviate problems with maximum-likelihood training on noisy datasets. We demonstrate experimentally on three benchmarks.,Noisy Self-Knowledge Distillation for Text Summarization,"In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.",  
"Until recently, linear time-invariant Gaussian modeling has dominated the development of time series modeling and feature extraction. In the case of EEG signals, where the underlying theory regarding the dynamical law governing the generation of these signals is not completely understood, a case can be made for using improved signal processing models.",Nonlinear Considerations in EEG Signal Classification,"In this paper, we investigate the effect of incorporating modeling of nonlinearity on the classification of electroencephalogram (EEG) signals using an artificial neural network (ANN). It is observed that the ANN’s predictive ability is improved after preprocessing EEG signals using a particular nonlinear modeling technique, viz. a bilinear model, compared with those obtained by using a particular classical linear analysis method, viz. an autoregressive (AR) model. Until recently, linear time-invariant Gaussian modeling has dominated the development of time series modeling and feature extraction. The advantage of such classical models lies in the fact that a complete signal processing theory is available. In the case of EEG signals, where the underlying theory regarding the dynamical law governing the generation of these signals (e.g., the underlying physiological factors) is not completely understood, a case can be made for using improved signal processing models that are not subject to linear constraints. Such models should recognize important features of the observed data that may not be well modeled by a linear time-invariant model. It is known that EEG signals are nonstationary, and it is possible that they may be nonlinear as well. Thus, one way of gaining further insights on the structure of EEG signals is to introduce nonlinear models and higher order spectra. This paper compares the results of classification using a linear AR model with those obtained from a bilinear model. It is shown that in certain cases, the nonlinearity of EEG signals is an important factor that ought to be taken into consideration during preprocessing of the signals prior to the classification task.", 
Nonnegative matrix factorization ( NMF) is a powerful feature extraction method for nonnegative data. The basic idea is to decompose the magnitude spectra of EEG signals from six channels via NMF.,Nonnegative Matrix Factorization for EEG Signal Classification,"Nonnegative matrix factorization (NMF) is a powerful feature extraction method for nonnegative data. This paper applies NMF to feature extraction for Electroencephalogram (EEG) signal classification. The basic idea is to decompose the magnitude spectra of EEG signals from six channels via NMF. Primary experiments on signals from one subject performing two tasks show high classification accuracy rate based on linear discriminant analysis. Our best results are close to 98% when training data and testing data from the same day, and 82% when training data and testing data from different days.", 
"Text summarization and categorization are some of the most demanding information retrieval tasks. In this article, we explore the effects that part of speech tagging has on the summarization procedure of an existing system. We are thus enhancing the personalization algorithm that the system utilizes with various features derived from the user's profile.",Noun retrieval effect on text summarization and delivery of personalized news articles to the user's desktop,"Text summarization and categorization, as well as personalization of the results, have always been some of the most demanding information retrieval tasks. Deploying a generalized, multi-functional mechanism that produces good results for the aforementioned tasks seems to be a panacea for most of the text-based, information retrieval needs. In this article, we present the keyword extraction techniques, exploring the effects that part of speech tagging has on the summarization procedure of an existing system. Moreover, we describe the profiling features that are used as an extension to an already constructed news indexing system, PeRSSonal. We are thus enhancing the personalization algorithm that the system utilizes with various features derived from the user’s profile, such as the list of viewed articles and the time spent on them. In addition, we analyze the system’s interconnection channels that are used with the client-side desktop application that was developed and we evaluate the approaches that we propose.", 
"Machine learning (ML) and data mining (DM) techniques have a vital role in healthcare systems. It is proven that a chance of 12% error remains in the diagnosis of the diseases by the medical practitioners. The highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research.",Novel Feature Reduction (NFR) Model With Machine Learning and Data Mining Algorithms for Effective Disease Risk Prediction,"Presently, the application of machine learning (ML) and data mining (DM) techniques have a vital role in healthcare systems and wisely convert all obtainable data into beneficial knowledge. It is proven from the literature works that a chance of 12% error remains in the diagnosis of the diseases by the medical practitioners. Moreover, for effective disease risk prediction in medical analysis, more emphasis is accorded to the area under the curve (AUC) with accuracy as an evaluation metric. However, the role of the AUC has not been previously characterized notably. In this research article, a novel feature reduction (NFR) model that is aligned with the ML and DM algorithms is proposed to reduce the error rate and further improve the performance. The proposed NFR model comprises of two approaches and uses the AUC in addition to the accuracy to achieve a robust and effective disease risk prediction. The first approach is based on a heuristic process evaluating performance by reducing features with respect to the improvement in the AUC besides the accuracy as evaluation metrics, working to obtain the best subset of highly contributing features in the prediction. The second approach evaluates the accuracy and AUC of all individual features and forms the subsets with the highest accuracies, AUCs, and least difference between them, which are combined in various combinations to achieve the best-reduced set of highly relevant features. For this purpose, the benchmarked public heart datasets of the ML repository of the University of California, Irvine (UCI) are tested; the results are promising. The highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research with a significant improvement of 25% in the performance of the running time of the algorithm.", 
Incorporating vehicle-to-everything (V2X) and D2D communication in a vehicular environment has become a research focus area due to the proliferation of fifth-generation (5G) technology. This paper presents a design for a novel cellular-5G VANET architecture intended to address all prior problems with vehicular ad hoc networks. All vehicles in the field in focus are divided into clusters using an adaptive mobility aware path similarity (A-MAPS) algorithm.,Novel path similarity aware clustering and safety message dissemination via mobile gateway selection in cellular 5G-based V2X and D2D communication for urban environment,"Incorporating vehicle-to-everything (V2X) and device-to-device (D2D) communication in a vehicular environment has become a research focus area due to the proliferation of fifth-generation (5G) technology. However, establishing V2X and D2D communication remains a challenging issue in urban scenarios because, by the nature of their role and usage, vehicles are subject to dynamic changes in their mobility and direction, as well as being affected by traffic congestion. Moreover, safety message dissemination over vehicular ad hoc networks (VANETs) without the introduction of a broadcast storm poses a considerable problem. This paper presents a design for a novel cellular-5G VANET architecture intended to address all prior problems in VANETs and improve quality of service (QoS). To this end, all vehicles in the field in focus are divided into clusters using an adaptive mobility aware path similarity (A-MAPS) algorithm. Cluster formation and cluster head (CH) selection are performed on the basis of a number of pivotal criteria, including future path similarity, which is vital for urban environments. To establish reliable vehicleto-vehicle (V2V) and D2D communication, a Bayesian rule-based fuzzy logic (BRFL) algorithm is introduced that determines the optimal forwarder for V2V and the optimal device for D2D communication. Furthermore, vehicle-to-infrastructure (V2I) and vehicle-to-pedestrian (V2P) communication are handled with a novel two-fitness hypotrochoid spiral optimization (2F-HSO) algorithm that is derived using fitness functions. For safety message dissemination, two different message types (Accident and Traffic) are considered, and a safety aware hierarchical tree for dissemination (SA-HTD) is constructed to handle their dissemination. The proposed cellular-5G VANET is modeled in an OMNeT++ simulator. Results indicate improvement in packet delivery ratio (PDR), throughput, transmission delay, and dissemination delay.", 
"Summarization deals with extracting summaries from immense chunks of texts. This analysis relies on the summarization method that uses NLTK, Gensim, Spacy. The analysis provides a comprehensive guide to sensitivity analysis of model parameters.",NOVEL TECHNIQUES USED FOR AUTOMATIC TEXT SUMMARIZATION,"This Paper describes about Natural language processing (NLP) that is absolute to have a large impact on human lives. Generally, Text summarization is split into 2 classes, Extractive summarization and theoretical abstractive. Summarization deals with extracting summaries from immense chunks of texts. There's lots of text material accessible on the web. Thus there's a haul of looking out relevant documents from the amount of document accessible and extracting relevant info from the web. To stop this downside, Text summarization is that the method of distinguishing the foremost necessary purposeful info from the document or set of connected documents and compression them into a shorter version conserving it's overall that means. This analysis relies on the summarization method that uses NLTK, Gensim, Spacy. The analysis provides a comprehensive guide to sensitivity analysis of model parameters concerning Gensim, Spacy, NLTK. Text summarization technique is one amongst the applications of natural summarization and NLTK summarization with an analysis of graphical user interface based application result.", 
In this paper we have given a novel statistical approach to summarize the given Odia text. We rank each sentence in the document by assigning a weight value to each word of the sentence.,Odia Text Summarization using Stemmer,"Lot of work has already been done for automatic text summarization. In this paper we have given a novel statistical approach to summarize the given Odia text. In our approach extraction of relevant sentences is done which can give the actual concept of the input document in a concise form. We rank each sentence in the document by assigning a weight value to each word of the sentence. The sentences are extracted as per their rank, which will lead to a good summary of the given text.", 
"Cities are increasingly turning towards specialized technologies to address issues related to society, ecology, morphology and many others. This paper reviews the urban potential of AI and proposes a new framework binding AI technology and cities. It aims to increase the livability of the urban fabric while boosting economic growth and opportunities.","On big data, artificial intelligence and smart cities","Cities are increasingly turning towards specialized technologies to address issues related to society, ecology, morphology and many others. The emerging concept of Smart Cities highly encourages this prospect by promoting the incorporation of sensors and Big Data through the Internet of Things (IoT). This surge of data brings new possibilities in the design and management of cities just as much as economic prospects. While Big Data processing through Artificial Intelligence (AI) can greatly contribute to the urban fabric, sustainability and livability dimensions however must not be overlooked in favour of technological ones. This paper reviews the urban potential of AI and proposes a new framework binding AI technology and cities while ensuring the integration of key dimensions of Culture, Metabolism and Governance; which are known to be primordial in the successful integration of Smart Cities for the compliance to the Sustainable Development Goal 11 and the New Urban Agenda. This paper is aimed towards Policy Makers, Data Scientists and Engineers who are looking at enhancing the integration of Artificial Intelligence and Big Data in Smart Cities with an aim to increase the livability of the urban fabric while boosting economic growth and opportunities.", 
"Heart disease is the leading cause of death. Half of all heart attacks and strokes occur in people who have not been flagged as 'at risk' The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.",On Deep Neural Networks for Detecting Heart Disease,"Heart disease is the leading cause of death, and experts estimate that approximately half of all heart attacks and strokes occur in people who have not been flagged as ’at risk.’ Thus, there is an urgent need to improve the accuracy of heart disease diagnosis. To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. Our main contribution is the design, evaluation, and optimization of DNN architectures of increasing depth for heart disease diagnosis. This work led to the discovery of a novel five layer DNN architecture – named Heart Evaluation for Algorithmic Risk-reduction and Optimization Five (HEARO-5) – that yields best prediction accuracy. HEARO-5’s design employs regularization optimization and automatically deals with missing data and/or data outliers. To evaluate and tune the architectures we use kway cross-validation as well as Matthews correlation coefficient (MCC) to measure the quality of our classifications. The study is performed on the publicly available Cleveland dataset of medical information, and we are making our developments open source, to further facilitate openness and research on the use of DNNs in medicine. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.", 
Aims to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units. Findings suggest the possibility for future efforts towards more efficient systems.,On the Abstractiveness of Neural Document Summarization,"Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We attempted to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units. Upon the observation that many abstractive systems tend to be near-extractive in practice, we also implemented a pure copy system, which achieved comparable results as abstractive summarizers while being far more computationally efficient. These findings suggest the possibility for future efforts towards more efficient systems that could better utilize the vocabulary in the original document.", 
"Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text pre processing decisions on the performance of a standard neural text classifier.",On the Role of Text Preprocessing in Neural Network Architectures An Evaluation Study on Text Categorization and Sentiment Analysis,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.", 
During maintenance developers cannot read the entire code of large systems. They need a way to get a quick understanding of source code entities so they can efficiently identify and then focus on the ones related to their task at hand. We create such descriptions using techniques from automatic text summarization.,On the Use of Automated Text Summarization Techniques for Summarizing Source Code,"During maintenance developers cannot read the entire code of large systems. They need a way to get a quick understanding of source code entities (such as, classes, methods, packages, etc.), so they can efficiently identify and then focus on the ones related to their task at hand. Sometimes reading just a method header or a class name does not tell enough about its purpose and meaning, while reading the entire implementation takes too long. We study a solution which mitigates the two approaches, i.e., short and accurate textual descriptions that illustrate the software entities without having to read the details of the implementation. We create such descriptions using techniques from automatic text summarization. The paper presents a study that investigates the suitability of various such techniques for generating source code summaries. The results indicate that a combination of text summarization techniques is most appropriate for source code summarization and that developers generally agree with the summaries produced.", 
"Properly categorized applications allow stakeholders to identify requirements and predict maintenance problems. Manual categorization is expensive, tedious, and laborious, so automatic categorization approaches are gaining widespread importance. For different legal and organizational reasons, the applications' source code is often not available, thus making it difficult to automatically categorize these applications.",On Using Machine Learning to Automatically Classify Software Applications into Domain Categories,"Software repositories hold applications that are often categorized to improve the effectiveness of various maintenance tasks. Properly categorized applications allow stakeholders to identify requirements related to their applications and predict maintenance problems in software projects. Manual categorization is expensive, tedious, and laborious – this is why automatic categorization approaches are gaining widespread importance. Unfortunately, for different legal and organizational reasons, the applications’ source code is often not available, thus making it difficult to automatically categorize these applications. In this paper, we propose a novel approach in which we use Application Programming Interface (API) calls from third-party libraries for automatic categorization of software applications that use these API calls. Our approach is general since it enables different categorization algorithms to be applied to repositories that contain both source code and bytecode of applications, since API calls can be extracted from both the source code and byte-code. We compare our approach to a state-of-the-art approach that uses machine learning algorithms for software categorization, and conduct experiments on two large Java repositories: an open-source repository containing 3,286 projects and a closed-source repository with 745 applications, where the source code was not available. Our contribution is twofold: we propose a new approach that makes it possible to categorize software projects without any source code using a small number of API calls as attributes, and furthermore we carried out a comprehensive empirical evaluation of automatic categorization approaches.", 
New way to parallelize the training of convolutional neural networks across multiple,One weird trick for parallelizing convolutional neural networks,I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks., 
"In the past decade, improvements in the production of in-expensive PC equipment and software has permitted more refined real-time signal processing in BCI systems. The goal of this work is to construct a system producing accuracy comparable to Roesler's K* classifier, Cameron et al. 's (RRF+K*) classifiers and at the same time providing enough speed to be used in an online BCI framework.",Online Eye State Recognition from EEG Data Using Deep architectures,"In the past decade, improvements in the production of in-expensive PC equipment and software has permitted more refined real-time signal processing in BCI systems. In the literature, Deep learning concepts have not been applied to EEG data analysis in a systematic manner. This paper applies various existing Deep learning architectures and algorithms for the classification of EEG data applied to eye state detection. The deep learning based classifier systems presented in this work are comparable to the state of the art classifiers devised by Roesler and Suenderman (2013), and Cameron et al.(2015). The goal of this work is to construct a system producing accuracies comparable to Roesler’s K* classifier, Cameron et al. ’s (RRF+K*) classifiers and at the same time providing enough speed to be used in an online BCI framework. In order to meet the constraints, following architectures were designed: A Multi layered neural network with ReLU and drop-out, deep belief networks based unsupervised learning, drop-out masks on deep neural networks. Specifically, we compare our results with K*, RRF, (K*+RRF), ada(RJ48F) classifiers. Also an in-depth analysis of binary class features has been done using t-SNE based visualizations while fitting elliptical contours to the features. Prior research suggests that instance-based/lazy learners like the K* algorithm are likely to be too slow to be used in a BCI framework, with ada(RJ48F) model performing decently well. But our chosen deep neural network architectures produce higher classification accuracies and have lower convergence times making them even faster within the time specifications of real-time classification and control applications.", 
OnSeS is a novel short text summarization method. It makes full use of word2vec to represent a word and utilizes neural network model to generate each word of the summary. The experimental results reveal that our proposed fully data-driven approach outperforms state-of-the-art method.,"OnSeS, A Novel Online Short Text Summarization Based on BM25 and Neural Network","The last decade has witnessed a dramatic growth of social networks, such as Twitter, Sina Microblog, etc. Messages/short texts on these platforms are generally of limited length, causing difficulties for machines to understand. Moreover, it is rarely possible for users to read and understand all the content due to the large quantity. So it is imperative to cluster and extract the viewpoints of these short texts. To solve this, the representation of a word is enriched with additional features from external, but it is demanding in terms of computational and time resources. In this paper, we proposed OnSeS, a novel short text summarization method which makes full use of word2vec to represent a word and utilizes neural network model to generate each word of the summary. OnSeS consists of three phrases: 1) clustering short texts using the ??-means algorithm; 2) ranking content of each cluster by building a graph-based ranking model using BM25; 3) generating main point of each cluster with the help of neural machine translation model on the top ranked sentence. The experimental results reveal that our proposed fully data-driven approach outperforms state-of-the-art method.", 
The medical domain suffers typically from the problem of information overload. It is essential for physicians and researchers in medicine and biology to have quick and efficient access to up-to-date information. The proposed system combines both document clustering and text summarization technique.,Ontology enhanced clustering based summarization of medical documents,"The growing amount of data, the short of structured information and the information diversity have made information and knowledge management a real challenge. Even though larger quantities of data are merely available, easier access to the required information at the right time and in the most appropriate form is still difficult. Particularly the medical domain suffers typically from the problem of information overload since it is essential for physicians and researchers in medicine and biology to have quick and efficient access to up-to-date information according to their interests and requirements. Methodologies are needed to support users whose knowledge of medical vocabularies is inadequate to find the desired information and for medical experts who search for information outside their field of expertise. In order to effectively utilize the vast amount of biomedical information and to provide a solution to information overload problem, the proposed system combines both document clustering and text summarization technique. In the proposed system the user query is revised by mapping query with synonyms and semantically related concepts using MeSH ontology knowledge source. Based on the revised query medical documents are retrieved from trustworthy online sources and those documents are clustered to generate cluster wise summary", 
ProMine has two main contributions; one is the semantic-based text mining approach for automatically identifying domain-specific knowledge elements. The other is the automatic categorization of these extracted knowledge elements by using Wiktionary.,Ontology Maintenance Through Semantic Text Mining An Application for IT Governance Domain,"Manual ontology population and enrichment is a complex task that require professional experience involving a lot of efforts. The authors’ paper deals with the challenges and possible solutions for semi-automatic ontology enrichment and population. ProMine has two main contributions; one is the semantic-based text mining approach for automatically identifying domain-specific knowledge elements; the other is the automatic categorization of these extracted knowledge elements by using Wiktionary. ProMine ontology enrichment solution was applied in IT audit domain of an e-learning system. After seven cycles of the application ProMine, the number of automatically identified new concepts are significantly increased and ProMine categorized new concepts with high precision and recall.", 
"Ontology summarization has been recognized as a very useful technique to facilitate Ontology understanding. A number of efforts have emerged lately that apply different criteria, addressing different features of ontology. There lacks consensus on a number of issues fundamental to the development of the field. We present an analysis of this technique and its approaches.","Ontology summarization, an analysis and an evaluation","Ontology summarization has been recognized as a very useful technique to facilitate ontology understanding and then support ontology reuse as a new or supplementing technique. A number of efforts have emerged lately that apply different criteria, addressing different features of ontology, to extract ontology summaries. However, those efforts are ad-hoc in that there lacks consensus on a number of issues fundamental to the development of the field, such as a definition for ontology summarization, use case scenarios etc. Also, there lack sufficient evaluations and analysis, e.g. comparison among them and with other similar techniques, to provide meaning guidelines for users of this technique. With the aim to provide solutions to those fundamental issues, in this work, we present an analysis of this technique and its approaches. With the help of an objective evaluation method, we investigate what features of ontology are important in ontology summarization.", 
"This paper analyzes the main literature in the field of text summarization systems. It presents the structure and features of Texminer, a software that facilitates summarization of texts on Port and Coastal Engineering. The authors were able to corroborate that the summaries obtained usingTexminer are more efficient than those derived through other systems.",Ontology-based text summarization. The case of Texminer,"The purpose of this paper is to look into the latest advances in ontology-based text summarization systems, with emphasis on the methodologies of a socio-cognitive approach, the structural discourse models and the ontology-based text summarization systems. The paper analyzes the main literature in this field and presents the structure and features of Texminer, a software that facilitates summarization of texts on Port and Coastal Engineering. Texminer entails a combination of several techniques, including: socio-cognitive user models, Natural Language Processing, disambiguation and ontologies. After processing a corpus, the system was evaluated using as a reference various clustering evaluation experiments conducted by Arco (2008) and Hennig et al. (2008). The results were checked with a support vector machine, Rouge metrics, the F-measure and calculation of precision and recall. The experiment illustrates the superiority of abstracts obtained through the assistance of ontology-based techniques. The authors were able to corroborate that the summaries obtained using Texminer are more efficient than those derived through other systems whose summarization models do not use ontologies to summarize texts. Thanks to ontologies, main sentences can be selected with a broad rhetorical structure, especially for a specific knowledge domain.", 
"To enter scientific evidence into a U.S. court, a tool must be reliable and relevant. To date, there have been few legal challenges to digital evidence. This paper examines the Dau",Open Source Digital Forensics Tools,"This paper addresses digital forensic analysis tools and their use in a legal setting. To enter scientific evidence into a United States court, a tool must be reliable and relevant. The reliability of evidence is tested by applying “Daubert” guidelines. To date, there have been few legal challenges to digital evidence, but as the field matures this will likely change. This paper examines the Daubert guidelines and shows that open source tools may more clearly and comprehensively meet the guidelines than closed source tools", 
The Web has become an excellent source for gathering consumer opinions. Techniques are now being developed to exploit these sources. We first discuss several aspects of the problem in the AI context.,Opinion Extraction and Summarization on the Web,"The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sources containing such opinions, e.g., product reviews, forums, discussion groups, and blogs. Techniques are now being developed to exploit these sources to help organizations and individuals to gain such important information easily and quickly. In this paper, we first discuss several aspects of the problem in the AI context, and then present some results of our existing work published in KDD-04 and WWW-05.", 
"Reviews contain user's opinion about product, event or topic. It is difficult for web users to read and understand contents from large number of reviews. Important and useful information can be extracted from reviews through opinion mining.",Opinion Mining and Summarization of Hotel Reviews,"Everyday many users purchases product, book travel tickets, buy goods and services through web. Users also share their views about product, hotel, news, topic etc on web in the form of reviews, blogs, comments etc. Many users read review information given on web to take decisions such as buying products, watching movie, going to restaurant etc. Reviews contain user’s opinion about product, event or topic. It is difficult for web users to read and understand contents from large number of reviews. Important and useful information can be extracted from reviews through opinion mining and summarization process. We presented machine learning and SentiWordNet based method for opinion mining from hotel reviews and sentence relevance score based method for opinion summarization of hotel reviews. We obtained about 87% of accuracy of hotel review classification as positive or negative review by machine learning method. The classified and summarized hotel review information helps web users to understand review contents easily in a short time.", 
Study proposes a novel multi-text summarization technique for identifying the top-k most informative sentences of hotel reviews. Researchers used content and sentiment similarities to determine the similarity of two sentences. The k-medoids clustering algorithm was used to partition sentences into k groups. The medoids from these groups were then selected as the final summarization results.,Opinion mining from online hotel reviews â€“ A text summarization approach,"Online travel forums and social networks have become the most popular platform for sharing travel information, with enormous numbers of reviews posted daily. Automatically generated hotel summaries could aid travelers in selecting hotels. This study proposes a novel multi-text summarization technique for identifying the top-k most informative sentences of hotel reviews. Previous studies on review summarization have primarily examined content analysis, which disregards critical factors like author credibility and conflicting opinions. We considered such factors and developed a new sentence importance metric. Both the content and sentiment similarities were used to determine the similarity of two sentences. To identify the top-k sentences, the k-medoids clustering algorithm was used to partition sentences into k groups. The medoids from these groups were then selected as the final summarization results. To evaluate the performance of the proposed method, we collected two sets of reviews for the two hotels posted on TripAdvisor.com. A total of 20 subjects were invited to review the text summarization results from the proposed approach and two conventional approaches for the two hotels. The results indicate that the proposed approach outperforms the other two, and most of the subjects believed that the proposed approach can provide more comprehensive hotel information", 
This paper employs an Opposition-Based Learning as ML approach for enhancing the initial population of the Differential Evolution algorithm in problem of text summarization. The objective of this proposed enhancement is to adjust the algorithm booting instead of relying on random numbers generations only. The study findings conclude that our proposed method outperformed a classical DE and other baseline methods in terms of F-measure.,Opposition Differential Evolution Based Method for Text Summarization,"The Evolutionary Algorithms (EAs) save sufficient data about problem features, search space, and population information during the runtime. Accordingly, the machine learning (ML) techniques were employed for examining these data to improve the EAs search performance compared with their classical versions. This paper employs an Opposition-Based Learning as ML approach for enhancing the initial population of the Differential Evolution algorithm in problem of text summarization. In addition, it investigates the use of the OBL technique in integer-based evolutionary populations. The objective of this proposed enhancement is to adjust the algorithm booting instead of relying on random numbers generations only. Basically, all methodology steps in this paper were presented by a previous study whereas the differences between both of them will be shown later. So, this paper tries to estimate the improvement size the OBL can achieve and compare the results with a traditional DE-based text summarization application and other baseline methods. The DUC2002 data set was assigned as a test bed and the ROUGE toolkit used to evaluate the methods performances. The experimental results showed that our proposed method assured the need for learning and improve the random-based EAs before proceed generating the solutions. The study findings conclude that our proposed method outperformed a classical DE and other baseline methods in terms of F-measure. OBL was broadly tested before in numerical test beds, in this paper it will be tested on text-based test bed news article of text summarization problem.", 
"OWCs refer to wireless communication technologies which utilize optical carriers in infrared, visible light, or ultraviolet bands of electromagnetic spectrum. An accurate and efficient channel models are crucial for the OWC link design. This paper first provides a brief history of OWCs. It also considers OWC channel scenarios and their utilization trade-off.",Optical Wireless Communication Channel Measurements and Models,"Optical wireless communications (OWCs) refer to wireless communication technologies which utilize optical carriers in infrared, visible light, or ultraviolet bands of electromagnetic spectrum. For the sake of an OWC link design and performance evaluation, a comprehensive understanding and an accurate prediction of link behavior are indispensable. Therefore, accurate and efficient channel models are crucial for the OWC link design. This paper first provides a brief history of OWCs. It also considers OWC channel scenarios and their utilization trade-off in terms of optical carrier, range, mobility, and power efficiency. Furthermore, the main optical channel characteristics that affect the OWC link performance are investigated. A comprehensive overview of the most important OWCs channel measurement campaigns and channel models, primarily for wireless infrared communications and visible light communications, are presented. OWCs channel models are further compared in terms of computation speed, complexity, and accuracy. The survey considers indoor, outdoor, underground, and underwater communication environments. Finally, future research directions in OWCs channel measurements and models are addressed.", 
"Recent advancements in human– computer interaction research have led to the possibility of emotional communication via brain–computer interface systems. In this study, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals. The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.",Optimal feature selection and Deep Learning Ensembles Method for emotion recognition from human brain EEG sensors,"Recent advancements in human– computer interaction research has led to the possibility of emotional communication via brain–computer interface systems for patients with neuropsychiatric disorders or disabilities. In this study, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that non-invasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this study, the scalp EEG data of 21 healthy subjects (12–14 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine (SVM), k-nearest neighbor (KNN), linear discriminant analysis (LDA), Naive Bayes, Random Forest, deep-learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method. Keywords: EEG pattern recognition, Hjorth parameter, EEG feature extraction, EEG emotion recognition", 
The goal of text summarization is to reduce the size of the text while preserving its important information and overall meaning. In this paper we tested all possible combinations of seven features and then reported the best one for particular document. We analyzed the results for all 10 documents taken from DUC 2002 dataset.,Optimal Features Set for Extractive Automatic Text Summarization,"The goal of text summarization is to reduce the size of the text while preserving its important information and overall meaning. With the availability of internet, data is growing leaps and bounds and it is practically impossible summarizing all this data manually. Automatic summarization can be classified as extractive and abstractive summarization. For abstractive summarization we need to understand the meaning of the text and then create a shorter version which best expresses the meaning, While in extractive summarization we select sentences from given data itself which contains maximum information and fuse those sentences to create an extractive summary. In this paper we tested all possible combinations of seven features and then reported the best one for particular document. We analyzed the results for all 10 documents taken from DUC 2002 dataset using ROUGE evaluation matrices", 
The proliferation of the Internet and huge amount of data it transfers means that automatic text summarization is becoming more important. In this paper we first analyze some state of the art methods to text,Optimizing machine learning approach based on fuzzy logic in text summarization,"With the proliferation of the Internet and the huge amount of data it transfers, automatic text summarization is becoming more important. In this paper we first analyze some state of the art methods to text summarization., We also try to analyze one of the previous text summarization methods, ""Machine learning Approach"", and eliminate its shortcomings .Finally we present an approach to the design of an automatic text summarizer that generates a summary using fuzzy logic to obtain better results compared to previous methods.", 
"This paper attempts to build a strong summarizer DivSelect+CNNLM by presenting new algorithms to optimize each of them. It proposes CNNLM, a novel neural network language model (NNLM) based on convolutional neural network (CNN) It models sentence redundancy by cosine similarity.",Optimizing sentence modeling and selection for document summarization,"Extractive document summarization aims to conclude given documents by extracting some salient sentences. Often, it faces two challenges: 1) how to model the information redundancy among candidate sentences; 2) how to select the most appropriate sentences. This paper attempts to build a strong summarizer DivSelect+CNNLM by presenting new algorithms to optimize each of them. Concretely, it proposes CNNLM, a novel neural network language model (NNLM) based on convolutional neural network (CNN), to project sentences into dense distributed representations, then models sentence redundancy by cosine similarity. Afterwards, it formulates the selection process as an optimization problem, constructing a diversified selection process (DivSelect) with the aim of selecting some sentences which have high prestige, meantime, are dis-similar with each other. Experimental results on DUC2002 and DUC2004 benchmark data sets demonstrate the effectiveness of our approach", 
In this paper we first analyze some state of the art methods for text summarization. We discuss what the main,Optimizing Text Summarization Based on Fuzzy Logic,In this paper we first analyze some state of the art methods for text summarization. We discuss what the main disadvantages of these methods are and then propose a new method using fuzzy logic. Comparisons of results show that our method beats most methods which use machine learning as their core., 
This paper is an attempt to find a new method for summarizing Persian texts based on features available in Persian language and the use of fuzzy logic. We compare the new approach with other methods to show how effective the new method will be.,Optimizing_Persian_Text_Summarization_Based_on_Fuzzy Logic Approach,"With the sudden increase of information in the world and especially on the internet, text summarization has received great attention these days. This paper is an attempt to find a new method for summarizing Persian texts based on features available in Persian language and the use of fuzzy logic. We compare the new approach with other methods to show how effective the new method will be. In this paper we first analyze some state of the art methods to text summarization., We also try to analyze one of the previous text summarization methods , ""Machine learning Approach"", and eliminate its shortcomings .Finally we present an approach to the design of an automatic text summarizer that generates a summary using fuzzy logic to obtain better results compared to previous methods.", 
"Abundant data helps to train a robust recognition system, while a good object classifier can help to collect a large amount of images. The goal of this work is to use the tremendous resources of the web to learn robust object category models for detecting and searching for objects in real-world cluttered scenes. Our algorithm is capable of automatically collecting much larger object category datasets for 22 randomly selected classes from the Caltech 101 dataset.",OPTIMOL Automatic Online Picture Collection via Incremental Model Learning,"The explosion of the Internet provides us with a tremendous resource of images shared online. It also confronts vision researchers the problem of finding effective methods to navigate the vast amount of visual information. Semantic image understanding plays a vital role towards solving this problem. One important task in image understanding is object recognition, in particular, generic object categorization. Critical to this problem are the issues of learning and dataset. Abundant data helps to train a robust recognition system, while a good object classifier can help to collect a large amount of images. This paper presents a novel object recognition algorithm that performs automatic dataset collecting and incremental model learning simultaneously. The goal of this work is to use the tremendous resources of the web to learn robust object category models for detecting and searching for objects in real-world cluttered scenes. Humans contiguously update the knowledge of objects when new examples are observed. Our framework emulates this human learning process by iteratively accumulating model knowledge and image examples. We adapt a non-parametric latent topic model and propose an incremental learning framework. Our algorithm is capable of automatically collecting much larger object category datasets for 22 randomly selected classes from the Caltech 101 dataset. Furthermore, our system offers not only more images in L.-J. Li () · L. Fei-Fei Dept. of Computer Science, Princeton University, Princeton, USA e-mail: jial@cs.princeton.edu L. Fei-Fei e-mail: feifeili@cs.princeton.edu L. Fei-Fei Dept. of Computer Science, Stanford University, Stanford, USA e-mail: feifeili@cs.stanford.edu each object category but also a robust object category model and meaningful image annotation. Our experiments show that OPTIMOL is capable of collecting image datasets that are superior to the well known manually collected object datasets Caltech 101 and LabelMe", 
"This paper presents use of multiple features to classify various objects of the outdoor natural scene image. Proposed system aims to classify images of the sky, water and green land. All these features are used together to train probabilistic neural network for classification.",Outdoor Natural Scene Object Classification Using Probabilistic Neural Network,"Region labeling for outdoor scenes to identify sky, green land, water, snow etc. facilitates content-based image retrieval systems. This paper presents use of multiple features to classify various objects of the outdoor natural scene image. Proposed system aims to classify images of the sky, water and green land. As all these nature components are irregular in shape, they can be classified using color and texture features. Color features of the object are extracted by using segmentation in La*b* color space. In the process of texture feature calculation, the image is initially divided into smaller grids. Global GLCM based statistical texture features are calculated using statistical features of these local grids. Results show that color and statistical texture features are not sufficient to differentiate sky and water body. To achieve discrimination between these two objects, a new edge-based horizontal line texture feature is proposed. The proposed feature is used to differentiate between sky and water objects based on the density of horizontal lines. All these features are used together to train probabilistic neural network for classification. The system has achieved improvement of 5% to 8% in F-measure, when all these features are used together for classification of natural scene objects", 
"Software-defined vehicular networks (SDVNs) can help fulfill the performance and management requirements for VANETs. The distinctive features of SDN, such as its flexibility and programmability, can help with VANets' performance.",Overcoming the Key Challenges to Establishing Vehicular Communication Is SDN the Answer,"Considerable development in software-based configurable hardware has paved the way for a new networking paradigm called software-defined vehicular networks (SDVNs). The distinctive features of SDN, such as its flexibility and programmability, can help fulfill the performance and management requirements for VANETs. Although several studies exist on VANET and SDN, a tutorial on SDVNs is still lacking. In this article, we initially investigate recent premier research advances in the SDVN paradigm. Then we categorize and classify SDVN concepts and establish a taxonomy based on important characteristics, such as services, access technologies, network architectural components, opportunities, operational modes, and system components. Furthermore, we identify and outline the key requirements for SDVNs. Finally, we enumerate and outline future research challenges.", 
This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We introduce a novel deep learning approach to localization by learning to predict object boundaries.,"OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks","We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", 
This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We introduce a novel deep learning approach to localization by learning to predict object boundaries.,"OverFeat-Integrated Recognition, Localization and Detection using Convolutional Networks","We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", 
"PageRank is based on a relationship between search results and the structure of web links. The PageRank vector needs to be calculated, that implies calculations for a stationary distribution, stochastic matrix. Even though it is a simple formula, PageRank runs a successful business.",PageRank Algorithm,"The way in which the displaying of the web pages is done within a search is not a mystery. It involves applied math and good computer science knowledge for the right implementation. This relation involves vectors, matrixes and other mathematical notations. The PageRank vector needs to be calculated, that implies calculations for a stationary distribution, stochastic matrix. The matrices hold the link structure and the guidance of the web surfer. As links are added every day, and the number of websites goes beyond billions, the modification of the web link’s structure in the web affects the PageRank. In order to make this work, search algorithms need improvements. Problems and misbehaviors may come into place, but this topic pays attention to many researches which do improvements day by day. Even though it is a simple formula, PageRank runs a successful business. PageRank may be considered as the right example where applied math and computer knowledge can be fitted together", 
Multi-document text summarization is an important task in many fields of knowledge. This work focuses on the parallelization of the Multi-Objective Artificial Bee Colony (MOABC) algorithm. An efficiency of 86.72% has been reported for 64 threads.,Parallelizing a multi-objective optimization approach for extractive multi-document text summarization,"Currently, automatic multi-document text summarization is an important task in many fields of knowledge, due to the continuous exponential growth of information on the Internet. Nevertheless, this task is computationally demanding. In the last years, automatic text summarization has been addressed by using multi-objective optimization approaches. In particular, recently, the Multi-Objective Artificial Bee Colony (MOABC) algorithm has obtained very good results. This work focuses on the parallelization of this approach. Several steps have been carried out for this goal. After a time profiling of the algorithm, a runtime comparison has been performed between the use of different random number generators within the algorithm. Then, a parallel implementation of the MOABC algorithm has been designed following its original scheme, in which the main steps are parallelized, and different parallel schedules have been studied and compared. Finally, a second design based on the asynchronous behavior of the bee colony in nature has been implemented and compared. Experiments have been carried out with datasets from Document Understanding Conference (DUC). The results show that the asynchronous design improves greatly the parallel design, being more than 55 times faster with 64 threads than the standard design. An efficiency of 86.72% has been reported for 64 threads.", 
The method proposed utilizes computational geometry algorithms (convex polygon intersections) to classify unknown EEGs. The correct classification scores obtained on real EEG data experiments (91% in the worst case) are promising.,PARAMETRIC PERSON IDENTIFICATION FROM THE EEG USING COMPUTATIONAL GEOMETRY,"Person identification based on features extracted parametrically from the EEG spectrum is investigated in this work. The method proposed utilizes computational geometry algorithms (convex polygon intersections), appropriately modified, in order to classify unknown EEGs. The signal processing step includes EEG spectral analysis for feature extraction, by fitting a linear model of the AR type on the alpha rhythm EEG signal. The correct classification scores obtained on real EEG data experiments (91% in the worst case) are promising in that they corroborate existing evidence that EEG carries genetically specific information and is therefore appropriate as a basis for person identification methods", 
Smart parking systems use services of third-party parking recommender system to provide recommendations of personalized parking spots. Indiscriminate sharing of users' data with an untrusted system may breach privacy. Users' behavior and mobility patterns could be inferred by analyzing their past history. We present two solutions that preserve privacy of users by using k-anonymity and differential privacy techniques.,Parking recommender system privacy preservation through anonymization and differential privacy,"Recent advancements in the Internet of Things (IoT) have enabled the development of smart parking systems that use services of third-party parking recommender system to provide recommendations of personalized parking spot to users based on their past experience. However, the indiscriminate sharing of users’ data with an untrusted (or semitrusted) parking recommender system may breach the privacy because users’ behavior and mobility patterns could be inferred by analyzing their past history. Therefore, in this article, we present two solutions that preserve privacy of users in parking recommender systems while analyzing the past parking history using k-anonymity (anonymization) and differential privacy (perturbation) techniques. Specifically, given an original parking database containing users’ parking information, the k-anonymity mechanism constructs an anonymized database, while differential privacy perturbs the query response using the Laplace mechanism, making the users indistinguishable in both approaches, hence preserving the privacy. Experimental results on a data set constructed from real parking measurements evaluate the trade-off between privacy and utility, therefore enabling users to receive parking spots recommendations while preserving their privacy.", 
"Image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search. Yet such models are not compatible with geometry-aware re-ranking methods. This work revisits both retrieval stages, namely initial search and re- ranking.",PARTICULAR OBJECT RETRIEVAL WITH INTEGRAL MAX-POOLING OF CNN ACTIVATIONS,"Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image reranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.", 
"This article presents an extractive text summarization technique for single document using partition based clustering algorithms. The importance of each sentence in a document is attributed with three features namely, term score, keywords and average cosine similarity.",Partitioned-Based Clustering Approaches for Single Document Extractive Text Summarization,"This article presents an extractive text summarization technique for single document using partition based clustering algorithms. Clustering of sentences is performed where the importance of each sentence in a document is attributed with three features namely, term score, keywords and average cosine similarity. Two clustering techniques, namely, k-means and fuzzy C-means are considered. To generate the summary, sentences are selected using two similarity calculation methods and the results are obtained for different compression rates (20%–60%). The results are quite promising with respect to the references used for evaluation.", 
EEG recording is often used to verify the diagnosis of brain death in clinical practice. A deep CNN was trained to obtain the similarity degree of the patients' EEG signals with the clinical diagnosed symptoms. This method can evaluate the condition of the brain damage patients.,Patient's EEG Data Analysis via Spectrogram Image with a Convolution Neural Network,"Electroencephalogram (EEG) recording is relatively safe for the patients who are in deep coma or quasi brain death, so it is often used to verify the diagnosis of brain death in clinical practice. The objective of this paper is to apply deep learning method to EEG signal analysis in order to confirm clinical brain death diagnosis. A novel approach using spectrogram images produced from EEG signals as the input dataset of Convolution Neural Network (CNN) is proposed in this paper. A deep CNN was trained to obtain the similarity degree of the patients’ EEG signals with the clinical diagnosed symptoms. This method can evaluate the condition of the brain damage patients and can be a reliable reference of quasi brain death diagnosis", 
"Several clustering algorithms often share common traits, especially if they exhibit significant overlap in processing steps. We have integrated the signatures of these patterns in the DSL compilation for parallelism identification and automatic parallel code generation. Our system is able to achieve near-optimal speedup while requiring a fraction of the programming effort, making it an ideal choice for the data analytics community.",Pattern-based Automatic Parallelization of Representative-based Clustering Algorithms,"Ease of programming and optimal parallel performance have historically been on the opposite side of a tradeoff, forcing the user to choose. With the advent of the Big Data era and rapid evolution of sequential algorithms, the data analytics community can no longer afford the tradeoff. We observed that several clustering algorithms often share common traits – particularly, algorithms belonging to same class of clustering exhibit significant overlap in processing steps. Here, we present our observation on domain patterns in Representative-based clustering algorithms and how they manifest as clearly identifiable programming patterns when mapped to a Domain Specific Language (DSL). We have integrated the signatures of these patterns in the DSL compiler for parallelism identification and automatic parallel code generation. Our experiments on different state-of-the-art parallelization frameworks shows that our system is able to achieve near-optimal speedup while requiring a fraction of the programming effort, making it an ideal choice for the data analytics community.", 
"Deep learning methods have achieved great successes in pedestrian detection, owing to its ability to learn discriminative features from raw pixels. We propose a novel deep model to learn high-level features from multiple tasks and multiple data sources. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech [9] and ETH [10] datasets.",Pedestrian Detection aided by Deep Learning Semantic Tasks,"Deep learning methods have achieved great successes in pedestrian detection, owing to its ability to learn discriminative features from raw pixels. However, they treat pedestrian detection as a single binary classification task, which may confuse positive with hard negative samples (Fig.1 (a)). To address this ambiguity, this work jointly optimize pedestrian detection with semantic tasks, including pedestrian attributes (e.g. ‘carrying backpack’) and scene attributes (e.g. ‘vehicle’, ‘tree’, and ‘horizontal’). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task deep model is carefully designed to coordinate tasks and reduce discrepancies among datasets. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech [9] and ETH [10] datasets where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.", 
"This paper improves the network structure of YOLO algorithm. Three Passthrough layers were added to the original YOLo network. The improvement was tested on the INRIA pedestrian dataset. This method can effectively improve the detection accuracy of pedestrians, while reducing the false detection rate and the missed detection rate, and the detection speed can reach 25 frames per second.",Pedestrian Detection Based on YOLO Network Model,"After going through the deep network, there will be some loss of pedestrian information, which will cause the disappearance of gradients, causing inaccurate pedestrian detection.This paper improves the network structure of YOLO algorithm and proposes a new network structure YOLO-R.First, three Passthrough layers were added to the original YOLO network. The Passthrough layer consists of the Route layer and the Reorg layer. Its role is to connect the shallow layer pedestrian features to the deep layer pedestrian features and link the high and low resolution pedestrian features.The role of the Route layer is to pass the pedestrian characteristic information of the specified layer to the current layer, and then use the Reorg layer to reorganize the feature map so that the currently-introduced Route layer feature can be matched with the feature map of the next layer.The three Passthrough layers added in this algorithm can well transfer the network's shallow pedestrian fine-grained features to the deep network, enabling the network to better learn shallow pedestrian feature information.This paper also changes the layer number of the Passthrough layer connection in the original YOLO algorithm from Layer 16 to Layer 12 to increase the ability of the network to extract the information of the shallow pedestrian features.The improvement was tested on the INRIA pedestrian dataset. The experimental results show that this method can effectively improve the detection accuracy of pedestrians, while reducing the false detection rate and the missed detection rate,and the detection speed can reach 25 frames per second.", 
"Pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences. We evaluated our best model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills.","PEGASUS, Pre-training with Extracted Gap-sentences for Abstractive Summarization","Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.", 
"VANET is a special kind of MANET adapted to the communication between vehicles. It has the characteristics of moving at high speed, and is classified into V2I and V2V communication methods. The purpose of this paper is to implement a simulation of a Vehicluar Ad-Hoc network for urban environment.",Performance Analysis According to RSU Range of VANET-based,"VANET is a special kind of MANET adapted to the communication between vehicles. It has the characteristics of moving at high speed, and is classified into V2I and V2V communication methods. In order to develop VANET-based vehicle wireless communication technology, it is necessary to establish a test environment and synchronize vehicle mobility in various road scenarios. However, when the experimental environment is built, the equipment is expensive, and the road traffic situation is difficult to predict. The purpose of this paper is to implement a simulation of a Vehicluar Ad-Hoc network for urban environment that can be used for testing purposes. Such environment can be used to design better MAC protocols in VANET. In this paper, we propose a scheme for experiments in a similar environment to actual urban traffic, and analyze the performance by dividing it into V2I and V2V nodes according to the RSU coverage.", 
"Automatic text summarization is the task of deriving a meaningful and concise brief from a given text. One key aspect of text summarizing is accurate identification of keywords from the given textual content. In this paper, the performance of three popular algorithms were investigated by measuring their effectiveness in identifying",Performance analysis of keyword extraction algorithms assessing extractive text summarization,"Automatic text summarization is the task of deriving a meaningful and concise brief from a given text while retaining the concept and key information conveyed by the original text. So far, numerous approaches and algorithms have been devised to achieve this goal with certain accuracy and effectiveness. One key aspect of text summarization is accurate identification of keywords from the given textual content. In this paper, the relative performance of three popular algorithms, namely TextRank, LexRank and Latent Semantic Analysis for keyword extraction were investigated by measuring their effectiveness in identifying keywords from set of articles. The performance of each of these algorithms were contrasted with those of handwritten summaries of the same articles. The most effective algorithm was identified from the empirical results.", 
Vehicular Ad-hoc Networks (VANET) facilitate vehicles to share safety and non-safety information through messages. The main goal behind sharing this information is to enhance road safety and reduce road accidents by alerting the driver about the unexpected hazards. Routing of messages in VANET is challenging due to packet delays arising from high mobility of vehicles.,Performance Analysis of Message Dissemination Techniques in VANET using Fog Computing,"Vehicular Ad-hoc Networks (VANET) is a derived subclass of Mobile Ad-hoc Networks (MANET) with vehicles as mobile nodes. VANET facilitate vehicles to share safety and non-safety information through messages. Safety information includes road accidents, natural hazards, roadblocks, etc. Non-safety information includes tolling information, traveler information, etc. The main goal behind sharing this information is to enhance road safety and reduce road accidents by alerting the driver about the unexpected hazards. However, routing of messages in VANET is challenging due to packet delays arising from high mobility of vehicles, frequently changing topology and high density of vehicles, leading to frequent route breakages and packet losses. This report summarizes the performance analysis of safety and non-safety message dissemination techniques in VANET based on the fog computing technique. Three main metrics to improve the performance of message dissemination are: 1) delay, 2) probability of message delivery, and 3) throughput. Analysis of such metrics plays an important role to improve the performance of existing message dissemination techniques. Simulations are usually conducted based on the metrics using ns-2 and Java discrete event simulator. The above three performance metrics and results published in literature help one to understand and increase the performance of various message dissemination techniques in a VANET environment", 
"IEEE 802.11p protocol has defined the specific system design strategy for dedicated short range communication (DSRC) In this paper, a DSRC system based on IEEE 802. 11p has been built in simulation system with soft decision applied in its data demodulation process.",Performance Analysis of Soft Decision Applied Demodulation Process in Dedicated Short Range Communication Based on IEEE 802.11p Protocol,"The p group in IEEE 802.11 protocol suite solves the problem of wireless access in vehicular environment (WAVE), for the very IEEE 802.11p protocol has defined the specific system design strategy for dedicated short range communication (DSRC). Serving as a macro frame, IEEE 802.11p still requires technical definition in detail when it has been put into application. Consider that a high reliable data demodulation process can be important in providing better communication performance, and consider that soft decision technique has already been verified as a effective way in improving the quality of data demodulation, it is meaningful to discuss the technical design and the effectiveness in introducing soft decision into the data demodulation process in IEEE 802.11p based DSRC system. Related research on performance analysis of IEEE 802.11p focus mostly on various communication scenarios of WAVE, with data demodulation process being seldom considered. In this paper, a DSRC system based on IEEE 802.11p has been built in simulation system with soft decision applied in its data demodulation process, and the performance of such system has been analyzed in detail.", 
"In the recent years, the research community has shown interest in the development of brain–computer interface applications which assist physically challenged people to communicate with their brain electroencephalogram (EEG) signal. Representation of these EEG signals for mental task classification in terms of relevant features is important to achieve higher performance.",Performance enhancement of mental task classification using EEG signal-a study of multivariate feature selection methods,"In the recent years, the research community has shown interest in the development of brain–computer interface applications which assist physically challenged people to communicate with their brain electroencephalogram (EEG) signal. Representation of these EEG signals for mental task classification in terms of relevant features is important to achieve higher performance in terms of accuracy and computation time. For feature extraction from the EEG, empirical mode decomposition and wavelet transform are more appropriate as they are suitable for the analysis of non-linear and non-stationary time series signals. However, the size of the feature vector obtained from them is huge and may hinder the performance of mental task classification. To obtain a minimal set of relevant and non-redundant features for classification, six popular multivariate filter methods have been investigated which are based on different criteria: distance measure, causal effect and mutual information. Experimental results demonstrate that the classification accuracy improves while the computation time reduces considerably with the use of each of the six multivariate feature selection methods. Among all the combinations of feature extraction and selection methods that are investigated, the combination of wavelet transform and linear regression performs the best.", 
"Vehicular Ad hoc Networks (VANET) is one of the emerging mobile ad hoc networking paradigms (MANET) In VANETs, vehicle nodes communicate with each other using wireless links. The topology of the network changes rapidly and routing protocols are still a crucial issue.",Performance Evaluation of DYMO and OLSRv2 Routing,"Vehicular Ad hoc Networks (VANET) is one of the emerging mobile ad hoc networking paradigms (MANET), where the self-organizing and infrastructure-less nature of MANET structure. In VANETs, vehicle nodes communicate with each other using wireless links. However, the nodes are highly mobile and the topology of the network changes rapidly. Therefore, the design of routing protocols in VANETs is still a crucial issue. In this paper, we examine two of the most recent routing protocols that proposed for MANETs to select the optimal path between source-destination pairs, namely: Dynamic MANET On-demand (DYMO) and Optimized Link State Routing version 2 (OLSRv2) routing protocols. We evaluated and compared the performance of both protocols based on different parameters under various simulation scenarios. The result has shown that the DYMO protocol has higher through put and packet delivery ratio compare to the OLSRv2 protocol. However, the OLSRv2 protocols has better performance in the terms of average jitter and end to end delay compare to the DYMO protocol based on the paper’s scenario. OLSRv2 should be selected if the system concerns about time delay and jitter, otherwise it should be selected DYMO to achieve higher throughput and high packet delivery.", 
"This study provides an understanding of the SVM method for news categorization on Indonesian news dataset. Use of Information Gain as feature selection improve accuracy than without any feature selection. Our model give satisfying result with 98,057% accuracy of Indonesia news classification.",Performance Improvement Of Support Vector Machine (SVM) With Information Gain On Categorization Of Indonesian News Documents,"More news articles which are unstoppable increasing, causing problems with grouping news according to appropriate kind of label. Therefore it is necessary to deal with the problem of grouping news by it’s category like business news, political news, and sports news. The categorization of news document belong to text classification domain, a Machine Learning topic as an approach that addressed this problem. Various algorithms have been used in previous studies such as Bayesian techniques, k-Nearest Neighborhood, Neural Networks, and Support Vector Machine (SVM). This study provides an understanding of the SVM method for news categorization on Indonesian news dataset that contain several types of news category. Problems in text classification is the number of features that affecting classification performance with SVM. Use of Information Gain as feature selection improve accuracy than without any feature selection. Our model give satisfying result with 98,057 % accuracy of Indonesia news classification. Improvement 2,9 points from 95,11% by SVM technique without feature selection.", 
"Proposed method consists of three phases: training a supervised NER model, recognizing named entities of the text, and generating a summary. We compared our unsupervised method with the best supervised Farsi methods and we achieved an overall improvement of ROUGE-2 recall score of 10.2%.",Persian Automatic Text Summarization Based on Named Entity Recognition,"In this paper, we propose an unsupervised method for summarizing Farsi texts based on our neural named entity recognition (NER) system. This method consists of three phases: training a supervised NER model, recognizing named entities of the text, and generating a summary. The proposed method is an unsupervised extractive single-document summarization method. Although the proposed method is language independent, we focus on Farsi text summarization in this work. Firstly, we produce a word embedding based on Hamshahri2 corpus. Secondly, we train a neural network on Arman NER corpus. Then, the proposed algorithm ranks the sentences of the text based on the named entities in each sentence and produces the summary. Finally, the proposed method is evaluated on Pasokh single-document data set using the ROUGE evaluation measure. Without using any handcrafted features, our proposed method achieves state-of-the-art results. We compared our unsupervised method with the best supervised Farsi methods, and we achieved an overall improvement of ROUGE-2 recall score of 10.2%.", 
"This paper introduces Parsumist-a text summarization system for Persian documents. It exploits a combination of statistical, semantic and heuristic-improved methods. It can generate generic or topic/ query-driven extracts summaries for single-or multiple Persian papers.",Persian Document Summarization by Parsumist,"The rapid growth of online information services has created the problem of information explosion. Automatic text summarization techniques are essential for dealing with this problem. The process of compacting a source document to reduce its complexity and length while retaining its most important contents is called text summarization. This paper introduces Parsumist-a text summarization system for Persian documents. It exploits a combination of statistical, semantic and heuristic-improved methods. It can generate generic or topic/query-driven extracts summaries for single-or multiple Persian documents. In this paper, we first review the related work in this field, especially for Persian text summarization. We then present the architecture of Parsumist, its components and features. The last section evaluates the system and compares it to other systems that exist.", 
"In this paper, we present new method for Persian Text Summarization based on fractal theory. The main goal of this method is",Persian Text Summarization Using Fractal Theory,"The importance of text summarization grows rapidly as the amount of information increases exponentially. In this paper, we present new method for Persian Text Summarization based on fractal theory. The main goal of this method is using hierarchical structure of document and improves that for Persian language. The result shows that our method improves performance of extractive summarization.", 
Direct connection between ElectroEncephaloGram (EEG) and the genetic information of individuals has been investigated by neurophysiologists and psychiatrists since 1960's. In this work the full EO EEG signal of healthy individuals are estimated by an autoregressive (AR) model and AR parameters are extracted as features.,Person Identification by Using AR Model for EEG Signals,"A direct connection between ElectroEncephaloGram (EEG) and the genetic information of individuals has been investigated by neurophysiologists and psychiatrists since 1960’s; and it opens a new research area in the science. This paper focuses on the person identification based on feature extracted from the EEG which can show a direct connection between EEG and the genetic information of subjects. In this work the full EO EEG signal of healthy individuals are estimated by an autoregressive (AR) model and the AR parameters are extracted as features. Here for feature vector constitution, two methods have been proposed; in the first method the extracted parameters of each channel are used as a feature vector in the classification step which employs a competitive neural network and in the second method a combination of different channel parameters are used as a feature vector. Correct classification scores at the range of 80% to 100% reveal the potential of our approach for person classification/identification and are in agreement to the previous researches showing evidence that the EEG signal carries genetic information. The novelty of this work is in the combination of AR parameters and the network type (competitive network) that we have used. A comparison between the first and the second approach imply preference of the second one", 
"Information overloading is the main issue to impede this potential. Teachers manually convert content to suit the mobile learning require a huge effort. Automatic text summarization can reduce this cost significantly, but it may have negative impact on the understanding of the meaning conveyed. The proposed solution provides a proper and efficient approach to help mobile learners.","Personalized Text Content Summarizer for Mobile Learning, An Automatic Text Summarization System with Relevance Based Language Model","Although millions of text contents and multimedia published on the Web have potential to be shared as the learning contents for mobile learning, effectively extracting useful information from them is an extremely difficult problem. Oft-decried information overloading is the main issue to impede this potential. Many approaches have been proposed to revise and reinforce content to provide the appropriate delivery for mobile learning. However, approaches of manually converting content to suit the mobile learning require a huge effort on the part of the teachers and the instructional designers. Automatic text summarization can reduce this cost significantly, but it may have negative impact on the understanding of the meaning conveyed, as well as the risk of producing a standard summary for all learners without reflecting their interests and preferences. In this paper, a personalized text-based content summarizer is introduced to address an approach to help mobile learners to retrieve and process information more quickly, based on their interests and preferences. In this work, probabilistic language modeling techniques are adapted to build a user model and an extractive text summarization system to generate the personalized and automatic summary for mobile learning. Experimental results have indicated that the proposed solution provides a proper and efficient approach to help mobile learners by summarizing important content quickly and adaptively.", 
"We present a system for inserting new objects into existing photographs by querying a vast image-based object library. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions.",Photo Clip Art,"We present a system for inserting new objects into existing photographs by querying a vast image-based object library, precomputed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged.", 
Part-based One-vs-One Features (POOFs) are highly discriminative intermediate features. Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. POOFs are useful for fine-grained visual categorization.,"POOF Part-Based One-vs-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation","From a set of images in a particular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.", 
"Pedestrian path prediction is a challenging problem because scenes are often crowded or contain obstacles. In this article, we propose classifying pedestrian trajectories into a number of route classes. We name the prediction of pedestrian paths by LSTM (PoPPL) our algorithm. We have evaluated PoPPL against other state-of-the-art methods on two public data sets.",PoPPL Pedestrian Trajectory Prediction by LSTM With Automatic Route Class Clustering,"Pedestrian path prediction is a very challenging problem because scenes are often crowded or contain obstacles. Existing state-of-the-art long short-term memory (LSTM)-based prediction methods have been mainly focused on analyzing the influence of other people in the neighborhood of each pedestrian while neglecting the role of potential destinations in determining a walking path. In this article, we propose classifying pedestrian trajectories into a number of route classes (RCs) and using them to describe the pedestrian movement patterns. Based on the RCs obtained from trajectory clustering, our algorithm, which we name the prediction of pedestrian paths by LSTM (PoPPL), predicts the destination regions through a bidirectional LSTM classification network in the first stage and then generates trajectories corresponding to the predicted destination regions through one of the three proposed LSTM-based architectures in the second stage. Our algorithm also outputs probabilities of multiple predicted trajectories that head toward the destination regions. We have evaluated PoPPL against other state-of-the-art methods on two public data sets. The results show that our algorithm outperforms other methods and incorporating potential destination prediction improves the trajectory prediction accuracy", 
This paper addresses the issue of automatic summarization for Korean texts. It presents a novel keywordextraction-based Korean text summarization (KKTS) algorithm.,POS-Tagging Enhanced Korean Text Summarization,"Information explosion causes a serious scarcity of people’s time and a severe divergence of people’s attention. This paper addresses the issue of automatic summarization for Korean texts and presents a novel keywordextraction-based Korean text summarization (KKTS) algorithm. We investigate the enhancement of POS-tagging to the KKTS algorithm according to three kinds of text feature: noun words, predicate words, and all words. The experimental results show that our POS-tagging enhanced KKTS algorithm according to noun words can achieve the best performance in the Korean summarization task.", 
"Disease prediction systems have played an important role in people's life. In this paper, we propose an efficient and privacy-preserving disease prediction system. In PPDP, patients' historical medical data are encrypted and outsourced to the cloud server. The risk of diseases for new coming medical data can be computed based on the prediction models.",PPDP An efficient and privacy-preserving disease prediction scheme in cloud-based e-Healthcare system,"Disease prediction systems have played an important role in people’s life, since predicting the risk of diseases is essential for people to lead a healthy life. The recent proliferation of data mining techniques has given rise to disease prediction systems. Specifically, with the vast amount of medical data generated every day, Single-Layer Perceptron can be utilized to obtain valuable information to construct a disease prediction system. Although the disease prediction system is quite promising, many challenges may limit it in practical use, including information security and prediction efficiency. In this paper, we propose an efficient and privacy-preserving disease prediction system, called PPDP. In PPDP, patients’ historical medical data are encrypted and outsourced to the cloud server, which can be further utilized to train prediction models by using Single-Layer Perceptron learning algorithm in a privacy-preserving way. The risk of diseases for new coming medical data can be computed based on the prediction models. In particular, PPDP builds on new medical data encryption, disease learning and disease prediction algorithms that novelly utilize random matrices. Security analysis indicates that PPDP offers a required level of privacy protection. In addition, real experiments on different datasets show that computation costs of data encryption, disease learning and disease prediction are several magnitudes lower than existing disease prediction schemes", 
Automatic Text summarization is the process of reducing a text document to create a summary that relates only important points of the original document. It's very hard for human being to manually summarize large documents of text.,Pragmatic analysis based document summarization,Automatic Text summarization is the process of reducing a text document to create a summary that relates only important points of the original document. Now a day’s huge information available so there is interest in automatic Text summarization. It’s very hard for human being to manually summarize large documents of text. Hence we use Text Summarization techniques. Basically Text Summarization Techniques classified in two types 1. Abstraction 2. Extraction. In this Paper We Proposed Abstraction Type of Text Summarizations by using pragmatic analysis. This Summary being generated by Matlab and serially transmitted to PIC microcontroller and displayed on LCD., 
More than 77 000 cases of 2019-nCoV infection have been confirmed in China. It has interferred with ordinary medical practice of oral and maxillofacial surgery seriously. This paper suggests the necessary medical protective measures.,Precaution of 2019 novel coronavirus infection in department of oral and maxillofacial surgery,"The epidemic of the 2019 novel coronavirus (2019-nCoV) infection has presented as a critical period. Until February 23rd 2020, more than 77 000 cases of 2019-nCoV infection have been confirmed in China, which has a great impact on economy and society. It has also interferred with ordinary medical practice of oral and maxillofacial surgery seriously. In order to protect the oral and maxillofacial surgery medical staff from 2019-nCoV infection during the outbreak period, this paper suggests the necessary medical protective measures for oral and maxillofacial surgery outpatients and wards", 
"The prognosis of cardiovascular and cerebrovascular events for patients suffering from hypertension is considered of a high importance. This study paves the way towards utilizing machine learning models and heart rate variability. It provides a simple, yet effective, and continuous prediction approach when compared to other available techniques, it says. It also assists clinicians in decision making by providing a simple and effective prediction approach.",Predicting Hypertensive Patients With Higher Risk of Developing Vascular Events Using Heart Rate Variability and Machine Learning,"The prognosis of cardiovascular and cerebrovascular events for patients suffering from hypertension is considered of a high importance in preventing any further development of cardiac diseases. Despite of the ability of current gold standard techniques in predicting vascular events risks, they still lack the required clinical efficiency. In this vein, the study proposed herein provides an investigation on the feasibility of using heart rate variability (HRV) utilized through a machine learning approach to predict hypertensive patients at higher risk of developing vascular events. Initially, HRV features were extracted from all patient’s data using time-domain, frequency-domain, non-linear, and fragmentation metrics. The extraction of features was based on a 24-hour cycle analysis segmented into four time periods; namely late-night, early-morning, afternoon, and evening. Analysis of all features was performed using a one-way analysis of variance (ANOVA) test on period by period basis. Furthermore, the selection of best features was performed following a Chi-squared test for demographic and HRV features. Then, a model based on decision trees and random under-sampling boosting (RUSBOOST) was trained using demographic features, HRV features, and a combination of both features. The performance of the trained model achieved a maximum accuracy of 97.08% using the combined set of features during the afternoon time period. In addition, the precision and F1-score in predicting high risk patients reached 81.25% and 86.67%, respectively. The overall area under the curve for the model was at 0.98, suggesting a high performance in the sensitivity and specificity measures. This study paves the way towards utilizing machine learning models and heart rate variability for the prognosis of vascular events in hypertensive patient. Furthermore, it assists clinicians in decision making by providing a simple, yet effective, and continuous prediction approach when compared to other available techniques", 
Issue-tracking systems (e.g. JIRA) have increasingly been used in many software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether an issue is at risk of being delayed against its deadline. Risk factors with good discriminative power were selected to build predictive models.,Predicting the delay of issues with due dates in software projects,"Issue-tracking systems (e.g. JIRA) have increasingly been used in many software projects. An issue could represent a software bug, a new requirement or a user story, or even a project task. A deadline can be imposed on an issue by either explicitly assigning a due date to it, or implicitly assigning it to a release and having it inherit the release’s deadline. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether an issue is at risk of being delayed against its deadline. A set of features (hereafter called risk factors) characterizing delayed issues were extracted from eight open source projects: Apache, Duraspace, Java.net, JBoss, JIRA, Moodle, Mulesoft, and WSO2. Risk factors with good discriminative power were selected to build predictive models to predict if the resolution of an issue will be at risk of being delayed. Our predictive models are able to predict both the the extend of the delay and the likelihood of the delay occurrence. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 79 % precision, 61 % recall, 68 % F-measure, and 83 % Area Under the ROC Curve. Our predictive models also have low error rates: on average 0.66 for Macro-averaged Mean Cost-Error and 0.72 Macro-averaged Mean Absolute Error", 
This paper proposes an effective and robust architecture for heart failure prediction. We can obtain mass unstructured data from EHR time series to identify links between diagnostic events and ultimately predict when a patient will be diagnosed. The main contribution is to predict heart failure using a neural network.,Predicting the Risk of Heart Failure With EHR Sequential Data Modeling,"Electronic health records (EHRs) contain patient diagnostic records, physician records, and records of hospital departments. For heart failure, we can obtain mass unstructured data from EHR time series. By analyzing and mining these time-based EHRs, we can identify the links between diagnostic events and ultimately predict when a patient will be diagnosed. However, it is difficult to use the existing EHR data directly, because they are sparse and non-standardized. Thus, this paper proposes an effective and robust architecture for heart failure prediction. The main contribution of this paper is to predict heart failure using a neural network (i.e., to predict the possibility of cardiac illness based on patient’s electronic medical data). Specifically, we employed one-hot encoding and word vectors to model the diagnosis events and predicted heart failure events using the basic principles of a long short-term memory network model. Evaluations based on a real-world data set demonstrate the promising utility and efficacy of the proposed architecture in the prediction of the risk of heart failure.", 
"Ventricular fibrillation (VF) is a type of cardiac arrhythmia that results in heart quivering instead of normal pumping. To date, early cardiopulmonary resuscitation and defibrillation are the only effective VF treatment. In this study, we developed a new deep learning method to predict the onset of VF. The prediction results showed that the proposed two-dimensional short-time Fourier transform (2D STFT)/continuous wavelet transform (CWT) convolutional neural network (CNN) model can reach a recall of 99% and an accuracy of 97%.",Predicting Ventricular Fibrillation Through Deep Learning,"Ventricular fibrillation (VF) is a type of cardiac arrhythmia. This chaotic cardiac electrical activity results in heart quivering instead of normal pumping. To date, early cardiopulmonary resuscitation (CPR) and defibrillation are the only effective VF treatment. Acute myocardial infarction is the most common cause of VF, and cardiomyopathy, myocarditis, electrolyte imbalance, cardiotoxic medication, and even ion channel abnormality can cause VF. Physicians have attempted to identify specific patterns in electrocardiography (ECG) that might predict VF in the short term. For example, ST segment changes might imply coronary artery occlusion with myocardial ischemia, increasing VF risk. However, in most cases, VF occurs abruptly without any early warning. Machine learning is used to extract information usually neglected by the human brain. In deep learning, a cascade of multiple layers of processing is used to extract features. Machine learning is used to classify different types and outcomes of cardiac arrhythmias that are difficult to recognize directly. In this study, we developed a new deep learning method to predict the onset of VF. ECG from MIT-BIH databases were used as the training and validation data sets; the prediction results showed that the proposed two-dimensional short-time Fourier transform (2D STFT)/continuous wavelet transform (CWT) convolutional neural network (CNN) model can reach a recall of 99% and an accuracy of 97%. We also compared the proposed 2D model with 1D and 2D time-domain CNN models. The results showed that the 1D CNN and 2D time-domain models can achieve an accuracy of 60.5% and 56%, respectively.", 
Deep learning is a machine learning technique that learns its own features. It can be used to discover new knowledge from retinal fundus images. We predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images.,Predicting Cardiovascular Risk Factors from Retinal Fundus Photographs using Deep Learning,"Traditionally, medical discoveries are made by observing associations and then designing experiments to test these hypotheses. However, observing and quantifying associations in images can be difficult because of the wide variety of features, patterns, colors, values, shapes in real data. In this paper, we use deep learning, a machine learning technique that learns its own features, to discover new knowledge from retinal fundus images. Using models trained on data from 284,335 patients, and validated on two independent datasets of 12,026 and 999 patients, we predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as such as age (within 3.26 years), gender (0.97 AUC), smoking status (0.71 AUC), HbA1c (within 1.39%), systolic blood pressure (within 11.23mmHg) as well as major adverse cardiac events (0.70 AUC). We further show that our models used distinct aspects of the anatomy to generate each prediction, such as the optic disc or blood vessels, opening avenues of further research.", 
"New analytical approaches can help translate understanding of early functional connectivity into predictive models of neurodevelopmental outcome. One approach to achieving this goal is multivering pattern analysis, a machine-learning, pattern classification approach well-suited for high-dimensional neuroimaging data. In this study, we evaluated resting state-functional MRI data from 50 preterm-born infants (born at 23-29 weeks gestation and without moderate-severe brain injury)",Prediction of Brain Maturity in Infants Using Machine-Learning Algorithms,"Recent resting-state functional MRI investigations have demonstrated that much of the large-scale functional network architecture supporting motor, sensory and cognitive functions in older pediatric and adult populations is present in term- and prematurely-born infants. Application of new analytical approaches can help translate the improved understanding of early functional connectivity provided through these studies into predictive models of neurodevelopmental outcome. One approach to achieving this goal is multivariate pattern analysis, a machine-learning, pattern classification approach well-suited for high-dimensional neuroimaging data. It has previously been adapted to predict brain maturity in children and adolescents using structural and resting state-functional MRI data. In this study, we evaluated resting state-functional MRI data from 50 preterm-born infants (born at 23-29 weeks gestation and without moderate-severe brain injury) scanned at term equivalent postmenstrual age compared with data from 50 term-born control infants studied within the first week of life. Using 214 regions of interest, binary support vector machines distinguished term from preterm infants with 84% accuracy (p<0.0001). Inter- and intra-hemispheric connections throughout the brain were important for group categorization, indicating widespread changes in the brain’s functional network architecture associated with preterm birth are detectable by term equivalent age. Support vector regression enabled quantitative estimation of birth gestational age in single subjects using only term equivalent resting state-functional MRI data, indicating the present approach is sensitive to the degree of disruption of brain development associated with preterm birth (using gestational age as a surrogate for the extent of disruption). This suggests support vector regression may provide a means for predicting neurodevelopmental outcome in individual infants.", 
Cardiovascular disease is one of the major causes of mortality around the world. Prediction of cardiovascular disease is more important in the clinical survey analysis. Predictions can be achieved by selecting a correct combination of prediction models and features. Pre-processing will be done first considering the clinical data.,Prediction of Cardiovascular Disease using Machine Learning,"Machine learning is a technique converts the raw clinical data into an informational data that helps for decision making and prediction. Cardiovascular disease is one of the major causes of mortality around the world. It is considered in a large scale, so prediction of cardiovascular disease is more important in the clinical survey analysis as day by day it gets increased. The amount of data in the health club is huge. As cardiovascular is one of the major causes for death there are some data analytical techniques that predicts the occurrence of cardiovascular disease. It can be achieved through selecting a correct combination of prediction models and features. Prediction models were developed using different classification techniques based on feature selection and there are certain algorithms which provide varied and improved accuracy. Here prediction model is developed using Random Forest classification technique - Method for classification, regression by constructing a multitude of decision trees at training time. Developed by aggregating tree Avoids over fitting can deal with large number of features. Helps with feature selection based on importance where necessary features only classified. Pre-processing will be done first considering the clinical data. It will be spited into train and test data with which accuracy can be achieved.", 
"Coronavirus disease COVID-19 leads to severe pneumonia and it is estimated to create a high impact on the healthcare system. An urgent need for early diagnosis is required for precise treatment, which in turn reduces the pressure in the health care system. Some of the standard image diagnosis available is Computed Tomography (CT) scan and Chest X-Ray (CXR) In this research, the state-of-the-art techniques used is Genetic Deep Learning Convolutional Neural Network.",Prediction of COVID-19 Using Genetic Deep Learning Convolutional Neural Network (GDCNN),"Rapid spread of Coronavirus disease COVID-19 leads to severe pneumonia and it is estimated to create a high impact on the healthcare system. An urgent need for early diagnosis is required for precise treatment, which in turn reduces the pressure in the health care system. Some of the standard image diagnosis available is Computed Tomography (CT) scan and Chest X-Ray (CXR). Even though a CT scan is considered a gold standard in diagnosis, CXR is most widely used due to widespread, faster, and cheaper. This study aims to provide a solution for identifying pneumonia due to COVID-19 and healthy lungs (normal person) using CXR images. One of the remarkable methods used for extracting a high dimensional feature from medical images is the Deep learning method. In this research, the state-of-the-art techniques used is Genetic Deep Learning Convolutional Neural Network (GDCNN). It is trained from the scratch for extracting features for classifying them between COVID-19 and normal images. A dataset consisting of more than 5000 CXR image samples is used for classifying pneumonia, normal and other pneumonia diseases. Training a GDCNN from scratch proves that, the proposed method performs better compared to other transfer learning techniques. Classification accuracy of 98.84%, the precision of 93%, the sensitivity of 100%, and specificity of 97.0% in COVID-19 prediction is achieved. Top classification accuracy obtained in this research reveals the best nominal rate in the identification of COVID-19 disease prediction in an unbalanced environment. The novel model proposed for classification proves to be better than the existing models such as ReseNet18, ReseNet50, Squeezenet, DenseNet-121, and Visual Geometry Group (VGG16).", 
Deep learning (DL) solutions for prediction of driver's cognitive states (drowsy or alert) using EEG data. CCNN and CCNN-R outperform deep neural networks (DNN) as well as other non-DL algorithms. DL with raw EEG inputs achieves better performance than ICA features.,Prediction of Driver's Drowsy and Alert States From EEG Signals with Deep Learning,"We investigate in this paper deep learning (DL) solutions for prediction of driver’s cognitive states (drowsy or alert) using EEG data. We discussed the novel channel-wise convolutional neural network (CCNN) and CCNN-R which is a CCNN variation that uses Restricted Boltzmann Machine in order to replace the convolutional filter. We also consider bagging classifiers based on DL hidden units as an alternative to the conventional DL solutions. To test the performance of the proposed methods, a large EEG dataset from 3 studies of driver’s fatigue that includes 70 sessions from 37 subjects is assembled. All proposed methods are tested on both raw EEG and Independent Component Analysis (ICA)-transformed data for cross-session predictions. The results show that CCNN and CCNN-R outperform deep neural networks (DNN) and convolutional neural networks (CNN) as well as other non-DL algorithms and DL with raw EEG inputs achieves better performance than ICA features.", 
"Heart disease is increasing rapidly due to number of reasons. If we predict cardiac arrest in the early stages, it will be very helpful to cured this disease. In this paper, we enlighten the number of techniques in Artificial Neural Network (ANN)",PREDICTION OF HEART DISEASE USING ARTIFICIAL NEURAL NETWORK,"Heart disease is increasing rapidly due to number of reasons. If we predict cardiac arrest (dangerous conditions of heart) in the early stages, it will be very helpful to cured this disease. Although doctors and health centres collect data daily, but mostly are not using machine learning and pattern matching techniques to extract the knowledge that can be very useful in prediction. Bioinformatics is the real world application of machine learning to extract patterns from the datasets using several data mining techniques. In this research paper, data and attributes are taken from the UCI repository. Attribute extraction is very effective in mining information for the prediction. By utilizing this, various patterns can be derived to predict the heart disease earlier. In this paper, we enlighten the number of techniques in Artificial Neural Network (ANN). The accuracy is calculated and visualized such as ANN gives 94.7% but with Principle Component Analysis (PCA) accuracy rate improve to 97.7%.", 
"Heart disease accounts to be the leading cause of death worldwide. It is difficult for medical practitioners to predict the heart attack as it is a complex task. Data mining algorithms such as J48, Naïve Bayes",Prediction of Heart Disease using Classification Algorithms,"The heart disease accounts to be the leading cause of death worldwide. It is difficult for medical practitioners to predict the heart attack as it is a complex task that requires experience and knowledge. The health sector today contains hidden information that can be important in making decisions. Data mining algorithms such as J48, Naïve Bayes, REPTREE, CART, and Bayes Net are applied in this research for predicting heart attacks. The research result shows prediction accuracy of 99%. Data mining enable the health sector to predict patterns in the dataset.", 
"Heart diseases are currently a major cause of death in the world. This problem is severe in developing countries in Africa and Asia. A heart disease predicted at earlier stages not only helps the patients prevent it, but can also help the medical practitioners learn the major causes of a heart attack.",Prediction of Heart Disease Using Deep Convolutional Neural Networks,"Heart diseases are currently a major cause of death in the world. This problem is severe in developing countries in Africa and Asia. A heart disease predicted at earlier stages not only helps the patients prevent it, but I can also help the medical practitioners learn the major causes of a heart attack and avoid it before its actual occurrence in patient. In this paper, we propose a method named CardioHelp which predicts the probability of the presence of cardiovascular disease in a patient by incorporating a deep learning algorithm called convolutional neural networks (CNN). The proposed method is concerned with temporal data modeling by utilizing CNN for HF prediction at its earliest stage. We prepared the heart disease dataset and compared the results with state-of-the-art methods and achieved good results. Experimental results show that the proposed method outperforms the existing methods in terms of performance evaluation metrics. The achieved accuracy of the proposed method is 97%.", 
"Heart disease is mainly related to contraction or blocked blood vessels in the heart. Heart disease occurs not only in adults but also in children. In this paper, we propose and analyzed classification accuracy, precision and sensitivity by four tree based classification algorithms. We concluded that feature selection methods Pearson correlation and Lasso Regularization with random forest ensemble method provide better results 99% accuracy. We analyzed and find the random Forest ensemble method predicted better result compare to other algorithms in the previous year's works.",Prediction of Heart Disease Using Feature Selection and Random Forest Ensemble Method,"The heart is very soft and sensitive part of body by which brain handles blood related system in body. The heart disease that greatly affects in body as like: pulmonary artery, atalata, enzaina and birth defects included. Heart disease is mainly related to contraction or blocked blood vessels in the heart. The symptoms of heart disease depend on the type of disease. Heart disease occurs not only in adults but also in children. The infection affecting the tissues is known as percarditis. In this, the tissues closest to the heart are affected. Infections affecting the lining of the heart muscle are known as myocardium .The study of medical datasets is made very intuitive by machine learning algorithms. The machine learning algorithms provide techniques to identify dataset attributes and the relationship between them. In this research work, we used heart disease related information from UCI repository. The dataset contained 1025 Instances with 14 attributes, sick and nonstick patients in target variable. In this paper, we proposed and analyzed classification accuracy, precision and sensitivity by four tree based classification algorithms: M5P, random Tree and Reduced Error Pruning with Random forest ensemble method. All the prediction based algorithms have applied after the features selection of heart patient’s dataset. In this paper, we used three features based algorithms: Pearson Correlation, Recursive Features Elimination and Lasso Regularization. The data table analyzed by different feature selection methods for better prediction. All the analysis is done by three experimental setup; First experiment applied Pearson Correlation on M5P, random Tree, Reduced Error Pruning and Random forest ensemble method. In the second experiment we used Recursive Features Elimination and applied on above four tree based algorithms. In the third experiment we used Lasso Regularization and applied on as above tree based algorithms. After all the performance we analyzed and calculated classification accuracy, precision and sensitivity. With the results, we finally concluded that feature selection methods Pearson correlation and Lasso Regularization with random forest ensemble method provide better results 99% accuracy. We analyzed and find the random forest ensemble method predicted better result compare to other algorithms in the previous year’s works.", 
"There is an opulence of data available within the healthcare systems. However, there is a scarcity of useful analysis tool to find hidden relationships in data. Some experiment has been conducted to compare the execution of predictive data mining technique on the same dataset.",Prediction of Heart Disease Using Machine Learning Algorithms,"The successful experiment of data mining in highly visible fields like marketing, e-business, and retail has led to its application in other sectors and industries. Healthcare is being discovered among these areas. There is an opulence of data available within the healthcare systems. However, there is a scarcity of useful analysis tool to find hidden relationships in data. This research intends to provide a detailed description of Naïve Bayes and decision tree classifier that are applied in our research particularly in the prediction of Heart Disease. Some experiment has been conducted to compare the execution of predictive data mining technique on the same dataset, and the consequence reveals that Decision Tree outperforms over Bayesian classification.", 
"The machine learning algorithm neural networks has proven to be the most accurate and reliable algorithm and hence used in the proposed system. The algorithm can predict the vulnerability of a heart disease given basic symptoms like age, sex, pulse rate etc.",Prediction of Heart Disease Using Machine Learning,"with the rampant increase in the heart stroke rates at juvenile ages, we need to put a system in place to be able to detect the symptoms of a heart stroke at an early stage and thus prevent it. It is impractical for a common man to frequently undergo costly tests like the ECG and thus there needs to be a system in place which is handy and at the same time reliable, in predicting the chances of a heart disease. Thus we propose to develop an application which can predict the vulnerability of a heart disease given basic symptoms like age, sex, pulse rate etc. The machine learning algorithm neural networks has proven to be the most accurate and reliable algorithm and hence used in the proposed system.", 
The knowledge discovery in database (KDD) is alarmed with development of methods and techniques for making use of data. The proposed PP-RNN uses multiple RNNs for learning from diagnosis code sequences of patients in order to predict occurrences of high-risk diseases. This proposed method is a critical issue to predict the heart disease diagnosis of adult disease patients.,PREDICTION OF HEART DISEASE USING RNN ALGORITHM,"An infrastructure builds in the data mining platform which is reliable to challenge the commercial and non- commercial IT development communities of data streams in high dimensional data cluster modeling. The knowledge discovery in database (KDD) is alarmed with development of methods and techniques for making use of data. The data size is generally growing from day to day. One of the most important steps of the KDD is the data mining which is ability to extract useful knowledge hidden in this large amount of data. Both the data mining and healthcare industry have emerged some of reliable early Heart diseases detection systems and other various healthcare related systems from the clinical and diagnosis data. In this project we propose the enhanced data mining algorithm for healthcare application. This proposed method is a critical issue to predict the heart disease diagnosis of adult disease patients due to the possibility of spreading to high-risk symptoms in medical fields. Most studies for predicting prognosis have used complex data from patients such as biomedical images, biomarkers, and pathological measurements. We demonstrate a language model-like method for predicting high-risk prognosis from diagnosis histories of patients using deep recurrent neural networks (RNNs), i.e., prognosis prediction using RNN (PPRNN). The proposed PP-RNN uses multiple RNNs for learning from diagnosis code sequences of patients in order to predict occurrences of high-risk diseases. Finally our experimental result shows our proposed method can achieve more accuracy result.", 
"Predicting the readmission rate early one can alleviate the financial and medical consequences of hospital readmissions. Adding a text model to the deep learning model improves performance, increasing accuracy and F1- score by 2% and 6%.",Prediction of hospital readmission for heart disease-A deep learning approach,"Hospital readmissions consume large amounts of medical resources and negatively impact the healthcare system. Predicting the readmission rate early one can alleviate the financial and medical consequences. Most related studies only select the patient's structural features or text features for modeling analysis, which offer an incomplete picture of the patient. Based on structured data (including demographic data, clinical data, administrative data) and medical record text, this paper uses deep learning methods to construct an optimal model for hospital readmission prediction, tested on a dataset of heart disease patients’ 30-day readmission. The results show that when only structured data is used, the deep learning model is much better than the Naive Bayes model and slightly better than the Support Vector Machine model. Adding a text model to the deep learning model improves performance, increasing accuracy and F1- score by 2% and 6%, respectively. This indicates that textual information contributes greatly to hospital readmission predictions.", 
"End stage renal disease (ESRD) is the last stage of chronic kidney disease that requires dialysis or a kidney transplant to survive. Many studies reported a higher risk of mortality in ESRD patients compared with patients without ESRD. In this paper, we develop a model to predict postoperative complications, major cardiac event, for patients who underwent any type of surgery. We compare several widely-used machine learning models through experiments with our collected data yellow of size 3220, and achieved F1 score of 0.797 with the random forest model. Based on experimental results, we found that features related to operation (e.g., anesthesia time, operation time, crystal, and colloid) have the biggest impact on model performance, and also found the best combination of features. We believe that this study will allow physicians to provide more appropriate therapy to the ESRD patients by providing information on potential postoperative complications.",Prediction of Postoperative Complications for Patients of End Stage Renal Disease,"End stage renal disease (ESRD) is the last stage of chronic kidney disease that requires dialysis or a kidney transplant to survive. Many studies reported a higher risk of mortality in ESRD patients compared with patients without ESRD. In this paper, we develop a model to predict postoperative complications, major cardiac event, for patients who underwent any type of surgery. We compare several widely-used machine learning models through experiments with our collected data yellow of size 3220, and achieved F1 score of 0.797 with the random forest model. Based on experimental results, we found that features related to operation (e.g., anesthesia time, operation time, crystal, and colloid) have the biggest impact on model performance, and also found the best combination of features. We believe that this study will allow physicians to provide more appropriate therapy to the ESRD patients by providing information on potential postoperative complications.", 
"Early prediction of mortality for patients with heart failure is crucial for guiding clinical decision-making. In this paper, we developed a risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF) The developed iRSF-based risk model could serve as a valuable tool for clinicians.",Predictive Modeling of Hospital Mortality for Patients With Heart Failure by Using an Improved Random Survival Forest,"Identification of different risk factors and early prediction of mortality for patients with heart failure are crucial for guiding clinical decision-making in Intensive care unit cohorts. In this paper, we developed a comprehensive risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF). Utilizing a novel split rule and stopping criterion, the proposed iRSF was able to identify more accurate predictors to separate survivors and nonsurvivors and thus improve discrimination ability. Based on the public MIMIC II clinical database with 8 059 patients, 32 risk factors, including demographics, clinical, laboratory information, and medications, were analyzed and used to develop the risk model for patients with heart failure. Compared with previous studies, more critical laboratory predictors were identified that could reveal difficult-to-manage comorbidities, including aspartate aminotransferase, alanine aminotransferase, total bilirubin, serum creatine, blood urea nitrogen, and their inherent effects on events; these were determined to be critical indicators for predicting heart failure mortality with the proposed iRSF. The experimental results showed that the developed risk model was superior to those used in previous studies and the conventional random survival forest-based model with an out-of-bag C-statistic value of 0.821. Therefore, the developed iRSF-based risk model could serve as a valuable tool for clinicians in heart failure mortality prediction.", 
Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI) The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions.,Preference Classification Using Electroencephalography (EEG) and Deep Learning,"Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI). The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions such as in forensic neuroscience, rehabilitative medicine, affective entertainment and neuro-marketing. One particularly useful yet rarely explored areas of EEG-based emotion classification is preference recognition [1], which is simply the detection of like versus dislike. Within the limited investigations into preference classification, all reported studies were based on musicallyinduced stimuli except for a single study which used 2D images. We present two EEG-based preference classification studies: using (1) kNN for a 10-subject EEG classification problem; (2) deep learning for an expanded 16-subject EEG classification problem. We show that inter-subject variability introduces significant classification problems when larger cohorts of test subjects are used and that deep learning shows promising results in terms of addressing this inter-subject variability problem in EEG-based preference classification.", 
Punjabi Text Summarization is the process of condensing the source Punjabi text into a shorter version. It comprises two phases: 1) Pre Processing and 2) Processing.,Preprocessing Phase of Punjabi Language Text Summarization,"Punjabi Text Summarization is the process of condensing the source Punjabi text into a shorter version, preserving its information content and overall meaning. It comprises two phases: 1) Pre Processing 2) Processing. Pre Processing is structured representation of the Punjabi text. This paper concentrates on Pre processing phase of Punjabi Text summarization. Various sub phases of pre processing are: Punjabi words boundary identification, Punjabi language stop words elimination, Punjabi language noun stemming, finding Common English Punjabi noun words, finding Punjabi language proper nouns, Punjabi sentence boundary identification, and identification of Punjabi language Cue phrase in a sentence.", 
"Pretraining-based encoder-decoder framework can generate the output sequence based on the input sequence. Model achieves new state-of-the-art on both CNN/Daily Mail and New York Times datasets. To the best of our knowledge, our approach is the first method which applies the BERT into text generation tasks.",Pretraining-Based Natural Language Generation for Text Summarization,"In this paper, we propose a novel pretraining-based encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the encoder of our model, we encode the input sequence into context representations using BERT. For the decoder, there are two stages in our model, in the first stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the refined word for each masked position. To the best of our knowledge, our approach is the first method which applies the BERT into text generation tasks. As the first step in this direction, we evaluate our proposed method on the text summarization task. Experimental results show that our model achieves new state-of-the-art on both CNN/Daily Mail and New York Times datasets.", 
Software project management inspires and urges the spirit of software developing team members. Success of every project based on right selection of team members that ensures to meet the desired requirements. The aim of study is to extract and prioritize issues faced by vendors of global software development (GSD) organizations.,Prioritizing the Issues extracted for Getting Right People on Right Project in Software Project Management From Vendorsâ€™ Perspective,"Software project management inspires and urges the spirit of software developing team members which continues until project completion. Obviously, success of every project based on right selection of team members that ensures to meet the desired requirements of any software developing project. The fundamental aim of current study is to extract and prioritize issues faced by vendors of global software development (GSD) organizations during the selection of right team with having aim to complete the project successfully. As a methodology, a systematic literature review (SLR) used for data extraction and categorization, a questionnaire survey adopted for data validation, and a hierarchical analytical process (AHP) used for prioritizing extracted findings. A total of 12 issues are extracted and grouped into 3 categories (association, teamwork, and fascination). The overall result showed that ‘‘association’’ is the most important category as compare to other categories. Similarly, communication and coordination issues, team’s consistency and stability issues, and lack of expertise issues, etc are highlighted as the most critical issues during selection of right people for the right project from vendors’ perspective.", 
"IoT systems require security, seamless authentication, robustness and easy maintenance services. The decentralized nature of blockchain has resolved many security, maintenance, and authentication issues of IoT systems. However, the network is public, so transactional details and encrypted keys are open and visible to everybody in that network.","Privacy preservation in blockchain based IoT systems Integration issues, prospects, challenges, and future research directions","Modern Internet of Things (IoT) systems are paving their path for a revolutionized world in which majority of our objects of everyday use will be interconnected. These objects will be able to link and communicate with each other and their surroundings in order to automate majority of our tasks. This interconnection of IoT nodes require security, seamless authentication, robustness and easy maintenance services. In order to provide such salient features, blockchain comes out as a viable solution. The decentralized nature of blockchain has resolved many security, maintenance, and authentication issues of IoT systems. Therefore, an immense increase in applications of blockchainbased IoT systems can be seen from the past few years. However, blockchain-based IoT network is public, so transactional details and encrypted keys are open and visible to everybody in that network. Thus, any adversary can infer critical information of users from this public infrastructure. In this paper, we discuss the privacy issues caused due to integration of blockchain in IoT applications by focusing over the applications of our daily use. Furthermore, we discuss implementation of five privacy preservation strategies in blockchain-based IoT systems named as anonymization, encryption, private contract, mixing, and differential privacy. Finally, we discuss challenges, and future directions for research in privacy preservation of blockchain-based IoT systems. This paper can serve as a basis of development of future privacy preservation strategies to address several privacy problems of IoT systems operating over blockchain.", 
Categorization of mobile apps is essential for app stores in maintaining a huge quantity of apps efficiently and securely. The problem in existing methods is that the apps are uploaded from untrusted sources and the static features extracted for categorization can be easily masked by obfuscation or encryption. We propose a new privacy-preserving categorization method based on learning patterns from a large scale of usage data.,Privacy-preserving categorization of mobile applications based on large-scale usage data,"Categorization of mobile applications (apps) according to their functionalities is essential for app stores in maintaining a huge quantity of apps efficiently and securely. The problem in existing methods is that the apps are uploaded from untrusted sources and the static features extracted for categorization can be easily masked by obfuscation or encryption. To solve this problem and improve the categorization accuracy, we propose to extract features from usage data generated by apps running on mobile devices. Usage data, such as average running time or number of active users of an app, is hard to be manipulated by untrusted developers, while different types of apps generate different usage patterns. Based on this observation, we propose a new privacy-preserving categorization method of mobile apps based on learning patterns from a large scale of usage data. Firstly, the usage data collected from different users is anonymized by shuffling. Then we formalize the usage data as time series, extract and cluster usage data for each app based on Dynamic Time Warping. We utilize the Shape Features to segment the clustered time series and transform them into feature vectors. Finally, we adopt five machine learning methods to train and test the categorization models on 3,086 apps. The results show that SVM performs the best. When we exclude apps with the small number of the usage data flows under 50,000, the categorization performance (F1-score) of our method is improved to be over 96%, which is significantly better than the previous methods", 
Medical devices are more vulnerable to numerous security threats and attacks than other network devices. PrivacyProtector includes the ideas of secret sharing and share repairing (in case of data loss or compromise) for patients' data privacy. The framework uses a distributed database consisting of multiple cloud servers.,Privacyprotector Privacy-protected patient data collection in IoT-based healthcare systems,"In IoT-based healthcare, medical devices are more vulnerable to numerous security threats and attacks than other network devices. Current solutions are able to provide protection to patients’ data during data transmission to some extent, but cannot prevent some sophisticated threats and attacks such as collusion attacks and data leakage. In this article, we first investigate the challenges with privacy protected data collection. Then we propose a practical framework called PrivacyProtector, patient privacy protected data collection, with the objective of preventing these types of attacks. PrivacyProtector includes the ideas of secret sharing and share repairing (in case of data loss or compromise) for patients’ data privacy. Since it is the first time, we apply the Slepian-Wolf-coding-based secret sharing (SW-SSS) in PrivacyProtector. In the framework, we use a distributed database consisting of multiple cloud servers, which ensures that the privacy of patients’ personal data can remain protected as long as one of the servers remains uncompromised. We also present a patient access control scheme in which multiple cloud servers collaborate in shared construction to offer patients’ data to healthcare providers without revealing the content of the data. The privacy performance analysis has shown that the PrivacyProtector framework is secure and privacy-protected against various attacks.", 
"Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, the semantically important words must be separated from the low-content function words. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization.",Probabilistic document modeling for syntax removal in text summarization,"Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric", 
"This work proposes an approach to address the problem of improving content selection in automatic text summarization by using probabilistic neural network (PNN) This approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword.",Probabilistic neural network based text summarization,"This work proposes an approach to address the problem of improving content selection in automatic text summarization by using probabilistic neural network (PNN). This approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword, sentence centrality, sentence resemblance to the title, sentence inclusion of name entity, sentence inclusion of numerical data, sentence relative length, Bushy path of the sentence and aggregated similarity for each sentence to generate summaries. First we investigate the effect of each sentence feature on the summarization task. Then we use all features in combination to train the probabilistic neural network (PNN) in order to construct a text summarizer model.", 
Natural Language Processing (NLP) has been receiving increasing attention in the past few years. This work proposes an automatic system for product categorization using only their titles. The proposed system employs a state-of-the-art deep neural network to extract features from the titles to be used as input in different machine learning models.,Product Categorization by Title Using Deep Neural Networks as Feature Extractor,"Natural Language Processing (NLP) has been receiving increasing attention in the past few years. In part, this is related to the huge flow of data being made available everyday on the internet, which increased the need for automatic tools capable of analyzing and extracting relevant information, especially from the text. In this context, text classification became one of the most studied tasks on the NLP domain. The objective is to assign predefined categories or labels to text or sentences. Important applications include sentence classification, sentiment analysis, spam detection, among many others. This work proposes an automatic system for product categorization using only their titles. The proposed system employs a state-of-the-art deep neural network as a tool to extract features from the titles to be used as input in different machine learning models. The system is evaluated in the large-scale Mercado Libre dataset, which has the common characteristics of real-world problems such as imbalanced classes, unreliable labels, besides having a large number of samples: 20,000,000 in total. The results showed that the proposed system was able to correctly categorize the products with a balanced accuracy of 86.57% on the local test split of the Mercado Libre dataset. It also surpassed the fourth place on the public rank of the MeLi Data Challenge with 91.19% of balanced accuracy, which represents less than 1% of the difference to the winner.", 
Electroencephalogram (EEG) based brain-computer interface (BCI) is a useful communication tool between human brain and external devices. Accurate and effective EEG classification plays an important role in performance of BCI applications. Authors propose a dictionary pair learning (DPL) method for EEG signal classification.,Projective dictionary pair learning for EEG signal classification in brain computer interface applications,"Electroencephalogram (EEG) based brain-computer interface (BCI) is a useful communication tool between human brain and external devices. Accurate and effective EEG classification plays an important role in performance of BCI applications. In this paper, we propose a dictionary pair learning (DPL) method for EEG signal classification. In this method, we can learn a dictionary without costly L0 and L1 calculation and sparse coefficients have been calculated by linear projection instead of nonlinear sparse coding. We analyzed the performance of new method using EEG data from IIIa and IVa databases of BCI competition III. Experimental results showed that proposed method provides higher classification performance compared with other dictionary learning methods such as label consistent K Singular value decomposition (LC-KSVD). Based on our results, accuracy rates are as follows: 81.25%, 100%, 60.2%, 83.04% and 79.37% for subjects “aa”, “al”, “av”, “aw” and “ay”, respectively from Iva database. Also, the average accuracy rate of 85.7% has been achieved for two-class classification of IIIa database.", 
"Spam is the most troublesome Internet phenomenon challenging large global companies. This study describes three machine-learning algorithms to filter spam from valid emails with low error rates and high efficiency. Several widely used techniques include C4.5 decision tree classifier, multilayer perceptron and Naïve Bayes classifier.",Proposed efficient algorithm to filter spam using machine learning,"Electronic spam is the most troublesome Internet phenomenon challenging large global companies, including AOL, Google, Yahoo and Microsoft. Spam causes various problems that may, in turn, cause economic losses. Spam causes traffic problems and bottlenecks that limit memory space, computing power and speed. Spam causes users to spend time removing it. Various methods have been developed to filter spam, including black list/white list, Bayesian classification algorithms, keyword matching, header information processing, investigation of spam-sending factors and investigation of received mails. This study describes three machine-learning algorithms to filter spam from valid emails with low error rates and high efficiency using a multilayer perceptron model. Several widely used techniques include C4.5 decision tree classifier, multilayer perceptron and Naïve Bayes classifier, all of which are used for training data whether in the form of spam or valid emails. Finally, the results are discussed, and outputs of considered techniques are examined in relation to the proposed model", 
"Two-party authenticated key agreement (2PAKA) protocols allow two users to generate a shared and fresh session key. Some recent studies showed that 2PAKA protocols based on these assumptions are insecure in post-quantum environments. To resolve this issue, we have designed a lattice-based 2PAka protocol with the intractability of the ring-learning-with-errors problem.",Provably secure two-party authenticated key agreement protocol for post-quantum environments,"A two-party authenticated key agreement (2PAKA) protocol is a cryptographic tool employed widely to allow two users to generate a shared and fresh session key between them in each session over an insecure network. The authenticated version of a two-party key agreement protocol is popular because it can easily withstand the impersonation of the user. In the literature, many 2PAKA protocols have been put forward with the intractability assumptions of the discrete logarithm (DLP) problem and integer factorization problem (IFP). Some recent studies showed that the 2PAKA protocols based on these assumptions are insecure in post-quantum environments. To resolve this issue, we have designed a lattice-based 2PAKA (LB-2PAKA) protocol with the intractability of the ring-learning-with-errors (RLWE) problem. The proposed LB-2PAKA protocol is also analyzed in the random oracle model to measure provable security and to estimate the breaching time. To evaluate the performance, we used the LatticeCrypto Library and estimated the running time of our LB-2PAKA protocol. Besides, we analyzed the communication cost requirement of our LB-2PAKA protocol.", 
"Pseudonymity can satisfy both security and privacy requirements. A large body of work emerged in recent years proposing pseudonym solutions tailored to vehicular networks. This survey covers pseudonym schemes based on public key and identity-based cryptography, group signatures and symmetric authentication.",Pseudonym Schemes in Vehicular Networks A Survey,"Safety-critical applications in cooperative vehicular networks require authentication of nodes and messages. Yet, privacy of individual vehicles and drivers must be maintained. Pseudonymity can satisfy both security and privacy requirements. Thus, a large body of work emerged in recent years, proposing pseudonym solutions tailored to vehicular networks. In this survey, we detail the challenges and requirements for such pseudonym mechanisms, propose an abstract pseudonym lifecycle, and give an extensive overview and categorization of the state of the art in this research area. Specifically, this survey covers pseudonym schemes based on public key and identity-based cryptography, group signatures and symmetric authentication. We compare the different approaches, give an overview of the current state of standardization, and identify open research challenges.", 
"Feature-based approaches play an important role and are widely applied in extractive summarization. In this paper, we use particle swarm optimization (PSO) to evaluate the effectiveness of different features used to summarize Arabic text. The experimental results illustrate that Arabs summarize texts simply, focusing on the first sentence of each paragraph.",PSO-Based Feature Selection for Arabic Text Summarization,"Feature-based approaches play an important role and are widely applied in extractive summarization. In this paper, we use particle swarm optimization (PSO) to evaluate the effectiveness of different state-of-the-art features used to summarize Arabic text. The PSO is trained on the Essex Arabic summaries corpus data to determine the best particle that represents the most appropriate simple/combination of eight informative/structure features used regularly by Arab summarizers. Based on the elected features and their relevant weights in each PSO iteration, the input text sentences are scored and ranked to extract the top ranking sentences in the form of an output summary. The output summary is then compared with a reference summary using the cosine similarity function as the fitness function. The experimental results illustrate that Arabs summarize texts simply, focusing on the first sentence of each paragraph.", 
"Government is promoting digitization of data which results in large volume of data. To manage digital data, some approach is required to retrieve the data efficiently. This paper focuses on the technique for retrieving the data (text) in compact form or summarizes form.",PSO-Based Text Summarization Approach Using Sentiment Analysis,"In the present era of technology, most of the human activities are controlled and monitored by the electronic devices and still, people are working for more advanced technology and hence to fulfill the customers requirement. Government is also promoting digitization of data which results in large volume of data. To manage digital data, some approach is required to retrieve the data efficiently. Till now, so many techniques have been proposed for retrieving data in original form as well as compact form. This paper focuses on the technique for retrieving the data (text) in compact form or summarizes form. To achieve this goal, the concept of Particle Swarm Optimization (PSO) with sentiment analysis has been used. PSO has been used in the field of text summarization and the result is remarkable. Besides PSO, Sentiment Analysis (SA) has been proved its importance in the same research field.", 
"PICOSS is a data-curator platform for volcano-seismic data analysis, including detection, segmentation and classification. PICOSS has exportability and standardization at its core. Users can select automatic or manual workflows to select and label seismic data.",Python Interface for the Classification of Seismic Signals,"Over the last decade machine learning has become increasingly popular for the analysis and characterization of volcano-seismic data. One of the requirements for the application of machine learning methods to the problem of classifying seismic time series is the availability of a training dataset; that is a suite of reference signals, with known classification used for initial validation of the machine outcome. Here, we present PICOSS (Python Interface for the Classification of Seismic Signals), a modular data-curator platform for volcano-seismic data analysis, including detection, segmentation and classification. PICOSS has exportability and standardization at its core; users can select automatic or manual workflows to select and label seismic data from a comprehensive suite of tools, including deep neural networks. The modular implementation of PICOSS includes a portable and intuitive graphical user interface to facilitate essential data labelling tasks for large-scale volcano seismic studies", 
Unmanned aerial vehicle (UAV)-assisted routing protocols have been proposed for vehicular ad hoc networks. Few studies have investigated load balancing algorithms to accommodate future traffic growth. We propose Q-learning based load balancing routing (Q-LBR) to prevent problems caused by traffic congestion.,Q-LBR Q-Learning Based Load Balancing Routing for UAV-Assisted VANET,"Although various unmanned aerial vehicle (UAV)-assisted routing protocols have been proposed for vehicular ad hoc networks, few studies have investigated load balancing algorithms to accommodate future traffic growth and deal with complex dynamic network environments simultaneously. In particular, owing to the extended coverage and clear line-of-sight relay link on a UAV relay node (URN), the possibility of a bottleneck link is high. To prevent problems caused by traffic congestion, we propose Q-learning based load balancing routing (Q-LBR) through a combination of three key techniques, namely, a low-overhead technique for estimating the network load through the queue status obtained from each ground vehicular node by the URN, a load balancing scheme based on Q-learning and a reward control function for rapid convergence of Q-learning. Through diverse simulations, we demonstrate that Q-LBR improves the packet delivery ratio, network utilization and latency by more than 8, 28 and 30%, respectively, compared to the existing protocol.", 
"This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems. Compared to previous approaches, our framework is able to combine different metrics and evaluate the quality of a set of metrics without any a-priori weighting of their relative importance.","QARLA, A Framework for the Evaluation of Text Summarization Systems","This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems. The input of the framework is a set of manual (reference) summaries, a set of baseline (automatic) summaries and a set of similarity metrics between summaries. It provides i) a measure to evaluate the quality of any set of similarity metrics, ii) a measure to evaluate the quality of a summary using an optimal set of similarity metrics, and iii) a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results. Compared to previous approaches, our framework is able to combine different metrics and evaluate the quality of a set of metrics without any a-priori weighting of their relative importance. We provide quantitative evidence about the effectiveness of the approach to improve the automatic evaluation of text summarisation systems by combining several similarity metrics", 
"5G networks and vehicular ad-hoc networks (VANETs) will result in intelligent transportation and safety services and in-vehicle entertainment services. Cars, mobile phones and other communication devices/sensors will benefit from off-loading of network data on unlicensed bands. Offloading can improve network load, offer guaranteed bit rate services and reduce control signaling overhead.",Quality of User Experience in 5G-VANET,"The coalescence of 5G networks and vehicular ad-hoc networks (VANETs) will result in intelligent transportation and safety services and in-vehicle entertainment services. As a result, the plethora of connected devices (cars, mobile phones and other communication devices/sensors) will benefit from off-loading of network data on unlicensed bands. Offloading can 1) improve network load 2) offer guaranteed bit rate services and 3) reduce control signaling overhead, hence resulting in the overall improvement in user experience. In this paper we briefly discuss the enabling technologies, various communication scenarios within the 5G-VANET and the crucial user experience perspective. It should be noted that service acceptance depends heavily on user opinion formulated as per their experience. We further address the multi-layer Quality of Experience (QoE) assessment model and propose the way forward to enhance user experience within 5G-VANET. Since it is a work in progress, we discuss the importance of how and where the network performance measurements should be made and their effect on the overall user experience with future contributions in form of network simulations.", 
This paper analyzes the topic identification stage of single-document automatic text summarization across four different domains. We present a study that explores the summary space of each domain via an exhaustive search strategy.,Quantifying the limits and success of extractive summarization systems across domains,"This paper analyzes the topic identification stage of single-document automatic text summarization across four different domains, consisting of newswire, literary, scientific and legal documents. We present a study that explores the summary space of each domain via an exhaustive search strategy, and finds the probability density function (pdf) of the ROUGE score distributions for each domain. We then use this pdf to calculate the percentile rank of extractive summarization systems. Our results introduce a new way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain.", 
"This paper proposes a novel approach for a language independent automatic summarization approach. It combines three main approaches. The Rhetorical Structure Theory (RST), the query processing approach, and the Network Representationapproach. The output is an answer, which also gives the user an opportunity to find additional information.",Query Answering Approach Based on Document Summarization,"The growing of online information obliged the availability of a thorough research in the domain of automatic text summarization within the Natural Language Processing (NLP) community.The aim of this paper is to propose a novel approach for a language independent automatic summarization approach that combines three main approaches. The Rhetorical Structure Theory (RST), the query processing approach, and the Network Representationapproach (NRA). RST, as a theory of major aspect for the structure of natural text, is used to extract the semantic relation behind the text.Query processing approachclassifies the question type and finds the answer in a way that suits the user’s needs. The NRA is used to create a graph representing the extracted semantic relation. The output is an answer, which not only responses to the question, but also gives the user an opportunity to find additional information that is related to the question.We implemented the proposed approach. As a case study, the implemented approachis applied on Arabic text in the agriculture field. The implemented approach succeeded in summarizing extension documents according to user's query. The approach results have been evaluated using Recall, Precision and F-score measures", 
"In this paper we introduce a query based, Arabic text, single document summarization. We use an Arabic corpus to extract domain knowledge represented by topic related concepts/ keywords. The user's query is expanded once by using the Arabic WordNet thesaurus and then by adding the domain specific knowledge base to the expansion. For the summarization dataset, Essex Arabic Summaries Corpus was used.",Query Based Arabic Text Summarization,"With the problem of increased web resources and the huge amount of information available, the necessity of having automatic summarization systems appeared. Since summarization is needed the most in the process of searching for information on the web, where the user aims at a certain domain of interest according to his query, in this case domain-based summaries would serve the best. Despite the existence of plenty of research work in the domain-based summarization in English, there is lack of them in Arabic due to the shortage of existing knowledge bases. In this paper we introduce a query based, Arabic text, single document summarization using an existing Arabic language thesaurus and an extracted knowledge base. We use an Arabic corpus to extract domain knowledge represented by topic related concepts/ keywords and the lexical relations among them. The user’s query is expanded once by using the Arabic WordNet thesaurus and then by adding the domain specific knowledge base to the expansion. For the summarization dataset, Essex Arabic Summaries Corpus was used. It has many topic based articles with multiple human summaries. The performance appeared to be enhanced when using our extracted knowledge base than to just use the WordNet.", 
"In this paper, we present a model for generating summaries of text documents with respect to a query. This is known as querybased summarization. We adapt an existing dataset of news article summaries for the",Query-Based Abstractive Summarization Using Neural Networks,"In this paper, we present a model for generating summaries of text documents with respect to a query. This is known as querybased summarization. We adapt an existing dataset of news article summaries for the task and train a pointer-generator model using this dataset. The generated summaries are evaluated by measuring similarity to reference summaries. Our results show that a neural network summarization model, similar to existing neural network models for abstractive summarization, can be constructed to make use of queries to produce targeted summaries.", 
"Sanskrit consists of lots of literature available in the form of epics, stories, puranas, Vedas, and many more. Most of the Sanskrit documents have been digitized and made available online. Many tools for summarization have been developed for English and foreign languages.",Query-Based Extractive Text Summarization for Sanskrit,"Sanskrit consists of lots of literature available in the form of epics, stories, puranas, Vedas, and many more. Most of the Sanskrit documents have been digitized and made available online. Searching for the required information from the plentiful documents available is a tedious task. Automatic summarization serves the purpose in such situations. Many tools for summarization have been developed for English and foreign languages. The research for such kind of tools in Sanskrit is under exploration. In this paper, we propose three query-based summary generation methods to obtain extractive summary for single document written in Sanskrit. The methods are based on average term frequency-inverse sentence frequency, the VSM (Vector Space Model) and a graph-based technique using PageRank. All the techniques are compared and evaluated on the basis of performance.", 
"Information Retrieval (IR) systems such as search engines retrieve a large set of documents, images and videos in response to a user query. Computational methods such as Automatic Text Summarization (ATS) reduce this information load enabling users to find information quickly. The challenges to ATS include both the time complexity and the accuracy of summarization.",Query-based Multi-Document Summarization by Clustering of Documents,"Information Retrieval (IR) systems such as search engines retrieve a large set of documents, images and videos in response to a user query. Computational methods such as Automatic Text Summarization (ATS) reduce this information load enabling users to find information quickly without reading the original text. The challenges to ATS include both the time complexity and the accuracy of summarization. Our proposed Information Retrieval system consists of three different phases: Retrieval phase, Clustering phase and Summarization phase. In the Clustering phase, we extend the Potential-based Hierarchical Agglomerative (PHA) clustering method to a hybrid PHA-ClusteringGain-K-Means clustering approach. Our studies using the DUC 2002 dataset show an increase in both the efficiency and accuracy of clusters when compared to both the conventional Hierarchical Agglomerative Clustering (HAC) algorithm and PHA.", 
In this paper we use a deep auto-encoder for extractive query-based summarization. We propose constructing a local vocabulary for each document and adding a small random noise to the input. We also propose using inputs with added noise in an Ensemble Noisy Auto-Encoder.,Query-based single document summarization using an ensemble noisy auto-encoder,"In this paper we use a deep auto-encoder for extractive query-based summarization. We experiment with different input representations in order to overcome the problems stemming from sparse inputs characteristic to linguistic data. In particular, we propose constructing a local vocabulary for each document and adding a small random noise to the input. Also, we propose using inputs with added noise in an Ensemble Noisy Auto-Encoder (ENAE) that combines the top ranked sentences from multiple runs on the same input with different added noise. We test our model on a publicly available email dataset that is specifically designed for text summarization. We show that although an auto-encoder can be a quite effective summarizer, adding noise to the input and running a noisy ensemble can make improvements.", 
"query-based text summarization is aimed at extracting essential information that answers the query from original text. The answer is presented in a minimal, often predefined, number of words. We introduce a new unsupervised approach based on the minimum description length (MDL) principle that employs Krimp compression algorithm.",Query-based summarization using MDL principle,"Query-based text summarization is aimed at extracting essential information that answers the query from original text. The answer is presented in a minimal, often predefined, number of words. In this paper we introduce a new unsupervised approach for query-based extractive summarization, based on the minimum description length (MDL) principle that employs Krimp compression algorithm (Vreeken et al., 2011). The key idea of our approach is to select frequent word sets related to a given query that compress document sentences better and therefore describe the document better. A summary is extracted by selecting sentences that best cover query-related frequent word sets. The approach is evaluated based on the DUC 2005 and DUC 2006 datasets which are specifically designed for query-based summarization (DUC, 2005 2006). It competes with the best results.", 
This paper presents a survey of recent extractive query-based summarization techniques. We explore approaches for single document and multi-document summarization. Knowledge-based and,"Query-Based Summarization, A survey","This paper presents a survey of recent extractive query-based summarization techniques. We explore approaches for single document and multi-document summarization. Knowledge-based and machine learning methods for choosing the most relevant sentences from documents with respect to a given query are considered. Further, we expose tailored summarization techniques for particular domains like medical texts. The most recent developments in the field are presented with opinion summarization of blog entries.", 
"Existing graph- or hypergraph-based summarizers use graph-based ranking algorithms to produce individual scores of relevance for the sentences. These systems fail to measure the topics jointly covered by the sentences forming the summary, which tends to produce redundant summaries. We propose a new topic model based on the semantic clustering of terms to discover the topics present in a corpus. These topics are modeled as the hyperedges of a hypergraph in which the nodes are the sentences and a summary is then produced.",Query-oriented text summarization based on hypergraph transversals,"The rise in the amount of textual resources available on the Internet has created the need for tools of automatic document summarization. The main challenges of query-oriented extractive summarization are (1) to identify the topics of the documents and (2) to recover query-relevant sentences of the documents that together cover these topics. Existing graph- or hypergraph-based summarizers use graph-based ranking algorithms to produce individual scores of relevance for the sentences. Hence, these systems fail to measure the topics jointly covered by the sentences forming the summary, which tends to produce redundant summaries. To address the issue of selecting non-redundant sentences jointly covering the main query-relevant topics of a corpus, we propose a new method using the powerful theory of hypergraph transversals. First, we introduce a new topic model based on the semantic clustering of terms in order to discover the topics present in a corpus. Second, these topics are modeled as the hyperedges of a hypergraph in which the nodes are the sentences. A summary is then produced by generating a transversal of nodes in the hypergraph. Algorithms based on the theory of submodular functions are proposed to generate the transversals and to build the summaries. The proposed summarizer outperforms existing graph- or hypergraph-based summarizers by at least 6% of ROUGE-SU4 F-measure on DUC 2007 dataset. It is moreover cheaper than existing hypergraph-based summarizers in terms of computational time complexity", 
"Automatic text summarization enables us to access the most important content in the shortest possible time. In this paper, features are extracted from the sentences, each of which evaluates the importance of the sentences from an aspect. This paper has shown that use of more suitable features leads to improved summaries generated.",Query-oriented text summarization using sentence extraction technique,"Today there is a huge amount of information from a lot of various resources such as World Wide Web, news articles, e-books and emails. On the one hand, human beings face a shortage of time, and on the other hand, due to the social and occupational needs, they need to obtain the most important information from various resources. Automatic text summarization enables us to access the most important content in the shortest possible time. In this paper a query-oriented text summarization technique is proposed by extracting the most informative sentences. To this end, a number of features are extracted from the sentences, each of which evaluates the importance of the sentences from an aspect. In this paper 11 of the best features are extracted from each of the sentences. This paper has shown that use of more suitable features leads to improved summaries generated. In order to evaluate the automatic generated summaries, the ROUGE criterion has been used.", 
Questions Classification (QC) plays an important role in question-answering systems. We identify different patterns and use machine learning algorithms to classify them. A framework is proposed for question classification using a grammar-based approach (GQCC) The results show that the GQCC using J48 classifier has outperformed other classification methods with 90.1% accuracy.,Question categorization and classification using grammar based approach,"Question-answering has become one of the most popular information retrieval applications. Despite that most question-answering systems try to improve the user experience and the technology used in finding relevant results, many difficulties are still faced because of the continuous increase in the amount of web content. Questions Classification (QC) plays an important role in question-answering systems, with one of the major tasks in the enhancement of the classification process being the identification of questions types. A broad range of QC approaches has been proposed with the aim of helping to find a solution for the classification problems; most of these are approaches based on bag-of-words or dictionaries. In this research, we present an analysis of the different type of questions based on their grammatical structure. We identify different patterns and use machine learning algorithms to classify them. A framework is proposed for question classification using a grammar-based approach (GQCC) which exploits the structure of the questions. Our findings indicate that using syntactic categories related to different domain-specific types of Common Nouns, Numeral Numbers and Proper Nouns enable the machine learning algorithms to better differentiate between different question types. The paper presents a wide range of experiments the results show that the GQCC using J48 classifier has outperformed other classification methods with 90.1% accuracy.", 
"In this paper we present our system called QueSTS, which does the above task by filtering and aggregating important query relevant sentences. Our approach captures the contextual relationships among sentences of all input documents and represents them as an ""integrated graph""","QueSTS, A Query Specific Text Summarization System","Effective extraction of query relevant information present within documents on the web is a nontrivial task. In this paper we present our system called QueSTS, which does the above task by filtering and aggregating important query relevant sentences distributed across a set of documents. Our approach captures the contextual relationships among sentences of all input documents and represents them as an “integrated graph”. These relationships are exploited and several subgraphs of integrated graph which consist of sentences that are highly relevant to the query and that are highly related to each other are constructed. These subgraphs are ranked by our scoring model. The highest ranked subgraph which is rich in query relevant information and also has sentences that are highly coherent is returned as a query specific summary.", 
"Boosted decision trees are among the most popular learning techniques in use today. They exhibit fast speeds at test time, but relatively slow training renders them impractical for applications with real-time learning requirements. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude.",Quickly Boosting Decision Trees - Pruning Underachieving Features Early,"Boosted decision trees are among the most popular learning techniques in use today. While exhibiting fast speeds at test time, relatively slow training renders them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it is used in conjunction with existing Boosting algorithms and other sampling methods to achieve even greater speedups.", 
"It is estimated that the number of connected devices will exceed twenty billions in 2020. Machine-to-machine (M2M) devices will account for nearly half of total connected devices. Radio resource management (RRM) in conventional H2H communications aims at improving spectrum efficiency and energy efficiency. Similarly, RRM also plays a vital role in M2M communications.",Radio Resource Management in Machine-to-Machine Communications - A Survey,"In futuristic wireless communications, a massive number of devices need to access networks with diverse quality of service (QoS) requirements. It is estimated that the number of connected devices will exceed twenty billions in 2020, and machine-to-machine (M2M) devices will account for nearly half of total connected devices. However, existing cellular systems and wireless standards, designed primarily for human-to-human (H2H) communications focusing on reducing access latency, increasing data rate and system throughput, are not well suited for M2M communications that require massive connections, diverse QoS requirements, and low energy consumption. Radio resource management (RRM) in conventional H2H communications aims at improving spectrum efficiency (SE) and energy efficiency (EE). Similarly, RRM also plays a vital role in M2M communications. In this paper, we make a comprehensive survey on state-of-theart research activities on RRM in M2M communications. First, we discuss the issues on RRM for MTC in LTE/LTE-A cellular networks including access control, radio resource allocation, power management, and the latest 3GPP standards supporting M2M communications. Acknowledging the fact that a single technology can not support all M2M applications, we discuss RRM issues for unlicensed band radio access technologies (RATs) in M2M capillary networks, including IEEE 802.11ah, Bluetooth Low Energy (BLE), ZigBee, and smart metering networks (SUNs). We also survey M2M RRM methods in heterogeneous networks consisting of cellular networks, capillary networks, and ultra dense networks (UDNs). Finally, we review recent standard activities and discuss the open issues and research challenges",  
"In the recent times, the requirement for generation of a multi-document summary has gained a lot of attention. This is due to the information explosion in the web media. In our proposed system, we have developed a random forest classifier based multi- document summarization system.",Random forest classifier based multi-document summarization system,"In the recent times, the requirement for generation of multi-document summary has gained a lot of attention among the researchers due to the information explosion in the web media. Mostly, the text summarization technique uses the sentence extraction technique where the salient sentences in the multiple documents are extracted and presented as a summary. In our proposed system, we have developed a random forest classifier based multi-document summarization system that differentiates the sentences in the multiple documents as one belonging to the summary or not belonging to the summary. For this each sentence in the documents is represented by a set of feature scores. Classifier is trained using feature scores and summary information of each sentence in the document set. Feature scores of sentences of multiple documents to be summarized are given as the test document for the classifier. From the output of the classifier, sentences that belonging to the summary class, a required size summary is generated using Maximal Marginal Relevance. The experiments are conducted using the DUC 2002 dataset and its corresponding summary. Experimental results show the quality of the summary generated by this method is good in terms of relevance and novelty.", 
"This paper looks at the above technique in detail, and proposes several improvements. MRISUM is found to outperform both RISUM and RISUM+ significantly, and also outperforms LSA+TRM approach.",Random Indexing and Modified Random Indexing based approach for extractive text summarization,"Random Indexing based extractive text summarization has already been proposed in literature. This paper looks at the above technique in detail, and proposes several improvements. The improvements are both in terms of formation of index (word) vectors of the document, and construction of context vectors by using convolution instead of addition operation on the index vectors. Experiments have been conducted using both angular and linear distances as metrics for proximity. As a consequence, three improved versions of the algorithm, viz. RISUM, RISUM+ and MRISUM were obtained. These algorithms have been applied on DUC 2002 documents, and their comparative performance has been studied. Different ROUGE metrics have been used for performance evaluation. While RISUM and RISUM+ perform almost at par, MRISUM is found to outperform both RISUM and RISUM+ significantly. MRISUM also outperforms LSA+TRM based summarization approach. The study reveals that all the three Random Indexing based techniques proposed in this study produce consistent results when linear distance is used for measuring proximity.", 
Sentence ranking problem aims to rank sentences in opinionated text based on their usefulness for helping users understand the detailed reasons of sentiments. Experiment results show that the proposed methods are effective.,Ranking explanatory sentences for opinion summarization,"We introduce a novel sentence ranking problem called explanatory sentence extraction (ESE) which aims to rank sentences in opinionated text based on their usefulness for helping users understand the detailed reasons of sentiments (i.e., “explanatoriness""). We propose and study several general methods for scoring the explanatoriness of a sentence. We create new data sets and propose a new measure for evaluation. Experiment results show that the proposed methods are effective, outperforming a state of the art sentence ranking method for standard text summarization.", 
Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task. We propose a,Ranking Sentences for Extractive Summarization with Reinforcement Learning,Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans., 
"R2N2 can be used to rank sentences for multi-document summarization. It formulates the sentence ranking task as a hierarchical regression process. Experiments on the DUC 2001, 2002 and 2004 multi- document summarization datasets show that R2 N2 outperforms state-of-the-art extractive summarization approaches.",Ranking with recursive neural networks and its application to multi-document summarization,"We develop a Ranking framework upon Recursive Neural Networks (R2N2) to rank sentences for multi-document summarization. It formulates the sentence ranking task as a hierarchical regression process, which simultaneously measures the salience of a sentence and its constituents (e.g., phrases) in the parsing tree. This enables us to draw on word-level to sentence-level supervisions derived from reference summaries. In addition, recursive neural networks are used to automatically learn ranking features over the tree, with hand-crafted feature vectors of words as inputs. Hierarchical regressions are then conducted with learned features concatenating raw features. Ranking scores of sentences and words are utilized to effectively select informative and nonredundant sentences to generate summaries. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that R2N2 outperforms state-of-the-art extractive summarization approaches", 
"Automatic responses to brief expression changes from a neutral face have been recently isolated in the human brain using fast periodic visual stimulation (FPVS) and scalp electroencephalography. In different stimulation sequences, an expressive (angry, disgusted, happy, fearful, or sad) or neutral face arose every 5 pictures. Frequency- domain analysis indicated a robust expression-specific brain response over occipito-temporal sites for each emotion and neutrality.",Rapid and automatic discrimination between facial expressions in the human brain,"Automatic responses to brief expression changes from a neutral face have been recently isolated in the human brain using fast periodic visual stimulation (FPVS) coupled with scalp electroencephalography (EEG). Based on these observations, here we isolate specific neural signatures for the rapid categorization of each of 5 basic expressions, i.e., when they are directly discriminated from all other facial expressions. Scalp EEG was recorded in 15 participants presented with pictures alternating at a rapid 6 Hz rate (i.e., one fixation/face, backward- and forward-masked). In different stimulation sequences, an expressive (angry, disgusted, happy, fearful, or sad) or a neutral face arose every 5 pictures (i.e., at 6/5 = 1.2 Hz), among pictures of the same individual expressing the other emotions randomly. Frequency-domain analysis indicated a robust (i.e., recorded in every individual participant) and objective (i.e., at the predefined 1.2 Hz frequency and its harmonics) expression-specific brain response over occipito-temporal sites for each emotion and neutrality. In this context of variable expressions, while neural responses to the different expressions (Anger, Disgust, Happiness, Sadness) were dissimilar qualitatively, a much larger specific signature for neutral faces as compared to facial expressions was found. Interestingly, Fear also elicited a strong contrasted response with other facial expressions, associated with a specific neural signature over ventral occipito-temporal sites. Collectively, these findings reveal that specific EEG signatures for different facial expressions can be isolated in the human brain, pointing to partially different neural substrates. In addition, they provide support for a strong and highly selective neural response to fear at the system-level, in line with the importance of this emotional expression for biological survival.", 
"Posting pictures is a necessary part of advertising a home for sale. Agents typically sort through dozens of images from which to pick the most complimentary ones. This is a manual effort involving annotating images accompanied by descriptions. Here, we propose an approach based on computer vision methodology to radically increase the efficiency of such tasks.",Real Estate Image Classification,"Posting pictures is a necessary part of advertising a home for sale. Agents typically sort through dozens of images from which to pick the most complimentary ones. This is a manual effort involving annotating images accompanied by descriptions (bedroom, bathroom, attic, etc.). When volumes are small, manual annotation is not a problem, but there is a point where this becomes too burdensome and ultimately infeasible. Here, we propose an approach based on computer vision methodology to radically increase the efficiency of such tasks. We present a high-confidence image classification framework, whose inputs are images and outputs are labels. The core of the classification algorithm is long short term memory (LSTM), and fully connected neural networks, along with a substantial preprocessing using ‘contrast-limited adaptive histogram equalization (CLAHE) for image enhancement. Since, there is no standard benchmark containing a comprehensive dataset of well-annotated real estate images, we introduce Real Estate Image (REI) database for evaluating the image classification algorithms. Therein we demonstrate empirics based on our proposed framework on the new REI dataset, as well as on the SUN dataset.", 
"Face to face communication is a real-time process operating at a time scale in the order of 40 milliseconds. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time. It has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator.",Real Time Face Detection and Facial Expression Recognition Development and Applications to Human Computer Interaction.,"Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [15, 2]. The expression recognizer receives image patches located by the face detector. A Gabor representation of the patch is formed and then processed by a bank of SVM classifiers. A novel combination of Adaboost and SVM’s enhances performance. The system was tested on the Cohn-Kanade dataset of posed facial expressions [6]. The generalization performance to new subjects for a 7- way forced choice correct. Most interestingly the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system has been deployed on a wide variety of platforms including Sony’s Aibo pet robot, ATR’s RoboVie, and CU animator, and is currently being evaluated for applications including automatic reading tutors, assessment of human-robot intera", 
"Road crashes and related forms of accidents are a common cause of injury and death among the human population. Driver drowsiness accounts for approximately 100,000 accidents per year in the U.S. alone. This approach is based on a deep learning method that can be implemented on Android applications with high accuracy.",Real-time Driver Drowsiness Detection for Android Application Using Deep Neural Networks Techniques,"Road crashes and related forms of accidents are a common cause of injury and death among the human population. According to 2015 data from the World Health Organization, road traffic injuries resulted in approximately 1.25 million deaths worldwide, i.e. approximately every 25 seconds an individual will experience a fatal crash. While the cost of traffic accidents in Europe is estimated at around 160 billion Euros, driver drowsiness accounts for approximately 100,000 accidents per year in the United States alone as reported by The American National Highway Traffic Safety Administration (NHTSA). In this paper, a novel approach towards real-time drowsiness detection is proposed. This approach is based on a deep learning method that can be implemented on Android applications with high accuracy. The main contribution of this work is the compression of heavy baseline model to a lightweight model. Moreover, minimal network structure is designed based on facial landmark key point detection to recognize whether the driver is drowsy. The proposed model is able to achieve an accuracy of more than 80%", 
"Weapons detection on Closed-circuit television (CCTV) has been studied recently, being extremely useful in the field of security, counter-terrorism, and risk mitigation. This article presents a new dataset obtained from a real CCTV installed in a university and the generation of synthetic images to which Faster R-CNN was applied.",Real-time gun detection in CCTV An open problem,"Object detectors have improved in recent years, obtaining better results and faster inference time. However, small object detection is still a problem that has not yet a definitive solution. The autonomous weapons detection on Closed-circuit television (CCTV) has been studied recently, being extremely useful in the field of security, counter-terrorism, and risk mitigation. This article presents a new dataset obtained from a real CCTV installed in a university and the generation of synthetic images, to which Faster R-CNN was applied using Feature Pyramid Network with ResNet-50 resulting in a weapon detection model able to be used in quasi real-time CCTV (90 ms of inference time with an NVIDIA GeForce GTX-1080Ti card) improving the state of the art on weapon detection in a two stages training. In this work, an exhaustive experimental study of the detector with these datasets was performed, showing the impact of synthetic datasets on the training of weapons detection systems, as well as the main limitations that these systems present nowadays. The generated synthetic dataset and the real CCTV dataset are available to the whole research community", 
"Proposed system uses a set of Support Vector Machines (SVMs) for classifying 6 basic emotions and neutral expression along with checking mouth status. Using the same SVM models, the mobile app is running on Samsung Galaxy S3 with 2.4 fps. The accuracy of real-time mobile emotion recognition is about 72%.",Real-time Mobile Facial Expression Recognition System â€“ A Case Study,"This paper presents a mobile application for real time facial expression recognition running on a smart phone with a camera. The proposed system uses a set of Support Vector Machines (SVMs) for classifying 6 basic emotions and neutral expression along with checking mouth status. The facial expression features for emotion recognition are extracted by Active Shape Model (ASM) fitting landmarks on a face and then dynamic features are generated by the displacement between neutral and expression features. We show experimental results with 86% of accuracy with 10 folds cross validation in 309 video samples of the extended Cohn-Kanade (CK+) dataset. Using the same SVM models, the mobile app is running on Samsung Galaxy S3 with 2.4 fps. The accuracy of real-time mobile emotion recognition is about 72% for 6 posed basic emotions and neutral expression by 7 subjects who are not professional actors.", 
"Speech summarization technology is expected to play an important role in building speech archives and improving the efficiency of spoken document retrieval. Fundamental problems with speech summarization include speech recognition errors, disfluencies, and difficulties of sentence segmentation. How to objectively evaluate speech summarizing results is an important issue.",Recent advances in automatic speech summarization,"Speech summarization technology, which extracts important information and removes irrelevant information from speech, is expected to play an important role in building speech archives and improving the efficiency of spoken document retrieval. However, speech summarization has a number of significant challenges that distinguish it from general text summarization. Fundamental problems with speech summarization include speech recognition errors, disfluencies, and difficulties of sentence segmentation. Typical speech summarization systems consist of speech recognition, sentence segmentation, sentence extraction, and sentence compaction components. Most of the research has focuses on sentence extraction, using LSA (Latent Semantic Analysis), MMR (Maximal Marginal Relevance), or feature-based approaches, among which no decisive method has yet been found. Proper sentence segmentation is also essential to achieve good summarization performance. How to objectively evaluate speech summarization results is an important issue. Several measures, including families of SumACCY and ROUGE measures, have been proposed, and correlation analyses between subjective and objective evaluation scores have been performed. Although these measures are useful for ranking various summarization methods, they do not correlate well with human evaluations, especially when spontaneous speech is targeted.", 
The task of automatic document summarization aims at generating short summaries for originally long documents. A good summary should cover the most important information of the original document or a cluster of documents. Numerous approaches for automatic summarization have been developed to date.,Recent advances in document summarization,"The task of automatic document summarization aims at generating short summaries for originally long documents. A good summary should cover the most important information of the original document or a cluster of documents, while being coherent, nonredundant and grammatically readable. Numerous approaches for automatic summarization have been developed to date. In this paper we give a self-contained, broad overview of recent progress made for document summarization within the last 5 years. Specifically, we emphasize on significant contributions made in recent years that represent the state-of-the-art of document summarization, including progress on modern sentence extraction approaches that improve concept coverage, information diversity and content coherence, as well as attempts from summarization frameworks that integrate sentence compression, and more abstractive systems that are able to produce completely new sentences. In addition, we review progress made for document summarization in domains, genres and applications that are different from traditional settings. We also point out some of the latest trends and highlight a few possible future directions.", 
Vehicular ad hoc networks (VANET) are of an increasing importance as they enable accessing a large variety of ubiquitous services. It is of paramount importance to ensure VANETs security as their deployment in the future must not compromise the safety and privacy of their users.,Recent Advances in VANET Security A Survey,"Vehicular ad hoc networks (VANET) are emerging as a prominent form of mobile ad hoc networks (MANETs) and as an effective technology for providing a wide range of safety applications for vehicle passengers. Nowadays, VANETs are of an increasing importance as they enable accessing a large variety of ubiquitous services. Such increase is also associated with a similar increase in vulnerabilities in these inter-vehicular services and communications, and consequently, the number of security attacks and threats. It is of paramount importance to ensure VANETs security as their deployment in the future must not compromise the safety and privacy of their users. The successful defending against such VANETs attacks prerequisite deploying efficient and reliable security solutions and services, and the research in this field is still immature and is continuously and rapidly growing. As such, this paper is devoted to provide a structured and comprehensive overview of the recent research advances on VANETS security services, surveying the state-of-the-art on security threats, vulnerabilities and security services, while focusing on important aspects that are not well-surveyed in the literature such as VANET security assessment tools.", 
"There is growing interest among the research community for developing new approaches to automatically summarize the text. Since the advent of text summarization in 1950s, researchers have been trying to improve techniques for generating summaries. Summary can be generated through extractive as well as abstractive methods. During a decade, several extractive approaches have been developed for automatic summary generation.","Recent automatic text summarization techniques, a survey","As information is available in abundance for every topic on internet, condensing the important information in the form of summary would benefit a number of users. Hence, there is growing interest among the research community for developing new approaches to automatically summarize the text. Automatic text summarization system generates a summary, i.e. short length text that includes all the important information of the document. Since the advent of text summarization in 1950s, researchers have been trying to improve techniques for generating summaries so that machine generated summary matches with the human made summary. Summary can be generated through extractive as well as abstractive methods. Abstractive methods are highly complex as they need extensive natural language processing. Therefore, research community is focusing more on extractive summaries, trying to achieve more coherent and meaningful summaries. During a decade, several extractive approaches have been developed for automatic summary generation that implements a number of machine learning and optimization techniques. This paper presents a comprehensive survey of recent text summarization extractive approaches developed in the last decade. Their needs are identified and their advantages and disadvantages are listed in a comparative manner. A few abstractive and multilingual text summarization approaches are also covered. Summary evaluation is another challenging issue in this research field. Therefore, intrinsic as well as extrinsic both the methods of summary evaluation are described in detail along with text summarization evaluation conferences and workshops. Furthermore, evaluation results of extractive summarization approaches are presented on some shared DUC datasets. Finally this paper concludes with the discussion of useful future directions that can help researchers to identify areas where further research is needed.", 
We use distance functions to detect and segment objects in novel images. We evaluate the detection and segmentation performance of our algorithm on real-world outdoor scenes from the LabelMe dataset. We also show some promising qualitative image parsing results.,Recognition by Association via Learning Per-exemplar Distances,"We pose the recognition problem as data association. In this setting, a novel object is explained solely in terms of a small set of exemplar objects to which it is visually similar. Inspired by the work of Frome et al., we learn separate distance functions for each exemplar; however, our distances are interpretable on an absolute scale and can be thresholded to detect the presence of an object. Our exemplars are represented as image regions and the learned distances capture the relative importance of shape, color, texture, and position features for that region. We use the distance functions to detect and segment objects in novel images by associating the bottom-up segments obtained from multiple image segmentations with the exemplar regions. We evaluate the detection and segmentation performance of our algorithm on real-world outdoor scenes from the LabelMe [15] dataset and also show some promising qualitative image parsing results.",  
Motor imagery intention is one of the hot current research focuses of brain-computer interface (BCI) studies. It can help patients with physical dyskinesia to convey their movement intentions. This paper proposes a new deep multi-view feature learning method for the classification task of motor imagery electroencephalogram (EEG) signals.,Recognition of EEG Signal Motor Imagery Intention Based on Deep Multi-View Feature Learning,"Recognition of motor imagery intention is one of the hot current research focuses of brain-computer interface (BCI) studies. It can help patients with physical dyskinesia to convey their movement intentions. In recent years, breakthroughs have been made in the research on recognition of motor imagery task using deep learning, but if the important features related to motor imagery are ignored, it may lead to a decline in the recognition performance of the algorithm. This paper proposes a new deep multi-view feature learning method for the classification task of motor imagery electroencephalogram (EEG) signals. In order to obtain more representative motor imagery features in EEG signals, we introduced a multi-view feature representation based on the characteristics of EEG signals and the differences between different features. Different feature extraction methods were used to respectively extract the time domain, frequency domain, time-frequency domain and spatial features of EEG signals, so as to made them cooperate and complement. Then, the deep restricted Boltzmann machine (RBM) network improved by t-distributed stochastic neighbor embedding (t-SNE) was adopted to learn the multi-view features of EEG signals, so that the algorithm removed the feature redundancy while took into account the global characteristics in the multi-view feature sequence, reduced the dimension of the multi-visual features and enhanced the recognizability of the features. Finally, support vector machine (SVM) was chosen to classify deep multi-view features. Applying our proposed method to the BCI competition IV 2a dataset we obtained excellent classification results. The results show that the deep multi-view feature learning method further improved the classification accuracy of motor imagery tasks.", 
Age-related macular degeneration (AMD) is a major cause of visual impairment in people older than 50 years. AMD affects essential tasks such as reading and face recognition. We investigated the mechanisms underlying the deficit in recognition of facial expressions in an AMD population with low vision. We observed that AMD participants mostly identified emotions using the lower part of the face (mouth),Recognition of facial emotion in low vision A flexible usage of facial features,"Age-related macular degeneration (AMD) is a major cause of visual impairment in people older than 50 years in Western countries, affecting essential tasks such as reading and face recognition. Here we investigated the mechanisms underlying the deficit in recognition of facial expressions in an AMD population with low vision. Pictures of faces displaying different emotions with the mouth open or closed were centrally displayed for 300 ms. Participants with AMD with low acuity (mean 20/200) and normally sighted age-matched controls performed one of two emotion tasks: detecting whether a face had an expression or not (expressive/non expressive (EXNEX) task) or categorizing the facial emotion as happy, angry, or neutral (categorization of expression (CATEX) task). Previous research has shown that healthy observers are mainly using high spatial frequencies in an EXNEX task while performance at a CATEX task was preferentially based on low spatial frequencies. Due to impaired processing of high spatial frequencies in central vision, we expected and observed that AMD participants failed at deciding whether a face was expressive or not but categorized normally the emotion of the face (e.g., happy, angry, neutral). Moreover, we observed that AMD participants mostly identified emotions using the lower part of the face (mouth). Accuracy did not differ between the two tasks for normally sighted observers. The results indicate that AMD participants are able to identify facial emotion but must base their decision mainly on the low spatial frequencies, as they lack the perception of finer details", 
Conventional techniques require multiple verification components and these only exist commercially for operation up to 110GHz. Multiple components can lead to significant errors due to imperfections in waveguide flange misalignments. The reconfigurable component is designed so that its electrical properties can be changed quickly to a broad range of values.,Reconfigurable Waveguide for Vector Network Analyzer Verification,"A novel simple approach to the verification process for millimeter-wave vector network analyzer waveguide calibration is reported using a single reconfigurable component verification kit. Conventional techniques require multiple verification components and these only exist commercially for operation up to 110 GHz. At millimeter-wave frequencies, the use of multiple components can lead to significant errors due to imperfections in waveguide flange misalignments during the multiple component connections. The reconfigurable component is designed so that its electrical properties can be changed quickly to a broad range of predetermined values without introducing additional errors due to changes in flange alignment. Once connected, the component can be reconfigured to introduce relative changes in the reflected and transmitted signals. For millimeter-wave metrology, where mechanical precision is of paramount importance, this single-component verification approach represents an attractive solution. A proof-of-concept verification process is described, based on full-wave electromagnetic modeling, hardware implementation, and validation measurements using standard WR-15 waveguide (50–75 GHz).", 
This paper presents a framework that assists software engineers in recovering a software project's architecture from its source code. The architectural recovery process is an iterative one that combines clustering based on contextual and structural information in the code base with developer feedback.,Reconstructing and Evolving Software Architectures,"During a long maintenance period, software projects experience architectural erosion and drift, making maintenance tasks more challenging to perform for software engineers unfamiliar with the code base. This paper presents a framework that assists software engineers in recovering a software project’s architecture from its source code. The architectural recovery process is an iterative one that combines clustering based on contextual and structural information in the code base with incremental developer feedback. This process converges when the developer is satisfied with the proposed decomposition of the software, and, as an additional benefit, the framework becomes tuned to aid future evolution of the project. The paper provides both analytic and empirical evaluations of the obtained results; experimental results show a reasonably superior performance of our framework over alternative conventional methods. The proposed framework utilizes a novel compartmentalization technique Coordinated Clustering of Heterogeneous Datasets (CCHD) that relies on contextual and structural information in the code base, but, unlike most previous approaches, does not require specific weights for each information type, which allows it to adapt to different project types and domains.", 
"Humans have an amazing ability to instantly grasp the overall 3D structure of a scene. This ability is completely missing in most popular recognition algorithms. In this paper, we take the first step towards constructing the surface layout, a labeling of the image into geometric classes.",Recovering Surface Layout from an Image,"Humans have an amazing ability to instantly grasp the overall 3D structure of a scene—ground orientation, relative positions of major landmarks, etc.—even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this “surface layout” of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis. In this paper, we take the first step towards constructing the surface layout, a labeling of the image into geometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.", 
"Heart disease is one of the crucial impacts of mortality in the country. In clinical data analysis, predicting cardiovascular disease is a primary challenge. This paper aims to find the key features of the prediction of cardiovascular diseases through the use of machine learning techniques. The prediction model is adding various combinations of features and various established methods of classification.",Recursion Enhanced Random Forest With an Improved Linear Model (RERF-ILM) for Heart Disease Detection on the Internet of Medical Things Platform,"Nowadays, Heart disease is one of the crucial impacts of mortality in the country. In clinical data analysis, predicting cardiovascular disease is a primary challenge. Deep learning (DL) has been demonstrated to be effective in helping to determine and forecast a huge amount of data produced by the health industry. In this paper, the proposed Recursion enhanced random forest with an improved linear model (RFRF-ILM) to detect heart disease. This paper aims to find the key features of the prediction of cardiovascular diseases through the use of machine learning techniques. The prediction model is adding various combinations of features and various established methods of classification. it produces a better level of performance with precision through the heart disease prediction model. In this study, the factors leading to cardiovascular disease can be diagnosed. A comparison of important variables showed with the Internet of Medical Things (IoMT) platform, for data analysis. This indicates that coronary artery disease develops more often in older ages. Also important in this disease’s outbreak is high blood pressure. For this purpose, measures must be taken to prevent this disease and Diabetes provides a further aspect that should be taken into consideration in the occurrence of coronary artery disease with 96.6 % accuracy,96.8% stability ratio and 96.7% F-measure ratio.", 
"Text summarization is the process of generating a shorter form of text from a source document to preserve salient information. Many existing works for text summarization are evaluated by using recall-oriented understudy scores. Because Korean is an agglutinative language that combines various morphemes into a word, ROUGE is not suitable for Korean summarization.",Reference and Document Aware Semantic Evaluation Methods for Korean Language Summarization,"Text summarization refers to the process that generates a shorter form of text from the source document preserving salient information. Many existing works for text summarization are generally evaluated by using recall-oriented understudy for gisting evaluation (ROUGE) scores. However, as ROUGE scores are computed based on n-gram overlap, they do not reflect semantic meaning correspondences between generated and reference summaries. Because Korean is an agglutinative language that combines various morphemes into a word that express several meanings, ROUGE is not suitable for Korean summarization. In this paper, we propose evaluation metrics that reflect semantic meanings of a reference summary and the original document, Reference and Document Aware Semantic Score (RDASS). We then propose a method for improving the correlation of the metrics with human judgment. Evaluation results show that the correlation with human judgment is significantly higher for our evaluation metrics than for ROUGE scores.", 
"This paper aims to provide a proper understanding of regular expressions keyword search technique used in computer forensics investigation. In this paper, we perform the experiments using Prodiscover tool to reduce the search space by identifying and filtering the known files.",Regex an experimental approach for searching in cyber forensic,"The expeditious improvement and advancement in technology have shaped computers as ammunition, which may lead to a huge loss if used for wrong motives. The objective of computer forensics involves correctly examine and collect the digital evidence in such a manner so that they are justifiable in court. Due to the growing importance of cyber world security and the sincerity of cybercrime, it is essential for security professionals to know the technology used in forensics and keyword searching is the most handful technique for identifying potential evidence from the pool of files. This paper aims to provide a proper understanding of regular expressions keyword search technique used in computer forensics investigation. In this paper, we perform the experiments using Prodiscover tool to reduce the search space by identifying and filtering the known files to speed up the searching process of evidence identification.", 
"Best-performing methods were complex ensemble systems that combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision by more than 50%. We call the resulting model an R-CNN or Region-based Convolutional Network.",Region-based Convolutional Networks for Accurate Object Detection and Segmentation,"Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50% relative to the previous best result on VOC 2012—achieving a mAP of 62.4%. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/?rbg/rcnn.", 
"The common spatial patterns (CSP) algorithm is commonly used to extract spatial filters for the classification of electroencephalogram (EEG) signals in the context of brain-computer interfaces. CSP's performance is limited when the number of available training samples is small. We propose a regularized common spatial pattern (R-CSP), which can outperform CSP by 8.5% on average.",Regularized Common Spatial Patterns with Generic Learning for EEG Signal Classification,"The common spatial patterns (CSP) algorithm is commonly used to extract discriminative spatial filters for the classification of electroencephalogram (EEG) signals in the context of brain-computer interfaces (BCIs). However, CSP is based on a sample-based covariance matrix estimation. Therefore, its performance is limited when the number of available training samples is small. In this paper, the CSP method is considered in such a small-sample setting. We propose a regularized common spatial patterns (R-CSP) algorithm by incorporating the principle of generic learning. The covariance matrix estimation in R-CSP is regularized through two regularization parameters to increase the estimation stability while reducing the estimation bias due to limited number of training samples. The proposed method is tested on data set IVa of the third BCI competition and the results show that R-CSP can outperform the classical CSP algorithm by 8.5% on average. Moreover, the regularization introduced is particularly effective in the small-sample setting.",  
"We propose Regularized Max Pooling (RMP) for image classification. RMP classifies an image (or an image region) by extracting feature vectors at multiple subwindows at multiple locations and scales. Unlike Spatial Pyramid Matching, RMP",Regularized Max Pooling for Image Categorization,"We propose Regularized Max Pooling (RMP) for image classification. RMP classifies an image (or an image region) by extracting feature vectors at multiple subwindows at multiple locations and scales. Unlike Spatial Pyramid Matching where the subwindows are defined purely based on geometric correspondence, RMP accounts for the deformation of discriminative parts. The amount of deformation and the discriminative ability for multiple parts are jointly learned during training. RMP outperforms the state-of-the-art performance by a wide margin on the challenging PASCAL VOC2012 dataset for human action recognition on still images.", 
"Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code. Authors say state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. They say their new code summarization approach using a hierarchical attention network incorporates code features, including type-augmented abstract syntax trees and program control flows. Their approach outperforms the baselines by around 22% to 45% in BLEU-1 and outperforms other approaches.",Reinforcement-Learning-Guided Source Code Summarization via Hierarchical Attention,"Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays “attention”) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22% to 45% in BLEU-1 and outperforms the state-of-the-art approaches by around 5% to 60% in terms of S-BLEU and C-BLEU.", 
Portfolio selection is an important yet challenging task in AI for FinTech. One of the key issues is how to represent the non-stationary price series of assets in a portfolio. We propose a novel Relation-aware Transformer (RAT) to handle these aspects.,Relation-Aware Transformer for Portfolio Policy Learning,"Portfolio selection is an important yet challenging task in AI for FinTech. One of the key issues is how to represent the non-stationary price series of assets in a portfolio, which is important for portfolio decisions. The existing methods, however, fall short of capturing: 1) the complicated sequential patterns for asset price series and 2) the price correlations among multiple assets. In this paper, under a deep reinforcement learning paradigm for portfolio selection, we propose a novel Relation-aware Transformer (RAT) to handle these aspects. Specifically, being equipped with our newly developed attention modules, RAT is structurally innovated to capture both sequential patterns and asset correlations for portfolio selection. Based on the extracted sequential features, RAT is able to make profitable portfolio decisions regarding each asset via a newly devised leverage operation. Extensive experiments on real-world crypto-currency and stock datasets verify the state-of-the-art performance of RAT.", 
"Representation of facial expressions using continuous dimensions has shown to be inherently more expressive and psychologically meaningful than using categorized emotions. Many sub-problems have arisen in this new field that remain only partially understood. This paper presents empirical studies addressing these problems. Using the NVIE database, results show that the fusion of LBP and FAP features performs the best.",Representation of Facial Expression Categories in Continuous Arousal-Valence Space Feature and Correlation,"Representation of facial expressions using continuous dimensions has shown to be inherently more expressive and psychologically meaningful than using categorized emotions, and thus has gained increasing attention over recent years. Many sub-problems have arisen in this new field that remain only partially understood. A comparison of the regression performance of different texture and geometric features and investigation of the correlations between continuous dimensional axes and basic categorized emotions are two of these. This paper presents empirical studies addressing these problems, and it reports results from an evaluation of different methods for detecting spontaneous facial expressions within the arousal-valence dimensional space (AV). The evaluation compares the performance of texture features (SIFT, Gabor, LBP) against geometric features (FAP-based distances), and the fusion of the two. It also compares the prediction of arousal and valence, obtained using the best fusion method, to the corresponding ground truths. Spatial distribution, shift, similarity, and correlation are considered for the six basic categorized emotions (i.e. anger, disgust, fear, happiness, sadness, surprise). Using the NVIE database, results show that the fusion of LBP and FAP features performs the best. The results from the NVIE and FEEDTUM databases reveal novel findings about the correlations of arousal and valence dimensions to each of six basic emotion categories.", 
"In the last five years, the trend of ""Big Data"" has emerged and become a core element of Business Intelligence research. Computer Science and management information systems are two core disciplines that drive research associated with Big Data and Business Intelligence. ""Data mining"", ""social media"" and ""information system"" are high frequency keywords.",Research Landscape of Business Intelligence and Big Data analytics A bibliometrics study,"Business Intelligence that applies data analytics to generate key information to support business decision making, has been an important area for more than two decades. In the last five years, the trend of “Big Data” has emerged and become a core element of Business Intelligence research. In this article, we review academic literature associated with “Big Data” and “Business Intelligence” to explore the development and research trends. We use bibliometric methods to analyze publications from 1990 to 2017 in journals indexed in Science Citation Index Expanded (SCIE), Social Science Citation Index (SSCI) and Arts & Humanities Citation Index (AHCI). We map the time trend, disciplinary distribution, high-frequency keywords to show emerging topics. The findings indicate that Computer Science and management information systems are two core disciplines that drive research associated with Big Data and Business Intelligence. “Data mining”, “social media” and “information system” are high frequency keywords, but “cloud computing”, “data warehouse” and “knowledge management” are more emphasized after 2016.", 
Wearable biosensors based on the Internet of Things provide a new solution for heart disease analysis and prediction of sudden death from heart failure. How to deal with a large amount of data generated by heart sound signals is an urgent problem to be solved. We propose a parallel compressive sensing model for the multichannel synchronous acquisition of heart sounds.,Research on Parallel Compressive Sensing and Application of Multi-Channel Synchronous Acquisition of Heart Sound Signals,"The rapid development of wearable biosensors based on the Internet of Things provides a new solution for heart disease analysis and prediction of sudden death from heart failure. However, how to deal with a large amount of data generated by heart sound signals is an urgent problem to be solved. Based on the characteristics of heart sound signals, we propose a parallel compressive sensing model for the multichannel synchronous acquisition of heart sound signals. Furthermore, we provide a series of experiments to assess the performance of the model. The experimental results demonstrate that the reconstruction speed of the proposed model is 9–10 times faster than that of the block sparse Bayesian learning algorithm and the orthogonal matching pursuit algorithm, and the reconstruction effect is better. Meanwhile, the proposed model can effectively reconstruct the normal heart sound signal and abnormal heart sound signal of fourchannel synchronous acquisition. Therefore, the proposed model is feasibility", 
"Platooning is a promising intelligent transportation framework that can improve road capacity, on-road safety, and fuel efficiency. An efficient resource allocation (RA) approach is required for the timely and successful delivery of inter-vehicle information within multiplatoons. Subchannel allocation scheme and power control mechanism are proposed for LTE-based inter- vehicle communications.",Resource Allocation for Cellular-based Inter-Vehicle Communications in Autonomous Multiplatoons,"With the significant population growth in megacities everywhere, traffic congestion is becoming a severe impediment, leading to long travel delays and large economic loss on a global scale. Platooning is a promising intelligent transportation framework that can improve road capacity, on-road safety, and fuel efficiency. Furthermore, enabling inter-vehicle communications within a platoon and among platoons (in a multiplatoon) can potentially enhance platoon control by keeping constant intervehicle and inter-platoon distances. However, an efficient resource allocation (RA) approach is required for the timely and successful delivery of inter-vehicle information within multiplatoons. In this paper, subchannel allocation scheme and power control mechanism are proposed for LTE-based inter-vehicle communications in a multiplatooning scenario. We jointly consider the evolved multimedia broadcast multicast services (eMBMS) and device-to-device (D2D) multicast communications to enable intraand inter-platoon communications such that a desired tradeoff between the required cellular resources and the imposed communication delay can be achieved. Simulation results are given to demonstrate that the proposed approaches can reduce the communication delay comparing to D2D-unicast based RA scheme, especially in a multiplatoon scenario with a large number of vehicles.", 
Fast channel variations caused by high mobility in a vehicular environment need to be properly accounted for. We attempt to maximize the ergodic capacity of V2I connections while ensuring reliability guarantee for each V2V link. Novel algorithms that yield optimal resource allocation and are robust to channel variations are proposed.,Resource Allocation for D2D-Enabled Vehicular Communications,"The widely deployed cellular network, assisted with device-to-device (D2D) communications, can provide a promising solution to support efficient and reliable vehicular communications. Fast channel variations caused by high mobility in a vehicular environment need to be properly accounted for when designing resource allocation schemes for the D2D-enabled vehicular networks. In this paper, we perform spectrum sharing and power allocation based only on slowly varying large-scale fading information of wireless channels. Pursuant to differing requirements for different types of links, i.e., high capacity for vehicle-to-infrastructure (V2I) links and ultra reliability for vehicle-to-vehicle (V2V) links, we attempt to maximize the ergodic capacity of V2I connections while ensuring reliability guarantee for each V2V link. Sum ergodic capacity of all V2I links is first taken as the optimization objective to maximize the overall V2I link throughput. Minimum ergodic capacity maximization is then considered to provide a more uniform capacity performance across all V2I links. Novel algorithms that yield optimal resource allocation and are robust to channel variations are proposed. Their desirable performance is confirmed by computer simulation.", 
"Responsive text summarization is an approach to web design aimed at allowing desktop web pages to be read in response to the size of the device a user is browsing with. RTS implements the TextRank algorithm, so it can be exploited to generate very short summaries or longer summaries.",Responsive text summarization,"Responsive text summarization (RTS) is an approach to web design aimed at allowing desktop web pages to be read in response to the size of the device a user is browsing with. RTS implements the TextRank algorithm, so it can be exploited to generate very short summaries (more apt for mobile devices, where screen space is at a premium) or longer, more explicative summaries (more apt e.g. for phablets or laptops). We validate the feasibility of RTS on blog sites and show that runtime performance incurred by our current implementation is negligible. We also show that reading time can be reduced by a factor of 4 without impacting substantially the summary quality. In general, anyone interested in making their web contents concise but informative could benefit from this work.", 
This study is to judge whether this combination method of multispectral image and convolutional neural network (CNN) method can,Resting-state EEG signal classification of amnestic mild cognitive impairment with type 2 diabetes mellitus based on multispectral image and convolutional neural network,The purpose of this study is to judge whether this combination method of multispectral image and convolutional neural network (CNN) method can be used to distinguish amnestic mild cognitive impairment (aMCI) with Type 2 diabetes mellitus (T2DM) and normal controls (NC) with T2DM effectively.., 
Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection. It is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations. This paper conducts a rigorous evaluation of these new techniques.,Return of the Devil in the Details Delving Deep into Convolutional Nets,"The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.", 
Research has shifted towards abstractive summation and real-time summarization. This paper provides a broad and systematic review of research in the field of text summarization published from 2008 to 2019. There are 85 journal and conference publications which are the results of the extraction of selected studies.,Review of automatic text summarization techniques & methods,"Text summarization automatically produces a summary containing important sentences and includes all relevant important information from the original document. One of the main approaches, when viewed from the summary results, are extractive and abstractive. An extractive summary is heading towards maturity and now research has shifted towards abstractive summation and real-time summarization. Although there have been so many achievements in the acquisition of datasets, methods, and techniques published, there are not many papers that can provide a broad picture of the current state of research in this field. This paper provides a broad and systematic review of research in the field of text summarization published from 2008 to 2019. There are 85 journal and conference publications which are the results of the extraction of selected studies for identification and analysis to describe research topics/trends, datasets, preprocessing, features, techniques, methods, evaluations, and problems in this field of research. The results of the analysis provide an in-depth explanation of the topics/trends that are the focus of their research in the field of text summarization; provide references to public datasets, preprocessing and features that have been used; describes the techniques and methods that are often used by researchers as a comparison and means for developing methods. At the end of this paper, several recommendations for opportunities and challenges related to text summarization research are mentioned", 
This paper discusses abstractive text summarization techniques. It highlights the parametric evaluation of these techniques. Existing techniques extract important sentences from original document and generate summary. This technique may not present conflicting information properly.,Review on Abstractive Text Summarization Techniques (ATST) for single and multi documents,"In recent times, there is an enormous amount of data available on the internet. It is laborious for users to encapsulate large amount of data manually. Automatic text summarization can solve this problem by generating summary automatically. It can be categorized into extractive and abstractive text summarization techniques. Existing techniques of extractive text summarization extract important sentences from original document and generate summary without any modification of actual data. This technique may not present conflicting information properly. Abstractive text summarization can solve this problem by representing the extracted sentences into another understandable semantic form. This paper discusses abstractive text summarization techniques and highlights the parametric evaluation of these techniques.", 
Summary is shorter text that covers important information from original text. Manual creation of summary is difficult task. Research community is developing new approaches for automatic text summarization.,Review on text summarization evaluation methods,There is ample amount of information available on internet. Important information can be gained by creating summary from available information. Manual creation of summary is difficult task. Hence research community is developing new approaches for automatic text summarization that creates summary automatically. Summary is shorter text that covers important information from original text. This paper discusses basics of automatic text summarization. To evaluate automatic summaries are also challenging task. The challenges in evaluating summaries are also described. Methods for evaluation of summary- Both intrinsic and extrinsic are described in detail. The paper concludes with some suggestions for future directions for summary evaluation., 
"Summarization of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. We show that, contrary to the common belief, RouGE is not much reliable in evaluating scientific articles.",Revisiting Summarization Evaluation for Scientific Articles,"Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE’s effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", 
"When compressed air Class A foam used in firefighting covers the surface of an object, free drainage and changes of structural occur. It was observed that the evolution of the foam structure was consistent with the drainage process in terms of the time nodes division at room temperature. The foam evolved from a spherical structure to an ellipsoidal structure at about 900 s. It then began to evolve toward a polygonal structure at 3000 s, and eventually became a polyglot at 3600 s.",Rheological characterization and prediction model of compressed air Class A foam,"When the compressed air Class A foam used in firefighting covers the surface of the object, free drainage and changes of structural occur. The evolution of the foam and the law of drainage and rheological properties were investigated. It was observed by experiments that the evolution of the foam structure was consistent with the drainage process in terms of the time nodes division at room temperature. The foam evolved from a spherical structure to an ellipsoidal structure at about 900 s, began to evolve toward a polygonal structure at 3000 s, and eventually became a polygonal structure at 3600 s. At the same time, the foam drainage mass rate can be divided into three stages: increasing stage (300–900 s), dropping stage (900–3000 s), and stabilizing stage (3000–3600 s). The similarity of the time nodes determines the interaction between the two phenomena. At room temperature, when the liquid volume fraction is between 0.03 and 0.14, a linear relationship between the foam yield stress and the liquid volume fraction could be established. Furthermore, the Herschel–Bulkley model was modified by introducing a structural damage factor k(t) to describe the change in foam rheological properties over time.", 
We show surface-level markers of pragmatic intent can be used to recognize important sentences in text. We focus on using automated detection of rhetorical figures,Rhetorical Figuration as a Metric in Text Summarization,"We show that surface-level markers of pragmatic intent can be used to recognize the important sentences in text and can thereby improve the performance of text summarization systems. In particular, we focus on using automated detection of rhetorical figures—characteristic syntactic patterns of persuasive language—to provide information for an additional metric to enhance the performance of the MEAD summarizer.", 
"Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. Best-performing methods are complex ensemble systems that combine multiple low-level image features with high-level context. We propose a simple and scalable detection algorithm that improves mean average precision by more than 30%.",Rich feature hierarchies for accurate object detection and semantic segmentation,"Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features", 
Risks are an inherent part of any software project. Risk management in software development projects is still often neglected. This paper aims to identify and to map risk factors in environments of software projects. The most mentioned risk factors were the lack of technical skills by the staff.,Risk factors in software development projects a systematic,"Risks are an inherent part of any software project. The presence of risks in environments of software development projects requires the perception so that the associated factors do not lead projects to failure. The correct identification and monitoring of these factors can be decisive for the success of software development projects and software quality. However, in practice, risk management in software development projects is still often neglected and one of the reasons is due to the lack of knowledge of risk factors that promoted a low perception of them in the environment. This paper aims to identify and to map risk factors in environments of software development projects. We conducted a systematic literature review through a database search, as well as we performed an assessment of quality of the selected studies. All this process was conducted through a research protocol. We identified 41 studies. In these works, we extracted and classified risk factors according to the software development taxonomy developed by Software Engineering Institute (SEI). In total, 148 different risk factors were categorized. The found evidences suggest that risk factors relating to software requirements are the most recurrent and cited. In addition, we highlight that the most mentioned risk factors were the lack of technical skills by the staff. Therefore, the results converged to the need for more studies on these factors as fundamental items for reduction of failure level of a software development project.", 
"Random Multimodel Deep Learning (RMDL) is a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RDML produces consistently better performance than standard methods over a broad range of data types and classification problems.",RMDL Random Multimodel Deep Learning for Classification,"The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. RDML can accept as input a variety data to include text, video, images, and symbolic. This paper describes RMDL and shows test results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and 20newsgroup. These test results show that RDML produces consistently better performance than standard methods over a broad range of data types and classification problems", 
"High accuracy, fast learning, and online performance make this P300 speller a potential communication tool for severely disabled individuals, who have lost all other means of communication. We show that the training time can be further reduced by a factor of two from its current value of about 20 min.",Robust Classification of EEG Signal for Brainâ€“Computer Interface,"We report the implementation of a text input application (speller) based on the P300 event related potential. We obtain high accuracies by using an SVM classifier and a novel feature. These techniques enable us to maintain fast performance without sacrificing the accuracy, thus making the speller usable in an online mode. In order to further improve the usability, we perform various studies on the data with a view to minimizing the training time required. We present data collected from nine healthy subjects, along with the high accuracies (of the order of 95% or more) measured online. We show that the training time can be further reduced by a factor of two from its current value of about 20 min. High accuracy, fast learning, and online performance make this P300 speller a potential communication tool for severely disabled individuals, who have lost all other means of communication and are otherwise cut off from the world, provided their disability does not interfere with the performance of the speller.", 
"Many road detection algorithms require pre-learned information, which may be unreliable. This paper proposes a general road shape prior to enforce the detected region to be road-shaped. The training data is automatically generated from a predicted road region of the current image.",ROBUST ROAD DETECTION FROM A SINGLE IMAGE USING ROAD SHAPE PRIOR,"Many road detection algorithms require pre-learned information, which may be unreliable as the road scene is usually unexpectable. Single image based (i.e., without any pre-learned information) road detection techniques can be adopted to overcome this problem, while their robustness needs improving. To achieve robust road detection from a single image, this paper proposes a general road shape prior to enforce the detected region to be road-shaped by encoding the prior into a graph-cut segmentation framework, where the training data is automatically generated from a predicted road region of the current image. By iteratively performing the graph-cut segmentation, an accurate road region will be obtained. Quantitative and qualitative experiments on the challenging SUN Database validate the robustness and efficiency of our method. We believe that the road shape prior can also be used to yield improvements for many other road detection algorithms.", 
"In this paper, we propose Local Binary Pattern Histogram Fourier features (LBP-HF) These features outperform non-invariant and earlier version of rotation. Invariant LBP and the MR8 descriptor in texture classification, material categorization and face recognition tests.",Rotation Invariant Image Description with Local Binary Pattern Histogram Fourier Features,"In this paper, we propose Local Binary Pattern Histogram Fourier features (LBP-HF), a novel rotation invariant image descriptor computed from discrete Fourier transforms of local binary pattern (LBP) histograms. Unlike most other histogram based invariant texture descriptors which normalize rotation locally, the proposed invariants are constructed globally for the whole region to be described. In addition to being rotation invariant, the LBP-HF features retain the highly discriminative nature of LBP histograms. In the experiments, it is shown that these features outperform non-invariant and earlier version of rotation invariant LBP and the MR8 descriptor in texture classification, material categorization and face recognition tests.", 
Major challenge in brain–computer interface research is the accurate classification of time-varying electroencephalographic (EEG) signals. Semisupervised learning methods can utilize both labeled and unlabeled data to improve performance over supervised approaches. It has been reported that the unlabeling data may undermine the performance of SSL in some cases.,Safe Semi-Supervised Extreme Learning Machine for EEG Signal Classification,"One major challenge in the current brain–computer interface research is the accurate classification of time-varying electroencephalographic (EEG) signals. The labeled EEG samples are usually scarce, while the unlabeled samples are available in large quantities and easy to collect in real applications. Semisupervised learning (SSL) methods can utilize both labeled and unlabeled data to improve performance over supervised approaches. However, it has been reported that the unlabeled data may undermine the performance of SSL in some cases. To improve the safety of SSL, we proposed a new safety-control mechanism by analyzing the differences between unlabeled data analysis in supervised and semi-supervised learning. We then develop and implement a safe classification method based on the semi-supervised extreme learning machine (SS-ELM). Following this approach, the Wasserstein distance is used to measure the similarities between the predictions obtained from ELM and SS-ELM algorithms, and a different risk degree is thereby calculated for each unlabeled data instance. A risk-based regularization term is then constructed and embedded into the objective function of the SS-ELM. Extensive experiments were conducted using benchmark and EEG datasets to evaluate the effectiveness of the proposed method. Experimental results show that the performance of the new algorithm is comparable to SS-ELM and superior to ELM on average. It is thereby shown that the proposed method is safe and efficient for the classification of EEG signals.", 
"Attention mechanism plays a dominant role in the sequence generation models. Salience estimation for words, phrases or sentences is a critical component in text summarization. We propose a Multi-Attention Learning framework which contains two new attention learning components for salience estimation.",Salience Estimation with Multi-Attention Learning for Abstractive Text Summarization,"Attention mechanism plays a dominant role in the sequence generation models and has been used to improve the performance of machine translation and abstractive text summarization. Different from neural machine translation, in the task of text summarization, salience estimation for words, phrases or sentences is a critical component, since the output summary is a distillation of the input text. Although the typical attention mechanism can conduct text fragment selection from the input text conditioned on the decoder states, there is still a gap to conduct direct and effective salience detection. To bring back direct salience estimation for summarization with neural networks, we propose a Multi-Attention Learning framework which contains two new attention learning components for salience estimation: supervised attention learning and unsupervised attention learning. We regard the attention weights as the salience information, which means that the semantic units with large attention value will be more important. The context information obtained based on the estimated salience is incorporated with the typical attention mechanism in the decoder to conduct summary generation. Extensive experiments on some benchmark datasets in different languages demonstrate the effectiveness of the proposed framework for the task of abstractive summarization.", 
"In this paper we apply LayerWise Relevance Propagation to a sequenceto-sequence attention model. We obtain unexpected saliency maps and discuss the rightfulness of these ""explanations"" We argue that we need a quantitative way of testing the counterfactual case.",Saliency Maps Generation for Automatic Text Summarization,"Saliency map generation techniques are at the forefront of explainable AI literature for a broad range of machine learning applications. Our goal is to question the limits of these approaches on more complex tasks. In this paper we apply LayerWise Relevance Propagation (LRP) to a sequenceto-sequence attention model trained on a text summarization dataset. We obtain unexpected saliency maps and discuss the rightfulness of these “explanations”. We argue that we need a quantitative way of testing the counterfactual case to judge the truthfulness of the saliency maps. We suggest a protocol to check the validity of the importance attributed to the input and show that the saliency maps obtained sometimes capture the real use of the input features by the network, and sometimes do not. We use this example to discuss how careful we need to be when accepting them as explanation.", 
"Language model (LM) pre-training has resulted in impressive performance and sample efficiency on a variety of language understanding tasks. It remains unclear how to best use pre-trained LMs for generation tasks such as abstractive summarization. We instead use a pre- trained decoder-only network, where the same Transformer LM both encodes the source and generates the summary.",Sample Efficient Text Summarization Using a Single Pre-Trained Transformer,"Language model (LM) pre-training has resulted in impressive performance and sample efficiency on a variety of language understanding tasks. However, it remains unclear how to best use pre-trained LMs for generation tasks such as abstractive summarization, particularly to enhance sample efficiency. In these sequence-to-sequence settings, prior work has experimented with loading pre-trained weights into the encoder and/or decoder networks, but used non-pre-trained encoder-decoder attention weights. We instead use a pre-trained decoder-only network, where the same Transformer LM both encodes the source and generates the summary. This ensures that all parameters in the network, including those governing attention over source states, have been pre-trained before the fine-tuning step. Experiments on the CNN/Daily Mail dataset show that our pre-trained Transformer LM substantially improves over pre-trained Transformer encoder-decoder networks in limited-data settings. For instance, it achieves 13.1 ROUGE2 using only 1% of the training data (?3000 examples), while pre-trained encoder-decoder models score 2.3 ROUGE-2.", 
Text classification is one of the most popular Natural Language Processing (NLP) tasks. Less attention was directed towards this task in Arabic due to lack of rich representative resources for training an Arabic text classifier. We introduce a large Single-labeled Arabic News Articles Dataset of textual data collected from three news portals.,SANAD-Single-label Arabic News Articles Dataset for automatic text categorization,"Text Classification is one of the most popular Natural Language Processing (NLP) tasks. Text classification (aka categorization) is an active research topic in recent years. However, much less attention was directed towards this task in Arabic, due to the lack of rich representative resources for training an Arabic text classifier. Therefore, we introduce a large Single-labeled Arabic News Articles Dataset (SANAD) of textual data collected from three news portals. The dataset is a large one consisting of almost 200k articles distributed into seven categories that we offer to the research community on Arabic computational linguistics. We anticipate that this rich dataset would make a great aid for a variety of NLP tasks on Modern Standard Arabic (MSA) textual data, especially for single label text classification purposes. We present the data in raw form. SANAD is composed of three main datasets scraped from three news portals, which are AlKhaleej, AlArabiya, and Akhbarona", 
Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks. The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image.,Scalable Object Detection using Deep Neural Networks,"Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for crossclass generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.", 
A novel seizure detection method based on the deep bidirectional long short-term memory (Bi-LSTM) network is proposed in this paper. The deep architecture is designed by combining two independent LSTM networks with the opposite propagation directions. A mean sensitivity of 93.61% and a mean specificity of 91.85% were achieved on a long-term scalp EEG database.,Scalp EEG classification using deep Bi-LSTM network for seizure detection,"Automatic seizure detection technology not only reduces workloads of neurologists for epilepsy diagnosis but also is of great significance for treatments of epileptic patients. A novel seizure detection method based on the deep bidirectional long short-term memory (Bi-LSTM) network is proposed in this paper. To preserve the nonstationary nature of EEG signals while decreasing the computational burden, the local mean decomposition (LMD) and statistical feature extraction procedures are introduced. The deep architecture is then designed by combining two independent LSTM networks with the opposite propagation directions: one transmits information from the front to the back, and another from the back to the front. Thus the deep model can take advantage of the information both before and after the currently analyzing moment to jointly determine the output state. A mean sensitivity of 93.61% and a mean specificity of 91.85% were achieved on a long-term scalp EEG database. The comparisons with other published methods based on either traditional machine learning models or convolutional neural networks demonstrated the improved performance for seizure detection.", 
"Proposed method is a summarization-based hybrid algorithm which comprises a preprocessing phase. Unimportant words which are not frequently used in the document are removed. The proposed method is compared with CSSA, SMTC and Max- capture methods.",Scientific Documents Clustering Based on Text Summarization,"In this paper a novel method is proposed for scientific document clustering. The proposed method is a summarization-based hybrid algorithm which comprises a preprocessing phase. In the summarization phase unimportant words which are not frequently used in the document are removed. This process reduces the amount of data for the clustering purpose. In this proposed method after the preprocessing phase, Term Frequency/Inverse Document Frequency (TFIDF) is calculated for all words in the document and BM25 in caluculated for words in sentences and summed over the document to score each word in document level. In next phase, Text summarization is performed based on BM25 scores. After that document clustering is done according to the scores of calculated TFIDF. The hybrid progress of the proposed scheme, from preprocessing phase to cluster labeling, gains a rapid and efficient clustering method which is evaluated by 400 English texts extracted from scientific articles of 11 different topics. The proposed method is compared with CSSA, SMTC and Max-Capture methods. The results demonstrate the proficiency of the proposed scheme in terms of computation time, and comparative efficiency using F-measure criterion", 
Researcher propose a model of summarizing a single article. Model can be further used to summarize an entire topic. Model is based on analyzing others' viewpoint of target article's contributions.,Scientific Paper Summarization Using Citation Summary Networks,"Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others’ viewpoint of the target article’s contributions and the study of its citation summary network using a clustering approach", 
This article presents the novel concept of Internet-of-Agro-Things (IoAT) with an example of automated plant disease prediction. It consists of solar enabled sensor nodes which help in continuous sensing and automating agriculture. The deployed prototype was deployed for two months and has achieved a robust performance by sustaining in varied weather conditions.,sCrop A Internet-of-Agro-Things (IoAT) Enabled Solar Powered Smart Device for Automatic Plant Disease Prediction,"Internet-of-Things (IoT) is omnipresent, ranging from home solutions to turning wheels for the fourth industrial revolution. This article presents the novel concept of Internet-of-Agro-Things (IoAT) with an example of automated plant disease prediction. It consists of solar enabled sensor nodes which help in continuous sensing and automating agriculture. The existing solutions have implemented a battery powered sensor node. On the contrary, the proposed system has adopted the use of an energy efficient way of powering using solar energy. It is observed that around 80% of the crops are attacked with microbial diseases in traditional agriculture. To prevent this, a health maintenance system is integrated with the sensor node, which captures the image of the crop and performs an analysis with the trained Convolutional Neural Network (CNN) model. The deployment of the proposed system is demonstrated in a real-time environment using a microcontroller, solar sensor nodes with a camera module, and an mobile application for the farmers visualization of the farms. The deployed prototype was deployed for two months and has achieved a robust performance by sustaining in varied weather conditions and continued to remain rust-free. The proposed deep learning framework for plant disease prediction has achieved an accuracy of 99.2% testing accuracy.", 
"Supporting vehicle generated data traffic will become extremely challenging in 5G-based vehicular networks. This is mainly due to the high mobility of vehicles on the road and the high complexity of 5G HetNets. An SDN enabled 5G VANET is proposed in this article, where neighboring vehicles are clustered adaptively according to real-time road conditions.",SDN Enabled 5G-VANET Adaptive Vehicle Clustering and Beamformed Transmission for Aggregated Traffc,"With the anticipated arrival of autonomous vehicles, supporting vehicle generated data traffic due to the dramatically increased use of in-vehicle mobile Internet access will become extremely challenging in 5G-based vehicular networks. This is mainly due to the high mobility of vehicles on the road and the high complexity of 5G HetNets. In order to support the increasing traffic and improve HetNet management, an SDN enabled 5G VANET is proposed in this article, where neighboring vehicles are clustered adaptively according to real-time road conditions using SDN’s global information gathering and network control capabilities. With proposed dual cluster head design and dynamic beamforming coverage, both trunk link communication quality and network robustness of vehicle clusters are significantly enhanced. Furthermore, an adaptive transmission scheme with selective modulation and power control is proposed to improve the capacity of the trunk link between the cluster head and base station. With cooperative communication between the mobile gateway candidates, the latency of traffic aggregation and distribution is also reduced. Computer simulation results show that the proposed design substantially improved 5G users’ bit error rate and trunk link throughput rate.", 
Self-driving vehicles which would shoulder the burden of driving and set free human on board are gradually becoming a reality. Supporting of growing in-vehicle data traffic will be challenging in future 5G and vehicular networks. High mobility nature of vehicles and the densified irregular distribution on road especially during rush time.,SDN Enabled Dual Cluster Head Selection and Adaptive Clustering in 5G-VANET,"Nowadays, self-driving vehicles which would shoulder the burden of driving and set free human on board are gradually becoming a reality. Consequently, the supporting of growing in-vehicle data traffic will be challenging in future 5G and vehicular networks, due to the high mobility nature of vehicles and the densified irregular distribution on road especially during rush time. Therefore in this paper, a Software-Defined Networking (SDN) enabled integrated 5G-VANET architecture is proposed to improve heterogeneous network (HetNet) management and aggregate vehicle traffic through IEEE 802.11p; a novel vehicle clustering method and dual cluster head design are then introduced to reduce signaling overhead and enhance the overall communication quality in 5G-VANET HetNet under the coordination of SDN. It is also proved by simulation that the proposed design reduced 5G users’ blocking probability to the operators services with a back-up cluster head (CH) in each cluster, and also realized adaptive clustering without excessive SDN’s processing delay", 
"Vehicular ad-Hoc Networks (VANETs) have been promoted as a key technology that can provide a wide variety of services. They are proposed to be part of the upcoming Fifth Generation (5G) technology, integrated with Software Defined Networking (SDN)",SDN VANETs in 5G An architecture for resilient security services,"Vehicular ad-Hoc Networks (VANETs) have been promoted as a key technology that can provide a wide variety of services such as traffic management, passenger safety, as well as travel convenience and comfort. VANETs are now proposed to be part of the upcoming Fifth Generation (5G) technology, integrated with Software Defined Networking (SDN), as key enabler of 5G. The technology of fog computing in 5G turned out to be an adequate solution for faster processing in delay sensitive application, such as VANETs, being a hybrid solution between fully centralized and fully distributed networks. In this paper, we propose a three-way integration between VANETs, SDN, and 5G for a resilient VANET security design approach, which strikes a good balance between network, mobility, performance and security features. We show how such an approach can secure VANETs from different types of attacks such as Distributed Denial of Service (DDoS) targeting either the controllers or the vehicles in the network, and how to trace back the source of the attack. Our evaluation shows the capability of the proposed system to enforce different levels of real-time user-defined security, while maintaining low overhead and minimal configuration.", 
"The SEAL model achieves state-of-the-art results on existing long-form summarization tasks, and outperforms strong baseline models on a new dataset/task we introduce, Search2Wiki. Since content selection is explicit in the SEAL model, a desirable side effect is that the selection can be inspected for enhanced interpretability.","SEAL, Segment-wise Extractive-Abstractive Long-form Text Summarization","Most prior work in the sequence-to-sequence paradigm focused on datasets with input sequence lengths in the hundreds of tokens due to the computational constraints of common RNN and Transformer architectures. In this paper, we study long-form abstractive text summarization, a sequence-to-sequence setting with input sequence lengths up to 100,000 tokens and output sequence lengths up to 768 tokens. We propose SEAL, a Transformer-based model, featuring a new encoder-decoder attention that dynamically extracts/selects input snippets to sparsely attend to for each output segment. Using only the original documents and summaries, we derive proxy labels that provide weak supervision for extractive layers simultaneously with regular supervision from abstractive summaries. The SEAL model achieves state-of-the-art results on existing long-form summarization tasks, and outperforms strong baseline models on a new dataset/task we introduce, Search2Wiki, with much longer input text. Since content selection is explicit in the SEAL model, a desirable side effect is that the selection can be inspected for enhanced interpretability.", 
"Deep neural networks have been used for remarkable success in text summarization. There is no clear understanding of why they perform so well, or how they might be improved. This paper seeks to better understand how neural extractive summarization systems could benefit.","Searching for Effective Neural Extractive Summarization, What Works and What's Next","The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 and our project homepage2", 
"Several crop-related diseases affect the productivity in the agriculture sector. Researchers had already used many techniques for this purpose, but some vision-related techniques are yet to be explored. This research paper provides a novel technique to detect crop diseases with the help of convolutional encoder networks using crop leaf images. Different convolution filters like 2×2 and 3×3 are used in proposed work.",Seasonal Crops Disease Prediction and Classification Using Deep Convolutional Encoder Network,"Agriculture plays a significant role in the growth and development of any nation’s economy. But, the emergence of several crop-related diseases affects the productivity in the agriculture sector. To cope up this issue and to make aware the farmers to prevent the expansion of diseases in crops and to implement effective management, crop disease diagnosis plays its significant role. Researchers had already used many techniques for this purpose, but some vision-related techniques are yet to be explored. Commonly used techniques are support vector machine, k-means clustering, radial basis functions, genetic algorithm, image processing techniques like filtering and segmentation, deep structured learning techniques like convolutional neural network. We have designed a hybrid approach for detection of crop leaf diseases using the combination of convolutional neural networks and autoencoders. This research paper provides a novel technique to detect crop diseases with the help of convolutional encoder networks using crop leaf images. We have obtained our result over a 900-image dataset, out of which 600 constitute the training set and 300 test set. We have considered 3 crops and 5 kinds of crop diseases. The proposed network was trained in such a way that it can distinguish the crop disease using the leaf images. Different convolution filters like 2×2 and 3×3 are used in proposed work. It was observed that the proposed architecture achieved variation in accuracy for the different number of epochs and for different convolution filter size. We reached 97.50% accuracy for 2×2 convolution filter size in 100 epochs, while 100% accuracy for 3×3 filter size which is better than other conventional methods.", 
"Security of Internet of Things (IoT) devices has been receiving a lot of attention in recent years. We investigate the potential of machine learning techniques in enhancing the security of IoT devices. We focus on the deployment of supervised, unsupervised learning techniques, and reinforcement learning for both host?based and network-based security.",Securing Internet of Things (IoT) with machine learning,"Advances in hardware, software, communication, embedding computing technologies along with their decreasing costs and increasing performance have led to the emergence of the Internet of Things (IoT) paradigm. Today, several billions of Internet?connected devices are part of the IoT ecosystem. IoT devices have become an integral part of the information and communication technology (ICT) infrastructure that supports many of our daily activities. The security of these IoT devices has been receiving a lot of attention in recent years. Another major recent trend is the amount of data that is being produced every day which has reignited interest in technologies such as machine learning and artificial intelligence. We investigate the potential of machine learning techniques in enhancing the security of IoT devices. We focus on the deployment of supervised, unsupervised learning techniques, and reinforcement learning for both host?based and network?based security solutions in the IoT environment. Finally, we discuss some of the challenges of machine learning techniques that need to be addressed in order to effectively implement and deploy them so that they can better protect IoT devices.", 
Vehicular Ad hoc Networks (VANETs) and Software Defined Networking (SDN) represent the critical enablers of 5G technology with the development of next-generation intelligent vehicular networks and applications. The security and robustness of the complete architecture is still questionable and have been largely neglected by the research community. The deployment and integration of different entities and several architectural components drive new security threats and vulnerabilities.,Security and design requirements for software-defined VANETs,"The evolving of Fifth Generation (5G) networks is becoming more readily available as a significant driver of the growth of new applications and business models. Vehicular Ad hoc Networks (VANETs) and Software Defined Networking (SDN) represent the critical enablers of 5G technology with the development of next-generation intelligent vehicular networks and applications. In recent years, researchers have focused on the integration of SDN and VANET, and looked at different topics related to the architecture, the benefits of software-defined VANET services, and the new functionalities to adapt them. However, the security and robustness of the complete architecture is still questionable and have been largely neglected by the research community. Moreover, the deployment and integration of different entities and several architectural components drive new security threats and vulnerabilities. In this paper, first, we survey the state-of-the-art SDN based Vehicular ad-hoc Network (SDVN) architectures for their networking infrastructure design, functionalities, benefits, and challenges. Then we discuss these architectures against major security threats that violate the key security services such as availability, privacy, authentication, and data integrity. We also discuss different countermeasures for these threats. Finally, we present the lessons learned with the directions of future research work towards provisioning stringent security solutions in new SDVN architectures. To the best of our knowledge, this is the first work that presents a comprehensive survey and security analysis on SDVN architectures, and we believe that it will help researchers to address various challenges (e.g., flexible network management, control and high resource utilization, and scalability) in vehicular communication systems which are required to improve the future Intelligent Transportation Systems (ITS).", 
Short-range communication (DSRC) applications are being developed to prevent automobile accidents. This study illustrates security evaluation of a DSRC wireless application in vehicular environments. We set up a simulation of a working road safety unit (RSU) on real DSRC devices. We performed Denial of Service attacks to determine how few packets need to be dropped to cause automobile crashes.,Security Evaluation of a Dedicated Short Range Communications (DS,"Applications using dedicated short-range communication (DSRC) are being developed to prevent automobile accidents. Many DSRC implementations, applications and network stacks are not mature. They have not been adequately tested and verified. This study illustrates security evaluation of a DSRC wireless application in vehicular environments (DSRC/WAVE) protocol implementation. We set up a simulation of a working road safety unit (RSU) on real DSRC devices. Our experiments work on the Cohda testbed with DSRC application wsm-channel. We extended the functionality of wsm-channel, an implementation of WAVE short message protocol (WSMP) for broadcasting GPS data in vehicular communications, to broadcast car information and RSU instructions. Next we performed Denial of Service attacks to determine how few packets need to be dropped to cause automobile crashes. Hidden Markov Models (HMM) are constructed using sniffed side channel information, since operational packets would be encrypted. The inferred HMM tracks the protocol status over time. Simulation experiments test the HMM predictions showing that we were able to drop necessary packets using side channels. The attack simulation following timing side-channel worked best to drop necessary packets with 2.5 % false positive rate (FPR) while the attack following size worked with 9.5% FPR.", 
Vehicular Ad-hoc Network(VANET) attained vast interest and research initiatives is also increased due to the range of solutions it can provide. This paper presents the safety challenges and existing threads in the VANET system.,Security Issues inVehicular Ad-hoc Network(VANET),"Over the years, area of Vehicular Ad-hoc Network(VANET) attained vast interest and research initiatives is also increased due to the range of solutions it can provide. Information safety is considered as most critical issue in any network system and it also the case in VANET. In VANETs wireless conversation between automobiles thus attackers breach confidentiality, privacy, and authenticity properties which impact further protection. This paper presents the safety challenges and existing threads in the VANET system. This paper focus on basic security requirements which must be fulfilled for securing VANETs. Different classes of VANET attacks are also discussed in this paper.", 
"VANET a subgroup of mobile ad-hoc network (MANET), refers to a group of intelligent nodes i.e. (vehicles) on the road. These intelligent vehicles interact with one other or with the road side unit (RSU) In VANET messages are conveyed in an open wireless channels.",Security of Vehicular Ad-Hoc Networks (VANET) A survey,"In the previous couple of years, various types of researchers concentrate on Vehicular Ad-hoc networks (VANET) field due to various facilities it provides. VANET a subgroup of mobile ad-hoc network (MANET), refers to a group of intelligent nodes i.e. (vehicles) on the road. These intelligent vehicles interact with one other or with the road side unit (RSU) for providing safer roads and a more efficient driving experience and providing security against attackers. In VANET messages are conveyed in an open wireless channels Security is therefore the most concerning issue in VANET. In this paper several types of the security issues, requirements, attacks, attackers in VANET have been described and some recent solutions to solve the security problems with their advantages and disadvantages have been discussed.", 
"IoT facilitates operations through ubiquitous connectivity by providing Internet access to all the devices with computing capabilities. Security, privacy and trust are still the major concerns for such networks. Lack of enforcement of these requirements introduces non-negligible threats to devices and platforms. This paper presents comparisons of state-of-the-art solutions for IoT which are applicable to security, privacy, and trust.","Security, Privacy and Trust for Smart Mobile-Internet of Things (M-IoT) A Survey","With an enormous range of applications, the Internet of Things (IoT) has magnetized industries and academicians from everywhere. IoT facilitates operations through ubiquitous connectivity by providing Internet access to all the devices with computing capabilities. With the evolution of wireless infrastructure, the focus from simple IoT has been shifted to smart, connected and mobile IoT (M-IoT) devices and platforms, which can enable low-complexity, low-cost and efficient computing through sensors, machines, and even crowdsourcing. All these devices can be grouped under a common term of M-IoT. Even though the positive impact on applications has been tremendous, security, privacy and trust are still the major concerns for such networks and insufficient enforcement of these requirements introduces non-negligible threats to M-IoT devices and platforms. Thus, it is important to understand the range of solutions which are available for providing a secure, privacy-compliant, and trustworthy mechanism for M-IoT. There is no direct survey available, which focuses on security, privacy, trust, secure protocols, physical layer security and handover protections in M-IoT. This paper covers such requisites and presents comparisons of statethe-art solutions for IoT which are applicable to security, privacy, and trust in smart and connected M-IoT networks. Apart from these, various challenges, applications, advantages, technologies, standards, open issues, and roadmap for security, privacy and trust are also discussed in this paper.", 
"The current state-of-the-art is based on exhaustive search. We propose to generate many approximate locations over few and precise object delineations. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image.",Segmentation as Selective Search for Object Recognition,"For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.", 
"Text mining techniques include categorization of text, summarization, topic detection, concept extraction, search and retrieval, document clustering, etc. Text mining can also be employed to detect a document's main topic/theme. Areas of applications for text mining include publishing, media, telecommunications,",Selection criteria for text mining approaches,"Text mining techniques include categorization of text, summarization, topic detection, concept extraction, search and retrieval, document clustering, etc. Each of these techniques can be used in finding some non-trivial information from a collection of documents. Text mining can also be employed to detect a document’s main topic/theme which is useful in creating taxonomy from the document collection. Areas of applications for text mining include publishing, media, telecommunications, marketing, research, healthcare, medicine, etc. Text mining has also been applied on many applications on the World Wide Web for developing recommendation systems. We propose here a set of criteria to evaluate the effectiveness of text mining techniques in an attempt to facilitate the selection of appropriate technique.", 
"One hundred twenty eight channel EEG signal was used in the experiments. Signal was recorded for 40 people, during the process of imagining right and left hand movements. Feature extraction was performed using frequency analysis with a resolution of 1Hz.",Selection of EEG signal features for ERDERS classification using genetic algorithms,"The article presents the use of genetic algorithm (GA) to select and classify ERD/ERS patterns. One hundred twenty eight channel EEG signal was used in the experiments. The signal was recorded for 40 people, during the process of imagining right and left hand movements. Feature extraction was performed using frequency analysis (FFT) with the resolution of 1Hz. So the features were spectral lines associated with particular electrodes. The selection of features, calculated for all people, was made with GA. The fitness function used in GA was EEG signal classification error calculated using LDA classifier and 5-CV test. The average accuracy of the classification for all people in 8-30Hz band was 0.85, while for the top 10 results 0.92.", 
"In this paper, EEG signals of 20 schizophrenic patients and 20 age-matched control participants are analyzed. The objective is to determine the more informative channels and finally distinguishing the two groups. The results show improved accuracy of classification in relatively low computational time with the two-stage feature selection method, says the paper. It also shows that a computationally fast algorithm with excellent classification results is obtained.  ",Selection of relevant features for EEG signal classification of schizophrenic patients,"In this paper, EEG signals of 20 schizophrenic patients and 20 age-matched control participants are analyzed with the objective of determining the more informative channels and finally distinguishing the two groups. For each case, 22 channels of EEG were recorded. A two-stage feature selection algorithm is designed, such that, the more informative channels are first selected to enhance the discriminative information. Two methods, bidirectional search and plus-L minus-R (LRS) techniques are employed to select these informative channels. The interesting point is that most of selected channels are located in the temporal lobes (containing the limbic system) that confirm the neuro-phychological differences in these areas between the schizophrenic and normal participants. After channel selection, genetic algorithm (GA) is employed to select the best features from the selected channels. In this case, in addition to elimination of the less informative channels, the redundant and less discriminant features are also eliminated. A computationally fast algorithm with excellent classification results is obtained. Implementation of this efficient approach involves several features including autoregressive (AR) model parameters, band power, fractal dimension and wavelet energy. To test the performance of the final subset of features, classifiers including linear discriminant analysis (LDA) and support vector machine (SVM) are employed to classify the reduced feature set of the two groups. Using the bidirectional search for channel selection, a classification accuracy of 84.62% and 99.38% is obtained for LDA and SVM, respectively. Using the LRS technique for channel selection, a classification accuracy of 88.23% and 99.54% is also obtained for LDA and SVM, respectively. Finally, the results are compared and contrasted with two well-known methods namely, the single-stage feature selection (evolutionary feature selection) and principal component analysis (PCA)-based feature selection. The results show improved accuracy of classification in relatively low computational time with the two-stage feature selection.", 
"Model consists of a sentence encoder, a selective gate network, and an attention equipped decoder. It is built with recurrent neural networks. Second level representation is tailored for sentence summarization task, which leads to better performance.",Selective Encoding for Abstractive Sentence Summarization,"We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-ofthe-art baseline models.", 
This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. We show that our selective search enables the use of the powerful Bag-of-Words model for recognition.,Selective Search for Object Recognition,"This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition.", 
"Authors: reliance on road side units or centralized trusted authority nodes to provide security is critical. They introduce a self-organized secure framework, deployed in vehicular ad hoc networks. Their approach combines the useful features of both Shamir secret sharing with a trust-based technique to ensure continuity of achieving all security services.",Self-organized secure framework for VANET,"While authentication is a necessary requirement to provide security in vehicular ad hoc networks, user's personal information such as identity and location must be kept private. The reliance on road side units or centralized trusted authority nodes to provide security services is critical because both are vulnerable, thus cannot be accessed by all users, which mean security absence. In this paper, we introduce a self-organized secure framework, deployed in vehicular ad hoc networks. The proposed framework solution is designed not only to provide an effective, integrated security and privacy-preserving mechanism but also to retain the availability of all security services even if there are no road side units at all and/or the trusted authority node is compromised. A decentralized tier-based security framework that depends on both trusted authority and some fully trusted nodes cooperated to distribute security services is presented. Our approach combines the useful features of both Shamir secret sharing with a trust-based technique to ensure continuity of achieving all security services. Mathematical analysis of security issues that the proposed framework achieves as well as the availability of offering security services is provided. Proposed framework examination was done to show the performance in terms of storage, computation complexity, and communication overhead as well as its resilience against various types of attacks. Comparisons with different types of security schemes showed that the protocol developed gave better results in most comparison parameters while being unique ensuring continuity of security services delivery.", 
Converting English sentences into expressions or Interlingua is called Universal Networking Language (UNL) UNL represents knowledge in the form of graphical format. nodes represent concepts and links represent relations between concepts. According to the highest weight age the document is summarized.,Semantic based text summarization using universal networking language,"Text Summarization is extracting the important information from the document by leaving out the irrelevant information, and to reduce the details and collects them in a compressed way. Normally text summarization is done in single or multi documents. The advantage on processing time can be achieved in the text summarization. Converting English sentences into expressions or Interlingua is called Universal Networking Language (UNL). The given source document is preprocessed by eliminating tables and images. The preprocessed document is fed into sentence splitter and then to word separator. The given word is sent to Morphological Analyzer to find the root word. This root word is fed into UNL dictionary for finding the corresponding concepts and attributes. By using the heuristic rules, we identify the relations between concepts. UNL represents knowledge in the form of graphical format, where nodes represent concepts and links represent relations between concepts. It represents the whole document not the sentences in particular. The graph algorithm is used to find the weight age of links connected to the Universal Word. According to the highest weight age the document is summarized.", 
"If two fragments of source code are identical to each other, they are called code clones. Code clones introduce difficulties in software maintenance and cause bug propagation. We present a machine learning framework to automatically detect clones in software.",Semantic Clone Detection Using Machine Learning,"If two fragments of source code are identical to each other, they are called code clones. Code clones introduce difficulties in software maintenance and cause bug propagation. In this paper, we present a machine learning framework to automatically detect clones in software, which is able to detect Types-3 and the most complicated kind of clones, Type-4 clones. Previously used traditional features are often weak in detecting the semantic clones The novel aspects of our approach are the extraction of features from abstract syntax trees (AST) and program dependency graphs (PDG), representation of a pair of code fragments as a vector and the use of classification algorithms. The key benefit of this approach is that our tool can find both Syntactic and Semantic clones extremely well. Our evaluation indicates that using our new AST and PDG features is a viable methodology, since they improve detecting clones on the IJaDataset2.0.", 
Automatic text summarization can be defined as a process of extracting and describing important information from given document using computer algorithms. Researchers propose an approach for summarizing Hindi text based on semantic graph of the document using Particle Swarm Optimization (PSO) algorithm.,Semantic Graph Based Automatic Text Summarization for Hindi Documents Using Particle Swarm Optimization,"Automatic text summarization can be defined as a process of extracting and describing important information from given document using computer algorithms. A number of techniques have been proposed by researchers in the past for summarization of English text. Automatic summarization of Indian text has received a very little attention so far. In this paper, we propose an approach for summarizing Hindi text based on semantic graph of the document using Particle Swarm Optimization (PSO) algorithm. PSO is one of the most powerful bio-inspired algorithms used to obtain optimal solution. The subject-object-verb (SOV) triples are extracted from the document. These triples are used to construct semantic graph of the document. A classifier is trained using PSO algorithm which is then used to generate semantic sub-graph and to obtain document summary.", 
"Text Summarization helps users to manage the vast amount of information available, by condensing documents' content. The approach summaries the input document by creating a rich semantic graph for the original document, reducing the generated graph, and then generating the abstractive summary.",Semantic graph reduction approach for abstractive Text Summarization,"One of the important Natural Language Processing applications is Text Summarization, which helps users to manage the vast amount of information available, by condensing documents’ content and extracting the most relevant facts or topics included. Text Summarization can be classified according to the type of summary: extractive, and abstractive. Extractive summary is the procedure of identifying important sections of the text and producing them verbatim while abstractive summary aims to produce important material in a new generalized form. In this paper, a novel approach is presented to create an abstractive summary for a single document using a rich semantic graph reducing technique. The approach summaries the input document by creating a rich semantic graph for the original document, reducing the generated graph, and then generating the abstractive summary from the reduced graph. Besides, a simulated case study is presented to show how the original text was minimized to fifty percent.", 
"Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification. The system is able to localize segment boundaries at a level of accuracy which is beyond previous methods.",SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS,"Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.", 
This paper introduces a sentence to vector encoding framework suitable for advanced natural language processing. We demonstrate the application of the sentence representations for two different tasks – sentence paraphrasing and paragraph summarization.,Semantic sentence embeddings for paraphrasing and text summarization,"This paper introduces a sentence to vector encoding framework suitable for advanced natural language processing. Our latent representation is shown to encode sentences with common semantic information with similar vector representations. The vector representation is extracted from an encoderdecoder model which is trained on sentence paraphrase pairs. We demonstrate the application of the sentence representations for two different tasks – sentence paraphrasing and paragraph summarization, making it attractive for commonly used recurrent frameworks that process text. Experimental results help gain insight how vector representations are suitable for advanced language embedding.", 
"Proposed mechanism summarizes the text, based on ontology, and from the obtained summary, semantic similarity is calculated using word net 3.0. The hash value is then calculated using the winnowing algorithm. This hash value of the document is matched with others using the Dice coefcient to calculate the similarity index. Based on the threshold chosen for similarity, the documents are treated either as novel or not.",Semantic similarity and text summarization based novelty detection,"Current web crawlers search the queries at very high speed, but the problem of novelty detection or redundant information still persists. It consumes precious time and memory of users in search of the new document over the internet. In this paper, an innovative novelty detection mechanism is proposed, which can be appended with the current web crawlers. The proposed mechanism frst summarizes the text, based on ontology, and from the obtained summary, semantic similarity is calculated using word net 3.0. The hash value is then calculated using the winnowing algorithm. This hash value of the document is matched with others using the Dice coefcient to calculate the similarity index. Based on the threshold chosen for similarity, the document is treated either as novel or not. This proposed mechanism is implemented using SQL as backend and visual studio-2012 as frontend. The results show that the projected strategy not only reduces memory consumption but also decreases the number of resultant documents, hence minimize the user time for searching the data from the obtained results. Also, the proposed approach can be used with other search engines like Google, Yahoo, Bing, and Alta Vista to minimize superfuous documents.", 
"State-of-the-art topic modeling techniques tend to be data intensive, so this paper proposes a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. The results show that our approach produces stable, more interpretable, and more expressive topics.",Semantic topic models for source code analysis,"Topic modeling techniques have been recently applied to analyze and model source code. Such techniques exploit the textual content of source code to provide automated support for several basic software engineering activities. Despite these advances, applications of topic modeling in software engineering are frequently suboptimal. This can be attributed to the fact that current state-of-the-art topic modeling techniques tend to be data intensive. However, the textual content of source code, embedded in its identifiers, comments, and string literals, tends to be sparse in nature. This prevents classical topic modeling techniques, typically used to model natural language texts, to generate proper models when applied to source code. Furthermore, the operational complexity and multi-parameter calibration often associated with conventional topic modeling techniques raise important concerns about their feasibility as data analysis models in software engineering. Motivated by these observations, in this paper we propose a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. Ten software systems from different application domains are used to empirically calibrate and configure the proposed approach. The usefulness of generated topics is empirically validated using human judgment. Furthermore, a case study that demonstrates thet operation of the proposed approach in analyzing code evolution is reported. The results show that our approach produces stable, more interpretable, and more expressive topics", 
Software industry has not adopted continuous use of static architecture conformance checking. One hindrance is the needed mapping from source code elements to elements of the architecture. We present a novel approach of generating dependency and semantic information extracted from an initial set of mapped source code files.,Semi-Automatic Mapping of Source Code Using Naive Bayes,The software industry has not adopted continuous use of static architecture conformance checking. One hindrance is the needed mapping from source code elements to elements of the architecture. We present a novel approach of generating and combining dependency and semantic information extracted from an initial set of mapped source code files. We use this to train a Naive Bayes classifier that is then used to map the remainder of the source code files. We compare this approach with the HuGMe technique on six open source projects with known mappings. We find that our approach provides an average performance improvement of 0.22 and an average precision and recall F1-score improvement of 0.26 in comparison to HuGMe, 
"Automatic text classification technology based on machine learning can classify a large number of natural language documents. However, labeling all data by human is labor intensive and time consuming. In 2012, Li et al. invented a fully automatic categorization approach for text (FACT) based on supervised learning, where no manual labeling efforts are required. The method outperforms the supervised support vector machine in terms of both F1 performance and classification accuracy in most cases.",Semi-Supervised Learning in Large Scale Text Categorization,"The rapid development of the Internet brings a variety of original information including text information, audio information, etc. However, it is difficult to find the most useful knowledge rapidly and accurately because of its huge number. Automatic text classification technology based on machine learning can classify a large number of natural language documents into the corresponding subject categories according to its correct semantics. It is helpful to grasp the text information directly. By learning from a set of hand-labeled documents, we obtain the traditional supervised classifier for text categorization (TC). However, labeling all data by human is labor intensive and time consuming. To solve this problem, some scholars proposed a semi-supervised learning method to train classifier, but it is unfeasible for various kinds and great number of Web data since it still needs a part of hand-labeled data. In 2012, Li et al. invented a fully automatic categorization approach for text (FACT) based on supervised learning, where no manual labeling efforts are required. But automatically labeling all data can bring noise into experiment and cause the fact that the result cannot meet the accuracy requirement. We put forward a new idea that part of data with high accuracy can be automatically tagged based on the semantic of category name, then a semi-supervised way is taken to train classifier with both labeled and unlabeled data, and ultimately a precise classification of massive text data can be achieved. The empirical experiments show that the method outperforms the supervised support vector machine (SVM) in terms of both F1 performance and classification accuracy in most cases. It proves the effectiveness of the semi-supervised algorithm in automatic TC.", 
"In this paper, we use important features based on fuzzy logic to extract the sentences. The efficiency of the technique used for scoring the text sentences could produce good summary. The results show that the highest average precision, recall and F-measure for the summaries were obtained from fuzzy method.",Sentence Features Fusion for Text Summarization Using Fuzzy Logic,"The scoring mechanism of the text features is the unique way for determining the key ideas in the text to be presented as text summary. The efficiency of the technique used for scoring the text sentences could produce good summary. The feature scores are imprecise and uncertain, this marks the differentiation between the important features and unimportant is difficult task. In this paper, we introduce fuzzy logic to deal with this problem. Our approach used important features based on fuzzy logic to extract the sentences. In our experiment, we used 30 test documents in DUC2002 data set. Each document is prepared by preprocessing process: sentence segmentation, tokenization, removing stop word, and word stemming. Then, we use 9 important features and calculate their score for each sentence. We propose a method using fuzzy logic for sentence extraction and compare our results with the baseline summarizer and Microsoft Word 2007 summarizers. The results show that the highest average precision, recall, and F-measure for the summaries were obtained from fuzzy method.", 
"Preprocessing, term selection, term weighting, sentence weighting and sentence selection are the main issues in generating extractive summaries. In this paper, a method to optimize the combination of previous relevant features in each step based on a genetic algorithm is presented.",Sentence features relevance for extractive text summarization using genetic algorithms,"Preprocessing, term selection, term weighting, sentence weighting, and sentence selection are the main issues in generating extractive summaries of text sentences. Although many outstanding related works only are focused in the last step, they show sophisticated features in each one. In order to determine the relevance of the sentences (sentence selection step) many sentence features have been proposed in this task (in fact, these features are related to all the steps). Recently, some good related works have coincided in the same features but they present different ways for weighting these features. In this paper, a method to optimize the combination of previous relevant features in each step based on a genetic algorithm is presented. The proposed method not only outperforms previous related works in two standard document collections, but also shows the relevance of these features to this problem.", 
"Text summarization is one of the most challenging applications in the field of NLP. Identifying the degree of relationship among input sentences will help to reduce the inclusion of insignificant sentences in summarized text. In this paper, the main objective was tried to measure sentence similarities. Our proposed methods were extensively tested by using several English and Bengali texts.",Sentence Similarity Estimation for Text Summarization Using Deep Learning,"One of the key challenges of natural language processing (NLP) is to identify the meaning of any text. Text summarization is one of the most challenging applications in the field of NLP where appropriate analysis is needed of given input text. Identifying the degree of relationship among input sentences will help to reduce the inclusion of insignificant sentences in summarized text. Result of summarized text always may not identify by optimal functions, rather a better summarized result could be found by measuring sentence similarities. The current sentence similarity measuring methods only find out the similarity between words and sentences. These methods state only syntactic information of every sentence. There are two major problems to identify similarities between sentences. These problems were never addressed by previous strategies provided the ultimate meaning of the sentence and added the word order, approximately. In this paper, the main objective was tried to measure sentence similarities, which will help to summarize text of any language, but we considered English and Bengali here. Our proposed methods were extensively tested by using several English and Bengali texts, collected from several online news portals, blogs, etc. In all cases, the proposed sentence similarity measures mentioned here was proven effective and satisfactory.", 
"Text summarization is a procedure for making a packed form of a particular document that gives the users utilizable info. In text summarization, resemblance among several sentences in a text has a major role. This paper seeks to investigate different techniques of automatic summarization.",Sentence Similarity Techniques for Automatic Text Summarization,"The technology of summarizing documents automatically is increasing rapidly and may give an answer for the information overload quandary. These days, document summarization is assumed an imperative part of information retrieval. With expansive amounts of documents, giving the user a short version of every document incredibly encourages the errand of discovering required documents. Text summarization is a procedure for making a packed form of a particular document that gives the users utilizable info, and summarization of multi document is engender summary distributing the meaning of the most info either explicitly or implicitly from a group of documents about main topic. In text summarization, resemblance among several sentences in a text has a major role. As such, development of methods of summarization has taken into consideration the aspect of similarities between several sentences in a text. This paper seeks to investigate different techniques of automatic summarization based on the element of sentence resemblance. Comparison is also developed for functionalities of various techniques with respect to recall, precision and F-measure values.", 
This paper presents sentence extraction-based automatic speech summarization techniques for making abstracts from spontaneous presentations. We propose a summarization technique using dimension reduction based on singular value decomposition which focuses on the most salient topics of each presentation. We also investigate the combination of confidence measure and linguistic likelihood to effectively extract sentences with less recognition error.,Sentence-extractive automatic speech summarization and evaluation techniques,"This paper presents sentence extraction-based automatic speech summarization techniques for making abstracts from spontaneous presentations. We propose a summarization technique using dimension reduction based on singular value decomposition which effectively focuses on the most salient topics of each presentation. With this technique, sentence location information, which is used for text summarization, is combined to extract important sentences from the introduction and conclusion segments of each presentation. We also investigate the combination of confidence measure and linguistic likelihood to effectively extract sentences with less recognition error. Experimental results show that the dimension-reduction-based method incorporating sentence location information, the confidence measure, and linguistic likelihood achieves the best automatic speech summarization performance in the condition of 10% summarization ratio. This paper also presents objective methods for evaluating automatic speech summarization methods. The correlation analysis between subjective and objective evaluation scores confirms that summarization accuracy, sentence F-measure, and 2 and 3-gram recall are the most effective among the objective evaluation metrics investigated in this paper", 
"In this paper, we propose a new adversarial training framework for text summarization task. We model abstractive summarization using Generative Adversarial Network (GAN) We aim to minimize the gap between generated summaries and the ground-truth ones. Experimental results showed that our model significantly outperforms the state-of-the-art models, especially on long text corpus.",Sequence Generative Adversarial Network for Long Text Summarization,"In this paper, we propose a new adversarial training framework for text summarization task. Although sequence-tosequence models have achieved state-of-the-art performance in abstractive summarization, the training strategy (MLE) suffers from exposure bias in the inference stage. This discrepancy between training and inference makes generated summaries less coherent and accuracy, which is more prominent in summarizing long articles. To address this issue, we model abstractive summarization using Generative Adversarial Network (GAN), aiming to minimize the gap between generated summaries and the ground-truth ones. This framework consists of two models: a generator that generates summaries, a discriminator that evaluates generated summaries. Reinforcement learning (RL) strategy is used to guarantee the co-training of generator and discriminator. Besides, motivated by the nature of summarization task, we design a novel Triple-RNNs discriminator, and extend the off-the-shelf generator by appending encoder and decoder with attention mechanism. Experimental results showed that our model significantly outperforms the state-of-the-art models, especially on long text corpus", 
"In this work, we cast text summarization as a sequence-to-sequence problem. We apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation. We",Sequence-to-Sequence RNNs for Text Summarization,"In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)). Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et al. (2015) on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance.", 
"Location-based services (LBS) make driving more convience for drivers, but they may leak user's location information. heterogeneous data sensed by vehicles also increases the complexity of application development, authors say. Authors propose context-based location privacy protection middleware architecture, named PP-OSGi.",Service Recommendation Middleware Based on Location Privacy Protection in VANET,"With the help of location-based services (LBS), it makes driving more convience for drivers. However, because the untrusted LBS server may leak the user’s location information, the user’s privacy is threatened. Moreover, the existing methods of location privacy protection do not take into account the impact of context on privacy protection demand. In addition, heterogeneous data sensed by vehicles also increases the complexity of application development. In order to solve the above problems, we propose a context-based location privacy protection middleware architecture, named PP-OSGi. The middleware simplifies application development by shielding the heterogeneity of vehicle sensed data and upper-layer applications. Furthermore, in order to protect the real location information of service request vehicles under different vehicle densities in a certain area, we propose a dynamically adjustable k-anonymous (DAK) algorithm and a location privacy protection (DLP) algorithm based on a dummy location, which are all encapsulated in PP-OSGi. The DAK and DLP algorithms dynamically determine the location privacy protection strength in different contexts based on the user’s location privacy preference model, select an anonymous group of neighboring vehicles to construct a anonymous area, and obtain a dummy location of the service request vehicle. The experimental results show that, under the premise of protecting the location privacy of vehicles, the success rate of service requests is improved and the communication cost between vehicles is reduced.", 
Internet of vehicles (IoV) is an emerging paradigm for accommodating the requirements of future intelligent transportation systems (ITSs) In this paper we introduce the software-defined IoV (SD-Iov) which is able to tackle the above-mentioned issues. We present the architecture and develop a centralized vehicular connection management approach.,Service-Oriented Dynamic Connection Management for Software-Defined Internet of Vehicles,"Internet of vehicles (IoV) is an emerging paradigm for accommodating the requirements of future intelligent transportation systems (ITSs) with the overwhelming trend of equipping vehicles with versatile sensors and communications modules, and facilitating drivers and passengers with a variety of innovative ITS applications. However, the implementation of IoV still faces many challenges, such as flexible and efficient connections, quality of service guarantee, and multiple concurrent support requests. To this end, in this paper we introduce the software-defined IoV (SD-IoV), which is able to tackle the above-mentioned issues by adopting the software-defined networking framework. We first present the architecture of SD-IoV and develop a centralized vehicular connection management approach. Then, we aim to allocate dedicated communications resources and underlying vehicular nodes to satisfy each service. We formulate the dynamic vehicular connection as an overlay vehicular network creation (OVNC) problem. A comprehensive utility function is also designed to serve as the optimization objective of OVNC. Finally, we solve the OVNC problem by developing a graph-based genetic algorithm and a heuristic algorithm, respectively. Extensive simulation results are provided to demonstrate the effectiveness of our proposed solution of dynamic vehicular connection management.", 
New algorithm for cross- domain classification in aerial vehicle images based on generative adversarial networks (GANs) Siamese-GAN learns categorical representations for labeled and unlabeled images coming from two different domains. Experiments on several cross-domain datasets composed of extremely high resolution (EHR) images acquired by manned/unmanned aerial vehicles.,Siamese-GAN Learning Invariant Representations for Aerial Vehicle Image Categorization,"In this paper, we present a new algorithm for cross-domain classification in aerial vehicle images based on generative adversarial networks (GANs). The proposed method, called Siamese-GAN, learns invariant feature representations for both labeled and unlabeled images coming from two different domains. To this end, we train in an adversarial manner a Siamese encoder–decoder architecture coupled with a discriminator network. The encoder–decoder network has the task of matching the distributions of both domains in a shared space regularized by the reconstruction ability, while the discriminator seeks to distinguish between them. After this phase, we feed the resulting encoded labeled and unlabeled features to another network composed of two fully-connected layers for training and classification, respectively. Experiments on several cross-domain datasets composed of extremely high resolution (EHR) images acquired by manned/unmanned aerial vehicles (MAV/UAV) over the cities of Vaihingen, Toronto, Potsdam, and Trento are reported and discussed.", 
"Noise or irrelevant components can affect signal classification accuracy. In signal de-noising, the performance of any statistical method may be impacted by the noise. Authors propose the multi-scale principal component analysis (PCA) method for de- noising complex biomedical signals.",Signal Decomposition by Multi-scale PCA and Its Applications to Long-term EEG Signal Classifcation,"Data coming from a real-world complex system are usually contaminated by certain levels of noise or some irrelevant components, which do not contribute to improve signal classification accuracy. Also in signal de-noising, the performance of any statistical method used to recover the original signals may be impacted by the noise. In this paper, we propose the multi-scale principal component analysis (PCA) method, which combines discrete wavelet transform and PCA for de-noising and decomposing complex biomedical signals in both spatial and temporal domains for signal classification. We also develop a new classification method, called Empirical Classification (EC), based on the characteristics of data we analyzed. These methods were applied to a publicly available EEG database for the purpose of epilepsy diagnosis and epileptic seizure detection. Our study shows that signal decomposition by the multi-scale PCA method coupled with the EC method, leads to a highly promising classification accuracy in classifying epileptic EEG signals. Our methods are also applicable for classifying biomedical images.", 
"Common spatial pattern (CSP) is known to be effective in EEG signal classification for motor imagery based brain computer interface (MI-BCI) To achieve accurate classification in CSP, it is necessary to find frequency bands that relate to brain activities associated with BCI tasks. We propose discriminative filter bank CSP (DFBCSP) that designs finite impulse response filters.",Simultaneous Design of FIR Filter Banks and Spatial Patterns for EEG Signal Classification,"The spatial weights for electrodes called common spatial pattern (CSP) are known to be effective in EEG signal classification for motor imagery based brain computer interface (MI-BCI). To achieve accurate classification in CSP, it is necessary to find frequency bands that relate to brain activities associated with BCI tasks. Several methods that determine such a set of frequency bands have been proposed. However, the existing methods cannot find the multiple frequency bands by using only learning data. To address this problem, we propose discriminative filter bank CSP (DFBCSP) that designs finite impulse response filters and the associated spatial weights by optimizing an objective function which is a natural extension of that of CSP. The optimization is conducted by sequentially and alternatively solving subproblems into which the original problem is divided. By experiments, it is shown that DFBCSP can effectively extract discriminative features for MI-BCI. Moreover, experimental results exhibit that DFBCSP can detect and extract the bands related to brain activities of motor imagery.", 
This paper presents an extraction based single document text summarization technique using Genetic Algorithms. A given document is represented as a weighted Directed Acyclic Graph. A fitness function is defined to mathematically express the quality of a summary.,Single document extractive text summarization using Genetic Algorithms,"This paper presents an extraction based single document text summarization technique using Genetic Algorithms. A given document is represented as a weighted Directed Acyclic Graph. A fitness function is defined to mathematically express the quality of a summary in terms of some desired properties of a summary, such as, topic relation, cohesion and readability. Genetic Algorithm is designed to maximize this fitness function, and get the corresponding summary by extracting the most important sentences. Results are compared with a couple of other existing text summarization methods keeping the DUC2002 data as benchmark, and using the precision-recall evaluation technique. The initial results obtained seem promising and encouraging for future work in this area.", 
"The paper proposes an extractive text summarization technique for single documents using Neural Networks and Genetic Algorithms. The results are compiled using DUC2002 data as a benchmark. They are compared with techniques using Genetic Algorithm, Neural Network and a summarizer made by Microsoft.",Single Document Extractive Text Summarization Using Neural Networks and Genetic Algorithm,"The presented paper proposes an extractive text summarization technique for single documents using Neural Networks and Genetic Algorithms. The Neural Network helps to define a fitness function to express mathematically the quality of the generated summary through six desired properties which are theme similarity, cohesion, sentiment, readability, aggregate similarity and sentence position. Genetic Algorithm maximizes the above-mentioned fitness function, and extracts the most important sentences to create the extractive summary. The results are compiled using DUC2002 data as a benchmark and calculated using the precision-recall technique. They are compared with techniques using Genetic Algorithm, Neural Network and a summarizer made by Microsoft. The comparison between the results clearly demonstrates the superiority of the technique and is very encouraging for future work in this area.", 
"An approach for single document summaries based on local topic identification and word frequency is proposed. Documents can be clustered into local topic after sentences similarity is calculated. Then sentences from all local topics are selected by computing the word frequency. Using this proposed method, the information redundancy of each local topic and among local topic is reduced.",Single Document Summarization Based on Local Topic Identification and Word Frequency,"In this task, an approach for single document summaries based on local topic identification and word frequency is proposed. In recent years, there has been increased interest in automatic summarization. The physical features are often used and have been successfully applied to this field; it also has some disadvantages of non-redundancy, structure and coherence. Therefore, we introduced logical structure feature which has been successfully applied in multidocument summarization (MDS), and we designed a system to accomplish this task. Documents can be clustered into local topic after sentences similarity is calculated, which can be sorted by the scoring. Then sentences from all local topics are selected by computing the word frequency. Using this proposed method, the information redundancy of each local topic and among local topic is reduced. The information coverage ratio and structure of the summarization is improved.", 
"Many methods of text summarization combining sentence selection and sentence compression have been proposed. The dependency between words has been used in most of these methods, but not rhetorical structures. In this paper, we use a nested tree, in which nodes in the document tree representing dependency between sentences are replaced by",Single Document Summarization based on Nested Tree Structure,"Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences, i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts.", 
Text summarization is an old challenge in text mining. We extract a set of features from each sentence that helps identify its importance in the document. Clustering approach is useful to decide which type of data present in document.,Single Document Text Summarization Using Clustering Approach Implementing for News Article,"Text summarization is an old challenge in text mining but in dire need of researcher’s attention in the areas of computational intelligence, machine learning and natural language processing. We extract a set of features from each sentence that helps identify its importance in the document. Every time reading full text is time consuming. Clustering approach is useful to decide which type of data present in document. In this paper we introduce the concept of k-mean clustering for natural language processing of text for word matching and in order to extract meaningful information from large set of offline documents, data mining document clustering algorithm are adopted.", 
Extractive text summarization consists of selecting the most important units from the original text. Several interesting automatic approaches are proposed for this task. Some of them are focused on getting a better result rather than giving some assumptions about what humans use.,Single Extractive Text Summarization Based on a Genetic Algorithm,"Extractive text summarization consists in selecting the most important units (normally sentences) from the original text, but it must be done as closer as humans do. Several interesting automatic approaches are proposed for this task, but some of them are focused on getting a better result rather than giving some assumptions about what humans use when producing a summary. In this research, not only the competitive results are obtained but also some assumptions are given about what humans tried to represent in a summary. To reach this objective a genetic algorithm is proposed with special emphasis on the fitness function which permits to contribute with some conclusions.", 
This paper proposes a single document summarization method based on the trimming of a discourse tree. We formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem. We then solve it with integer linear programming (ILP),Single-Document Summarization as a Tree Knapsack Problem,"Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem, a Maximum Coverage Problem or a Budgeted Median Problem. These methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. Thus, summaries generated by these methods may lack logical coherence. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores.", 
In this study we have evaluated the existing methods of automatic document summarization. We propose two approaches in English documents that is based on Latent semantic analysis. We have compared the performance of our systems with existing systems in the literature. The evaluation and comparisons of the summaries are performed with ROUGE-L.,Single-document summarization using latent semantic analysis,"In this study we have evaluated the existing methods of automatic document summarization system and we proposed two approaches in English documents that is based on Latent semantic analysis. Summary selection four existing and two proposed methods for automatic summarization are also used. The evaluated methods that are used include Gong and Liu, Steinberger and Jezek, Murray, Renal & Chaletta, Cross approach and the proposed methods are avesvd and ravesvd. Latent semantic analysis (LSA) is a technique that is used vectorial semantics, for analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA brings out latent relationships within a collection of documents rather than looking at each document isolated from the others. It looks at all the documents as a whole and the terms they contain to identify relationships between them. We have compared the performance of our systems with existing systems in the literature which was developed for this document summarization. The document set used for evaluation of our system is the Document Understanding Conferences (DUC) datasets are document summaries on corpus DUC-2002 and 2004. The evaluation and comparisons of the summaries are performed with ROUGE-L. Our evaluation results need improvement for better results.", 
Electroencephalogram (EEG) signal recorded during motor imagery (MI) has been widely applied in non-invasive brain–computer interface (BCI) We propose a new method based on the deep convolutional neural network (CNN) to perform feature extraction and classification for single-trial MI EEG.,Single-trial EEG classification of motor imagery using deep convolutional neural networks,"Electroencephalogram (EEG) signal recorded during motor imagery (MI) has been widely applied in non-invasive brain–computer interface (BCI) as a communication approach. In this paper, we propose a new method based on the deep convolutional neural network (CNN) to perform feature extraction and classification for single-trial MI EEG. Firstly, based on the spatio-temporal characteristics of EEG, a 5-layer CNN model is built to classify MI tasks (left hand and right hand movement); then the CNN model is applied in the experimental data set collected from subjects, and compared with other three conventional classification methods (power + SVM, CSP + SVM and AR + SVM). The results demonstrate that CNN can further improve classification performance: the average accuracy using CNN (86.41 ± 0.77%) is 9.24%, 3.80% and 5.16% higher than those using power + SVM, CSP + SVM and AR + SVM, respectively. The present study shows that the proposed method is effective to classify MI, and provides a practical method by non-invasive EEG signal in BCI applications.", 
Speller is a BCI application that uses the P300 waveform to essentially allow its user to communicate without using its peripheral nerves. Highest accuracy achieved was of 91.6% for a healthy subject and 88.1% for post-stroke victim.,Single-trial P300 classification using deep belief networks for a BCI system,"A brain-computer interface (BCI) aims to provide its users with the capability to interact with machines only through its brain activity. There is a special interest in developing BCIs targeted at people with mild or severe motor disabilities since this kind of technology would improve their lifestyles. The Speller is a BCI application that uses the P300 waveform to essentially allow its user to communicate without using its peripheral nerves. This paper focuses on the classification of the P300 waveform from single-trials obtained through EEG using deep belief networks (DBNs). This deep learning algorithm can identify relevant features automatically from the subject’s data, making its training requiring less pre-processing stages. The network was tested using signals recorded from healthy subjects and post-stroke victims. The highest accuracy achieved was of 91.6% for a healthy subject and 88.1% for a post-stroke victim", 
"5G networks will pave the way for video distribution over vehicular Networks (VANETs) The increased number of cells makes mobility management schemes a challenging task for 5G VANET. Vehicles frequently switch among different networks, leading to unnecessary handovers. In this sense, an inefficient handover algorithm delivers videos with poor Quality of Experience.",Skipping-based Handover Algorithm for Video Distribution Over Ultra-Dense VANET,"Next-generation networks will pave the way for video distribution over vehicular Networks (VANETs), which will be composed of ultra-dense heterogeneous radio networks by considering existing communication infrastructures to achieve higher spectral efficiency and spectrum reuse rates. However, the increased number of cells makes mobility management schemes a challenging task for 5G VANET, since vehicles frequently switch among different networks, leading to unnecessary handovers, higher overhead, and ping-pong effect. In this sense, an inefficient handover algorithm delivers videos with poor Quality of Experience (QoE), caused by frequent and ping-pong handover that leads to high packets/video frames losses. In this article, we introduce a multi-criteria skipping-based handover algorithm for video distribution over ultra-dense 5G VANET, called Skip-HoVe. It considers a skipping mechanism coupled with mobility prediction, Quality of Service (QoS)- and QoE-aware decision, meaning the handovers are made more reliable and less frequently. Simulation results show the efficiency of Skip-HoVe to deliver videos with Mean Opinion Score (MOS) 30% better compared to state-ofthe-art algorithms while maintaining a ping-pong rate around 2%.", 
"Sleep stage classification refers to identifying the various stages of sleep and is a critical step in an effort to assist physicians in the diagnosis and treatment of related sleep disorders. The limitations of manual sleep stage scoring have escalated the demand for developing Automatic Sleep Stage Classification (ASSC) systems. The proposed methodology achieves an average classification sensitivity, specificity and accuracy of 89.06%, 98.61% and 93.13% respectively.",Sleep Stage Classification Using EEG Signal Analysis-A Comprehensive Survey and New Investigation,"Sleep specialists often conduct manual sleep stage scoring by visually inspecting the patient’s neurophysiological signals collected at sleep labs. This is, generally, a very difficult, tedious and time-consuming task. The limitations of manual sleep stage scoring have escalated the demand for developing Automatic Sleep Stage Classification (ASSC) systems. Sleep stage classification refers to identifying the various stages of sleep and is a critical step in an effort to assist physicians in the diagnosis and treatment of related sleep disorders. The aim of this paper is to survey the progress and challenges in various existing Electroencephalogram (EEG) signal-based methods used for sleep stage identification at each phase; including pre-processing, feature extraction and classification; in an attempt to find the research gaps and possibly introduce a reasonable solution. Many of the prior and current related studies use multiple EEG channels, and are based on 30 s or 20 s epoch lengths which affect the feasibility and speed of ASSC for real-time applications. Thus, in this paper, we also present a novel and efficient technique that can be implemented in an embedded hardware device to identify sleep stages using new statistical features applied to 10 s epochs of single-channel EEG signals. In this study, the PhysioNet Sleep European Data Format (EDF) Database was used. The proposed methodology achieves an average classification sensitivity, specificity and accuracy of 89.06%, 98.61% and 93.13%, respectively, when the decision tree classifier is applied. Finally, our new method is compared with those in recently published studies, which reiterates the high classification accuracy performance.", 
Electroencephalogram (EEG) is a common base signal used to monitor brain activities and diagnose sleep disorders. Manual sleep stage scoring is a time-consuming task for sleep experts. SleepEEGNet is composed of deep convolutional neural networks (CNNs) to extract timeinvariant features and frequency information.,SleepEEGNet Automated sleep stage scoring with sequence to sequence deep learning approach,"Electroencephalogram (EEG) is a common base signal used to monitor brain activities and diagnose sleep disorders. Manual sleep stage scoring is a time-consuming task for sleep experts and is limited by inter-rater reliability. In this paper, we propose an automatic sleep stage annotation method called SleepEEGNet using a single-channel EEG signal. The SleepEEGNet is composed of deep convolutional neural networks (CNNs) to extract timeinvariant features, frequency information, and a sequence to sequence model to capture the complex and long short-term context dependencies between sleep epochs and scores. In addition, to reduce the effect of the class imbalance problem presented in the available sleep datasets, we applied novel loss functions to have an equal misclassified error for each sleep stage while training the network. We evaluated the performance of the proposed method on different single-EEG channels (i.e., Fpz-Cz and Pz-Oz EEG channels) from the Physionet Sleep-EDF datasets published in 2013 and 2018. The evaluation results demonstrate that the proposed method achieved the best annotation performance compared to current literature, with an overall accuracy of 84.26%, a macro F1-score of 79.66% and ? = 0.79. Our developed model can be applied to other sleep EEG signals and aid the sleep specialists to arrive at an accurate diagnosis.", 
There are around 20% to 50% of COVID-19 cases had diabetes across the different regions. This article discusses recommendations and associated risk for diabetic patients to balance glyncemic profile during the outbreak.,Smart Healthcare for Diabetes during COVID-19,The diabetic patients are at higher risk from novel coronavirus disease 2019 (COVID-19) that spreads through Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). There are around 20% to 50% of COVID-19 cases had diabetes across the different regions from the world. This article discusses recommendations and associated risk for diabetic patients to balance glyncemic profile during COVID-19 outbreak. It also discusses the case study of various countries with impact of COVID-19 for diabetic patients. It presents emerging smart healthcare that can potentially safeguard against COVID-19., 
"In recent years, remote heart monitoring systems are developed to harness advanced machine learning methods to identify heart disorders by processing electrocardiogram (ECG) signals. The main goal of this technology is providing quality healthcare for elderly and high-risk heart-patients. The developed methodology is general and applicable to other biomedical signals such as EEG, Pleth, and PPG.",Smart Heart Monitoring Early Prediction of Heart Problems Through Predictive Analysis of ECG Signals,"Health monitoring devices are integral parts of smart health in the era of smart connected communities. In recent years, remote heart monitoring systems are developed to harness advanced machine learning methods to identify heart disorders by processing electrocardiogram (ECG) signals. However, current technologies suffer from two important drawbacks: i) the lack of prediction capacity to predict heart abnormalities ahead of time, and ii) failure in capturing inter-patient variability. In this paper, we propose a novel two-step predictive framework for ECG signal processing, where a global classifier recognizes severe abnormalities (red alarms) by comparing the signal against a universal reference model. The seemingly normal signal samples undergo a subsequent deviation analysis and yellow alarms are called by identifying mild and yet informative signal morphology distortions comparing to the learned patient-specific baseline that can be indicative of upcoming heart conditions. To facilitate an accurate deviation analysis, a controlled nonlinear transformation with optimized parameters is proposed to increase the symmetry of signals for different abnormality classes in the feature space. The proposed method achieves a classification accuracy of 96.6% and provides a unique feature of predictive analysis by generating precaution warning messages about the elevated risk of heart abnormalities to take preventive actions according to physician orders. In particular, the chance of observing a severe problem (in terms of a red alarm) is raised by about 5% to 10% after observing a yellow alarm of the same type. The main goal of this technology is providing quality healthcare for elderly and high-risk heart-patients, however, the developed methodology is general and applicable to other biomedical signals such as EEG, Pleth, and PPG.", 
The transition of transportation sector from Internal Combustion Engines (ICE) to Electric Vehicles (EVs) has raised many concerns about their users. The development of Smart Station Search Assistance (S3A) system is an innovative step in this direction.,Smart Station Search Assistance for Electric Vehicleâ€”A Step Toward Smart City,"The transition of transportation sector from Internal Combustion Engines (ICE) to Electric Vehicles (EVs) has raised many concerns about their users which mainly include locating a suitable charging station for such vehicles. Presently, human intelligence-based methods are being used for the purpose. However, the development of Smart Station Search Assistance (S3A) system is an innovative step in this direction as it provides an optimal solution to locate a charging station as per user requirement subject to vehicle constraints. The use of optimization model in S3A for recommending an optimal charging station service has been justified. The main challenges which can affect the performance of the system have also been discussed.", 
"The Smart Corpus aims to create a smart-contracts' repository where smart contracts data are freely and immediately available and also classified. Smart Corpus can be easily extended, as the number of new smart- Contracts increases day by day. Each contract comes with its own associated software metrics as computed by the freely available software PASO. The Smart Corpus is a Corpus of Smart Contracts in an organized reasoned and up to date repository.",SMART-CORPUS AN ORGANIZED REPOSITORY OF ETHEREUM SMART CONTRACTS SOURCE CODE AND METRICS,"Many empirical software engineering studies show that there is a great need for repositories where source code is acquired, filtered and classified. During the last few years, Ethereum block explorer services have emerged as a popular project to explore and search Ethereum blockchain data such as transactions, addresses, tokens, smart-contracts’ source code, prices and other activities taking place on the Ethereum blockchain. Despite the availability of this kind of services, retrieving specific information useful to empirical software engineering studies, such as the study of smart-contracts’ software metrics might require many sub-tasks, such as searching specific transactions in a block, parsing files in HTML format and filtering the smart-contracts to remove duplicated code or unused smart-contracts. In this paper we afford this problem creating Smart Corpus’, a Corpus of Smart Contracts in an organized reasoned and up to date repository where Solidity source code and other metadata about Ethereum smart contracts can easily and systematically be retrieved. We present the Smart Corpus’ design and its initial implementation and we show how the data-set of smart contracts’ source code in a variety of programming languages can be queried and processed, get useful information on smart contracts and their software metrics. The Smart Corpus aims to create a smart-contracts’ repository where smart contracts data (source code, ABI and byte-code) are freely and immediately available and also classified based on the main software metrics identified in the scientific literature. Smart-contracts source code has been validated by EtherScan and each contract comes with its own associated software metrics as computed by the freely available software PASO. Moreover, Smart Corpus can be easily extended, as the number of new smart-contracts increases day by day.", 
Digital evidence can sometimes be difficult to extract from smartphone devices. Prior knowledge of smartphone forensic tools is paramount to a successful investigation. The main objective of this paper is intended to be consciousness-raising than suggesting best practices.,Smartphone Forensic Challenges,"Globally, the extensive use of smartphone devices has led to an increase in storage and transmission of enormous volumes of data that could be potentially be used as digital evidence in a forensic investigation. Digital evidence can sometimes be difficult to extract from these devices given the various versions and models of smartphone devices in the market. Forensic analysis of smartphones to extract digital evidence can be carried out in many ways, however, prior knowledge of smartphone forensic tools is paramount to a successful forensic investigation. In this paper, the authors outline challenges, limitations and reliability issues faced when using smartphone device forensic tools and accompanied forensic techniques. The main objective of this paper is intended to be consciousness-raising than suggesting best practices to these forensic work challenges.", 
"The proliferation of smartphones introduces new opportunities in digital forensics. Smartphones are equipped with sensors which can be used to infer the user's context. This context can aid in the rejection or acceptance of an alibi, or even reveal a suspect's actions or activities.",Smartphone sensor data as digital evidence,"The proliferation of smartphones introduces new opportunities in digital forensics. One of the reasons is that smartphones are usually equipped with sensors (e.g. accelerometer, proximity sensor, etc.), hardware which can be used to infer the user’s context. This context may be useful in a digital investigation, as it can aid in the rejection or acceptance of an alibi, or even reveal a suspect’s actions or activities. Nonetheless, sensor data are volatile, thus are not available in post-mortem analysis. Thus, the only way to timely acquire them, in case such a need arises during a digital investigation, is by software that collects them when they are generated by the suspect’s actions. In this paper we examine the feasibility of ad-hoc data acquisition from smartphone sensors by implementing a device agent for their collection in Android, as well as a protocol for their transfer. Then, we discuss our experience regarding the data collection of smartphone sensors, as well as legal and ethical issues that arise from their collection. Finally, we describe scenarios regarding the agent’s preparation and use in a digital investigation.", 
"The approach searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time. A smooth object is represented by its material appearance and imaged shape. Recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion.",Smooth Object Retrieval using a Bag of Boundaries,"We describe a scalable approach to 3D smooth object retrieval which searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time. The approach is illustrated on sculptures. A smooth object is represented by its material appearance (sufficient for foreground/background segmentation) and imaged shape (using a set of semi-local boundary descriptors). The descriptors are tolerant to scale changes, segmentation failures, and limited viewpoint changes. Furthermore, we show that the descriptors may be vector quantized (into a bag-of-boundaries) giving a representation that is suited to the standard visual word architectures for immediate retrieval of specific objects. We introduce a new dataset of 6K images containing sculptures by Moore and Rodin, and annotated with ground truth for the occurrence of twenty 3D sculptures. It is demonstrated that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion, and also that instances of the same shape can be retrieved even though they may be made of different materials", 
"Traditional summarization research has focused on extracting informative sentences from standard documents. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient algorithm is designed to learn the proposed factor graph model.",Social Context Summarization,"We study a novel problem of social context summarization for Web documents. Traditional summarization research has focused on extracting informative sentences from standard documents. With the rapid growth of online social networks, abundant user generated content (e.g., comments) associated with the standard documents is available. Which parts in a document are social users really caring about? How can we generate summaries for standard documents by considering both the informativeness of sentences and interests of social users? This paper explores such an approach by modeling Web documents and social contexts into a unified framework. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient algorithm is designed to learn the proposed factor graph model. Experimental results on a Twitter data set validate the effectiveness of the proposed model. By leveraging the social context information, our approach obtains significant improvement (averagely +5.0%- 17.3%) over several alternative methods (CRF, SVM, LR, PR, and DocLead) on the performance of summarization.", 
Paper proposes using soft histograms for the Local Binary Pattern (LBP) operator. This makes the operator more robust to noise,SOFT HISTOGRAMS FOR LOCAL BINARY PATTERNS,This paper proposes using soft histograms for the Local Binary Pattern (LBP) operator that has been widely applied in different image analysis tasks. This makes the operator more robust to noise and make its output continuous with respect to input. The proposed extension is shown to significantly enhance the performance of the operator with degraded images., 
"In recent years, there has been a growing interest in applying deep learning techniques to software categorization. We present an approach using a set of low-level features derived from lexical analysis of software code. We evaluate our approach by applying it to categorize popular Python projects from Github.",Software Categorization Using Low-Level Distributional Features,"In recent years, there has been a growing interest in applying deep learning techniques for automatic generation of software. To achieve this ambitious objective, a number of smaller research goals need to be reached, one of which is automatic categorization of software, used in numerous tasks of software intelligence. We present here an approach to this problem using a set of low-level features derived from lexical analysis of software code. We compare different feature sets for categorizing software and also apply different supervised machine learning algorithms to perform the classification task. The representation allows us to identify the most relevant libraries used for each class, and we use the best-performing classifier to accomplish this. We evaluate our approach by applying it to categorize popular Python projects from Github.", 
"This article proposes a software defined spaceair-ground integrated network architecture for supporting diverse vehicular services in a seamless, efficient, and cost-effective manner. The network would protect legacy services in the satellite, aerial, and terrestrial segments.",Software Defned Space-Air-Ground Integrated Vehicular Networks Challenges and Solutions,"This article proposes a software defined spaceair-ground integrated network architecture for supporting diverse vehicular services in a seamless, efficient, and cost-effective manner. First, the motivations and challenges for integration of space-air-ground networks are reviewed. Second, a software defined network architecture with a layered structure is presented. To protect the legacy services in the satellite, aerial, and terrestrial segments, resources in each segment are sliced through network slicing to achieve service isolation. Then available resources are put into a common and dynamic space-air-ground resource pool, which is managed by hierarchical controllers to accommodate vehicular services. Finally, a case study is carried out, followed by discussion on some open research topics.", 
"Existing automatic techniques for design pattern selection are limited to semi-formal specification, multi-class problem, adequate sample size and individual classifier training. We exploit a text categorization based approach via Fuzzy c-means (unsupervised learning technique) that targets to present a systematic way to group the similar design patterns. We also propose a new feature selection method Ensemble-IG to overcome the multi- class problem and improve the classification performance.",Software design patterns classification and selection using text categorization approach,"Numerous software design patterns have been introduced and cataloged either as a canonical or a variant solution to solve a design problem. The existing automatic techniques for design pattern(s) selection aid novice software developers to select the more appropriate design pattern(s) from the list of applicable patterns to solve a design problem in the designing phase of software development life cycle. : However, the existing automatic techniques are limited to the semi-formal specification, multi-class problem, an adequate sample size to make precise learning and individual classifier training in order to determine a candidate design pattern class and suggest more appropriate pattern(s). To address these issues, we exploit a text categorization based approach via Fuzzy c-means (unsupervised learning technique) that targets to present a systematic way to group the similar design patterns and suggest the appropriate design pattern(s) to developers related to the specification of a given design problem. We also propose an evaluation model to assess the effectiveness of the proposed approach in the context of several real design problems and design pattern collections. Subsequently, we also propose a new feature selection method Ensemble-IG to overcome the multi-class problem and improve the classification performance of the proposed approach.", 
"Study the possibilities to track provenance of software source code artifacts within the largest publicly accessible corpus of publicly available source code, the Software Heritage archive. Over 4 billion unique source code files and 1 billion commits capture their development histories across 50 million software projects.",Software Provenance Tracking at the Scale of Public Source Code,"We study the possibilities to track provenance of software source code artifacts within the largest publicly accessible corpus of publicly available source code, the Software Heritage archive, with over 4 billions unique source code files and 1 billion commits capturing their development histories across 50 million software projects. We perform a systematic and generic estimate of the replication factor across the different layers of this corpus, analysing how much the same artifacts (e.g., SLOC, files or commits) appear in different contexts (e.g., files, commits or source code repositories). We observe a combinatorial explosion in the number of identical source code files across different commits. To discuss the implication of these findings, we benchmark different data models for capturing software provenance information at this scale, and we identify a viable solution, based on the properties of isochrone subgraphs, that is deployable on commodity hardware, is incremental and appears to be maintainable for the foreseeable future.", 
"Connected vehicles have the potential to further improve road safety, transportation intelligence and enhance in-vehicle entertainment. Many enabling applications in 5G-VANET rely on sharing mobile data among vehicles. This is still a challenging issue due to the extremely large data volume and the prohibitive cost of transmitting such data using 5G cellular networks. We propose a graph theory based algorithm to efficiently solve the data sharing problem.",Software-Defined Cooperative Data Sharing in Edge Computing Assisted 5G-VANET,"It is widely recognized that connected vehicles have the potential to further improve the road safety, transportation intelligence and enhance the in-vehicle entertainment. By leveraging the 5G enabled Vehicular Ad hoc NETworks (VANET) technology, which is referred to as 5G-VANET, a flexible softwaredefined communication can be achieved with ultra-high reliability, low latency, and high capacity. Many enabling applications in 5G-VANET rely on sharing mobile data among vehicles, which is still a challenging issue due to the extremely large data volume and the prohibitive cost of transmitting such data using 5G cellular networks. This paper focuses on efficient cooperative data sharing in edge computing assisted 5G-VANET. First, to enable efficient cooperation between cellular communication and Dedicated Short-Range Communication (DSRC), we first propose a software-defined cooperative data sharing architecture in 5GVANET. The cellular link allows the communications between OpenFlow enabled vehicles and the Controller to collect contextual information, while the DSRC serves as the data plane, enabling cooperative data sharing among adjacent vehicles. Second, we propose a graph theory based algorithm to efficiently solve the data sharing problem, which is formulated as a maximum weighted independent set problem on the constructed conflict graph. Specifically, considering the continuous data sharing, we propose a balanced greedy algorithm, which can make the content distribution more balanced. Furthermore, due to the fixed amount of computing resources allocated to this softwaredefined cooperative data sharing service, we propose an integer linear programming based decomposition algorithm to make full use of the computing resources. Extensive simulations in NS3 and SUMO demonstrate the superiority and scalability of the proposed software-defined architecture and cooperative data sharing algorithms.", 
Software security vulnerabilities are one of the critical issues in the realm of computer security. Many different approaches have been proposed to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue.,Sofware Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques-A Survey,"Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and datamining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.", 
This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data. This is over a 20,Some Improvements on Deep Convolutional Neural Network Based Image Classification,"We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techniques include adding more image transformations to the training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year’s winner.", 
"SCAA is to find the real author of source code in a corpus. It is a privacy threat to open-source programmers, but, it may be significantly helpful to develop forensic based applications. Such as, ghostwriting detection, copyright dispute settlements, and other code analysis applications. The proposed work is analyzed on 1000 programmers' data, collected from Google Code Jam (GCJ)",Source Code Authorship Attribution Using Hybrid Approach of Program Dependence Graph and Deep Learning Model,"Source Code Authorship Attribution (SCAA) is to find the real author of source code in a corpus. Though, it is a privacy threat to open-source programmers, but, it may be significantly helpful to develop forensic based applications. Such as, ghostwriting detection, copyright dispute settlements, and other code analysis applications. The efficient features extraction is the key challenge for classifying real authors of specific source codes. In this paper, the Program Dependence Graph with Deep Learning (PDGDL) methodology is proposed to identify authors from different programming source codes. First, the PDG is implemented to extract control and data dependencies from source codes. Second, the preprocessing technique is applied to convert PDG features into small instances with frequency details. Third, the Term Frequency Inverse Document Frequency (TFIDF) technique is used to zoom the importance of each PDG feature in source code. Fourth, Synthetic Minority Over-sampling Technique (SMOTE) is applied to tackle the class imbalance problem. Finally, the deep learning algorithm is applied to extract coding styles’ features for each programmer and to attribute the real authors. The deep learning algorithm is further fine-tuned with drop out layer, learning error rate, loss and activation function, and dense layers for better accuracy of results. The proposed work is analyzed on 1000 programmers’ data, collected from Google Code Jam (GCJ). The dataset contains three different programming languages, i.e., C++, Java, C#. The results are appreciable in outperforming the existing techniques from the perspective of classification accuracy, precision, recall, and f-measure metrics.", 
Programming languages are the primary tools of the software development industry. The vast majority of published source code is manually specified or programmatically assigned. This work shows that the identification of the programming language can be done automatically by utilizing an artificial neural network.,Source Code Classification Using Neural Networks,"Programming languages are the primary tools of the software development industry. As of today, the programming language of the vast majority of the published source code is manually specified or programmatically assigned based solely on the respective file extension. This work shows that the identification of the programming language can be done automatically by utilizing an artificial neural network based on supervised learning and intelligent feature extraction from the source code files. We employ a multi-layer neural network - word embedding layers along with a Convolutional Neural Network - to achieve this goal. Our criteria for an automatic source code identification solution include high accuracy, fast performance, and large programming language coverage. The model achieves a 97% accuracy rate while classifying 60 programming languages.", 
"RSSCART is a hybrid machine learning approach of Random Subspace (RSS) and Classification And Regression Trees (CART) This model is a combination of the RSS method which is known as an efficient ensemble technique and the CART which is a state of the art classifier. The Luc Yen district of Yen Bai province, a prominent landslide prone area of Viet Nam, was selected for the model development.",Spatial prediction of landslides using hybrid machine learning approach based on Random Subspace and Classification and Regression Trees,"A hybrid machine learning approach of Random Subspace (RSS) and Classification And Regression Trees (CART) is proposed to develop a model named RSSCART for spatial prediction of landslides. This model is a combination of the RSS method which is known as an efficient ensemble technique and the CART which is a state of the art classifier. The Luc Yen district of Yen Bai province, a prominent landslide prone area of Viet Nam, was selected for the model development. Performance of the RSSCART model was evaluated through the Receiver Operating Characteristic (ROC) curve, statistical analysis methods, and the Chi Square test. Results were compared with other benchmark landslide models namely Support Vector Machines (SVM), single CART, Naïve Bayes Trees (NBT), and Logistic Regression (LR). In the development of model, ten important landslide affecting factors related with geomorphology, geology and geo-environment were considered namely slope angles, elevation, slope aspect, curvature, lithology, distance to faults, distance to rivers, distance to roads, and rainfall. Performance of the RSSCART model (AUC = 0.841) is the best compared with other popular landslide models namely SVM (0.835), single CART (0.822), NBT (0.821), and LR (0.723). These results indicate that performance of the RSSCART is a promising method for spatial landslide prediction.", 
"Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224×224) input image. In this work, we equip the networks with a more principled pooling strategy, ""spatial pyramid pooling"", to eliminate the above requirement. Our SPP-net achieves state-ofthe-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.",Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,"Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224×224) input image. This requirement is “artificial” and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-ofthe-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170× faster than the recent leading method R-CNN (and 24-64× faster overall), while achieving better or comparable accuracy on Pascal VOC 2007", 
"Spatial-LTM represents an image containing objects in a hierarchical way by oversegmented image regions of homogeneous appearances. Only one single latent topic is assigned to the image patches within each region, enforcing the spatial coherency of the model. The model can simultaneously segment and classify objects, even in the case of occlusion and multiple instances.",Spatially Coherent Latent Topic Model for Concurrent Segmentation and Classification of Objects and Scenes,"We present a novel generative model for simultaneously recognizing and segmenting object and scene classes. Our model is inspired by the traditional bag of words representation of texts and images as well as a number of related generative models, including probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA). A major drawback of the pLSA and LDA models is the assumption that each patch in the image is independently generated given its corresponding latent topic. While such representation provides an efficient computational method, it lacks the power to describe the visually coherent images and scenes. Instead, we propose a spatially coherent latent topic model (Spatial-LTM). Spatial-LTM represents an image containing objects in a hierarchical way by oversegmented image regions of homogeneous appearances and the salient image patches within the regions. Only one single latent topic is assigned to the image patches within each region, enforcing the spatial coherency of the model. This idea gives rise to the following merits of Spatial-LTM: (1) Spatial-LTM provides a unified representation for spatially coherent bag of words topic models; (2) Spatial-LTM can simultaneously segment and classify objects, even in the case of occlusion and multiple instances; and (3) SpatialLTM can be trained either unsupervised or supervised, as well as when partial object labels are provided. We verify the success of our model in a number of segmentation and classification experiments.", 
"5G is a paradigm-shifting communications technology that is envisioned to provide an even wider range of high-quality services than 4G. It promises to offer high bandwidth and ultra-low latency, which are desirable for voice and mobile broadband. It",Special issue on Advancements in 5G Networks Security,"5G is a paradigm-shifting communications technology that is envisioned to provide an even wider range of high-quality services than 4G. It promises to offer high bandwidth and ultra-low latency, which are desirable not only for voice and mobile broadband, but also for new vertical industries such as healthcare, public transport, manufacturing, media and entertainment. Therefore, secure network architectures, mechanisms, and protocols are necessary to for a foundation of 5G to address potential security threats. This special issue is focusing on original research results and achievements by scientists, designers, and developers working on various issues and challenges related to 5G networks security.", 
The system generates text summaries from input audio using three independent components. The absence of sentence boundaries in the recognized text complicates the summarization process. The readers marked two aspects of the summaries: readability and information relevance. The results of the proposed system were compared with the results of sentence summarization.,Speech-to-Text Summarization Using Automatic Phrase Extraction from Recognized Text,"This paper describes a summarization system that was developed in order to summarize news delivered orally. The system generates text summaries from input audio using three independent components: an automatic speech recognizer, a syntactic analyzer, and a summarizer. The absence of sentence boundaries in the recognized text complicates the summarization process. Therefore, we use a syntactic analyzer to identify continuous segments in the recognized text. We used 50 reference articles to perform our evaluation. The data are publicly available at http://nlp.ite.tul.cz/sumarizace. The results of the proposed system were compared with the results of sentence summarization in the reference articles. The evaluation was performed using co-occurrence of n-grams in the reference and generated summaries, and by readers mark-ups. The readers marked two aspects of the summaries: readability and information relevance. Experiments confirm that the generated summaries have the same information value as the reference summaries. However, readers state that phrase summaries are hard to read without the whole sentence context.", 
"This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features) SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness. It can be computed and compared much faster.",Speeded-Up Robust Features (SURF),"This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF’s usefulness in a broad range of topics in computer vision.", 
This paper proposes an innovative graph-based text summarization framework for generic single and multi document summarization. The summarizer benefits from two well-established text semantic representation techniques as well as the constantly evolving collective human knowledge in Wikipedia. Experimental results indicate that the proposed summarizer outperforms all state-of-the-art related comparators.,"SRL-ESA-TextSum, A text summarization approach based on semantic role labeling and explicit semantic analysis","Automatic text summarization attempts to provide an effective solution to today’s unprecedented growth of textual data. This paper proposes an innovative graph-based text summarization framework for generic single and multi document summarization. The summarizer benefits from two well-established text semantic representation techniques; Semantic Role Labelling (SRL) and Explicit Semantic Analysis (ESA) as well as the constantly evolving collective human knowledge in Wikipedia. The SRL is used to achieve sentence semantic parsing whose word tokens are represented as a vector of weighted Wikipedia concepts using ESA method. The essence of the developed framework is to construct a unique concept graph representation underpinned by semantic role-based multi-node (under sentence level) vertices for summarization. We have empirically evaluated the summarization system using the standard publicly available dataset from Document Understanding Conference 2002 (DUC 2002). Experimental results indicate that the proposed summarizer outperforms all state-of-the-art related comparators in the single document summarization based on the ROUGE-1 and ROUGE-2 measures, while also ranking second in the ROUGE-1 and ROUGE-SU4 scores for the multi-document summarization. On the other hand, the testing also demonstrates the scalability of the system, i.e., varying the evaluation data size is shown to have little impact on the summarizer performance, particularly for the single document summarization task. In a nutshell, the findings demonstrate the power of the role-based and vectorial semantic representation when combined with the crowd-sourced knowledge base in Wikipedia.", 
"We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size.",SSD Single Shot MultiBox Detector,"We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 input, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size", 
We investigate feature stability in the context of clinical prognosis derived from electronic medical records. We introduce Laplacian-based regularization into a regression model. We demonstrate better feature stability and goodness,Stabilizing High-Dimensional Prediction Models Using Feature Graphs,"We investigate feature stability in the context of clinical prognosis derived from high-dimensional electronic medical records. To reduce variance in the selected features that are predictive, we introduce Laplacian-based regularization into a regression model. The Laplacian is derived on a feature graph that captures both the temporal and hierarchic relations between hospital events, diseases, and interventions. Using a cohort of patients with heart failure, we demonstrate better feature stability and goodness-of-fit through feature graph stabilization.", 
"Vehicular Ad-hoc Network (VANET) is a modern era of dynamic information distribution among societies. In clustering, vehicles are grouped together to formulate a cluster based on certain rules. A trust-based clustering mechanism allows clusters to determine a trustworthy CH. The StabTrust successfully identifies malicious and compromised vehicles and provides robust security against several potential attacks.",StabTrust-A Stable and Centralized Trust-Based Clustering Mechanism for IoT Enabled Vehicular Ad-Hoc Networks,"Vehicular Ad-hoc Network (VANET) is a modern era of dynamic information distribution among societies. VANET provides an extensive diversity of applications in various domains, such as Intelligent Transport System (ITS) and other road safety applications. VANET supports direct communications between vehicles and infrastructure. These direct communications cause bandwidth problems, high power consumption, and other similar issues. To overcome these challenges, clustering methods have been proposed to limit the communication of vehicles with the infrastructure. In clustering, vehicles are grouped together to formulate a cluster based on certain rules. Every cluster consists of a limited number of vehicles/nodes and a cluster head (CH). However, the significant challenge for clustering is to preserve the stability of clusters. Furthermore, a secure mechanism is required to recognize malicious and compromised nodes to overcome the risk of invalid information sharing. In the proposed approach, we address these challenges using components of trust. A trust-based clustering mechanism allows clusters to determine a trustworthy CH. The novel features incorporated in the proposed algorithm includes trust-based CH selection that comprises of knowledge, reputation, and experience of a node. Also, a backup head is determined by analyzing the trust of every node in a cluster. The major significance of using trust in clustering is the identification of malicious and compromised nodes. The recognition of these nodes helps to eliminate the risk of invalid information. We have also evaluated the proposed mechanism with the existing approaches and the results illustrate that the mechanism is able to provide security and improve the stability by increasing the lifetime of CHs and by decreasing the computation overhead of the CH re-selection. The StabTrust also successfully identifies malicious and compromised vehicles and provides robust security against several potential attacks.", 
StarSum is a star bipartite graph which models sentences and their topic signature phrases. The approach ensures sentence similarity and content importance from the graph structure. A DUC experiment shows the effectiveness of StarSum compared to different baselines.,"StarSum, A Simple Star Graph for Multi-document Summarization","Graph-based approaches for multi-document summarization have been widely used to extract top sentences for a summary. Traditionally, the documents’ cluster is modeled as a graph of the cluster’s sentences only which might limit the ability of recognizing topically discriminative sentences in regard to other clusters. In this paper, we propose StarSum a star bipartite graph which models sentences and their topic signature phrases. The approach ensures sentence similarity and content importance from the graph structure. We extract sentences in an approach that guarantees diversity and coverage which are crucial for multi-document summarization. Regardless of the simplicity of the approach in ranking, a DUC experiment shows the effectiveness of StarSum compared to different baselines.", 
This paper describes a social media based traffic status monitoring system. It uses an association rules based iterative query expansion algorithm to extract real time transportation related tweets. The algorithm is used to extract tweets for incident management purpose.,"Steds, Social Media Based Transportation Event Detection with Text Summarization","Ubiquitous user-input contents on social media and online services have generated a tremendous amount of information. Such information has great potential applications in various areas such as events detection and text summarization. In this paper, a social media based traffic status monitoring system is established. The system is initiated by a transportation related keyword generation process. Then an association rules based iterative query expansion algorithm is applied to extract real time transportation related tweets for incident management purpose. We also confirm the feasibility of summarizing the redundant tweets to generate concise and comprehensible textual contents. Comparison results show that our query expansion method for tweets extraction outperforms the previous ones. Analysis and case studies further demonstrate the practical usefulness of our tweets summarization algorithm.", 
Summarization of long sequences into a concise statement is a core problem in natural language processing. We develop a framework to extend existing sequence encoders with a graph component. The resulting hybrid sequence-graph models outperform,Structured Neural Summarization,"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.", 
In this paper we present an automatic text summarization method based on natural language understanding. The system extracts the rhetorical structure of text and the compound of the rhetorical relations between sentences. It then cuts out less important parts from the extracted structure.,Study of Automatic Text Summarization Based on Natural Language Understanding,"In this paper, we present an automatic text summarization method based on natural language understanding by using RST (Rhetorical Structure Theory) and CIT (Comprehensive Information Theory). RST is an analytic framework designed to account for text structure at the clause level. CIT is an integrated concept of syntactic, semantic and pragmatic information. The system extracts the rhetorical structure of text and the compound of the rhetorical relations between sentences, and then cuts out less important parts from the extracted structure. Finally it analyses the sentences in the extracted structure to generate text summarization by using CIT.", 
Heart disease/syndrome is the major cause of death worldwide. Diabetic is a life threatening disease which prevent in several urbanized as well as emergent countries like India. Data mining is a well known practice used by health organizations for classification of diseases such as diabetes and cancer.,Study of machine learning algorithms for special disease prediction using principal of component analysis,"The worldwide study on causes of death due to heart disease/syndrome has been observed that it is the major cause of death. If recent trends are allowed to continue, 23.6 million people will die from heart disease in coming 2030. The healthcare industry collects large amounts of heart disease data which unfortunately are not “mined” to discover hidden information for effective decision making. In this paper, study of PCA has been done which finds the minimum number of attributes required to enhance the precision of various supervised machine learning algorithms. The purpose of this research is to study supervised machine learning algorithms to predict heart disease. Data mining has number of important techniques like categorization, preprocessing. Diabetic is a life threatening disease which prevent in several urbanized as well as emergent countries like India. The data categorization is diabetic patients datasets which is developed by collecting data from hospital repository consists of 1865 instances with dissimilar attributes. The examples in the dataset are two categories of blood tests, urine tests. In this research paper we discuss a variety of algorithm approaches of data mining that have been utilized for diabetic disease prediction. Data mining is a well known practice used by health organizations for classification of diseases such as diabetes and cancer in bioinformatics research.", 
Summarizers make it easier for users to understand the content without reading it completely. Data is an important component in almost every domain where research and analysis are required to solve the problems. Various Neural Network models are employed along with other machine translation models.,Study on Abstractive Text Summarization Techniques,"As there is an increase in the usage of digital applications, the availability of data generated has increased to a tremendous scale. Data is an important component in almost every domain where research and analysis are required to solve the problems. It is available in a structured or unstructured format. Therefore, in order to get corresponding data as per the applications purpose, easily and quickly from different sources of data on the internet, an online content summarizer is desired. Summarizers makes it easier for users to understand the content without reading it completely. Abstractive Text Summarizer helps in defining the content by considering the important words and helps in creating summaries that are in a human-readable format. The main aim is to make summaries in such a way that it should not lose its context. Various Neural Network models are employed along with other machine translation models to bring about a concise summary generation. This paper aims to highlight and study the existing contemporary models for abstractive text summarization and also to explore areas for further research.", 
Depression is a complex clinical entity that can pose challenges for clinicians. Machine learning methods have been developed to help improve the management of this disease. These methods utilize anatomical and physiological data acquired from neuroimaging to create models.,Studying depression using imaging and machine learning methods,"Depression is a complex clinical entity that can pose challenges for clinicians regarding both accurate diagnosis and effective timely treatment. These challenges have prompted the development of multiple machine learning methods to help improve the management of this disease. These methods utilize anatomical and physiological data acquired from neuroimaging to create models that can identify depressed patients vs. non-depressed patients and predict treatment outcomes. This article (1) presents a background on depression, imaging, and machine learning methodologies; (2) reviews methodologies of past studies that have used imaging and machine learning to study depression; and (3) suggests directions for future depression-related studies.", 
"Web advertising is one of the major sources of income for a large number of Web sites. A significant part of Web advertising consists of textual ads, the ubiquitous short text messages. There are two primary channels for distributing ads: Sponsored Search (or Paid Search Advertising) and Content Match.",Studying the Impact of Text Summarization on Contextual Advertising,"Web advertising, one of the major sources of income for a large number of Web sites, is aimed at suggesting products and services to the ever growing population of Internet users. A significant part of Web advertising consists of textual ads, the ubiquitous short text messages usually marked as sponsored links. There are two primary channels for distributing ads: Sponsored Search (or Paid Search Advertising) and Content Match (or Contextual Advertising). In this paper, we concentrate on the latter, which is devoted to display commercial ads within the content of third-party Web pages. In the literature, several approaches estimated the ad relevance based on co-occurrence of the same words or phrases within the ad and within the page. However, targeting mechanisms based solely on phrases found within the text of the page can lead to problems. In order to solve these problems, matching mechanisms that combine a semantic phase with the traditional syntactic phase have been proposed. We are mainly interested in studying the impact of the syntactic phase on contextual advertising. In particular, we perform a comparative study on text summarization in contextual advertising. Results show that implementing effective text summarization techniques may help to improve the corresponding contextual advertising system.", 
"Deep learning (DL) techniques are gaining more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks.",Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks,"Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.", 
"In this paper, we introduce a subcategory-aware object classification framework to boost category level object classification performance. Motivated by the observation of considerable intra-class diversities and inter-class ambiguities in many current object classification datasets, we split data into subcategories.",Subcategory-aware Object Classification,"In this paper, we introduce a subcategory-aware object classification framework to boost category level object classification performance. Motivated by the observation of considerable intra-class diversities and inter-class ambiguities in many current object classification datasets, we explicitly split data into subcategories by ambiguity guided subcategory mining. We then train an individual model for each subcategory rather than attempt to represent an object category with a monolithic model. More specifically, we build the instance affinity graph by combining both intraclass similarity and inter-class ambiguity. Visual subcategories, which correspond to the dense subgraphs, are detected by the graph shift algorithm and seamlessly integrated into the state-of-the-art detection assisted classification framework. Finally the responses from subcategory models are aggregated by subcategory-aware kernel regression. The extensive experiments over the PASCAL VOC 2007 and PASCAL VOC 2010 databases show the state-ofthe-art performance from our framework.", 
"Emotion recognition from Electroencephalography (EEG) is proved to be a good choice as it cannot be mimicked like speech signals or facial expressions. EEG signals of emotions are not unique and it varies from person to person as each one has different emotional responses to the same stimuli. In this paper, a subject-independent emotion recognition technique is proposed from EEG signals using Variational Mode Decomposition (VMD) and Deep",Subject Independent Emotion recognition from EEG using VMD and Deep Learning,"Emotion recognition from Electroencephalography (EEG) is proved to be a good choice as it cannot be mimicked like speech signals or facial expressions. EEG signals of emotions are not unique and it varies from person to person as each one has different emotional responses to the same stimuli. Thus EEG signals are subject dependent and proved to be effective for subject dependent emotion recognition. However, subject independent emotion recognition plays an important role in situations like emotion recognition from paralyzed or burnt face, where EEG of emotions of the subjects before the incidents are not available to build the emotion recognition model. Hence there is a need to identify common EEG patterns corresponds to each emotion independent of the subjects. In this paper, a subject independent emotion recognition technique is proposed from EEG signals using Variational Mode Decomposition (VMD) as a feature extraction technique and Deep Neural Network as the classifier. The performance evaluation of the proposed method with the benchmark DEAP dataset shows that the combination of VMD and Deep Neural Network performs better compared to the state of the art techniques in subject-independent emotion recognition from EEG.", 
This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. Our experiments with the NTCIR ACLIA test collections show,Subtree Extractive Summarization via Submodular Maximization,"This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 1 2 (1 ? e ?1 ). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm.", 
"In text summarization, relevance and coverage are two main criteria that decide the quality of a summary. We propose a new multi-document summarization approach SumCR via sentence extraction. The relevance value of each sentence in SumCR is obtained within a subset of similar sentences. Experimental studies on DUC benchmark data show the good performance of SumCR.","SumCR, A new subtopic-based extractive approach for text summarization","In text summarization, relevance and coverage are two main criteria that decide the quality of a summary. In this paper, we propose a new multi-document summarization approach SumCR via sentence extraction. A novel feature called Exemplar is introduced to help to simultaneously deal with these two concerns during sentence ranking. Unlike conventional ways where the relevance value of each sentence is calculated based on the whole collection of sentences, the Exemplar value of each sentence in SumCR is obtained within a subset of similar sentences. A fuzzy medoid-based clustering approach is used to produce sentence clusters or subsets where each of them corresponds to a subtopic of the related topic. Such kind of subtopic-based feature captures the relevance of each sentence within different subtopics and thus enhances the chance of SumCR to produce a summary with a wider coverage and less redundancy. Another feature we incorporate in SumCR is Position, i.e., the position of each sentence appeared in the corresponding document. The final score of each sentence is a combination of the subtopic-level feature Exemplar and the document-level feature Position. Experimental studies on DUC benchmark data show the good performance of SumCR and its potential in summarization tasks.", 
"In this paper, we focus on the task of automatic text summarization. We have developed a tool that summarizes the given text. We use several NLP features and machine learning techniques for text summarizing.","Sumdoc, A Unified Approach for Automatic Text Summarization","In this paper, we focus on the task of automatic text summarization. Lot of work has already been carried out on automatic text summarization though most of the work done in this field is on extracted summaries. We have developed a tool that summarizes the given text. We have used several NLP features and machine learning techniques for text summarizing. We have also showed how WordNet can be used to obtain abstractive summarization. We are using an approach that first extracts sentences from the given text by using ranking algorithm, by means of which we rank the sentence on the basis of many features comprising of some classical features as well as some novel ones. Then, after extracting candidate sentences, we investigate some of the words and phrases and transform them into their respective simple substitutes so as to make the final summary a hybrid summarization technique.", 
"We propose an end-to-end neural model for zero-shot abstractive text summarization of paragraphs. We show results for extractive and human baselines to demonstrate a large abstractive gap in performance. SummAE consists of a denoising auto-encoder that embeds sentences and paragraphs in a common space, from which either can be decoded.","SummAE, Zero-Shot Abstractive Text Summarization using Length-Agnostic Auto-Encoders","We propose an end-to-end neural model for zero-shot abstractive text summarization of paragraphs, and introduce a benchmark task, ROCSumm, based on ROCStories, a subset for which we collected human summaries. In this task, five-sentence stories (paragraphs) are summarized with one sentence, using human summaries only for evaluation. We show results for extractive and human baselines to demonstrate a large abstractive gap in performance. Our model, SummAE, consists of a denoising auto-encoder that embeds sentences and paragraphs in a common space, from which either can be decoded. Summaries for paragraphs are generated by decoding a sentence from the paragraph representations. We find that traditional sequence-to-sequence auto-encoders fail to produce good summaries and describe how specific architectural choices and pre-training techniques can significantly improve performance, outperforming extractive baselines. The data, training, evaluation code, and best model weights are opensourced.", 
Text summarization system uses hybrid approach of linguistic and statistical analysis. System is highly modular and entirely HTML-based.,"Summar, Combining linguistics and statistics for text summarization",We describe a text summarization system that moves beyond standard approaches by using a hybrid approach of linguistic and statistical analysis and by employing text-sort-specific knowledge of document structure and phrases indicating importance. The system is highly modular and entirely XML-based so that different components can be combined easily., 
Precision/recall schemes and summary accuracy measures are suggested as particularly suitable in evaluating speech summaries.,"Summarization Evaluation for Text and Speech, Issues and Approaches","This paper surveys current text and speech summarization evaluation approaches. It discusses advantages and disadvantages of these, with the goal of identifying summarization techniques most suitable to speech summarization. Precision/recall schemes, as well as summary accuracy measures which incorporate weightings based on multiple human decisions, are suggested as particularly suitable in evaluating speech summaries.", 
"The effective representation of documents, quantifying the relatedness of sentences, and selecting the most informative sentences are main challenges. We evaluate the efficacy of a graph-based summarizer using different types of context-free and contextualized embeddings. The word representations are produced by pre-training neural language models on large corpora of biomedical texts.",Summarization of biomedical articles using domain-specific word embeddings and graph ranking,"Text summarization tools can help biomedical researchers and clinicians reduce the time and effort needed for acquiring important information from numerous documents. It has been shown that the input text can be modeled as a graph, and important sentences can be selected by identifying central nodes within the graph. However, the effective representation of documents, quantifying the relatedness of sentences, and selecting the most informative sentences are main challenges that need to be addressed in graph-based summarization. In this paper, we address these challenges in the context of biomedical text summarization. We evaluate the efficacy of a graph-based summarizer using different types of context-free and contextualized embeddings. The word representations are produced by pre-training neural language models on large corpora of biomedical texts. The summarizer models the input text as a graph in which the strength of relations between sentences is measured using the domain specific vector representations. We also assess the usefulness of different graph ranking techniques in the sentence selection step of our summarization method. Using the common Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics, we evaluate the performance of our summarizer against various comparison methods. The results show that when the summarizer utilizes proper combinations of context-free and contextualized embeddings, along with an effective ranking method, it can outperform the other methods. We demonstrate that the best settings of our graph-based summarizer can efficiently improve the informative content of summaries and decrease the redundancy.", 
"In real world settings, collections and individual documents rarely stay unchanged over time. The World Wide Web is a prime example of a collection where information changes both frequently and significantly. The goal is to obtain a summary that describes the most significant changes made to a document during a given period. This paper proposes different approaches to generate summaries using extractive summarization techniques. It is observed that the approach using the LDA model outperforms all the other approaches.",Summarization of changes in dynamic text collections using Latent Dirichlet Allocation model,"In the area of Information Retrieval, the task of automatic text summarization usually assumes a static underlying collection of documents, disregarding the temporal dimension of each document. However, in real world settings, collections and individual documents rarely stay unchanged over time. The World Wide Web is a prime example of a collection where information changes both frequently and significantly over time, with documents being added, modified or just deleted at different times. In this context, previous work addressing the summarization of web documents has simply discarded the dynamic nature of the web, considering only the latest published version of each individual document. This paper proposes and addresses a new challenge - the automatic summarization of changes in dynamic text collections. In standard text summarization, retrieval techniques present a summary to the user by capturing the major points expressed in the most recent version of an entire document in a condensed form. In this new task, the goal is to obtain a summary that describes the most significant changes made to a document during a given period. In other words, the idea is to have a summary of the revisions made to a document over a specific period of time. This paper proposes different approaches to generate summaries using extractive summarization techniques. First, individual terms are scored and then this information is used to rank and select sentences to produce the final summary. A system based on Latent Dirichlet Allocation model (LDA) is used to find the hidden topic structures of changes. The purpose of using the LDA model is to identify separate topics where the changed terms from each topic are likely to carry at least one significant change. The different approaches are then compared with the previous work in this area. A collection of articles from Wikipedia, including their revision history, is used to evaluate the proposed system. For each article, a temporal interval and a reference summary from the article’s content are selected manually. The articles and intervals in which a significant event occurred are carefully selected. The summaries produced by each of the approaches are evaluated comparatively to the manual summaries using ROUGE metrics. It is observed that the approach using the LDA model outperforms all the other approaches.", 
"Only limited research exists on the process of gathering, organizing and presenting patient data. We develop a conceptual model for describing and understanding clinical summarization. The model identifies five distinct stages of summarization and a framework for future research. We describe a hypothetical case study to illustrate the application of this model in the primary care setting.","Summarization of clinical information, A conceptual model","To provide high-quality and safe care, clinicians must be able to optimally collect, distill, and interpret patient information. Despite advances in text summarization, only limited research exists on clinical summarization, the complex and heterogeneous process of gathering, organizing and presenting patient data in various forms. To develop a conceptual model for describing and understanding clinical summarization in both computer-independent and computer-supported clinical tasks. Based on extensive literature review and clinical input, we developed a conceptual model of clinical summarization to lay the foundation for future research on clinician workflow and automated summarization using electronic health records (EHRs). Our model identifies five distinct stages of clinical summarization: (1) Aggregation, (2) Organization, (3) Reduction and/or Transformation, (4) Interpretation and (5) Synthesis (AORTIS). The AORTIS model describes the creation of complex, task-specific clinical summaries and provides a framework for clinical workflow analysis and directed research on test results review, clinical documentation and medical decision-making. We describe a hypothetical case study to illustrate the application of this model in the primary care setting. Both practicing physicians and clinical informaticians need a structured method of developing, studying and evaluating clinical summaries in support of a wide range of clinical tasks. Our proposed model of clinical summarization provides a potential pathway to advance knowledge in this area and highlights directions for further research.", 
Text data has grown astronomically and this growth has produced a great demand for text summarization. We propose a new summarization process by text mining and social network,Summarization of Documents by Finding Key Sentences Based on Social Network Analysis,"Finding key sentences or paragraphs from a document is an important and challenging problem. In recent years, the amount of text data has grown astronomically and this growth has produced a great demand for text summarization. In the present study, we propose a new text summarization process by text mining and social network methods. To demonstrate the applicability of the proposed summarization procedure, we used Martin Luther King, Jr’s public speech", 
Text summarization is nothing but summarizing the content of given text document. In this paper propose a Malayalam text summarization system which is based on MMR technique with successive threshold. The key idea is to use a unit step function at each step to decide the maximum marginal relevance.,Summarization of Malayalam Document Using Relevance of Sentences,"Text summarization is an emerging technique for finding out the summary of the text document. Text summarization is nothing but summarizing the content of given text document. Text summarization has got so uses such as Due to the massive amount of information getting increased on internet; it is difficult for the user to go through all the information available on web. Summarization techniques need to be used to reduce the users time in reading the whole information available on web. In this paper propose a Malayalam text summarization system which is based on MMR technique with successive threshold. Here the sentences are selected based on the concept of maximal marginal relevance. The key idea is to use a unit step function at each step to decide the maximum marginal relevance and the number of sentences present in the summary would be equal to the number of paragraphs or the average number of sentences present in the text document, which can be achieved by using successive threshold approach. We apply MMR approach on Malayalam text summarization task and achieve comparable results to the state of the art.", 
The Semantic Link Network is a semantics modeling method for effective information services. This paper proposes a new text summarization approach that extracts SemanticLink Network from scientific paper. It ranks the nodes to select Top-k sentences to compose summary. The proposed approach can be applied to implementing other summarization applications.,Summarization of Scientific Paper Through Reinforcement Ranking on Semantic Link Network,"The Semantic Link Network is a semantics modeling method for effective information services. This paper proposes a new text summarization approach that extracts Semantic Link Network from scientific paper consisting of language units of different granularities as nodes and semantic links between the nodes, and then ranks the nodes to select Top-k sentences to compose summary. A set of assumptions for reinforcing representative nodes is set to reflect the core of paper. Then, Semantic Link Networks with different types of node and links are constructed with different combinations of the assumptions. Finally, an iterative ranking algorithm is designed for calculating the weight vectors of the nodes in a converged iteration process. The iteration approximately approaches a stable weight vector of sentence nodes, which is ranked to select Top-k high-rank nodes for composing summary. We designed six types of ranking models on Semantic Link Networks for evaluation. Both objective assessment and intuitive assessment show that ranking Semantic Link Network of language units can significantly help identify the representative sentences. This work not only provides a new approach to summarizing text based on extraction of semantic links from text but also verifies the effectiveness of adopting the Semantic Link Network in rendering the core of text. The proposed approach can be applied to implementing other summarization applications such as generating an extended abstract, the mind map and the bulletin points for making the slides of a given paper. It can be easily extended by incorporating more semantic links to improve text summarization and other information services.", 
"We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011) We show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora and user comments on news articles.",Summarization Through Submodularity and Dispersion,"We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.", 
Text summarization is one of the oldest problems in natural language processing. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program.,Summarization with a joint model for sentence extraction and compression,"Text summarization is one of the oldest problems in natural language processing. Popular approaches rely on extracting relevant sentences from the original documents. As a side effect, sentences that are too long but partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data.", 
"Research combining sentence scoring and decision tree method for automatic text summarization in Indonesian language. It uses the decision tree algorithm to choose which of sentences will be selected in summarization system. The result shows the highest f-measure score is 0, 80 and the average is 0,. 58.",Summarizing Indonesian text automatically by using sentence scoring and decision tree,"Text summarization is a process of compressing a text from the source to be a shorter version, but the version still contains the main information there. By reading the summary, the readers might be easy and fast to understand the contents instead of reading all the text. Because of that, it needs a method to understand, clarify, and present the whole information needed clearly and succinctly in the summary. So, it allows the readers save the time and energy. This research combining sentence scoring and decision tree method for automatic text summarization in Indonesian language. It uses the decision tree algorithm to choose which of sentences will be selected in summarization system. To produce the rules for decision tree, it uses 50 news texts as the training data. The produced-model from the training stage will be implemented for sentence selection process to the summarization system. The result shows the highest f-measure score is 0, 80 and the average is 0, 58. Based on this, it concludes that the result of document summarization using sentence scoring and decision tree shows a better accuracy score for news text document.", 
Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data.,Summarizing large text collection using topic modeling and clustering based on MapReduce framework,"Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the semantic similarity computation in summarization process. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data. In this paper, a novel framework based on MapReduce technology is proposed for summarizing large text collection. The proposed technique is designed using semantic similarity based clustering and topic modeling using Latent Dirichlet Allocation (LDA) for summarizing the large text collection over MapReduce framework. The summarization task is performed in four stages and provides a modular implementation of multiple documents summarization. The presented technique is evaluated in terms of scalability and various text summarization parameters namely, compression ratio, retention ratio, ROUGE and Pyramid score are also measured. The advantages of MapReduce framework are clearly visible from the experiments and it is also demonstrated that MapReduce provides a faster implementation of summarizing large text collections and is a powerful tool in Big Text Data analysis.", 
SummaRuNNer is a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents. It achieves performance better than or comparable to state-of-the-art.,SummaRuNNer A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents,"We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.", 
SummCoder is a novel methodology for generic extractive text summarization of single documents. It generates a summary according to three sentence selection metrics. The approach obtains comparable or better performance than the state-of-the-art methods for different ROUGE metrics.,"SummCoder, An unsupervised framework for extractive text summarization based on deep auto-encoders","In this paper, we propose SummCoder, a novel methodology for generic extractive text summarization of single documents. The approach generates a summary according to three sentence selection metrics formulated by us: sentence content relevance, sentence novelty, and sentence position relevance. The sentence content relevance is measured using a deep auto-encoder network, and the novelty metric is derived by exploiting the similarity among sentences represented as embeddings in a distributed semantic space. The sentence position relevance metric is a hand-designed feature, which assigns more weight to the first few sentences through a dynamic weight calculation function regulated by the document length. Furthermore, a sentence ranking and a selection technique are developed to generate the document summary by ranking the sentences according to the final score obtained through the fusion of the three sentences selection metrics. We also introduce a new summarization benchmark, Tor Illegal Documents Summarization (TIDSumm) dataset, especially to assist Law Enforcement Agencies (LEAs), that contains two sets of ground truth summaries, manually created, for 100 web documents extracted from onion websites in Tor (The Onion Router) network. Empirical results show that, on DUC 2002, on Blog Summarization, and on TIDSumm datasets, our text summarization approach obtains comparable or better performance than the state-of-the-art methods for different ROUGE metrics", 
"Scene categorization is a fundamental problem in computer vision. Scene understanding research has been constrained by the limited scope of currently-used databases. The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images.",SUN Database Large-scale Scene Recognition from Abbey to Zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.", 
Proposed sentence relevance estimation is based on normalization of NMF topic space and further weighting of each topic using sentences representation in topic space. The proposed method shows better summarization quality and performance than state of the art methods on DUC 2002 standard dataset.,Supervised and unsupervised text classification via generic summarization,"This paper presents a new generic text summarization method using Non-negative Matrix Factorization (NMF) to estimate sentence relevance. Proposed sentence relevance estimation is based on normalization of NMF topic space and further weighting of each topic using sentences representation in topic space. The proposed method shows better summarization quality and performance than state of the art methods on DUC 2002 standard dataset. In addition, we study how this method can improve the performance of supervised and unsupervised text classification tasks. In our experiments with Reuters-21578 and 20 Newsgroups benchmark datasets we apply developed text summarization method as a preprocessing step for further multi-label classification and clustering. As a result, the quality of classification and clustering has been significantly improved.", 
"SupMMD is a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. It makes use of similarity across multiple information sources, such as text features and knowledge based concepts.",SupMMD A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy,"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.", 
An approach to recognize the emotion responses during multimedia presentation using the electroencephalogram (EEG) signals is proposed. It will be able to provide key clues to develop EEG-inspired multimedia applications.,Support Vector Machine for EEG Signal Classification during Listening to Emotional Music,"An approach to recognize the emotion responses during multimedia presentation using the electroencephalogram (EEG) signals is proposed. The association between EEG signals and music-induced emotion responses was investigated in three factors, including: 1) the types of features, 2) the temporal resolutions of features, and 3) the components of EEG. The results showed that the spectrum power asymmetry index of EEG signal was a sensitive marker to reflect the brain activation related to emotion responses, especially for the low frequency bands of delta, theta and alpha components. Besides, the maximum classification accuracy was obtained around 92.73 % by using support vector machine (SVM) based on 60 features derived from all EEG components with the feature temporal resolution of one second. As such, it will be able to provide key clues to develop EEG-inspired multimedia applications, in which multimedia contents could be offered interactively according to the users’ immediate feedback.", 
Main idea identification is an essential reading comprehension skill. Co-teachers need to provide explicit instruction to meet the needs of students with learning disabilities. This article provides guidance for supporting the main idea identification and text summarization skills of middle school students.,Supporting Main Idea Identification and Text Summarization in Middle School Co-Taught Classes,"Being able to identify the main ideas within a complex multi-paragraph content-area text is an essential reading comprehension skill. It is especially important for content-area and special education co-teachers to provide explicit instruction in this skill to meet the needs of their students with learning disabilities who frequently struggle with understanding text they read. To help students with main idea identification, co-teachers can provide students with explicit instruction on how to generate a main idea statement for individual paragraphs or sections of a text. Co-teachers can extend this instruction by incorporating peer-mediated practice to help students strengthen their main idea statements. Finally, co-teachers can instruct students to use their statements to summarize the text. This article provides guidance for supporting the main idea identification and text summarization skills of middle school students in a co-taught classroom.", 
"Developers are struggling to keep up with the staggering amount of source code that needs to be read and understood. We propose an approach to automatically determine such descriptions, based on",Supporting program comprehension with source code summarization,"One of the main challenges faced by today’s developers is keeping up with the staggering amount of source code that needs to be read and understood. In order to help developers with this problem and reduce the costs associated with it, one solution is to use simple textual descriptions of source code entities that developers can grasp easily, while capturing the code semantics precisely. We propose an approach to automatically determine such descriptions, based on automated text summarization technology.", 
"This paper describes the vulnerabilities found in Android based Smart phones. It also describes the attacks like privilege escalation, privacy attack and other threats. In last section we discuss the possible countermeasure against the attacks and threats due to that.",Survey Collection Mobile Hacking App And,"Nowadays people are eagerly waiting to peep into the others personal details, it’s all about curiosity, passiveness or steeling the data. This may be for creating fun or sometimes in order to protect them from the different set of obstacles. With the help of the latest technology one can able to easily hack up the others phone but also it is not as easy as possible. The use of Smart phones has become very popular around the globe in this digital area and in recent days its user's number has increased at indefinite level, as everyone rush to explore the digital world. The people are switching towards Smart phones in order to access many applications such as financial, business, educational and social .This paper describes the vulnerabilities found in Android based Smart phones and also describes the attacks like privilege escalation, privacy attack and other threats which are associated with these particular devices. In last section we discuss the possible countermeasure against the attacks and threats due to that, the Android Smart phones become vulnerable.", 
"IoT-based real-time health monitoring system gains the attention of the researchers as well as the business sectors. The sensitive nature of the patient's healthcare data is very crucial and the compromise of such data may compromise one's life. In any IoT based healthcare system, data security is the vital issue.",Survey of Symmetric and Asymmetric Key Management Schemes in the context of IoT based Healthcare System,"Internet of Things (IoT) based real-time health monitoring system gains the attention of the researchers as well as the business sectors. This service turns out to be a blessing to the patients as well as the health care professionals. However, without data security, the practical implementation of this vital healthcare service is not feasible. The sensitive nature of the patient’s healthcare data is very crucial and the compromise of such data may compromise one’s life. Therefore, in any IoT based healthcare system, data security is the vital issue. In this direction of IoT based healthcare system, this paper presents a survey of symmetric and asymmetric key management schemes. Considering the cryptographic key (symmetric or asymmetric), we have discussed the relevant studies within a time period of 6 years. A comparison of these schemes is presented with respect to the security features and simultaneously described the pros and cons of the respected schemes.", 
"This paper provides a review on some of the significant research work done on abstractive text summarization. It includes data processing, word embedding, basic model architecture, training, and validation process. The paper narrates the current research in this field.",Survey on Abstractive Text Summarization,"This paper provides a review on some of the significant research work done on abstractive text summarization. The process of generating the summary from one or more text corpus, by keeping the key points in the corpus is called text summarization. The most prominent technique in text summarization is an abstractive and extractive method. The extractive summarization is purely based on the algorithm and it just copies the most relevant sentence/words from the input text corpus and creating the summary. An abstractive method generates new sentences/words that may/may not be in the input corpus. This paper focuses on the abstractive text summarization. This paper explains the overview of the various processes in abstractive text summarization. It includes data processing, word embedding, basic model architecture, training, and validation process and the paper narrates the current research in this field. It includes different types of architectures, attention mechanism, supervised and reinforcement learning, the pros and cons of different architecture. Systematic comparison of different text summarization models will provide the future direction of text summarization.", 
"The integration of Internet of Things (IoT) into cellular network infrastructure is gaining much attention. Vehicle to Everything (V2X) communications and services provide data links over existing cellular networks for IoT deployment. V2X networks are characterised by high mobility, high node density, dynamic network topology and time sensitivity.",Survey on existing authentication issues for cellular-assisted V2X communication,"The integration of Internet of Things (IoT) into cellular network infrastructure is gaining much attention from the research community and standardisation groups, where new cellular-based enabling communication technologies are proposed to provide data links over existing cellular networks for IoT deployment. The Long Term Evolution (LTE) network and the emerging 5G system (New Radio) are widely acknowledged as the main driving forces for the connected-vehicle in the mobile network domain. On the other hand, vehicular networks are characterised by high mobility, high node density, dynamic network topology, time sensitivity and high transmission reliability. These unique characteristics of vehicular networks introduce new security implications and therefore, necessitate enhancement of existing LTE security or provision of new security techniques, in order to ensure fast, secure and reliable vehicular communications under mobile network. In this paper, an introduction to Vehicle to Everything (V2X) communications and services alongside the corresponding service requirement is presented. Then, the potential benefits of using cellular infrastructure for V2X services and the reference architectures for cellular-based V2X systems are described. We then focus on the security requirements of V2X in cellular network, specifically; on the authentication of V2X entities, as any violation or breach of authentication process can expose the entire network to serious consequences. The paper discusses common V2X threats and surveys existing V2X authentication solutions proposed in the literature. Finally, paper presents security issues related to V2X communication in cellular network leading towards possible research challenges.", 
Text summarization is the process of generating summary of one or more documents. It conveys information present in the documents and is usually less than the original documents. Two main approaches focused are clustering techniques and machine learning technique (SVM),Survey on Extractive Text Summarization Approaches,"Due to increasing use of internet and online technologies or online data, there is vast increase in the electronic documents. When a data is being retrieved from such a huge collection of electronic documents, hundreds and thousands of documents are retrieved. Hence, for user, it is not possible to read all the retrieved documents. Also, these documents contain redundant information. In such situation summarization proves to be very useful that summarizes the retrieved documents. This has lead to intensive research in the area of automatic text summarization. It is widely used in other fields like natural language processing and machine learning. Text summarization is the process of generating summary of one or more documents which conveys information present in the documents and is usually less than the original documents. This research paper provides an overview of the extractive techniques for text summarization. Two main approaches focused are clustering techniques and machine learning technique (SVM).", 
Summarization reduces the complexity of a document while retaining its important features. There are wide varieties of approaches in Multi-document Text Summarization. Graph and Cluster Based methods proposed by various researchers in the field have been analysed. Using this information one can generate new or even hybrid methods in multi-document summarization.,Survey on Graph and Cluster Based approaches in Multi-document Text Summarization,"In today's era of World Wide Web, on-line information is increasing exponentially day by day. So there is a need to condense corpus of documents into useful information automatically. Automatic Text summarization plays an important role to extract salient feature from corpus of documents, which helps user to get useful information in short time and less effort. Summarization reduces the complexity of a document while retaining its important features. Recently, most researchers have transferred their efforts from single to multi document summarization but they have to be aware of the issues of redundancy, sentence ordering, fluency, etc. There are wide varieties of approaches in Multi-document Text Summarization like Graph Based, Cluster Based, Time Based and Term frequency -Inverse document frequency Based etc. The survey starts introducing Multi-document text Summarization (MDS) and then discusses various methods of MDS which fall under the Graph and Cluster Based methods. In this paper, we have analysed Graph and Cluster Based methods proposed by various researchers in the field and we sort out some of the problems in applied procedures and also pin out advantages, which would help future researchers working in the area, to get significant instruction for further analysis. Using this information one can generate new or even hybrid methods in Multi-document summarization.", 
Software vulnerability analysis methods based on machine learning is becoming an important research area of information security. The up-to-date and well-known works in this research area were analyzed deeply.,Survey on Software Vulnerability Analysis method based on Machine Learning,"With the increasingly rich of vulnerability related data and the extensive application of machine learning methods, software vulnerability analysis methods based on machine learning is becoming an important research area of information security. In this paper, the up-to-date and well-known works in this research area were analyzed deeply. A framework for software vulnerability analysis based on machine learning was proposed. And the existing works were described and compared, the limitations of these works were discussed. The future research directions on software vulnerability analysis based on machine learning were put forward in the end.", 
"Text classification can be defined that the task was automatically categorized a group documents into one or more predefined classes. In text classification technique, term weighting methods design suitable weights to the specific terms to enhance the text classification performance.",Survey on supervised machine learning techniques for automatic text classification,"Supervised machine learning studies are gaining more significant recently because of the availability of the increasing number of the electronic documents from different resources. Text classification can be defined that the task was automatically categorized a group documents into one or more predefined classes according to their subjects. Thereby, the major objective of text classification is to enable users for extracting information from textual resource and deals with process such as retrieval, classification, and machine learning techniques together in order to classify different pattern. In text classification technique, term weighting methods design suitable weights to the specific terms to enhance the text classification performance. This paper surveys of text classification, process of different term weighing methods and comparison between different classification techniques.", 
"Vehicular Ad hoc NETworks is a use case of mobile ad hoc networks where cars are the mobile nodes. VANETs exhibit several unique features such as the high mobility of nodes, short connection times, etc. conventional security mechanisms are not always effective.",Survey on VANET security challenges and possible cryptographic solutions,"In the near future, it is expected that vehicles which increasingly become an intelligent systems will be equipped with radio communications interfaces. Thus, vehicular networks can be formed and they are commonly known as VANETs (Vehicular Ad hoc NETworks), a use case of mobile ad hoc networks where cars are the mobile nodes. As VANETs exhibit several unique features such as the high mobility of nodes, short connection times, etc. conventional security mechanisms are not always effective. Consequently, a wide variety of research contributions have been recently presented to cope with the intrinsic characteristics of vehicular communication. This paper provides a summary of the recent state of the art of VANETs, it presents the communication architecture of VANETs and outlines the privacy and security challenges that need to be overcome to make such networks safety usable in practice. It identifies all existing security problems in VANETs and classifies them from a cryptographic point of view. It regroups, studies and compares also the various cryptographic schemes that have been separately suggested for VANETs, evaluates the efficiency of proposed solutions and explores some future trends that will shape the research in cryptographic protocols for intelligent transportation systems", 
The features are the main entries in text summarization. Treating all features equally causes poor summary generation. The particle swarm optimization is trained using DUC 2002 data to learn the weight of each feature. The experimental results shown that simple features are less effective than the combined features.,Swarm Based Features Selection for Text Summarization,"The features are the main entries in text summarization. Treating all features equally causes poor summary generation. In this paper, we investigate the effect of the feature structure on the features selection using particle swarm optimization. The particle swarm optimization is trained using DUC 2002 data to learn the weight of each feature. The features used are different in terms of the structure, where some features were formed as combination of more than one feature while others as simple or individual feature. Therefore the determining of the effectiveness of each type of features could lead to mechanism to differentiate between the features having high importance and those having low importance. We assume that the combined features have higher priority of getting selection more than the simple features. In each iteration, the particle swarm optimization selects some features, then corresponding weights of those features are used to score the sentences and the top ranking sentences are selected as summary. The selected features of each best summary are used in calculation of the final features weights. The experimental results shown that the simple features are less effective than the combined features", 
The treating of all text features with same level of importance can be considered the main factor causing creating a summary with low quality. Model creates summaries which are 43% similar to the manually generated summaries. The weights obtained from the training of the model were used to adjust the text features scores.,Swarm Based Text Summarization,"The scoring mechanism of the text features is the unique way for determining the key ideas in the text to be presented as text summary. The treating of all text features with same level of importance can be considered the main factor causing creating a summary with low quality. In this paper, we introduced a novel text summarization model based on swarm intelligence. The main purpose of the proposed model is for scoring the sentences, emphasizing on dealing with the text features fairly based on their importance. The weights obtained from the training of the model were used to adjust the text features scores, which could play an important role in the selection process of the most important sentences to be included in the final summary. The results show that the human summaries H1 and H2 are 49% similar to each other. The proposed model creates summaries which are 43% similar to the manually generated summaries, while the summaries produced by Ms Word summarizer are 39% similar.", 
Automatic text summarization systems aim to make their created summaries closer to human summaries. The system is built based on exploiting of the advantages of different techniques. The experimental results showed that our model got the best performance over all methods.,Swarm Diversity Based Text Summarization,"Automatic text summarization systems aim to make their created summaries closer to human summaries. The summary creation under the condition of the redundancy and the summary length limitation is a challenge problem. The automatic text summarization system which is built based on exploiting of the advantages of different techniques in form of an integrated model could produce a good summary for the original document. In this paper, we introduced an integrated model for automatic text summarization problem; we tried to exploit different techniques advantages in building of our model like advantage of diversity based method which can filter the similar sentences and select the most diverse ones and advantage of the differentiation between the most important features and less important using swarm based method. The experimental results showed that our model got the best performance over all methods used in this study.", 
Swarm LSA-PSO model shows promising results in context based text summarization using BOW clustering approach. The input text documents were downloaded from Document Understanding Conference (DUC) 2002 dataset. The terms matrix was constructed from co-occurrence of terms using Bag-of-Words.,Swarm LSA-PSO clustering model in text summarization,"The information overload problem has posed great challenge to internet users to retrieve relevant information accurately for the past decades. It is a tedious task for machine to intuitively mimic human linguists to summarize documents into meaningful text in abstractive manner. Quite often, the summarized text lacks cohesion and becomes difficult to comprehend. The objective of this paper is to investigate the proposed Swarm LSA-PSO model performs better than alternative methods. In this study, terms matrix was constructed from co-occurrence of terms using Bag-of-Words (BOW). The huge dimensions of terms were reduced using Singular Value Decomposition followed by K-Means PSO clustering for acquiring optimal number of concepts clusters. These key concepts were used to identify the main gist in documents for text summarization. The input text documents were downloaded from Document Understanding Conference (DUC) 2002 dataset. The preliminary results show that the swarm LSA-PSO model shows promising results in context based text summarization using BOW clustering approach.", 
"There has been a significant research in automatic text summarization using featurebased techniques. syntactic structure has not widely applied due to its difficulty of handling it in summarization process. In this approach, two neural networks are trained based on the feature score and the Syntactic structure of sentences. The proposed approach achieved F-measure of 80% for the compression ratio 50%.",Syntactic and sentence feature based hybrid approach for text summarization,"Recently, there has been a significant research in automatic text summarization using featurebased techniques in which most of them utilized any one of the soft computing techniques. But, making use of syntactic structure of the sentences for text summarization has not widely applied due to its difficulty of handling it in summarization process. On the other hand, feature-based technique available in the literature showed efficient results in most of the techniques. So, combining syntactic structure into the feature-based techniques is surely smooth the summarization process in a way that the efficiency can be achieved. With the intention of combining two different techniques, we have presented an approach of text summarization that combines feature and syntactic structure of the sentences. Here, two neural networks are trained based on the feature score and the syntactic structure of sentences. Finally, the two neural networks are combined with weighted average to find the sentence score of the sentences. The experimentation is carried out using DUC 2002 dataset for various compression ratios. The results showed that the proposed approach achieved F-measure of 80% for the compression ratio 50 % that proved the better results compared with the existing techniques.", 
"Automatic text summarization is the process of condensing an original document into shorter form. Fuzzy logic has appeared as a powerful theoretical framework for studying human reasoning. This paper is a systematic literature review to gather, analyze, and report the trends, gaps and prospects of using fuzzy logic.",Systematic literature review of fuzzy logic based text summarization,"‘Information Overload’is not a new term but with the massive development in technology which enables anytime, anywhere, easy and unlimited access; participation & publishing of information has consequently escalated its impact. Assisting users‘informational searches with reduced reading surfing time by extracting and evaluating accurate, authentic & relevant information are the primary concerns in the present milieu. Automatic text summarization is the process of condensing an original document into shorter form to create smaller, compact version from the abundant information that is available, preserving the content & meaning such that it meets the needs of the user. Though many summarization techniques have been proposed but there are no ‘silver bullets’to achieve the superlative results as of human generated summaries. Thus, the domain of text summarization is an active and dynamic field of study, practice & research with the continuous need to expound novel techniques for achieving comparable & effectual results. Fuzzy logic has appeared as a powerful theoretical framework for studying human reasoning and its application has been explored within the domain of text summarization in the past few years. This paper is a systematic literature review to gather, analyze, and report the trends, gaps and prospects of using fuzzy logic for automatic text summarization on the basis of the findings in original studies.", 
The electroencephalogram (EEG) signal captures the electrical activity of the brain and is an important source of information for studying neurological disorders. This article explores the application of least squares support vector machines (LS-SVM) to the task of epilepsy diagnosis through automatic EEG signal classification.,Tackling EEG signal classification with least squares support vector machines-A sensitivity analysis study,"The electroencephalogram (EEG) signal captures the electrical activity of the brain and is an important source of information for studying neurological disorders. The proper analysis of this biological signal plays an important role in the domain of brain–computer interface, which aims at the construction of communication channels between human brain and computers. In this paper, we investigate the application of least squares support vector machines (LS-SVM) to the task of epilepsy diagnosis through automatic EEG signal classification. More specifically, we present a sensitivity analysis study by means of which the performance levels exhibited by standard and least squares SVM classifiers are contrasted, taking into account the setting of the kernel function and of its parameter value. Results of experiments conducted over different types of features extracted from a benchmark EEG signal dataset evidence that the sensitivity profiles of the kernel machines are qualitatively similar, both showing notable performance in terms of accuracy and generalization. In addition, the performance accomplished by optimally configured LS-SVM models is also quantitatively contrasted with that obtained by related approaches for the same dataset.", 
"Paper presents a detailed analysis of three methods for achieving the detection of redundant information. Semantic-based methods are able to detect up to 90% of redundancy, compared to only 19% for lexical-based ones.",Tackling redundancy in text summarization through different levels of language analysis,"One of the main challenges to be addressed in text summarization concerns the detection of redundant information. This paper presents a detailed analysis of three methods for achieving such goal. The proposed methods rely on different levels of language analysis: lexical, syntactic and semantic. Moreover, they are also analyzed for detecting relevance in texts. The results show that semantic-based methods are able to detect up to 90% of redundancy, compared to only the 19% of lexical-based ones. This is also reflected in the quality of the generated summaries, obtaining better summaries when employing syntactic- or semantic-based approaches to remove redundancy", 
"Tamil Document Summarization using sub graph presents a method for extracting sentences from an individual document. Subject–Object–Predicate (SOP) triples are extracted from individual sentences to create a semantic graph of the original document and the corresponding human extracted summary. Using the Support Vector Machine (SVM) learning algorithm, a classifier has been trained to identify SOP triples from the document semantic graph that belong to the summary.",Tamil Document Summarization Using Semantic Graph Method,"Document summarization refers to the task of producing shorter version of the original document by selecting important sentences from the text. Tamil Document Summarization using sub graph presents a method for extracting sentences from an individual document to serve as a document summary or a pre-cursor to creating a generic document abstract. Language-Neutral Syntax (LNS) [3], a system of representation for natural language sentences has been used for considering the semantics of the documents. Syntactic analysis of the text that produces a logical form analysis has been applied for each sentence. Subject–Object–Predicate (SOP) triples are extracted from individual sentences to create a semantic graph [2] of the original document and the corresponding human extracted summary. Semantic Normalization is applied to SOP triples to reduce the number of nodes in the semantic graph of the original document. Using the Support Vector Machine (SVM) learning algorithm, a classifier has been trained to identify SOP triples from the document semantic graph that belong to the summary. The classifier is then used for automatic extraction of summaries from the test documents.", 
Relevance Prediction is a more intuitive measure of an individual's performance on a real-world task than interannotator agreement. This measure is shown to be a more reliable measure of task performance than LDC Agreement. The significance level for detected differences is higher for the former than for the latter.,Task-based evaluation of text summarization using Relevance Prediction,"This article introduces a new task-based evaluation measure called Relevance Prediction that is a more intuitive measure of an individual’s performance on a real-world task than interannotator agreement. Relevance Prediction parallels what a user does in the real world task of browsing a set of documents using standard search tools, i.e., the user judges relevance based on a short summary and then that same user—not an independent user—decides whether to open (and judge) the corresponding document. This measure is shown to be a more reliable measure of task performance than LDC Agreement, a current gold-standard based measure used in the summarization evaluation community. Our goal is to provide a stable framework within which developers of new automatic measures may make stronger statistical statements about the effectiveness of their measures in predicting summary usefulness. We demonstrate—as a proof-of-concept methodology for automatic metric developers—that a current automatic evaluation measure has a better correlation with Relevance Prediction than with LDC Agreement and that the significance level for detected differences is higher for the former than for the latter", 
Traditional taxi systems often suffer from inefficiencies due to uncoordinated actions as system capacity and customer demand change. Large amounts of information regarding customer demand and system status can be collected in real time. This information provides opportunities to perform various types of control and coordination for large-scale intelligent transportation systems.,Taxi Dispatch With Real-Time Sensing Data in Metropolitan Areas A Receding Horizon Control Approach,"Traditional taxi systems in metropolitan areas often suffer from inefficiencies due to uncoordinated actions as system capacity and customer demand change. With the pervasive deployment of networked sensors in modern vehicles, large amounts of information regarding customer demand and system status can be collected in real time. This information provides opportunities to perform various types of control and coordination for large-scale intelligent transportation systems. In this paper, we present a receding horizon control (RHC) framework to dispatch taxis, which incorporates highly spatiotemporally correlated demand/supply models and real-time Global Positioning System (GPS) location and occupancy information. The objectives include matching spatiotemporal ratio between demand and supply for service quality with minimum current and anticipated future taxi idle driving distance. Extensive trace-driven analysis with a data set containing taxi operational records in San Francisco, CA, USA, shows that our solution reduces the average total idle distance by 52%, and reduces the supply demand ratio error across the city during one experimental time slot by 45%. Moreover, our RHC framework is compatible with a wide variety of predictive models and optimization problem formulations. This compatibility property allows us to solve robust optimization problems with corresponding demand uncertainty models that provide disruptive event information.", 
Cloud computing presents many promising technological and economical opportunities. Many customers are reluctant to move their IT infrastructure to the cloud due to security concerns. Traditional approaches to evidence collection and recovery are no longer practical in the cloud. This paper focuses on the technical aspects of digital forensics in distributed cloud environments.,Technical Issues of Forensic Investigations in Cloud Computing Environments,"Cloud Computing is arguably one of the most discussed information technologies today. It presents many promising technological and economical opportunities. However, many customers remain reluctant to move their business IT infrastructure completely to the cloud. One of their main concerns is Cloud Security and the threat of the unknown. Cloud Service Providers (CSP) encourage this perception by not letting their customers see what is behind their virtual curtain. A seldomly discussed, but in this regard highly relevant open issue is the ability to perform digital investigations. This continues to fuel insecurity on the sides of both providers and customers. Cloud Forensics constitutes a new and disruptive challenge for investigators. Due to the decentralized nature of data processing in the cloud, traditional approaches to evidence collection and recovery are no longer practical. This paper focuses on the technical aspects of digital forensics in distributed cloud environments. We contribute by assessing whether it is possible for the customer of cloud computing services to perform a traditional digital investigation from a technical point of view. Furthermore we discuss possible solutions and possible new methodologies helping customers to perform such investigations.",  
People urgently need a technology that can automatically extract abstracts from text. Traditional extractive automatic abstract method can only extract keywords or key sentences. We propose a method based on knowledge graph technology to automatically extracts abstract texts.,Template Oriented Text Summarization via Knowledge Graph,"People are flooded with massive semi-structured and unstructured texts in their daily work life. The fast-paced lifestyle has forced us to get more focused information from these large amounts of text more quickly. So people urgently need a technology that can automatically extract abstracts from text. The traditional extractive automatic abstract method can only extract keywords or key sentences. Although the current popular sequence-to-sequence extraction methods have greatly improved compared with the traditional methods, they cannot be combined with the background information to obtain higher level abstraction. Therefore, we propose a method based on knowledge graph technology to automatically extract abstract texts. This method can not only obtain higher-level extraction from the text, but also can select template and question and answer to obtain a personalized abstract. We experimented on the CNN DAILYMAIL dataset. The results show that the abstract obtained by this method can reflect more textual information, and more in line with human reading habits, and can achieve personalized extraction, and can obtain close to the best ROUGE index results.", 
Historical records and expert assessments warned of a far-right anti-Muslim act of violence for some time. Study examined people's reported anxiety about the possibility of a terrorist attack in New Zealand. Results reveal a relatively strong association between terrorism anxiety and attitudes toward Muslims.,Terrorism Anxiety and Attitudes toward Muslims,"Many communities in New Zealand were left shaken following the terrorist attack against two Muslim mosques in Christchurch on March 15, 2019. However, historical records and expert assessments warned of a far-right anti-Muslim act of violence for some time. Our study examined people’s reported anxiety about the possibility of a terrorist attack in New Zealand using data from the 2017/2018 New Zealand Attitudes and Values Study (N = 17,072). Although anxiety regarding a potential terrorist attack was low, warmth toward Muslims correlated negatively with terrorism anxiety. Numerous other socio-demographic and attitudinal variables (e.g., age, gender, political orientation, nationalism, and aspects of personality) also correlated with terrorism anxiety. Collectively, our results reveal a relatively strong association between terrorism anxiety and attitudes toward Muslims. It remains an open question as to whether this association will endure over time, despite growing evidence of terrorism stemming from the far-right", 
"This research investigates the causes and impact of war against terrorism on Pakistan's economy, governance and social life. The data was collected through sources available on official websites, available literature and by using a questionnaire from 320 respondents living in FATA area.","Terrorism in Pakistan Genesis, Damages and Way Forward","Pakistan came into being as a truly democratic Islamic state, she was once a state with strong economy, stable government and policies. The golden principles of peace, liberty, and justice were preserved in its basic idea which led to the creation of an independent state. This research investigates the causes and impact of war against terrorism on Pakistan’s economy, governance and social life. The data was collected through sources available on official websites, available literature were taken into consideration and by using a questionnaire from 320 respondents living in FATA area. Evidence is highlighted through tables, percentage values and graphs. Empirical results show that terrorism has substantially affected Pakistan’s economy and governance in terms of unemployment, poverty, high cost of security in public and private sectors, uncertainty and high cost of doing business. Finally way forward to overcome the shortfall is suggested.", 
Since '9/11' Pakistan has been at the epicentre of both terrorism and the war against it. This special paper helps to explain the psychosocial perspective of terrorism in Pakistan that leads to violent radicalisation.,Terrorism in Pakistan the psychosocial context and why it matters,"Terrorism is often construed as a well-thoughtout, extreme form of violence to perceived injustices. The after effects of terrorism are usually reported without understanding the underlying psychological and social determinants of the terrorist act. Since ‘9/11’ Pakistan has been at the epicentre of both terrorism and the war against it. This special paper helps to explain the psychosocial perspective of terrorism in Pakistan that leads to violent radicalisation. It identifies the terrorist acts in the background of Pakistan’s history, current geopolitical and social scenario. The findings may also act as a guide on addressing this core issue.", 
Text Categorization is the task of automatically sorting a set of documents into categories. Document Summarization is an emerging technique for understanding the main purpose of any kind of documents.,Test model for text categorization and text summarization,Text Categorization is the task of automatically sorting a set of documents into categories from a predefined set and Text Summarization is a brief and accurate representation of input text such that the output covers the most important concepts of the source in a condensed manner. Document Summarization is an emerging technique for understanding the main purpose of any kind of documents. This paper presents a model that uses text categorization and text summarization for searching a document based on user query., 
"The vital characteristic of randomness is unpredictability. We propose a new randomness testing method with the artificial neural network (ANN) to verify which sources of random numbers are truly unpredictable. The testing results indicate that the random sequences from natural number ? and LCG fail to pass our randomness test, while the other two kinds of sequences pass the test successfully.",Testing Randomness Using Artificial Neural Network,"The vital characteristic of randomness is unpredictability. Thus any regularity will compromise the application of random numbers. Quantum random number generators (QRNGs) can provide intrinsic unpredictable randomness based on the nature of quantum physics, while pseudo-random number generator can not due to the origination of deterministic algorithms or physical processes. However, commonly used traditional test suits are rigorously to test the statistical properties, and the unpredictability of random numbers is still difficult to test. To verify which sources of random numbers are truly unpredictable, in this paper, we propose a new randomness testing method with the artificial neural network (ANN). Random number sequences generated by four different kinds of sources are tested, which are the natural number ?, the linear congruence generator (LCG) pseudo-random algorithm, the Mersenne Twister (MT) pseudorandom algorithm and the QRNG based on vacuum noise, respectively. The testing results indicate that the random sequences from natural number ? and LCG fail to pass our randomness test, while the other two kinds of sequences pass the test successfully due to the relatively simple structure of our ANN. As the complexity of the ANN and the amount of computing power involved increasing, this test method has potential to predict the MT random numbers, and it is also expected that the quantum random numbers can not pass the test with unpredictability no matter how complex the ANN is.", 
"Text summarization is useful for bring the short story of all the newspaper articles, email correspondence or to extract key elements for the search engine. Large unstructured data can be converted in such form that can be used for report making, compacting of web pages and review of the book.",Text analysis and information retrieval of text data,"Text summarization combines the process of POS tagging, term frequency and topical analysis. All these together are used to produce insightful summary of the document/documents. The concise version of the text document can be made using the concept of frequency of the terms and inverse frequency of documents. Text summarization is useful for bring the short story of all the newspaper articles, email correspondence or to extract key elements for the search engine. To compact the size, the sentences which are not near to the centroid is not to be considered in the output. To do that, the data which does not relate to the centroid topic has to be pruned. The output consists of only important data useful to the user. Large unstructured data can be converted in such form that can be used for report making, compacting of web pages and review of the book. In this, the summary from documents contains significant information, and is less than half of the original size. The output should be such that it fully satisfies the user’s query and understands the answer given to it.", 
Text summarization is an approach by which the size of one or more document is shortened and the shorten passage presents the core information of the document. Many methods have already implemented for English text and the effort for Bengali text is gaining.,Text analysis for Bengali Text Summarization using Deep Learning,"Text summarization is an approach by which the size of one or more document is shortened and the shorten passage presents the core information of the document. In this modern era of information technology, we are over flooded with online data which raised the necessity of summary of the original text. Many methods have already implemented for English text and the effort for Bengali text are gaining alongside. In this paper, we propose an extractive text summarization technique based on a deep learning model of Recurrent Neural Network (RNN) for single document summary. Our method is to classify the sentences as significant or not for the summary. We have used Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU) based RNN. Between them, we found LSTM more promising and we achieved average F1 scores- 0.63, 0.59, 0.56 for Rouge-1, Rouge-2 and Rouge-3 in some respects", 
"Secure patterns provide a solution for the security requirement of the software. There are large number of secure patterns, and it is difficult to choose an appropriate pattern. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization.",Text Categorization Approach for Secure Design,"Secure patterns provide a solution for the security requirement of the software. There are large number of secure patterns, and it is quite difficult to choose an appropriate pattern. Moreover, selection of these patterns needs security knowledge; generally, developers are not specialized in the domain of security knowledge. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization. A repository of secure design patterns is used as a data set and a repository of requirements artifacts in the form of software requirements specification (SRS) are used for this paper. A text categorization scheme, which begins with preprocessing, indexing of secure patterns, ends up by querying SRS features for retrieving secure design pattern using document retrieval model. For the evaluation of the proposed model, we have used three different domains’ SRS. These three SRS documents represent three different domains, i.e., e-commerce, social media, and desktop utility program. A traditional precision and recall method along with F-measure used for evaluation of information/document retrieval model is used to evaluate the results. F-measure for 17 different design problems shows around 81% accuracy with recall up to 0.69%.", 
Automatic text summarization essentially condenses a long document into a shorter format. It is a potential solution to the information overload. Several automatic summarizers exist in the literature capable of producing highquality summaries. They do not focus on preserving the underlying meaning and semantics of the text.,Text document summarization using word embedding,"Automatic text summarization essentially condenses a long document into a shorter format while preserving its information content and overall meaning. It is a potential solution to the information overload. Several automatic summarizers exist in the literature capable of producing highquality summaries, but they do not focus on preserving the underlying meaning and semantics of the text. In this paper, we capture and preserve the semantics of text as the fundamental feature for summarizing a document. We propose an automatic summarizer using the distributional semantic model to capture semantics for producing high-quality summaries. We evaluated our summarizer using ROUGE on DUC-2007 dataset and compare our results with other four different state-of-the-art summarizers. Our system outperforms the other reference summarizers leading us to the conclusion that usage of semantic as a feature for text summarization provides improved results and helps to further reduce redundancies from the input source.",  
This paper aims to perform text feature weighting for summarization of documents in bahasa Indonesia using genetic algorithm. We investigate the effect of the first ten sentence features on the summarization task. We use latent semantic feature to increase the accuracy. All feature score functions are used to train a genetic algorithm model to obtain a suitable combination of feature weights.,Text feature weighting for summarization of document Bahasa Indonesia using genetic algorithm,"This paper aims to perform text feature weighting for summarization of documents in bahasa Indonesia using genetic algorithm. There are eleven text features, i.e, sentence position (f1), positive keywords in sentence (f2), negative keywords in sentence (f3), sentence centrality (f4), sentence resemblance to the title (f5), sentence inclusion of name entity (f6), sentence inclusion of numerical data (f7), sentence relative length (f8), bushy path of the node (f9), summation of similarities for each node (f10), and latent semantic feature (f11). We investigate the effect of the first ten sentence features on the summarization task. Then, we use latent semantic feature to increase the accuracy. All feature score functions are used to train a genetic algorithm model to obtain a suitable combination of feature weights. Evaluation of text summarization uses F-measure. The Fmeasure is directly related to the compression rate. The results showed that adding f11 increases the F-measure by 3.26% and 1.55% for compression ratio of 10% and 30%, respectively. On the other hand, it decreases the F-measure by 0.58% for compression ratio of 20%. Analysis of text feature weight showed that only using f2, f4, f5, and f11 can deliver a similar performance using all eleven features.", 
"Text mining is a research field which tries to discover valuable information from unstructured texts. In this paper, we have discussed general idea of text mining and comparison of its techniques.","Text mining, Techniques and its application","Text mining has become an exciting research field as it tries to discover valuable information from unstructured texts. The unstructured texts which contain vast amount of information cannot simply be used for further processing by computers. Therefore, exact processing methods, algorithms and techniques are vital in order to extract this valuable information which is completed by using text mining. In this paper, we have discussed general idea of text mining and comparison of its techniques. In addition, we briefly discuss a number of text mining applications which are used presently and in future.", 
Study is focused on how the algorithm can be applied on number of documents. strengths and weaknesses of TD-IDF algorithm are compared. Future research directions are discussed.,"Text mining, use of TF-IDF to examine the relevance of words to documents","In this paper, the use of TF-IDF stands for (term frequencyinverse document frequency) is discussed in examining the relevance of key-words to documents in corpus. The study is focused on how the algorithm can be applied on number of documents. First, the working principle and steps which should be followed for implementation of TF-IDF are elaborated. Secondly, in order to verify the findings from executing the algorithm, results are presented, then strengths and weaknesses of TD-IDF algorithm are compared. This paper also talked about how such weaknesses can be tackled. Finally, the work is summarized and the future research directions are discussed", 
Text Summarization is a challenging problem these days. It is a useful task that gives support to many other tasks. It takes advantage of the techniques developed for related Natural Language Processing tasks.,"TEXT SUMMARIZATION , AN OVERVIEW","This paper presents an overview of Text Summarization. Text Summarization is a challenging problem these days. Due to the great amount of information we are provided with and thanks to the development of Internet technologies, needs of producing summaries have become more and more widespread. Summarization is a very interesting and useful task that gives support to many other tasks as well as it takes advantage of the techniques developed for related Natural Language Processing tasks. The paper we present here may help us to have an idea of what Text Summarization is and how it can be useful for.",  
"PubMed data can hold promise for extracting decision support information. This study evaluated the efficiency of a text summarization application called Semantic MEDLINE. It processed citations addressing the prevention and drug treatment of four disease topics. It outperformed conventional summarization in terms of recall, and outperformed the baseline method in both recall and precision. The application has potential in identifying decision support data for multiple needs.",Text summarization as a decision support aid,"PubMed data potentially can provide decision support information, but PubMed was not exclusively designed to be a point-of-care tool. Natural language processing applications that summarize PubMed citations hold promise for extracting decision support information. The objective of this study was to evaluate the efficiency of a text summarization application called Semantic MEDLINE, enhanced with a novel dynamic summarization method, in identifying decision support data. We downloaded PubMed citations addressing the prevention and drug treatment of four disease topics. We then processed the citations with Semantic MEDLINE, enhanced with the dynamic summarization method. We also processed the citations with a conventional summarization method, as well as with a baseline procedure. We evaluated the results using clinician-vetted reference standards built from recommendations in a commercial decision support product, DynaMed. For the drug treatment data, Semantic MEDLINE enhanced with dynamic summarization achieved average recall and precision scores of 0.848 and 0.377, while conventional summarization produced 0.583 average recall and 0.712 average precision, and the baseline method yielded average recall and precision values of 0.252 and 0.277. For the prevention data, Semantic MEDLINE enhanced with dynamic summarization achieved average recall and precision scores of 0.655 and 0.329. The baseline technique resulted in recall and precision scores of 0.269 and 0.247. No conventional Semantic MEDLINE method accommodating summarization for prevention exists. Semantic MEDLINE with dynamic summarization outperformed conventional summarization in terms of recall, and outperformed the baseline method in both recall and precision. This new approach to text summarization demonstrates potential in identifying decision support data for multiple needs.", 
"This paper has proposed a language-independent, semantic-aware approach that applies the harmony search algorithm to generate appropriate multi-document summaries. It learns the objective function from an extra set of reference summaries and then generates the best summaries according to the trained function. It was able to generally outperform other cited summarizer systems.","Text Summarization as a Multi-objective Optimization Task, Applying Harmony Search to Extractive Multi-Document Summarization","Today, automated extractive text summarization is one of the most common techniques for organizing information. In extractive summarization, the most appropriate sentences are selected from the text and build a representative summary. Therefore, probing for the best sentences is a fundamental task. This paper has coped with extractive summarization as a multi-objective optimization problem and proposed a language-independent, semantic-aware approach that applies the harmony search algorithm to generate appropriate multi-document summaries. It learns the objective function from an extra set of reference summaries and then generates the best summaries according to the trained function. The system also performs some supplementary activities for better achievements. It expands the sentences by using an inventive approach that aims at tuning conceptual densities in the sentences towards important topics. Furthermore, we introduced an innovative clustering method for identifying important topics and reducing redundancies. A sentence placement policy based on the Hamiltonian shortest path was introduced for producing readable summaries. The experiments were conducted on DUC2002, DUC2006 and DUC2007 datasets. Experimental results showed that the proposed framework could assist the summarization process and yield better performance. Also, it was able to generally outperform other cited summarizer systems.", 
"In this paper, we introduce an unsupervised graph based ranking model for text summarization. Our model builds a graph by collecting words, and their lexical relationships from the document. We apply a handful of available semantic information to enhance edgeweights (interconnectivity) between nodes.",Text summarization as an assistive technology,"Automated text summarization can be applied as an assistive tool for people with vision deficiency as well as with language understanding or attention deficit disorders. In this paper, we introduce an unsupervised graph based ranking model for text summarization. Our model builds a graph by collecting words, and their lexical relationships from the document. We apply a handful of available semantic information (definition, sentimental polarity) of words to enhance edgeweights (interconnectivity) between nodes (words). After applying a polarity based ranking algorithm over the graph we collect a subset of high-ranked and low-ranked words, name those as keywords. We, then, extract sentences that possess a higher rank defined by the rank vector of keywords. Sentences extracted in this manner correlate with each other and express the summary of the document quite successfully. Summaries formed by our model can appease readers with vision difficulties while keep them updated.", 
"Text classification (TC) or text categorization task is assigning a document to one or more predefined classes or categories. A common problem in TC is the high number of terms or features in document(s) to be classified. This problem can be solved by selecting the most important terms. In this study, an automatic text summarization is used for feature selection.",Text summarization as feature selection for arabic text classification,"Text classification (TC) or text categorization task is assigning a document to one or more predefined classes or categories. A common problem in TC is the high number of terms or features in document(s) to be classified (the curse of dimensionality). This problem can be solved by selecting the most important terms. In this study, an automatic text summarization is used for feature selection. Since text summarization is based on identifying the set of sentences that are most important for the overall understanding of document(s). We address the effectiveness of using summarization techniques on text classification. Another feature selection technique is used, which is Term Frequency (TF) on the same but full-text data set, i.e., before summarization. Support Vector Machine is used to classify our Arabic data set. The classifier performance is evaluated in terms of classification accuracy, precision, recall, and the execution time. Finally, a comparison is held between the results of classifying full documents and summarized documents.", 
Extractive compression is a challenging natural language processing problem. This work formulates neural extractive compression as a parse tree transduction problem. The proposed model can achieve state of the art performance on sentence compression benchmarks.,Text Summarization as Tree Transduction by Top-Down TreeLSTM,"Extractive compression is a challenging natural language processing problem. This work contributes by formulating neural extractive compression as a parse tree transduction problem, rather than a sequence transduction task. Motivated by this, we introduce a deep neural model for learning structure-tosubstructure tree transductions by extending the standard Long Short-Term Memory, considering the parent-child relationships in the structural recursion. The proposed model can achieve state of the art performance on sentence compression benchmarks, both in terms of accuracy and compression rate.", 
"The information overload faced by today's society has created a big challenge for people who want to look for relevant information from the internet. In this paper, we propose a text summarization model based on classification using Adaptive Neuro-Fuzzy Inference System (ANFIS) The model can learn to filter high quality summary sentences.",Text Summarization Based on Classification Using ANFIS,"The information overload faced by today’s society has created a big challenge for people who want to look for relevant information from the internet. There are a lot of online documents available and digesting such large texts collection is not an easy task. Hence, automatic text summarization is required to automate the process of summarizing text by extracting only the salient information from the documents. In this paper, we propose a text summarization model based on classification using Adaptive Neuro-Fuzzy Inference System (ANFIS). The model can learn to filter high quality summary sentences. We then compare the performance of our proposed model with the existing approaches which are based on neural network and fuzzy logic techniques. ANFIS was able to alleviate the limitations in the existing approaches and the experimental finding of this study shows that the proposed model yields better results in terms of precision, recall and F-measure on the Document Understanding Conference (DUC) data corpus.", 
"In this article, we present an original approach for text summarization using conceptual data classification. Concept date classification is used to extract the most interacting sentences from the main text. The system may be incorporated with the indexers of search engines over the Internet to find key words.",Text Summarization Based on Conceptual Data Classification,"In this article, we present an original approach for text summarization using conceptual data classification. We show how a given text can be summarized without losing meaningful knowledge and without using any semantic or grammatical concepts. In fact, concept date classification is used to extract the most interacting sentences from the main text and ignoring the other meaningless sentences in order to generate the text summary. The approach is tested on Arabic and English texts with different sizes and different topics and the obtained results are satisfactory. The system may be incorporated with the indexers of search engines over the Internet in order to find key words and other pertinent information of the new deployed Web pages that would be stored in databases for quick search.", 
"This work proposes an approach to address the problem of improving content selection in automatic text summarization. The approach is a trainable summarizer, which takes into account several features, for each sentence. The proposed approach performance is measured at several compression rates.",Text Summarization Based on Genetic Programming,"This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. This approach is a trainable summarizer, which takes into account several features, for each sentence to generate summaries. First, we investigate the effect of each sentence feature on the summarization task. Then we use all features in combination to train genetic programming (GP), vector approach and fuzzy approach in order to construct a text summarizer for each model. Furthermore, we use trained models to test summarization performance. The proposed approach performance is measured at several compression rates on a data corpus composed of 17 English scientific articles.", 
"Salience and coverage are two most important issues for summaries. In this paper, we propose a novel summarization model called Sentence Selection with Semantic Representation. The selection strategy used in SSSR is to select sentences that can reconstruct the original document with least distortion.",Text Summarization Based on Sentence Selection with Semantic Representation,"Text summarization is of great importance to solve information overload. Salience and coverage are two most important issues for summaries. Most existing models extract summaries by selecting the top sentences with highest scores without using the relationships between sentences, and usually represent the sentences simply basing on lexical or statistical features. As a result, those models can not achieve salience or coverage very well. In this paper, we propose a novel summarization model called Sentence Selection with Semantic Representation (SSSR). SSSR ensures both salience and coverage by learning semantic representations for sentences and applying a well-designed selection strategy to select summary sentences. The selection strategy used in SSSR is to select sentences that can reconstruct the original document with least distortion by means of linear combination. Besides, we improve our selection strategy by reducing redundant information. Then we learn two semantic representations for sentences: (1) weighted mean of word embeddings; (2) deep coding. Both of them are semantic and compact, and can capture similarities between sentences. Extensive experiments on datasets DUC2006 and DUC2007 validate our model", 
In recent times owing to the magnitude of data present digitally across networks over a wide range of databases the need for text summarization has never been higher. We apply the technique of calculating a threshold value from both the attribute and semantic structure of the individual words.,Text summarization basing on font and cue-phrase feature for a single document,"In recent times owing to the magnitude of data present digitally across networks over a wide range of databases the need for text summarization has never been higher. The following paper deals with summarization of text derived from the syntactic and semantic features of the words in the document. We apply the technique of calculating a threshold value from both the attribute and semantic structure of the individual words. The algorithm helps in calculating the threshold value in order to give weightage to a particular word in a document. Initially the document undergoes the preprocessing techniques; the obtained data will be kept in a data set, then on that data we will apply the proposed algorithm in order to get a summarized data.", 
This thesis discusses the state-of-the-art algorithms that use machine learning approach. We also present the means of evaluating summaries using ROUGE metrics.,Text summarization by machine learning,"This thesis discusses the state-of-the-art algorithms, focusing only on those that use machine learning approach, such as neural network based NetSum, or graph representation, such as LexRank. We also present the means of evaluating summaries using ROUGE metrics. A part of the work is experimental implementation of a method that takes advantage of frequent patterns extracted from text. We evaluate and compare the performance of our methods to traditional methods found in automatic text summarization.", 
"An unsupervised algorithm can help for clustering similar ideas (sentences) Then, for composing the summary, the most representative sentence is selected from each cluster. Several experiments in the standard DUC-2002 collection show that the proposed method obtains more favorable results than other approaches.",Text Summarization by Sentence Extraction Using Unsupervised Learning,"The main problem for generating an extractive automatic text summary is to detect the most relevant information in the source document. Although, some approaches claim being domain and language independent, they use high dependence knowledge like key-phrases or golden samples for machine-learning approaches. In this work, we propose a language- and domain-independent automatic text summarization approach by sentence extraction using an unsupervised learning algorithm. Our hypothesis is that an unsupervised algorithm can help for clustering similar ideas (sentences). Then, for composing the summary, the most representative sentence is selected from each cluster. Several experiments in the standard DUC-2002 collection show that the proposed method obtains more favorable results than other approaches.", 
"This paper analyzes the appropriateness and benefits of using summaries in semantic QA. The results show that QF summarization is better than generic (58% improvement) and short summaries are better than long (6.3% improvement). The use of TS within semantic. QA improves the performance for both named-entity-based (10%) and, especially, semantic-role-based QA (47.5%)","Text summarization contribution to semantic question answering, New approaches for finding answers on the web","As the Internet grows, it becomes essential to find efficient tools to deal with all the available information. Question answering (QA) and text summarization (TS) research fields focus on presenting the information requested by users in a more concise way. In this paper, the appropriateness and benefits of using summaries in semantic QA are analyzed. For this purpose, a combined approach where a TS component is integrated into a Web-based semantic QA system is developed. The main goal of this paper is to determine to what extent TS can help semantic QA approaches, when using summaries instead of search engine snippets as the corpus for answering questions. In particular, three issues are analyzed: (i) the appropriateness of query-focused (QF) summarization rather than generic summarization for the QA task, (ii) the suitable length comparing short and long summaries, and (iii) the benefits of using TS instead of snippets for finding the answers, tested within two semantic QA approaches (named entities and semantic roles). The results obtained show that QF summarization is better than generic (58% improvement), short summaries are better than long (6.3% improvement), and the use of TS within semantic QA improves the performance for both named-entity-based (10%) and, especially, semantic-role-based QA (47.5%).", 
A new technique to produce a summary of an original text investigated in this paper. The system develops many approaches to solve this problem that gave a high quality result. The model consists of four stages. Each sentence is ranked depending on many features such as the existence of the keywords/keyphrase in it.,Text Summarization Extraction System (TSES) Using Extracted Keywords,"A new technique to produce a summary of an original text investigated in this paper. The system develops many approaches to solve this problem that gave a high quality result. The model consists of four stages. The preprocess stages convert the unstructured text into structured. In first stage, the system removes the stop words, pars the text and assigning the POS (tag) for each word in the text and store the result in a table. The second stage is to extract the important keyphrases in the text by implementing a new algorithm through ranking the candidate words. The system uses the extracted keywords/keyphrases to select the important sentence. Each sentence ranked depending on many features such as the existence of the keywords/keyphrase in it, the relation between the sentence and the title by using a similarity measurement and other many features. The Third stage of the proposed system is to extract the sentences with the highest rank. The Forth stage is the filtering stage. This stage reduced the amount of the candidate sentences in the summary in order to produce a qualitative summary using KFIDF measurement.", 
"In this paper, random five features are used and investigated using a (pseudo) Genetic concept. The Document Understanding Conference (DUC2002) used to train our proposed model. The objective of this paper is to learn the weight (importance) of each feature.",Text summarization features selection method using pseudo Genetic-based model,"The features are considered the cornerstone of text summarization. The most important issue is what feature to be considered in a text summarization process. Including all the features in the summarization process may not be considered as an optimal solution. Therefore, other methods need to be deployed. In this paper, random five features used and investigated using a (pseudo) Genetic concept as an optimized trainable features selection mechanism. The Document Understanding Conference (DUC2002) used to train our proposed model; hence the objective of this paper is to learn the weight (importance) of each used feature. For each input document using the genetic concept, the size of the generation is defined and the chromosome dimension (genes) is equal to number of features used . Each gene is represents a feature and in binary format. A chromosome with high fitness value is selected to be enrolled in the final round. The average of each gene is computed for all best chromosomes and considered the weight of that feature. Our experimental result shows that our proposed model is able performing features selection process", 
Automatic text summarization as a tool has a great scope in the legal domain. Large volume of online data has necessitated the need for such a tool. An intentional weightage is given to the issue of legal text summation.,"Text Summarization for Big Data, A Comprehensive Survey","Availability of large volume of online data has necessitated the need for automatic text summarization. In this paper, a survey of various text summarization accomplishments that have been implemented in the recent past is discussed. An intentional weightage is given to the issue of legal text summarization, as summarization as a tool has a great scope in the legal domain. The paper starts with a brief introduction of automatic text summarization, then succinctly mentions the novel advances in extractive and abstractive text summarization techniques, and then moves to literature survey, and it finally winds up with some future work directions.", 
The design and implementation of the proposed systems is concerned with methods for summarizing of the retrieving information from a collection of documents. The quality of a system is measured by how useful it is to the typical users of the system.,Text Summarization for Information Retrieval using Pattern Recognition Techniques,"In the present work a model is proposed which is useful for text summarization of the given document by using pattern recognition techniques for improving the retrieval performance of the relevant information. The design and implementation of the proposed systems is concerned with methods for summarizing of the retrieving information from a collection of documents or corpuses. The quality of a system is measured by how useful it is to the typical users of the system. In the basic approach, a query is considered generated from an “ideal” document that satisfies the information need. The system’s job is then to estimate the likelihood of each document in the collection being the ideal document and rank them accordingly. The recent development of related techniques stimulates new modeling and estimation methods that are beyond the scope of the traditional approaches.", 
There is no existing system for summarizing Malayalam documents. This paper explains a statistical sentence scoring technique and a semantic graph based technique for text summarization.,Text summarization for Malayalam documents â€” An experience,The amount of data available in the internet is increasing at a very high speed. Text summarization has helped in making a better use of the information available online. Various methods were adopted to automate text summarization. However there is no existing system for summarizing Malayalam documents. In this paper we have investigated on developing efficient and effective methods to summarize Malayalam documents. This paper explains a statistical sentence scoring technique and a semantic graph based technique for text summarization., 
WWW has caused rapid growth of information explosion. Readers are overloaded with too many lengthy text documents. Oil and gas industry could not escape from this predicament. We develop an Automated Text Summarization System known as AutoTextSumm.,Text Summarization for Oil and Gas Drilling Topic,"Information sharing and gathering are important in the rapid advancement era of technology. The existence of WWW has caused rapid growth of information explosion. Readers are overloaded with too many lengthy text documents in which they are more interested in shorter versions. Oil and gas industry could not escape from this predicament. In this paper, we develop an Automated Text Summarization System known as AutoTextSumm to extract the salient points of oil and gas drilling articles by incorporating statistical approach, keywords identification, synonym words and sentence’s position. In this study, we have conducted interviews with Petroleum Engineering experts and English Language experts to identify the list of most commonly used keywords in the oil and gas drilling domain. The system performance of AutoTextSumm is evaluated using the formulae of precision, recall and F-score. Based on the experimental results, AutoTextSumm has produced satisfactory performance with F-score of 0.81.", 
"Text summarization system to produce a summary of oil and gas news articles. System integrated statistical approach with three underlying concepts: keyword occurrences, title of the news article and location of the sentence. System is able to produce an effective summary with an average recall value of 83% at the compression rate of 25%.",Text summarization for oil and gas news article,"Information is increasing in volumes; companies are overloaded with information that they may lose track in getting the intended information. It is a time consuming task to scan through each of the lengthy document. A shorter version of the document which contains only the gist information is more favourable for most information seekers. Therefore, in this paper, we implement a text summarization system to produce a summary that contains gist information of oil and gas news articles. The summarization is intended to provide important information for oil and gas companies to monitor their competitor’s behaviour in enhancing them in formulating business strategies. The system integrated statistical approach with three underlying concepts: keyword occurrences, title of the news article and location of the sentence. The generated summaries were compared with human generated summaries from an oil and gas company. Precision and recall ratio are used to evaluate the accuracy of the generated summary. Based on the experimental results, the system is able to produce an effective summary with the average recall value of 83% at the compression rate of 25%.", 
Automatic text summarization is an essential natural language processing (NLP) application. It goals to summarize a given textual content into a shorter model. The essential idea is to connect the semantic holes among content substance.,Text Summarization Framework Using Advanced ML Algo,"In recent years, much work has been performed to summarize meeting recordings, sport videos, movies, pictorial storylines and social multimedia. Automatic text summarization is an essential natural language processing (NLP) application that goals to summarize a given textual content into a shorter model. The quick development in media data transmission over the Internet requests content outline utilizing neural system from nonconcurrent blend of content. This paper speaks to a structure that uses the methods of NLP strategy to analyze the elaborative data contained in multi-modular insights and to improve the parts of content rundown. The essential idea is to connect the semantic holes among content substance. After, the created outline for significant data through multi-modular subject demonstrating. At long last, all the multi-modular components are considered to create a literary outline by expanding the significance, non-excess, believability and degree through the assigned collection of submodular highlights. The exploratory outcome shows that Text Summarization system outflanks other serious strategies.", 
"Legal text processing has become an important area of research. In this paper, we look at different text summarization techniques in the recent past. We briefly cover a few software tools used in legal text summarizing.","Text summarization from legal documents, a survey","Enormous amount of online information, available in legal domain, has made legal text processing an important area of research. In this paper, we attempt to survey different text summarization techniques that have taken place in the recent past. We put special emphasis on the issue of legal text summarization, as it is one of the most important areas in legal domain. We start with general introduction to text summarization, briefly touch the recent advances in single and multi-document summarization, and then delve into extraction based legal text summarization. We discuss different datasets and metrics used in summarization and compare performances of different approaches, first in general and then focused to legal text. we also mention highlights of different summarization techniques. We briefly cover a few software tools used in legal text summarization. We finally conclude with some future research directions.", 
This paper presents a text summarization in Android mobile devices. It is a challenge to browse large documents in a mobile device because of its small screen size and information overload problems. The objectives of the paper are to integrate WordNet 3.1 into the proposed system called TextSumIt.,Text Summarization in Android Mobile Devices,"This paper presents a text summarization in Android mobile devices. With the proliferation of small screen devices and advancement of mobile technology, the text summarization research has been inspired by the new paradigm shift in accessing information ubiquitously at anytime, anywhere and anyway on mobile devices. However, it is a challenge to browse large documents in a mobile device because of its small screen size and information overload problems. In this paper, a semantic and syntactic based summarization was attempted and implemented in a text summarizer. The objectives of the paper are two-fold. (1) To integrate WordNet 3.1 into the proposed system called TextSumIt which condenses single lengthy document into shorter summarized text. (2) To provide better readability to Android mobile users by displaying the salient ideas in bullets points. Documents were collected from DUC 2002 and Reuter news datasets. Experimental results show that the text summarization model improves the accuracy, readability and time saving in the text summarizer as compared with MS Word AutoSummarize.", 
"This is the first systematic review of text summarization in the biomedical domain. The study identified research gaps and provides recommendations for guiding future research. Recent research has focused on a hybrid technique comprising statistical, language processing and machine learning techniques. Further research is needed on the application of summarization to real research or patient care settings.","Text summarization in the biomedical domain, A systematic review of recent research","The amount of information for clinicians and clinical researchers is growing exponentially. Text summarization reduces information as an attempt to enable users to find and understand relevant source texts more quickly and effortlessly. In recent years, substantial research has been conducted to develop and evaluate various summarization techniques in the biomedical domain. The goal of this study was to systematically review recent published research on summarization of textual documents in the biomedical domain. MEDLINE (2000 to October 2013), IEEE Digital Library, and the ACM digital library were searched. Investigators independently screened and abstracted studies that examined text summarization techniques in the biomedical domain. Information is derived from selected articles on five dimensions: input, purpose, output, method and evaluation. Of 10,786 studies retrieved, 34 (0.3%) met the inclusion criteria. Natural language processing (17; 50%) and a hybrid technique comprising of statistical, Natural language processing and machine learning (15; 44%) were the most common summarization approaches. Most studies (28; 82%) conducted an intrinsic evaluation. This is the first systematic review of text summarization in the biomedical domain. The study identified research gaps and provides recommendations for guiding future research on biomedical text summarization. Recent research has focused on a hybrid technique comprising statistical, language processing and machine learning techniques. Further research is needed on the application and evaluation of text summarization in real research or patient care settings.", 
"This chapter gives an overview of recent advances in the field of biomedical text summarization. Different types of challenges are introduced, and methods are discussed concerning the type of challenge that they address. Future lines of work are pointed out and current trends are discussed.",Text Summarization in the Biomedical Domain,"This chapter gives an overview of recent advances in the field of biomedical text summarization. Different types of challenges are introduced, and methods are discussed concerning the type of challenge that they address. Biomedical literature summarization is explored as a leading trend in the field, and some future lines of work are pointed out. Underlying methods of recent summarization systems are briefly explained and the most significant evaluation results are mentioned. The primary purpose of this chapter is to review the most significant research efforts made in the current decade toward new methods of biomedical text summarization. As the main parts of this chapter, current trends are discussed and new challenges are introduced.", 
"The encoder-decoder model is widely used in text summarization research. Soft attention is used to obtain the required contextual semantic information during decoding. However, due to the lack of access to the key features, the generated summary deviates from the core content.",Text Summarization Method Based on Double Attention Pointer Network,"A good document summary should summarize the core content of the text. Research on automatic text summarization attempts to solve this problem. The encoder-decoder model is widely used in text summarization research. Soft attention is used to obtain the required contextual semantic information during decoding. However, due to the lack of access to the key features, the generated summary deviates from the core content. In this paper, we proposed an encoder-decoder model based on a double attention pointer network (DAPT). In DAPT, the self-attention mechanism collects key information from the encoder, the soft attention and the pointer network generate more coherent core content, and the fusion of both generates accurate and coherent summaries. In addition, the improved coverage mechanism is used to address the repetition problem and improve the quality of the generated summaries. Simultaneously, scheduled sampling and reinforcement learning (RL) are combined to generate new training methods to optimize the model. Experiments on the CNN/Daily Mail dataset and the LCSTS dataset show that our model performs as well as many state-of-the-art models. The experimental analysis shows that our model achieves higher summarization performance and reduces the occurrence of repetition.", 
We discuss text summarization in terms of maximum coverage problem and its variant. We explore some decoding algorithms including the ones never used in this summarization formulation. We augment the summarization model so that it takes into account the relevance,Text summarization model based on maximum coverage problem and its variant,"We discuss text summarization in terms of maximum coverage problem and its variant. We explore some decoding algorithms including the ones never used in this summarization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-andbound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC’04 on ROUGE-1 without stopwords", 
We propose a multi-document generic summarization model based on the budgeted median problem. Our model selects sentences to generate a summary so that every sentence in the document cluster,Text summarization model based on the budgeted median problem,We propose a multi-document generic summarization model based on the budgeted median problem. Our model selects sentences to generate a summary so that every sentence in the document cluster can be assigned to and be represented by a sentence in the summary as much as possible. The advantage of this model is that it covers the entire relevant part of the document cluster through sentence assignment and can incorporate asymmetric relations between sentences such as textual entailment., 
Automatic summarization plays an important role in document processing system and information retrieval system. In this paper we present a novel approach for text summarization of Hindi text document based on some linguistic rules. Dead wood words and phrases are also removed from the original document. Proposed system is tested on various Hindi inputs and accuracy of the system.,Text Summarization of Hindi Documents Using Rule Based Approach,Automatic summarization plays an important role in document processing system and information retrieval system. Generation of summary of a text document is a very important part of NLP. There are a number of scenarios where automatic construction of such summaries is useful. Text summarization is that process which convert a larger text into its shorter form maintaining its information. Summary of a longer text saves the reading time as it contain lesser number of lines but all important information of the original text document. In this paper we present a novel approach for text summarization of Hindi text document based on some linguistic rules. Dead wood words and phrases are also removed from the original document to generate the lesser number of words from the original text. Proposed system is tested on various Hindi inputs and accuracy of the system in form of number of lines extracted from original text containing important information of the original text document, 
"Text summarization solves the problem of extracting important information from huge amount of text data. One of the most commonly used methods is the Latent Semantic Analysis (LSA) In this paper, different LSA based",Text summarization of turkish texts using latent semantic analysis,"Text summarization solves the problem of extracting important information from huge amount of text data. There are various methods in the literature that aim to find out well-formed summaries. One of the most commonly used methods is the Latent Semantic Analysis (LSA). In this paper, different LSA based summarization algorithms are explained and two new LSA based summarization algorithms are proposed. The algorithms are evaluated on Turkish documents, and their performances are compared using their ROUGE-L scores. One of our algorithms produces the best scores.", 
The main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.,"Text Summarization Techniques, A Brief Survey","In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.", 
Sentence Connectivity is a textual characteristic that may be incorporated intelligently for the selection of sentences of a well meaning summary. The existing summarization methods do not utilize its potential fully. This paper introduces a novel method for singledocument text summarization.,Text Summarization through Entailment-based Minimum Vertex Cover,"Sentence Connectivity is a textual characteristic that may be incorporated intelligently for the selection of sentences of a well meaning summary. However, the existing summarization methods do not utilize its potential fully. The present paper introduces a novel method for singledocument text summarization. It poses the text summarization task as an optimization problem, and attempts to solve it using Weighted Minimum Vertex Cover (WMVC), a graph-based algorithm. Textual entailment, an established indicator of semantic relationships between text units, is used to measure sentence connectivity and construct the graph on which WMVC operates. Experiments on a standard summarization dataset show that the suggested algorithm outperforms related methods.", 
"This paper proposes two approaches to address text summarization. The first is a trainable summarizer, which takes into account several features to generate summaries. The second uses latent semantic analysis to derive the semantic matrix of a document or a corpus. The two novel approaches were measured at several compression rates.",Text summarization using a trainable summarizer and latent semantic analysis,"This paper proposes two approaches to address text summarization: modified corpus-based approach (MCBA) and LSA-based T.R.M. approach (LSA + T.R.M.). The first is a trainable summarizer, which takes into account several features, including position, positive keyword, negative keyword, centrality, and the resemblance to the title, to generate summaries. Two new ideas are exploited: (1) sentence positions are ranked to emphasize the significances of different sentence positions, and (2) the score function is trained by the genetic algorithm (GA) to obtain a suitable combination of feature weights. The second uses latent semantic analysis (LSA) to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. We evaluate LSA + T.R.M. both with single documents and at the corpus level to investigate the competence of LSA in text summarization. The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles. When the compression rate was 30%, an average f-measure of 49% for MCBA, 52% for MCBA + GA, 44% and 40% for LSA + T.R.M. in single-document and corpus level were achieved respectively.", 
"Pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR) The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences.",Text Summarization using Abstract Meaning Representation,"With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-ofthe-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality", 
"The amount of textual information available on the web is estimated by terra bytes. Such technique would speed up of reading, information accessing and decision making process. This paper investigates a graph based centrality algorithm on Arabic text summarization problem (ATS)",Text summarization using centrality concept,"The amount of textual information available on the web is estimated by terra bytes. Therefore constructing a software program to summarize web pages or electronic documents would be a useful technique. Such technique would speed up of reading, information accessing and decision making process. This paper investigates a graph based centrality algorithm on Arabic text summarization problem (ATS). The graph based algorithm depends on extracting the most important sentences in a documents or a set of documents (cluster). The algorithm starts computing the similarity between two sentences and evaluating the centrality of each sentence in a cluster based on centrality graph. Then the algorithm extracts the most important sentences in the cluster to include them in a summary. The algorithm is implemented and evaluated by human participants and by an automatic metrics. Arabic NEWSWIRE-a corpus is used as a data set in the algorithm evaluation. The result was very promising.", 
The Text Summarization is one of the problem under Natural Language Processing. The present text summarizer system uses either SVM or Clustering technique. In this work we propose a Hybrid approach to serve our purpose by cascading both techniques.,Text summarization using clustering technique and SVM technique,The Text Summarization is one of the problem under Natural Language Processing.This system which gives a single summarized document from multiple related documents. The summarizer provides an accurate result to the input query in the form of a precise text document by analyzing the text from various text document clusters. There are two methodologies- Clustering and Support Vector Machine (SVM) are used to solve this NLP problem.The present text summarizer system uses either SVM or Clustering technique. In this work we propose a Hybrid approach to serve our purpose by cascading both techniques to get an improved summary of data on related documents. We pre process the documents to get tokens obtained after stemming and stop word removal. The hybrid approach helps in summarizing the text documents efficiently by avoiding redundancy among the words in the document and ensures highest relevance to the input query.The guiding factors of our results are the ratio of input to output sentences after summarization., 
Text summarization is the most challenging task in information retrieval. Data reduction helps a user to find required information quickly. This paper presents a combined approach to document and sentence clustering.,Text Summarization using Clustering Technique,"A summarization system consists of reduction of a text document to generate a new form which conveys the key meaning of the contained text. Due to the problem of information overload, access to sound and correctly-developed summaries is necessary. Text summarization is the most challenging task in information retrieval. Data reduction helps a user to find required information quickly without wasting time and effort in reading the whole document collection. This paper presents a combined approach to document and sentence clustering as an extractive technique of summarization.", 
"In this paper, by using BabelNet knowledge base and its concept graph, a system for summarizing text is offered. The proposed method produces summaries with more quality and fewer redundancies. To compare and evaluate the performance of the proposed method, DUC2004 is used.",Text summarization using concept graph and BabelNet knowledge base,"With rapid increasing text information, the need for a computer system to processing and analyzing this information are felt. One of the systems that exist in analyzing and processing of text is a text summarization in which large volume of text is summarized based on different algorithms. In this paper, by using BabelNet knowledge base and its concept graph, a system for summarizing text is offered. In proposed approach, concepts of words by using BabelNet knowledge base are extracted and concept graphs are produced and sentences, according to concepts and resulting graph are rated. Therefore, these rating concepts are utilized in final summarization. Also, a replication control approach is proposed in a way that selected concepts in each state are punished and this causes to produce summaries with less redundancy. To compare and evaluate the performance of the proposed method, DUC2004 is used and ROUGE used as evaluation metric. The proposed method by compared to other methods produces summaries with more quality and fewer redundancies.", 
"Text summarization issue is highly attended by various researchers. In this paper, we used Cuckoo Search Optimization Algorithm (CSOA) to improve performance. The proposed approach is examined on Doc. 2002 standard documents and analyzed by Rouge evaluation software.",Text Summarization Using Cuckoo Search Optimization Algorithm,"Today, with rapid growth of the World Wide Web and creation of Internet sites and online text resources, text summarization issue is highly attended by various researchers. Extractive-based text summarization is an important summarization method which is included of selecting the top representative sentences from the input document. When, we are facing into large data volume documents, the extractive-based text summarization seems to be an unsolvable problem. Therefore, to deal with such problems, meta-heuristic techniques are applied as a solution. In this paper, we used Cuckoo Search Optimization Algorithm (CSOA) to improve performance of extractive-based summarization method. The proposed approach is examined on Doc. 2002 standard documents and analyzed by Rouge evaluation software. The obtained results indicate better performance of proposed method compared with other similar techniques", 
"We develop models and extract relevant features for automatic text summarization. We investigate the performance of different models on the DUC 2001 dataset. We segregated the summarization task into 2 main steps, the first being sentence ranking and the second step being sentence selection.",Text Summarization using Deep Learning and Ridge Regression,"We develop models and extract relevant features for automatic text summarization and investigate the performance of different models on the DUC 2001 dataset. Two different models were developed, one being a ridge regressor and the other one was a multi-layer perceptron. The hyperparameters were varied and their performance were noted. We segregated the summarization task into 2 main steps, the first being sentence ranking and the second step being sentence selection. In the first step, given a document, we sort the sentences based on their Importance, and in the second step, in order to obtain non-redundant sentences, we weed out the sentences that are have high similarity with the previously selected sentences.", 
Summarization is a way to give abstract form of large document so that the moral of the document can be communicated easily. In this paper we are implemented an efficient technique for text summarization to reduce the computational cost and time.,Text summarization using enhanced MMR technique,"Now a day when huge amount of documents and web contents are available, so reading of full content is somewhat difficult. Summarization is a way to give abstract form of large document so that the moral of the document can be communicated easily. Current research in automatic summarization is dominated by some effective, yet naive approaches: summarization through extraction, summarization through Abstraction and multi-document summarization. These techniques are used to building a summary of a document. Although there are a number of techniques implemented for the summarization of text for the single document or for the online web data or for any language. Here in this paper we are implemented an efficient technique for text summarization to reduce the computational cost and time and also the storage capacity.", 
Semantic Graph Model exploits the semantic information of sentence using FSGM. FSGM treats sentences as vertices and semantic relationship as the edges. It uses FrameNet and word embedding to calculate the similarity of sentences.,Text Summarization Using FrameNet-Based Semantic Graph Model,"Text summarization is to generate a condensed version of the original document. The major issues for text summarization are eliminating redundant information, identifying important difference among documents, and recovering the informative content. This paper proposes a Semantic Graph Model which exploits the semantic information of sentence using FSGM. FSGM treats sentences as vertexes while the semantic relationship as the edges. It uses FrameNet and word embedding to calculate the similarity of sentences. This method assigns weight to both sentence nodes and edges. After all, it proposes an improved method to rank these sentences, considering both internal and external information. The experimental results show that the applicability of the model to summarize text is feasible and effective.", 
Text summarization plays an important role in the area of natural language processing and text mining. Text summarization compresses the source text into a shorter version preserving its information content and overall meaning.,Text summarization using hierarchical clustering algorithm and expectation maximization clustering algorithm,"Due to an exponential growth in the generation of web data, the need for text summarization of web documents has become very critical. Web data can be accessed by different ways, large amount of data is available which makes searching for relevant pieces of information a difficult task. It is very complicated for human beings to manually summarize large documents of text. Text summarization plays an important role in the area of natural language processing and text mining. Text summarization is compressing the source text into a shorter version preserving its information content and overall meaning. In this paper, we are implementing initially phases of natural language processing that is splitting, tokenization, part of speech tagging, chunking and parsing. Secondly we are implementing Hierarchical clustering Algorithm and Expectation Maximization Clustering Algorithm to find out sentence similarity. Based on the value of sentences similarity, we are summarizing the text document", 
System can be proceeds by either statistic or both statistic and heuristic methods. Core of the proposed system is to find the best Parameter values for all used summarization rates.,Text Summarization Using Hybrid Methods,"This paper describes new approach which is a hybrid system for automatic text summarization which combines statistic and heuristic methods. The system can be proceeds by either statistic or both statistic and heuristic methods. As with the statistic method the summary is found based on some statistics features, and with the statistic and heuristic the summary is found based on combined statistic features and heuristic features like word frequency, position, length of sentences, and similarity with the document title. The core of the proposed system is to find the best parameter values of each used features for all used summarization rates.",  
"There are different approaches to creating well-formed summaries. One of the newest methods is the Latent Semantic Analysis (LSA) In this paper, different LSA-based summarization algorithms are explained.",Text summarization using Latent Semantic Analysis,"Text summarization solves the problem of presenting the information needed by a user in a compact form. There are different approaches to creating well-formed summaries. One of the newest methods is the Latent Semantic Analysis (LSA). In this paper, different LSA-based summarization algorithms are explained, two of which are proposed by the authors of this paper. The algorithms are evaluated on Turkish and English documents, and their performances are compared using their ROUGE scores. One of our algorithms produces the best scores and both algorithms perform equally well on Turkish and English document sets.", 
Combining PageRank and Random Indexing provides the best results on government texts. Adapting a text summarizer for a particular genre can improve text summarization.,Text Summarization using Random Indexing and PageRank,We present results from evaluations of an automatic text summarization technique that uses a combination of Random Indexing and PageRank. In our experiments we use two types of texts: news paper texts and government texts. Our results show that text type as well as other aspects of texts of the same type influence the performance. Combining PageRank and Random Indexing provides the best results on government texts. Adapting a text summarizer for a particular genre can improve text summarization., 
Text Summarization aims to generate concise and compressed form of original documents. The techniques used for text summarization may be categorized as extractive summarization and abstractive. We consider extractive techniques which are based on selection of important sentences within a document.,Text summarization using rough sets,"Text Summarization aims to generate concise and compressed form of original documents. The techniques used for text summarization may be categorized as extractive summarization and abstractive summarization. We consider extractive techniques which are based on selection of important sentences within a document. A major issue in extractive summarization is how to select important sentences, i.e., what criteria should be defined for selection of sentences which are eventually part of the summary. We examine this issue using rough sets notion of reducts. A reduct is an attribute subset which essentially contains the same information as the original attribute set. In particular, we defined and examined three types of matrices based on an information table, namely, discernibility matrix, indiscernibility matrix and equal to one matrix. Each of these matrices represents a certain type of relationship between the objects of an information table. Three types of reducts are determined based on these matrices. The reducts are used to select sentences and consequently generate text summaries. Experimental results and comparisons with existing approaches advocates for the use of the proposed approach in generating text summaries.", 
An automated text summarization tool has been developed using Sentence Scoring Method.,Text Summarization using Sentence Scoring Method,"In this project an automated text summarization tool has been developed using Sentence Scoring Method which involves finding the frequent terms, sentence ranking etc. Summary is extracted from the list of top ranked sentences. The size of summary can be specified by the user", 
Sentence-Level Semantic Graph Model (SLSGM) exploits the semantic information of sentence. SLSGM considers sentences as vertices and semantic relationship between sentences as edges. Model can be used for text summarization.,Text summarization using Sentence-Level Semantic Graph Model,"Text summarization is to generate a condensed version of the original document. The major issues for text summarization are eliminating redundant information, identifying important difference among documents and covering the informative content. In this paper, we propose a Sentence-Level Semantic Graph Model (SLSGM) which exploits the semantic information of sentence. SLSGM considers sentences as vertexes while the semantic relationship between sentences as the edges. We calculate the relevance values between sentences using semantic analysis and take the values as weights of edges while sentences’ values are scored by a variant of the traditional PageRank graph ranking algorithm considering both internal and external information. The experimental results show that the applicability of the model to text summarization is feasible and effective.", 
"Sentiment analysis is already being used in various domains for analysis of large scale text data interpretation and opinion mining. This work shows that sentiment analysis can also be used efficiently for the purpose of text summarization. The proposed scheme is found to be efficient, in particular for 50% summarization .",Text Summarization Using Sentiment Analysis for DUC Data,"Text Summarization is a way of determining the key concepts that are covered in the given text under consideration. Various techniques have been presented in literature and many are used in commercially available systems. The primary aim of the present work is to find efficacy of using sentiments for text summarization. In this work we present a computationally efficient technique based on sentiments of key words in text for the purpose of summarization. Sentiment analysis is already being used in various domains for analysis of large scale text data interpretation and opinion mining. The present work shows that sentiment analysis can also be used efficiently for the purpose of text summarization. We have tested our results on the standard DUC2002 datasets, and compared our results with different summarization approaches, viz. Random indexing based, LSA based, Graph based and Weighted graph based methods for different percentages of summarization. The proposed scheme is found to be efficient, in particular for 50% summarization.", 
In our approach extraction of relevant sentences is done which can give the actual concept of the input document in a concise form. We rank each sentence in the document by assigning a weight value to each word of the sentence.,Text Summarization using Term Weights,"Lot of work has already been done for automatic text summarization. In this paper we have given a novel statistical approach to summarize the given text. In our approach extraction of relevant sentences is done which can give the actual concept of the input document in a concise form. We rank each sentence in the document by assigning a weight value to each word of the sentence and a boost factor is also added to those terms which appear in bold, italic or underlined or any combination of these features. It helps us to extract more relevant sentences which will lead to a good summary of the given text.", 
We use a deep autoencoder to compute a feature space from the term-frequency (TF) input. We investigate the effect of adding small random noise to local TF as the input representation. We propose an ensemble of noisy AEs which we call the Ensemble Noisy Auto-Encoder (ENAE) ENAE is a stochastic version of an AE that adds noise to the input text and selects top sentences.,Text summarization using unsupervised deep learning,"We present methods of extractive query-oriented single-document summarization using a deep autoencoder (AE) to compute a feature space from the term-frequency (tf) input. Our experiments explore both local and global vocabularies. We investigate the effect of adding small random noise to local tf as the input representation of AE, and propose an ensemble of such noisy AEs which we call the Ensemble Noisy Auto-Encoder (ENAE). ENAE is a stochastic version of an AE that adds noise to the input text and selects the top sentences from an ensemble of noisy runs. In each individual experiment of the ensemble, a different randomly generated noise is added to the input representation. This architecture changes the application of the AE from a deterministic feed-forward network to a stochastic runtime model. Experiments show that the AE using local vocabularies clearly provide a more discriminative feature space and improves the recall on average 11.2%. The ENAE can make further improvements, particularly in selecting informative sentences. To cover a wide range of topics and structures, we perform experiments on two different publicly available email corpora that are specifically designed for text summarization. We used ROUGE as a fully automatic metric in text summarization and we presented the average ROUGE-2 recall for all experiments.", 
"Automatic text summarization has been an active field of research for many years. The advent of human-generated knowledge bases like Wikipedia offer a further possibility in text summarizing. In this paper, we study a novel approach that leverages Wikipedia in conjunction with graphbased ranking. We evaluate the performance of our proposed summarizer using the ROUGE metric.",Text summarization using Wikipedia,"Automatic text summarization has been an active field of research for many years. Several approaches have been proposed, ranging from simple position and word-frequency methods, to learning and graph based algorithms. The advent of human-generated knowledge bases like Wikipedia offer a further possibility in text summarization – they can be used to understand the input text in terms of salient concepts from the knowledge base. In this paper, we study a novel approach that leverages Wikipedia in conjunction with graphbased ranking. Our approach is to first construct a bipartite sentence–concept graph, and then rank the input sentences using iterative updates on this graph. We consider several models for the bipartite graph, and derive convergence properties under each model. Then, we take up personalized and query-focused summarization, where the sentence ranks additionally depend on user interests and queries, respectively. Finally, we present a Wikipedia-based multi-document summarization algorithm. An important feature of the proposed algorithms is that they enable real-time incremental summarization – users can first view an initial summary, and then request additional content if interested. We evaluate the performance of our proposed summarizer using the ROUGE metric, and the results show that leveraging Wikipedia can significantly improve summary quality. We also present results from a user study, which suggests that using incremental summarization can help in better understanding news articles.", 
"This paper proposes a novel method for text summarization using WordNet graph based sentence ranking. The proposed method utilizes the degree, betweenness, and closeness centrality measures. Web of Science (WoS) is used as the data source for the same.",Text Summarization Using WordNet Graph Based Sentence Ranking,"Text summarization refers to the task of generating a summary from a given document that tries to replicate the most significant information of the original document. A number of techniques are available in the literature regarding the same and sentence ranking is one of them. This paper proposes a novel method for text summarization using WordNet graph based sentence ranking. The proposed method utilizes the degree, betweenness, and closeness centrality measures. This paper also analyzes the current research work going on in text summarization and sentence ranking. Web of Science (WoS) is used as the data source for the same. The cooccurrence of all the keywords in the research papers pertaining to sentence ranking is also visualized.", 
"Summarization is the process of shortening a text document to make a summary that keeps the main points of the actual document. Keyword extraction usually is done by extracting relevant words having a higher frequency than others. In this work, we propose an algorithm that automatically extracts keyword for text summarization in Telugu e-newspaper datasets.",Text Summarization with Automatic Keyword Extraction in Telugu e-Newspapers,"Summarization is the process of shortening a text document to make a summary that keeps the main points of the actual document. Extractive summarizers work on the given text to extract sentences that best express the message hidden in the text. Most extractive summarization techniques revolve around the concept of finding keywords and extracting sentences that have more keywords than the rest. Keyword extraction usually is done by extracting relevant words having a higher frequency than others, with stress on important one’s. Manual extraction or annotation of keywords is a tedious process brimming with errors involving lots of manual effort and time. In this work, we proposed an algorithm that automatically extracts keyword for text summarization in Telugu e-newspaper datasets. The proposed method compares with the experimental result of articles having the similar title in five different Telugu e-newspapers to check the similarity and consistency in summarized results.", 
"Authors introduce a method to make extractions based on three factors of Readability, Cohesion and Topic relation. They use Harmony Search-based sentence selection to make such a summary. Their results indicate that the extracted summaries have better precision and recall than the other approaches.",Text summarization with harmony search algorithm-based sentence extraction,"Currently vast amounts of textual information exist in large repositories such as Web. To processes such a huge amount of information, automatic text summarization has been of great interests. Unlike many approaches which focus on sentence or paragraph extraction, in this research, we introduce a method to make extractions based on three factors of Readability, Cohesion and Topic relation. We use Harmony Search-based sentence selection to make such a summary. Once the summary is created, it is evaluated using a fitness function based on those three factors. The evaluation of the algorithm on a test collection is also presented in the paper. Our results indicate that the extracted summaries by our proposed scheme have better precision and recall than the other approaches.",  
Bidirectional Encoder Representations from Transformers (BERT; Devlin et al. 2019) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document. Our extractive model is built on top of this encoder by stacking several intersentence Transformer layers.,Text Summarization with Pretrained Encoders,"Bidirectional Encoder Representations from Transformers (BERT; Devlin et al. 2019) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several intersentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves stateof-the-art results across the board in both extractive and abstractive settings", 
Text summarization is used to provide a shorter version of the original text and keeping the overall meaning. One of the most commonly used methods is the Latent Semantic Analysis (LSA),"Text summarization within the latent semantic analysis framework, Comparative study","It is very difficult for human beings to manually summarize large documents of text. Text summarization solves this problem. Nowadays, Text summarization systems are among the most attractive research areas. Text summarization (TS) is used to provide a shorter version of the original text and keeping the overall meaning. There are various methods that aim to find out well-formed summaries. One of the most commonly used methods is the Latent Semantic Analysis (LSA). In this review, we present a comparative study among almost algorithms based on Latent Semantic Analysis (LSA) approach.", 
This thesis deals with the development of a new text summarization method that uses the latent semantic analysis (lsa) The method originally combines both lexical and anaphoric information. Using summaries in multilingual searching system led to better user orientation in the retrieved texts.,Text summarization within the LSA framework,"This thesis deals with the development of a new text summarization method that uses the latent semantic analysis (lsa). The language-independent analysis is able to capture interrelationships among terms, so that we can obtain a representation of document topics. This feature is exploited by the proposed summarization approach. The method originally combines both lexical and anaphoric information. Moreover, anaphora resolution is employed in correcting false references in the summary. Then, I describe a new sentence compression algorithm that takes advantage from the lsa properties. Next, I created a method which evaluates the similarity of main topics of an original text and its summary, motivated by the ability of lsa to extract topics of a text. Using summaries in multilingual searching system muse led to better user orientation in the retrieved texts and to faster searching when summaries were indexed instead of full texts.", 
This study explores summarization approaches along with their recent state-of-art models in single and multi-document summarization. This survey is intended to make an extensive study from features representation to sentence selection and summary generation.,"Text Summarization, An Essential Study","The proliferation of data from diverse sources makes humans insufficient in utilizing the knowledge properly at some instance. To quickly have an overview of abundant information, Text Summarization (TS) comes into play. TS will effectively extract the candidate sentences from the source and represent the saliency of whole knowledge. Over the decades Text Summarization techniques have been transformed by the usage of linguistics to advanced machine learning models, this study explores summarization approaches along with their recent state-of-art models in single and multi-document summarization. This survey is intended to make an extensive study from features representation to sentence selection and summary generation using machine learning, recent graph and evolutionary based methods. The overall investigation will help the researchers to effectively handle large quantities of data in building effective Natural Language Processing applications. Eventually, this study draws popular abstractive mechanisms and observations that would be helpful for the intended research", 
"This chapter introduces a taxonomy of summarization methods. We consider various aspects which can affect their categorization. A special attention is devoted to application of recent information reduction methods, based on algebraic transformations. We introduce experiences with the development of our web searching and summarization system.","Text Summarization, An Old Challenge and New Approaches","One of the most relevant todays problems called information overloading has increased the necessity of more sophisticated and powerful information compression methods - summarizers. This chapter firstly introduces a taxonomy of summarization methods, an overview of their principles from classical ones, over corpus based, to knowledge rich approaches. We consider various aspects which can affect their categorization. A special attention is devoted to application of recent information reduction methods, based on algebraic transformations. Our own LSA (Latent Semantic Analysis) based approach is included too. The next part is devoted to evaluation measures for assessing quality of a summary. The taxonomy of evaluation measures is presented and their features are discussed. Further, we introduce experiences with the development of our web searching and summarization system. Finally, some new ideas and a conception for the future of this field are mentioned.", 
This paper presents a system that converts a medical text into a table structure. The system's core technologies are medical event recognition modules and a negative event identification module. It also proposes an SVM-based classifier using syntactic information.,"Text2table, Medical text summarization system based on named entity recognition and modality identification","With the rapidly growing use of electronic health records, the possibility of large-scale clinical information extraction has drawn much attention. It is not, however, easy to extract information because these reports are written in natural language. To address this problem, this paper presents a system that converts a medical text into a table structure. This system’s core technologies are (1) medical event recognition modules and (2) a negative event identification module that judges whether an event actually occurred or not. Regarding the latter module, this paper also proposes an SVM-based classifier using syntactic information. Experimental results demonstrate empirically that syntactic information can contribute to the method’s accuracy.", 
This work combined two-tiered topic model with graph based TextRank method to extract better summary sentences. The proposed method's summary results outperform other topic model based summary results using ROUGE metrics.,TextRank enhanced Topic Model for Query focussed Text Summarization,"Topic model based query focused text summarization methods can meet the user’s needs as they are able to find subtopics and their correlations. Topic model based summarization method does not score the sentences directly as they generates topics distributions, which are used to score the sentences. Graph based methods rank sentences using syntactic and semantic information. In this work, a topic model based summarization method namely two-tiered topic model is combined with graph based TextRank method. The combined method, called TextRank enhanced Two-Tiered topic model, uses the important sentences obtained from TextRank in the generative process of two-tiered model to extract better summary sentences. The proposed method’s summary results outperform other topic model based summary results using ROUGE metrics evaluated on DUC 2005 dataset", 
"With the advent of Web 2.0, there exist many online platforms that results in massive textual data production. This study aims to present an overview of approaches that can be applied to extract valuable information nuggets residing within text. Two major tasks of automatic keyword extraction and text summarization are being reviewed.","Textual keyword extraction and summarization, State-of-the-art","With the advent of Web 2.0, there exist many online platforms that results in massive textual data production such as social networks, online blogs, magazines etc. This textual data carries information that can be used for betterment of humanity. Hence, there is a dire need to extract potential information out of it. This study aims to present an overview of approaches that can be applied to extract and later present these valuable information nuggets residing within text in brief, clear and concise way. In this regard, two major tasks of automatic keyword extraction and text summarization are being reviewed. To compile the literature, scientific articles were collected using major digital computing research repositories. In the light of acquired literature, survey study covers early approaches up to all the way till recent advancements using machine learning solutions. Survey findings conclude that annotated benchmark datasets for various textual data-generators such as twitter and social forms are not available. This scarcity of dataset has resulted into relatively less progress in many domains. Also, applications of deep learning techniques for the task of automatic keyword extraction are relatively unaddressed. Hence, impact of various deep architectures stands as an open research direction. For text summarization task, deep learning techniques are applied after advent of word vectors, and are currently governing state-of-the-art for abstractive summarization. Currently, one of the major challenges in these tasks is semantic aware evaluation of generated results.", 
Text mining is the discovery by automatically extracting information from different written resources and also by computer. In this survey paper we discuss such successful techniques and methods to give effectiveness over information retrieval. The types of situations where each technology may be useful in order to help users.,Text_Mining_Methods_and_Techniques,"In recent years growth of digital data is increasing, knowledge discovery and data mining have attracted great attention with coming up need for turning such data into useful information and knowledge. The use of the information and knowledge extracted from a large amount of data benefits many applications like market analysis and business management. In many applications database stores information in text form so text mining is the one of the most resent area for research. To extract user required information is the challenging issue. Text Mining is an important step of knowledge discovery process. Text mining extracts hidden information from notstructured to semi-structured data. Text mining is the discovery by automatically extracting information from different written resources and also by computer for extracting new, previously unknown information. This survey paper tries to cover the text mining techniques and methods that solve these challenges. In this survey paper we discuss such successful techniques and methods to give effectiveness over information retrieval in text mining. The types of situations where each technology may be useful in order to help users are also discussed.", 
Combining PageRank and Random Indexing provides the best results on government texts. Adapting a text summarizer for a particular genre can improve text summarization.,Text_Summarization_using_Random_Indexing_and_PageRank,We present results from evaluations of an automatic text summarization technique that uses a combination of Random Indexing and PageRank. In our experiments we use two types of texts: news paper texts and government texts. Our results show that text type as well as other aspects of texts of the same type influence the performance. Combining PageRank and Random Indexing provides the best results on government texts. Adapting a text summarizer for a particular genre can improve text summarization, 
Penelitian ini mengusulkan sebuah implementasi terkait dengan automasi peringkasan teks bertipe ekstraktif. Algoritma TF-IDF-EGA dikenal mampu untuk menghasilkan ringkasan Teks berdasarkan skor yang didapat pada setiap kalimat dalam teks.,TF-IDF-Enhanced Genetic Algorithm Untuk Extractive Automatic Text Summarization,"Penelitian ini mengusulkan sebuah implementasi terkait dengan automasi peringkasan teks bertipe ekstraktif dengan menggunakan metode TF-IDF-EGA. Dimana dalam permasalahan peringkasan teks dibutuhkan suatu solusi untuk meringkas teks dengan kalimat ringkasan yang dapat merepresentasikan keseluruhan teks yang ada. Algoritma TF-IDF dikenal mampu untuk menghasilkan ringkasan teks berdasarkan skor yang didapat pada setiap kalimat dalam teks. Namun hasil dari TF-IDF terkadang didapati hasil ringkasan yang terdiri dari kalimat yang tidak deskriptif, hal ini dikarenakan dalam peringkasannya TF-IDF hanya memilih beberapa kalimat yang memiliki skor tertinggi dan biasanya kalimat dengan skor tertinggi merupakan kalimat yang berisi kata-kata penting/kata-kata ilmiah tertentu sehingga kalimatnya tidak deskriptif. Algoritma EGA mampu untuk mengatasi permasalahan tersebut dengan cara memilih kalimat ringkasan yang memiliki nilai probabilitas tertentu sebagai hasil peringkasan teks.", 
Electroencephalogram (EEG)-based sleep stage analysis is helpful for diagnosis of sleep disorder. The accuracy of previous EEG-based method is still unsatisfactory. We propose an EEG- based automatic sleep stage classification method. The proposed method achieves the best accuracy of 88.83%.,The Analysis and Classify of Sleep Stage Using Deep Learning Network from Single-Channel EEG Signal,"Electroencephalogram (EEG)-based sleep stage analysis is helpful for diagnosis of sleep disorder. However, the accuracy of previous EEG-based method is still unsatisfactory. In order to improve the classification performance, we proposed an EEG-based automatic sleep stage classification method, which combined convolutional neural network (CNN) and time-frequency decomposition. The time-frequency image (TFI) of EEG signals is obtained by using the smoothed short-time Fourier transform. The features derived from the TFI have been used as an input feature of a CNN for sleep stage classification. The proposed method achieves the best accuracy of 88.83%. The experimental results demonstrate that deep learning method provides better classification performance compared to other methods.", 
"Deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance. Cancer prognosis is to estimate the fate of cancer, probabilities of cancer recurrence and progression, and to provide survival estimation to the patients. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients.",The Application of Deep Learning in Cancer Prognosis Prediction,"Deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance, etc. Cancer prognosis is to estimate the fate of cancer, probabilities of cancer recurrence and progression, and to provide survival estimation to the patients. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients. The improvement of biomedical translational research and the application of advanced statistical analysis and machine learning methods are the driving forces to improve cancer prognosis prediction. Recent years, there is a significant increase of computational power and rapid advancement in the technology of artificial intelligence, particularly in deep learning. In addition, the cost reduction in large scale next-generation sequencing, and the availability of such data through open source databases (e.g., TCGA and GEO databases) offer us opportunities to possibly build more powerful and accurate models to predict cancer prognosis more accurately. In this review, we reviewed the most recent published works that used deep learning to build models for cancer prognosis prediction. Deep learning has been suggested to be a more generic model, requires less data engineering, and achieves more accurate prediction when working with large amounts of data. The application of deep learning in cancer prognosis has been shown to be equivalent or better than current approaches, such as Cox-PH. With the burst of multi-omics data, including genomics data, transcriptomics data and clinical information in cancer studies, we believe that deep learning would potentially improve cancer prognosis", 
"Recent studies show uncompilable commits exist even in high-profile open-source software. Identifying broken code can shed light on how software quality evolves when developers do not follow best practices. This paper explores the relations between commit type, size and compilability.",The Characteristics and Impact of Uncompilable Code Changes on Software Quality Evolution,"Software repositories allow multiple developers to iteratively contribute commits, with the intention of improving the system. However, commits can negatively impact software quality, or even cause the software to become uncompilable. Recent studies show that uncompilable commits exist even in high-profile open-source software. Identifying broken code, a potential symptom of careless development, and analyzing how software changes when it becomes uncompilable can shed light on how software quality evolves when developers do not follow best practices. Since comprehensive software quality analysis tools are incapable of analyzing uncompilable commits, there is little insight as to what happens and how quality changes when a commit breaks the compilability. In this paper, starting from an analysis of the software quality metric changes that happen when the project become uncompilable, we explore the purposes of commits and the relations between commit type, size and compilability, analyzed across 68 open-source Java repositories", 
"Concerns about unequal access to new technologies, integrated within the concept of digital divide, has been a topic of study since the use of the Internet began to be generalized among the population. This study uses advanced data analysis techniques to analyze the main socioeconomic digital skills drivers of the Spanish population.",The digital divide in light of sustainable development An approach through,"The growing use of ICT in the workplace has meant that the labor market demands new skills and digital capabilities, either in the production of goods and services or in the need for workers with complementary skills. Concerns about unequal access to new technologies, integrated within the concept of digital divide, has been a topic of study since the use of the Internet began to be generalized among the population. Traditionally, research into the inequalities arising from the emergence of digital technologies has incorporated socioeconomic variables, especially gender, age, educational level, income, and habitat. This study uses advanced data analysis techniques to analyze the main socioeconomic digital skills drivers of the Spanish population. This information allows us to identify whether the population has training needs in relation to their digital competences, which will have a positive influence on improving the level of sustainable development of the country.", 
This study aims at investigating the effect of teaching the summarization of text as a cognitive strategy on achievement of male and female students' reading comprehension. 60 English undergraduates studying at the University of Tonekabon participated in the study.,The Effect of Text Summarization as a Cognitive Strategy on the Achievement of Male and Female Language Learners' Reading Comprehension,"This study aims at investigating the effect of teaching the summarization of text as a cognitive strategy on achievement of male and female students’ reading comprehension. 60 English undergraduates studying at the University of Tonekabon participated in this study. The participants were randomly assigned into the experimental and control groups. A series of treatment were provided for the participants in the experimental group. Two reading comprehension tests, as the pretest and posttest, were given to the students of both groups to observe their reading comprehension ability at the beginning and at the end of the treatment sessions. A Two-way ANOVA was run on the obtained data. The results revealed that the instruction of summarization strategy had a significant effect on the participants reading comprehension. Finally, the findings of this study suggest that teaching summarization strategy empowers students’ reading comprehension ability.", 
category representations can be broadly classified as within–category information or between– category information. Little is known about the factors contributing to the development and generalizability of different types of category representations. This study investigates the impact of training methodology and category structures. It finds a bias toward within– category representations regardless of the training methodology.,The effect of training methodology on knowledge representation in categorization,"Category representations can be broadly classified as containing within–category information or between–category information. Although such representational differences can have a profound impact on decision–making, relatively little is known about the factors contributing to the development and generalizability of different types of category representations. These issues are addressed by investigating the impact of training methodology and category structures using a traditional empirical approach as well as the novel adaptation of computational modeling techniques from the machine learning literature. Experiment 1 focused on rule–based (RB) category structures thought to promote between–category representations. Participants learned two sets of two categories during training and were subsequently tested on a novel categorization problem using the training categories. Classification training resulted in a bias toward between–category representations whereas concept training resulted in a bias toward within–category representations. Experiment 2 focused on information-integration (II) category structures thought to promote within– category representations. With II structures, there was a bias toward within–category representations regardless of training methodology. Furthermore, in both experiments, computational modeling suggests that only within–category representations could support generalization during the test phase. These data suggest that within–category representations may be dominant and more robust for supporting the reconfiguration of current knowledge to support generalization.", 
Text summarization is used to condense texts into the most important ideas. Reducing the amount of content transmitted may negatively impact the meaning conveyed within. This study investigates automatic text summarization to provide a tool set that reduces the quantity of textual content for mobile learning.,The effectiveness of automatic text summarization in mobile learning contexts,"Mobile learning benefits from the unique merits of mobile devices and mobile technology to give learners capability to access information anywhere and anytime. However, mobile learning also has many challenges, especially in the processing and delivery of learning content. With the aim of making the learning content suitable for the mobile environment, this study investigates automatic text summarization to provide a tool set that reduces the quantity of textual content for mobile learning support. Text summarization is used to condense texts into the most important ideas. However, reducing the amount of content transmitted may negatively impact the meaning conveyed within. Although many solutions of text summarization have been applied by intelligent tutoring systems for learning support, few of them have been quantitatively investigated for learning achievements of learners, especially in mobile learning context. This study focuses on a methodology for investigating the effectiveness of automatic text summarization used in mobile learning context. The experimental results demonstrate that our proposed summarization approach is able to generate summaries effectively, and those generated summaries are perceived as helpful to support mobile learning. The findings of this work indicate that properly summarized learning content is not only able to satisfy learning achievements, but also able to align content size with the unique characteristics and affordances of mobile devices", 
"Accessible taxis are infrequently used by elderly passengers in Hong Kong, due in part to high fares and long walking and wait times. The elderly using crutches or a wheelchair had a stronger preference for accessible taxis. A step-wise taxi fare subsidy scheme is recommended to integrate with current transport policy measures to look after the needs of the elderly. A supply-side subsidy is also recommended to support local taxi operators to purchase accessible taxis and increase the fleet size.",The effects of accessible taxi service and taxi fare subsidy scheme on the elderlyâ€™s willingness-to-travel,"Taxis provide a personalized, door-to-door, and demand-responsive service, and are an attractive transport mode for accommodating the needs of an aging population. Accessible taxis further enhance the level of comfort and accessibility by providing a larger compartment and a mechanical ramp to facilitate the boarding and alighting of wheelchairs. At present, however, both ordinary and accessible taxis are infrequently used by elderly passengers in Hong Kong, due in part to high fares and long walking and wait times. Because of these limitations, taxis cannot effectively cater for the travel demands of the elderly and hence cannot improve their mobility. In this study, 580 residents of Hong Kong aged 60 or above were interviewed regarding their decision to whether to make a trip by an ordinary/accessible taxi to attend a non-compulsory social activity in hypothetical scenarios. A total of 2320 observations were obtained and a series of binary logistic regression models were calibrated to identify the significant factors influencing the elderly’s willingness-to-travel. The model results show that the older elderly and those with a lower monthly expenditure were more reluctant to travel by taxi and preferred staying home. The elderly using crutches or a wheelchair had a stronger preference for accessible taxis rather than the non-walking aid users. The findings also indicate that the elderly were more sensitive to a taxi fare subsidy based on a percentage discount than one based on a fixed discount. Policy measures related to providing an accessible taxi service and taxi fare subsidy scheme to improve the mobility of the elderly are discussed: A step-wise taxi fare subsidy scheme is recommended to integrate with current transport policy measures to look after the needs of the elderly at different ages, and a supply-side subsidy is also recommended to support local taxi operators to purchase accessible taxis and increase the fleet size to minimize the wait time of users.", 
"Schematic threatening, friendly, and neutral faces were used to test the hypothesis that humans preferentially orient their attention toward threat. Results consistently showed faster and more accurate detection of threatening than friendly targets. Threatening angry faces were more quickly and accurately detected than were other negative faces.",The Face in the Crowd Revisited A Threat Advantage With Schematic Stimuli,"Schematic threatening, friendly, and neutral faces were used to test the hypothesis that humans preferentially orient their attention toward threat. Using a visual search paradigm, participants searched for discrepant faces in matrices of otherwise identical faces. Across 5 experiments, results consistently showed faster and more accurate detection of threatening than friendly targets. The threat advantage was obvious regardless of whether the conditions favored parallel or serial search (i.e., involved neutral or emotional distractors), and it was valid for inverted faces. Threatening angry faces were more quickly and accurately detected than were other negative faces (sad or ""scheming""), which suggests that the threat advantage can be attributed to threat rather than to the negative valence or the uniqueness of the target display.",  
Big Data Analytics (BDA) is an emerging phenomenon with the reported potential to transform how firms manage and enhance high value businesses performance. The purpose of this study is to investigate the impact of BDA on operations management in the manufacturing sector.,The impact of big data analytics on firms high value,"Big Data Analytics (BDA) is an emerging phenomenon with the reported potential to transform how firms manage and enhance high value businesses performance. The purpose of our study is to investigate the impact of BDA on operations management in the manufacturing sector, which is an acknowledged infrequently researched context. Using an interpretive qualitative approach, this empirical study leverages a comparative case study of three manufacturing companies with varying levels of BDA usage (experimental, moderate and heavy). The information technology (IT) business value literature and a resource based view informed the development of our research propositions and the conceptual framework that illuminated the relationships between BDA capability and organizational readiness and design. Our findings indicate that BDA capability (in terms of data sourcing, access, integration, and delivery, analytical capabilities, and people’s expertise) along with organizational readiness and design factors (such as BDA strategy, top management support, financial resources, and employee engagement) facilitated better utilization of BDA in manufacturing decision making, and thus enhanced high value business performance. Our results also highlight important managerial implications related to the impact of BDA on empowerment of employees, and how BDA can be integrated into organizations", 
This paper focuses on comparing the impact of the local attention in Long Short-Term Memory (LSTM) model to generate an abstractive text summarization. Developing a model using a dataset of Amazon Fine Food Reviews and evaluating it using dataset of GloVe.,The Impact of Local Attention in LSTM for Abstractive Text Summarization,"An attentional mechanism is very important to enhance a neural machine translation (NMT). There are two classes of attentions: global and local attentions. This paper focuses on comparing the impact of the local attention in Long Short-Term Memory (LSTM) model to generate an abstractive text summarization (ATS). Developing a model using a dataset of Amazon Fine Food Reviews and evaluating it using dataset of GloVe shows that the global attention-based model produces better ROUGE-1, where it generates more words contained in the actual summary. But, the local attention-based gives higher ROUGE-2, where it generates more pairs of words contained in the actual summary, since the mechanism of local attention considers the subset of input words instead of the whole input words.", 
"Internet of Vehicles (IOV) will require very low latency, much lower than is currently provided by existing communication networks. This paper proposes the advantages of using 5G communication model for future implementation of IOV environment. 5G communications will expand the possibilities of what mobile networks can do.",The Internet of Vehicles based on 5G Communications,"The recent development in the large domain of the Internet of Things is driving conventional change in Vehicle Adhoc Networks, now known as Internet of Vehicles (IOV), Vehicleto-Vehicle (V2V), Vehicle-to-Infrastructure(V2I) and some Intelligent Transport System (ITS) applications will require very low latency, much lower than is currently provided by existing communication networks. The connected vehicles market will be extremely large in terms of number of connected end-points and data exchanged. Over time, any vehicles will be given the potential to connect to anything at anytime, from people to physical things, processes, contents, working knowledge, timely pertinent information and goods of all sorts in entirely flexible, reliable and secure ways. 5G communications will expand the possibilities of what mobile networks can do, and extend upon what services they can deliver. 5G will provide the foundational infrastructure for building smart IOV Environment, which will push vehicle network performance and capability requirements to their extremes. This paper proposes the advantages of using 5G communication model for future implementation of IOV environment in term of low latency, extremely high bandwidth and reliability", 
The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection. It has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. It reviews the state-of-the-art in evaluated methods.,The PASCAL Visual Object Classes (VOC) Challenge,"The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension", 
"This report presents the results of the 2006 PASCAL Visual Object Classes Challenge. Details of the challenge, data, and evaluation",The Pascal Visual Object Classes Challenge 2006 (VOC2006) Results,"This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006). Details of the challenge, data, and evaluation are presented. Participants in the challenge submitted descriptions of their methods, and these have been included verbatim. This document should be considered preliminary, and subject to change.", 
"In 2020, OpenAI developed GPT-3, a neural language model that is capable of sophisticated natural language generation and completion of tasks like classification, question-answering, and summarization. The Center on Terrorism, Extremism, and Counterterrorism (CTEC) evaluated the G PT-3 API for the risk of weaponization by extremists. extremists who may attempt to use the model to amplify their ideologies and recruit to their communities.",The Radicalization Risks of GPT-3 and Advanced Neural Language Models,"In 2020, OpenAI developed GPT-3, a neural language model that is capable of sophisticated natural language generation and completion of tasks like classification, question-answering, and summarization. While OpenAI has not opensourced the model’s code or pre-trained weights at the time of writing, it has built an API to experiment with the model’s capacity. The Center on Terrorism, Extremism, and Counterterrorism (CTEC) evaluated the GPT-3 API for the risk of weaponization by extremists who may attempt to use GPT-3 or hypothetical unregulated models to amplify their ideologies and recruit to their communities. Our methods included: 1. using prompts adapted from right-wing extremist narratives and topics to evaluate ideological consistency, accuracy, and credibility, and 2. evaluating the efficacy of the model’s output in contributing to online radicalization into violent extremism. Experimenting with prompts representative of different types of extremist narrative, structures of social interaction, and radical ideologies, CTEC finds: • GPT-3 demonstrates significant improvement over its predecessor, GPT-2, in generating extremist texts. • GPT-3 shows strength in generating text that accurately emulates interactive, informational, and influential content that could be utilized for radicalizing individuals into violent far-right extremist ideologies and behaviors. • While OpenAI’s preventative measures are strong, the possibility of unregulated copycat technology represents significant risk for large-scale online radicalization and recruitment. In the absence of safeguards, successful and efficient weaponization that requires little experimentation is likely. • AI stakeholders, the policymaking community, and governments should begin investing as soon as possible in building social norms, public policy, and educational initiatives to preempt an influx of machine-generated disinformation and propaganda. Mitigation will require effective policy and partnerships across industry, government, and civil society. This project was made possible by the OpenAI API Academic Access Program.", 
The random electrode selection ensemble (RESE) aims to surmount the instability demerit of the Fisher discriminant feature extraction for BCI applications. Multiple individual classifiers are constructed. Theoretical analysis and classification experiments with real EEG signals indicate that the RESE approach is both effective and efficient.,The random electrode selection ensemble for EEG signal classification,"Pattern classification methods are a crucial direction in the current study of brain–computer interface (BCI) technology. A simple yet effective ensemble approach for electroencephalogram (EEG) signal classification named the random electrode selection ensemble (RESE) is developed, which aims to surmount the instability demerit of the Fisher discriminant feature extraction for BCI applications. Through the random selection of recording electrodes answering for the physiological background of user-intended mental activities, multiple individual classifiers are constructed. In a feature subspace determined by a couple of randomly selected electrodes, principal component analysis (PCA) is first used to carry out dimensionality reduction. Successively Fisher discriminant is adopted for feature extraction, and a Bayesian classifier with a Gaussian mixture model (GMM) approximating the feature distribution is trained. For a test sample the outputs from all the Bayesian classifiers are combined to give the final prediction for its label. Theoretical analysis and classification experiments with real EEG signals indicate that the RESE approach is both effective and efficient.", 
Most automated summarizers are extractive and select sentences verbatim. We examine how elementary discourse units (EDUs) from Rhetorical Structure Theory can be used to extend extractive summarizers. EDU segmentation is effective in preserving human-labeled summarization concepts.,The Role of Discourse Units in Near-Extractive Summarization,"Although human-written summaries of documents tend to involve significant edits to the source text, most automated summarizers are extractive and select sentences verbatim. In this work we examine how elementary discourse units (EDUs) from Rhetorical Structure Theory can be used to extend extractive summarizers to produce a wider range of human-like summaries. Our analysis demonstrates that EDU segmentation is effective in preserving human-labeled summarization concepts within sentences and also aligns with near-extractive summaries constructed by news editors. Finally, we show that using EDUs as units of content selection instead of sentences leads to stronger summarization performance in near-extractive scenarios, especially under tight budgets.", 
"Mobile technologies can be, and have been, exploited in terrorist activities. Study shows how mobile forensics techniques can be used to recover evidentiary artefacts from client devices. This study also highlights the extent of acquired evidence between Android and Windows devices, in which Android presents more evidentiary value.",The role of mobile forensics in terrorism investigations involving the use of cloud storage service and communication apps,"Mobile technologies can be, and have been, exploited in terrorist activities. In this paper, we highlight the importance of mobile forensics in the investigation of such activities. Specifically, using a series of controlled experiments on Android and Windows devices, we demonstrate how mobile forensics techniques can be used to recover evidentiary artefacts from client devices. There are three simulation scenarios, namely: (1) information propagation, (2) information concealment and (3) communications. The experiments used three popular cloud apps (Google Drive, Dropbox, and OneDrive), five communication apps (Messenger, WhatsApp, Telegram, Skype and Viber), and two email apps (GMail and Microsoft Outlook). The evidential data was collected and analysed using mobile forensics and network packet analyser tools. The correlation of evidence artefacts would support to infer illegal use of mobile devices. This study also highlights the extent of acquired evidence between Android and Windows devices, in which Android presents more evidentiary value.", 
"This paper reports on the further results of the ongoing research analyzing the impact of a range of commonly used statistical and semantic features in the context of extractive text summarization. The features experimented with include word frequency, inverse sentence and term frequencies, stopwords filtering, word senses and resolved anaphora.",The role of statistical and semantic features in single-document extractive summarization,"This paper reports on the further results of the ongoing research analyzing the impact of a range of commonly used statistical and semantic features in the context of extractive text summarization. The features experimented with include word frequency, inverse sentence and term frequencies, stopwords filtering, word senses, resolved anaphora and textual entailment. The obtained results demonstrate the relative importance of each feature and the limitations of the tools available. It has been shown that the inverse sentence frequency combined with the term frequency yields almost the same results as the latter combined with stopwords filtering that in its turn proved to be a highly competitive baseline. To improve the suboptimal results of anaphora resolution, the system was extended with the second anaphora resolution module. The present paper also describes the first attempts of the internal document data representation", 
Security community has advocated widespread adoption of secure communication tools to protect personal privacy. Recent studies have shown users may not understand the security features of tools they are using. Most participants develop a habit of using the less secure default chat mode at all times. We also uncover several user interface design issues that impact security.,The Security Blanket of the Chat World An Analytic Evaluation and a User Study of Telegram,"The computer security community has advocated widespread adoption of secure communication tools to protect personal privacy. Several popular communication tools have adopted end-to-end encryption (e.g., WhatsApp, iMessage), or promoted security features as selling points (e.g., Telegram, Signal). However, previous studies have shown that users may not understand the security features of the tools they are using, and may not be using them correctly. In this paper, we present a study of Telegram using two complementary methods: (1) a labbased user study (11 novices and 11 Telegram users), and (2) a hybrid analytical approach combining cognitive walk-through and heuristic evaluation to analyse Telegram’s user interface. Participants who use Telegram feel secure because they feel they are using a secure tool, but in reality Telegram offers limited security benefits to most of its users. Most participants develop a habit of using the less secure default chat mode at all times. We also uncover several user interface design issues that impact security, including technical jargon, inconsistent use of terminology, and making some security features clear and others not. For instance, use of the end-to-end-encrypted Secret Chat mode requires both the sender and recipient be online at the same time, and Secret Chat does not support group conversations.", 
"Text summarization enables users to reduce the amount of text that must be read while still assimilating core information. It is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. The BioChain and FreqDist methods outperform some common summarization systems, while the ChainFreq method improves upon the base approaches.",The use of domain-specific concepts in biomedical text summarization,"Text summarization is a method for data reduction. The use of text summarization enables users to reduce the amount of text that must be read while still assimilating the core information. The data reduction offered by text summarization is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. Such efforts are often hampered by the high-volume of publications. This paper presents two independent methods (BioChain and FreqDist) for identifying salient sentences in biomedical texts using concepts derived from domain-specific resources. Our semantic-based method (BioChain) is effective at identifying thematic sentences, while our frequency-distribution method (FreqDist) removes information redundancy. The two methods are then combined to form a hybrid method (ChainFreq). An evaluation of each method is performed using the ROUGE system to compare system-generated summaries against a set of manually-generated summaries. The BioChain and FreqDist methods outperform some common summarization systems, while the ChainFreq method improves upon the base approaches. Our work shows that the best performance is achieved when the two methods are combined. The paper also presents a brief physician’s evaluation of three randomly-selected papers from an evaluation corpus to show that the author’s abstract does not always reflect the entire contents of the full-text.", 
"Emotion recognition is of great significance in artificial intelligence, health care, distance education, military field and so on. The paper builds a stacked autoencoder deep learning classification network. The well-trained network is used to classify three emotion states including happy, neural and grief. Fourteen experiments are performed with 5-fold cross validation.",Three Class Emotions Recognition Based On Deep Learning Using Staked Autoencoder,"Emotion recognition is a hot spot in advanced humancomputer interaction system, which is of great significance in artificial intelligence, health care, distance education, military field and so on. The paper builds a stacked autoencoder deep learning classification network consist of an input layer, two autoencoder hidden layers and a softmax classifier output layer based on SJTU Emotion EEG Dataset (SEED). Pretrain the first autoencoder employed L-BFGS to optimize the cost function. Then pretrain the second autoencoder with the output of first autoencoder. Finally send to the softmax classifier. Pretrain each autoencoder in forward propagation, then fine-tuning the whole network in back propagation. The well-trained network is used to classify three emotion states including happy, neural and grief. The raw inputs are differential entropy of EEG signal in five rhythmic frequencies band and the differential entropy of whole EEG signal. Fourteen experiments are performed with 5-fold cross validation, the average classification accuracy of three class emotion states is 59.6%?66.27%?71.97%?78.48%?82.56% and 85.5%. The result shows the higher frequency band differential entropy like Gamma band is more relative to emotion reaction.", 
"Millimeter-wave imaging is well suited for the detection of concealed weapons or other contraband. Millimeter-waves are nonionizing, readily penetrate common clothing material, and are reflected from the human body. Practical weapon detection systems for airport or other high-throughput applications require high-speed scanning on the order of 3 to 10 s.",Three-Dimensional Millimeter-Wave Imaging for Concealed Weapon Detection,"Millimeter-wave imaging techniques and systems have been developed at the Pacific Northwest National Laboratory (PNNL), Richland, WA, for the detection of concealed weapons and contraband at airports and other secure locations. These techniques were derived from microwave holography techniques that utilize phase and amplitude information recorded over a two-dimensional aperture to reconstruct a focused image of the target. Millimeter-wave imaging is well suited for the detection of concealed weapons or other contraband carried on personnel since millimeter-waves are nonionizing, readily penetrate common clothing material, and are reflected from the human body and any concealed items. In this paper, a wide-bandwidth three-dimensional holographic microwave imaging technique is described. Practical weapon detection systems for airport or other high-throughput applications require high-speed scanning on the order of 3 to 10 s. To achieve this goal, a prototype imaging system utilizing a 27–33 GHz linear sequentially switched array and a high-speed linear scanner has been developed and tested. This system is described in detail along with numerous imaging results.", 
We are building an interactive visual text analysis tool that aids users in analyzing large collections of text. Our tool tightly integrates state-of-the-art text analytics with interactive visualization to maximize the value of both. We have applied our work to a number of text corpora and our evaluation shows promise.,"TIARA, Interactive, Topic-Based Visual Text Summarization and Analysis","We are building an interactive visual text analysis tool that aids users in analyzing large collections of text. Unlike existing work in visual text analytics, which focuses either on developing sophisticated text analytic techniques or inventing novel text visualization metaphors, ours tightly integrates state-of-the-art text analytics with interactive visualization to maximize the value of both. In this article, we present our work from two aspects. We first introduce an enhanced, LDA-based topic analysis technique that automatically derives a set of topics to summarize a collection of documents and their content evolution over time. To help users understand the complex summarization results produced by our topic analysis technique, we then present the design and development of a time-based visualization of the results. Furthermore, we provide users with a set of rich interaction tools that help them further interpret the visualized results in context and examine the text collection from multiple perspectives. As a result, our work offers three unique contributions. First, we present an enhanced topic modeling technique to provide users with a time-sensitive and more meaningful text summary. Second, we develop an effective visual metaphor to transform abstract and often complex text summarization results into a comprehensible visual representation. Third, we offer users flexible visual interaction tools as alternatives to compensate for the deficiencies of current text summarization techniques. We have applied our work to a number of text corpora and our evaluation shows promise, especially in support of complex text analyses", 
"Neurological disease such as the epilepsy is diagnosed using the analysis of electroencephalogram (EEG) recordings. Focal EEG signals are generated from epileptogenic areas, and the nonfocal signals are obtained from other regions of the brain. The classification of the focal and non-focal EEG signal are necessary for locating the epileptic areas during surgery for epilepsy.",Time-Frequency Domain Deep Convolutional Neural Network for the Classification of Focal and Non-Focal EEG Signals,"The neurological disease such as the epilepsy is diagnosed using the analysis of electroencephalogram (EEG) recordings. The areas of the brain associated with the consequence of epilepsy are termed as epileptogenic regions. The focal EEG signals are generated from epileptogenic areas, and the nonfocal signals are obtained from other regions of the brain. Thus, the classification of the focal and non-focal EEG signals are necessary for locating the epileptogenic areas during surgery for epilepsy. In this paper, we propose a novel method for the automated classification of focal and non-focal EEG signals. The method is based on the use of the synchrosqueezing transform (SST) and deep convolutional neural network (CNN) for the classification. The time-frequency matrices of EEG signal are evaluated using both Fourier SST (FSST) and wavelet SST (WSST). The twodimensional (2D) deep CNN is used for the classification using the time-frequency matrix of EEG signals. The experimental results reveal that the proposed method attains the accuracy, sensitivity, and specificity values of more than 99% for the classification of focal and non-focal EEG signals. The method is compared with existing approaches for the discrimination of focal and non-focal categories of EEG signals.", 
"Digital forensics is a growing field with a high need for qualified professionals but a lack of people to fill this need. There is a need for the creation of forensic tools to help streamline this process. In this paper, we propose a web based interactive data visualization timeline.",Timely A Chain of Custody Data Visualizer,"Digital forensics is a growing field with a high need for qualified professionals but a lack of people to fill this need. As a result, there is a need for the creation of forensic tools to help streamline this process and to allow those in the field and those that are breaking into the field to be able to learn and succeed in their respective careers. In the field, chain of custody reports are used to track and document changes in possession or ownership of evidence. While the written report is standard, timelines have been shown to be effective visualization tools in both organizing and displaying information, as well as educating those who use them. In this paper, we are proposing a web based interactive data visualization timeline that organizes the chain of custody and evidence information in an easy to understand and easily accessible interface.", 
VANET is a subset of MANET in which communication among the vehicles may be done using vehicle-to-vehicle infrastructure. There may be chances of attacks in VANET due to mobility of nodes and random change in topology. One of the prominent attack is Sybil attack in which attacker creates multiple false identities.,Timestamp Based Detection of Sybil Attack in VANET,"VANET is a subset of MANET in which communication among the vehicles may be done using vehicle -to- vehicle or roadside infrastructure. But there may be chances of attacks in VANET due to mobility of nodes and random change in topology. One of the prominent attack is Sybil attack in which attacker creates multiple false identities to disturb the functionality of VANET. In literature many solution have been proposed for detection and protection of vehicles from Sybil attack. In the current work, authors have proposed a Sybil node detection technique based on timestamp mechanism. In this work timestamp is a unique certificate provided by RSU to all vehicles on the road in VANET. In the proposed work for node discovery and data transmission, we used Ad-hoc On Demand Distance Vector (AODV) Routing protocol and timestamp as a hash function of public key and for detection of the Sybil node implemented through NS2 simulator", 
A suitable evolutionary text graph model may impart a better understanding of the texts and improve the summarization process. We propose a timestamped graph model that is motivated by human writing and reading processes. We apply this model to the standard DUC multi-document text summarization task.,"Timestamped Graphs, Evolutionary Models of Text for Multi-document Summarization","Current graph-based approaches to automatic text summarization, such as LexRank and TextRank, assume a static graph which does not model how the input texts emerge. A suitable evolutionary text graph model may impart a better understanding of the texts and improve the summarization process. We propose a timestamped graph (TSG) model that is motivated by human writing and reading processes, and show how text units in this model emerge over time. In our model, the graphs used by LexRank and TextRank are specific instances of our timestamped graph with particular parameter settings. We apply timestamped graphs on the standard DUC multi-document text summarization task and achieve comparable results to the state of the art.", 
"In this paper, we design time–frequency localized three-band biorthogonal linear phase wavelet filter bank for epileptic seizure electroencephalograph (EEG) signal classification. We use convex semidefinite programming to transform a nonconvex problem into a convex SDP. The filter banks outperform two-band filter banks in the classification of seizure and seizure-free EEG signals.",Time-frequency localized three-band biorthogonal wavelet filter bank using semidefinite relaxation and nonlinear least squares with epileptic seizure EEG signal classification,"In this paper, we design time–frequency localized three-band biorthogonal linear phase wavelet filter bank for epileptic seizure electroencephalograph (EEG) signal classification. Time–frequency localized analysis and synthesis low-pass filters are designed using convex semidefinite programming (SDP) by transforming a nonconvex problem into a convex SDP using semidefinite relaxation technique. Threeband parameterized lattice biorthogonal linear phase perfect reconstruction filter bank (BOLPPRFB) is chosen and nonlinear least squares algorithm is used to determine its parameters values that generate the designed analysis and synthesis low-pass filters such that the band-pass and high-pass filters are also well localized in time and frequency domain. The designed analysis and synthesis three-band wavelet filter banks are compared with the standard two-band filter banks like Daubechies maximally regular filter banks, Cohen–Daubechies–Feauveau (CDF) biorthogonal filter banks and orthogonal time–frequency localized filter banks. Kruskal–Wallis statistical test is employed to measure the statistical significance of the subband features obtained from the various two and three-band filter banks for epileptic seizure EEG signal classification. The results show that the designed three-band analysis and synthesis filter banks both outperform two-band filter banks in the classification of seizure and seizure-free EEG signals. The designed three-band filter banks and multi-layer perceptron neural network (MLPNN) are further used together to implement a signal classifier that provides classification accuracy better than the recently reported results for epileptic seizure EEG signal classification", 
"Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. In this study, by following preprocessing steps, a Feature Dependent Naives (FDNB) classification method is proposed.",Title A Feature Dependent Naive Bayes Approach and Its,"Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes (FDNB) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set.", 
"In this paper, we propose a web based and domain independent automatic text summarization method. The method focuses on generating an arbitrary length summary by extracting and assigning scores to semantically important information. It also takes font semantics of the text into consideration while scoring different entities of the document.","Too Long-Didn't Read, A Practical Web Based Approach towards Text Summarization","In today’s digital epoch, people share and read a motley of never ending electronic information, thus either a lot of time is wasted in deciphering all this information, or only a tiny amount of it is actually read. Therefore, it is imperative to contrive a generic text summarization technique. In this paper, we propose a web based and domain independent automatic text summarization method. The method focuses on generating an arbitrary length summary by extracting and assigning scores to semantically important information from the document, by analyzing term frequencies and tagging certain parts of speech like proper nouns and signal words. Another important characteristic of our approach is that it also takes font semantics of the text (like headings and emphasized texts) into consideration while scoring different entities of the document.", 
"This article aims to compile the current status of software tools used in the different RE activities to direct future development efforts. We selected 50 articles, 33 conference and 17 journal papers, from the initially retrieved 10,191 articles. The results from the data analysis enabled us to classify RE tools in 15 categories and 29 subcategories.",Tool-Supported Requirement Engineering A Categorization of the State of the Art,"The multiple activities that are part of Requirements Engineering (RE) are benefited from the support of software tools that assists the application of diverse techniques. Given the importance of this phase in software development and specifically in product safety, this article aims to compile the current status of software tools used in the different RE activities to direct future development efforts in this type of tools and identify the activities of the RE, the implemented techniques and which tools focus on the requirements associated with safety. We used systematic literature review method for reviewing the literature on RE tools. We used automatic search strategies for searching the relevant papers published between 1 January 2013 and 31 December 2018 and Meta-aggregation to synthesize the data extracted from the primary studies and to obtain a categorization of findings. We selected 50 articles, 33 conference and 17 journal papers, from the initially retrieved 10,191 articles. The results from the data analysis 248 findings enabled us to classify RE tools in 15 categories and 29 subcategories. This review has enabled us to identify the following areas for further research in software tool assisted RE: 1) is necessary to perform more research on the development of RE tools that specialize in software requirements associated to a quality attribute of interest as safety; 2) it is important to development RE tools activities different areas from requirement specification, validation and management; 3) in is important to conduct evaluation the RE tools founds in non-academic environment.", 
This paper attempts to exploit appropriate priors to generate topic aspect-oriented summarization. The underlying intuition of the proposed TAOS is that different topics can prefer different aspects. Different aspects can be represented by different preference of features. The results show our method can generate meaningful summarization in terms of ROUGE and Jensen–Shannon Divergence metrics.,Topic aspect-oriented summarization via group selection,"The summarization is desirable to efficiently apprehend the gist of the huge amount of data and becomes a significant challenge in many applications such as news article summarization and social media mining. Considering the summaries from multi-documents of one topic can describe various aspects of one given topic, this paper attempts to exploit appropriate priors to generate topic aspect-oriented summarization (abbreviated as TAOS). The underlying intuition of the proposed TAOS is that different topics can prefer different aspects and the different aspects can be represented by different preference of features(e.g., technical topic may prefer proper noun than sports topic). In order to materialize the intuition of TAOS, we first extract several groups of features according to topic factors, and then a group norm penalty (i.e., ?1=?2 norm) and latent variables are utilized to select overlapping groups of features. We compare our proposed approach with some state-of-the-art methods on DUC2003, DUC2004 datasets for text summarization and NUS-Wide dataset for image summarization. The results show our method can generate meaningful summarization in terms of ROUGE and Jensen–Shannon Divergence metrics", 
"The qualities of human readable summaries available in the datasets are not up to the mark, leading to issues in creating an accurate model for text summarization. The paper proposes a novel methodology for summarizing a corpus of documents to generate a coherent summary using topic modeling and classification technique. The outcomes of the empirical work show that the proposed model is more promising compared to the well-known text summaries.",Topic modeling combined with classification technique for extractive multi-document text summarization,"The qualities of human readable summaries available in the datasets are not up to the mark, leading to issues in creating an accurate model for text summarization. Although recent works have been largely built upon this issue and set up a strong platform for further improvements, they still have many limitations. Looking in this direction, the paper proposes a novel methodology for summarizing a corpus of documents to generate a coherent summary using topic modeling and classification technique. The objectives of the propose work are highlighted below: • A novel heuristic approach is introduced to find out the actual number of topics that exist in a corpus of documents which handles the stochastic nature of latent dirichlet allocation. • A large corpus of documents is handled by minimizing the huge set of sentences into a small set without losing the important one and thus providing a concise and information rich summary at the end. • Ensuring that the sentences are arranged as per their importance in the coherent summary. • Results of the experiment are compared with the state-of-the-art summary systems. The outcomes of the empirical work show that the proposed model is more promising compared to the well-known text summarization models.", 
Design pattern is a high-quality and reusable solution to a recurring software design problem. Paper proposes a novel approach for the automatic selection of the fit design pattern. This approach is based on using Latent Dirichlet Allocation (LDA) topic model.,Topic Modelling for Automatic Selection of Software Design Patterns,"Design pattern is a high-quality and reusable solution to a recurring software design problem. It is considered an important concept in the software engineering field due to its ability to enhance some of the quality attributes of the software systems including maintainability and extensibility. However, novice developers need to be provided by a tool to assist them in selecting the fit design pattern to solve a design problem. The paper proposes a novel approach for the automatic selection of the fit design pattern. This approach is based on using Latent Dirichlet Allocation (LDA) topic model. The topic is a set of words that often appear together. LDA is able to relate words with similar meaning and to differentiate between uses of words with multiple meanings. In this paper LDA is used to analyze the textual descriptions of design patterns and extract the topics then discover the similarity between the target problem scenario and the collection of patterns using Improved Sqrt-Cosine similarity measure (ISCS). The proposed approach was evaluated using Gang of four design patterns. The experimental results showed that the proposed approach outperforms approach based on the traditional vector space model of Unigrams.", 
"A context-based topic segmentation system based on a new informative similarity measure based on word co-occurrence. In particular, our evaluation with the state-of-the-art","Topic segmentation algorithms for text summarization and passage retrieval, An exhaustive evaluation","In order to solve problems of reliability of systems based on lexical repetition and problems of adaptability of languagedependent systems, we present a context-based topic segmentation system based on a new informative similarity measure based on word co-occurrence. In particular, our evaluation with the state-of-the-art in the domain i.e. the c99 and the TextTiling algorithms shows improved results both with and without the identification of multiword units.", 
"Automatic text summarization aims at condensing a document to a shorter version while preserving the key information. Most current state-ofthe-art (SOTA) abstractive summarization methods are based on the Transformer-based encoder-decoder architecture. This study proposes a topic-aware abstractive. summarization (TAAS) framework by leveraging the underlying semantic structure. of documents. TAAS outperforms BART by 2%, 8% and 12% regarding the F measure of ROUGE-1, RouGE-2, and ROUAGE-L, respectively.",Topic-Aware Abstractive Text Summarization,"Automatic text summarization aims at condensing a document to a shorter version while preserving the key information. Different from extractive summarization which simply selects text fragments from the document, abstractive summarization generates the summary in a word-by-word manner. Most current state-ofthe-art (SOTA) abstractive summarization methods are based on the Transformer-based encoder-decoder architecture and focus on novel self-supervised objectives in pre-training. While these models well capture the contextual information among words in documents, little attention has been paid to incorporating global semantics to better fine-tune for the downstream abstractive summarization task. In this study, we propose a topic-aware abstractive summarization (TAAS) framework by leveraging the underlying semantic structure of documents represented by their latent topics. Specifically, TAAS seamlessly incorporates a neural topic modeling into an encoder-decoder based sequence generation procedure via attention for summarization. This design is able to learn and preserve global semantics of documents and thus makes summarization effective, which has been proved by our experiments on real-world datasets. As compared to several cutting-edge baseline methods, we show that TAAS outperforms BART, a well-recognized SOTA model, by 2%, 8%, and 12% regarding the F measure of ROUGE-1, ROUGE-2, and ROUGE-L, respectively. TAAS also achieves comparable performance to PEGASUS and ProphetNet, which is difficult to accomplish given that training PEGASUS and ProphetNet requires enormous computing capacity beyond what we used in this study.", 
We consider the problem of query-focused multidocument summarization. Our approach combines queryfocused and thematic features to estimate the summaryrelevance of sentences. Experimental results show that our approach outperforms the best reported results on DUC 2006 data.,Topic-based multi-document summarization with probabilistic latent semantic analysis,"We consider the problem of query-focused multidocument summarization, where a summary containing the information most relevant to a user’s information need is produced from a set of topic-related documents. We propose a new method based on probabilistic latent semantic analysis, which allows us to represent sentences and queries as probability distributions over latent topics. Our approach combines queryfocused and thematic features computed in the latent topic space to estimate the summaryrelevance of sentences. In addition, we evaluate several different similarity measures for computing sentence-level feature scores. Experimental results show that our approach outperforms the best reported results on DUC 2006 data, and also compares well on DUC 2007 data.", 
"This approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system",Topical Coherence for Graph-based Extractive Summarization,"We present an approach for extractive single-document summarization. Our approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system with state-of-the-art results on scientific articles from PLOS Medicine and on DUC 2002 data. Human judges evaluate the coherence of summaries generated by our system in comparision to two baselines. Our approach obtains competitive performance.", 
Topological data analysis is a branch of machine learning that excels in studying high-dimensional data. Data objects with mixed numeric and categorical attributes are ubiquitous in real-world applications. The proposed method outperforms several state-of-the-art algorithms in the prediction of heart disease.,Topological Machine Learning for Mixed Numeric and Categorical Data,"Topological data analysis is a relatively new branch of machine learning that excels in studying high-dimensional data, and is theoretically known to be robust against noise. Meanwhile, data objects with mixed numeric and categorical attributes are ubiquitous in real-world applications. However, topological methods are usually applied to point cloud data, and to the best of our knowledge there is no available framework for the classification of mixed data using topological methods. In this paper, we propose a novel topological machine learning method for mixed data classification. In the proposed method, we use theory from topological data analysis such as persistent homology, persistence diagrams and Wasserstein distance to study mixed data. The performance of the proposed method is demonstrated by experiments on a real-world heart disease dataset. Experimental results show that our topological method outperforms several state-of-the-art algorithms in the prediction of heart disease.", 
"We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR) In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and",Toward Abstractive Summarization Using Semantic Representations,"We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results", 
"Main vision of Internet of Things is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. Internet of Vehicles is a vehicular instance of the Social IoT (SIoT), where vehicles are the key social entities in the machine-to-machine vehicular social networks.","Toward Social Internet of Vehicles Concept, Architecture, and Applications","The main vision of the Internet of Things (IoT) is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. As one of the key members of IoT, Internet of Vehicles (IoV) has seen steep advancement in communication technologies. Now, vehicles can easily exchange safety, efficiency, infotainment, and comfort-related information with other vehicles and infrastructures using vehicular ad hoc networks (VANETs). We leverage on the cloud-based VANETs theme to propose cyber-physical architecture for the Social IoV (SIoV). SIoV is a vehicular instance of the Social IoT (SIoT), where vehicles are the key social entities in the machine-to-machine vehicular social networks. We have identified the social structures of SIoV components, their relationships, and the interaction types. We have mapped VANETs components into IoT-A architecture reference model to offer better integration of SIoV with other IoT domains. We also present a communication message structure based on automotive ontologies, the SAE J2735 message set, and the advanced traveler information system events schema that corresponds to the social graph. Finally, we provide the implementation details and the experimental analysis to demonstrate the efficacy of the proposed system as well as include different application scenarios for various user groups.", 
Automation has been proposed or used to expedite most steps of the systematic review process. How these technologies work in practice and when (and when not) to use them is often not clear to practitioners.,Toward systematic review automation a practical guide to using machine learning tools in research synthesis,"Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.", 
"Unstructured data is growing very fast especially when analysing external data such as customers' reviews in social media. It is essential to determine the reputation of the source to the analysts, so that they can take into account the trust value of each source. Another important consideration is the semantics of extracted textual data.",Towards a Methodology for Social Business Intelligence in the Era of Big Social Data Incorporating Trust and Semantic Analysis,"Business intelligence applications support decision makers by providing meaningful information from extracted data mainly coming from operational databases and structured data sources. However, the volume of unstructured data is growing very fast especially when analysing external data such as customers’ reviews in social media. It is essential to determine the reputation of the source to the analysts, so that they can take into account the trust value of each source in their analysis. Another important consideration is the semantics of extracted textual data from which meaningful information is derived. The aim of this paper is to provide readers with an understanding of the central concepts and the current state-of-the-art in social trust and semantic analysis of big social data. We provide an in depth analysis of existing challenges and identify set of quality attributes to be used as guide for designing and evaluating architectures of big social trust.", 
"The intrinsic and extrinsic quality evaluation is an essential part of the summary evaluation methodology. Processing large text corpora using these methods is expensive from both the organizational and the financial perspective. For the first time, we propose crowdsourcing to evaluate the quality of query-based extractive text summaries.",Towards a reliable and robust methodology for crowd-based subjective quality assessment of query-based extractive text summarization,"The intrinsic and extrinsic quality evaluation is an essential part of the summary evaluation methodology usually conducted in a traditional controlled laboratory environment. However, processing large text corpora using these methods reveals expensive from both the organizational and the financial perspective. For the first time, and as a fast, scalable, and cost-effective alternative, we propose micro-task crowdsourcing to evaluate both the intrinsic and extrinsic quality of query-based extractive text summaries. To investigate the appropriateness of crowdsourcing for this task, we conduct intensive comparative crowdsourcing and laboratory experiments, evaluating nine extrinsic and intrinsic quality measures on 5-point MOS scales. Correlating results of crowd and laboratory ratings reveals high applicability of crowdsourcing for the factors overall quality, grammaticality, non-redundancy, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness. Further, we investigate the effect of the number of repetitions of assessments on the robustness of mean opinion score of crowd ratings, measured against the increase of correlation coefficients between crowd and laboratory. Our results suggest that the optimal number of repetitions in crowdsourcing setups, in which any additional repetitions do no longer cause an adequate increase of overall correlation coefficients, lies between seven and nine for intrinsic and extrinsic quality factors.", 
This paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single documents. The approach can naturally make full use of the reinforcement between sentences and keywords. The corpus-based approach is validated to work almost as well as the knowledge-,Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction,"Though both document summarization and keyword extraction aim to extract concise representations from documents, these two tasks have usually been investigated independently. This paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document under the assumption that the summary and keywords of a document can be mutually boosted. The approach can naturally make full use of the reinforcement between sentences and keywords by fusing three kinds of relationships between sentences and words, either homogeneous or heterogeneous. Experimental results show the effectiveness of the proposed approach for both tasks. The corpus-based approach is validated to work almost as well as the knowledge-based approach for computing word semantics", 
"Study aimed to devise an extractive summarization method for A-133 Single Audits. Audits assess if recipients of federal grants are compliant with program requirements for use of federal funding. Currently, these voluminous audits must be manually analyzed by officials for oversight, risk management, and prioritization purposes. This work highlights the inherent difficulty and subjective nature of automated summarization in a real-world application.",Towards automatic extractive text summarization of A-133 Single Audit reports with machine learning,"The rapid growth of text data has motivated the development of machine-learning based automatic text summarization strategies that concisely capture the essential ideas in a larger text. This study aimed to devise an extractive summarization method for A-133 Single Audits, which assess if recipients of federal grants are compliant with program requirements for use of federal funding. Currently, these voluminous audits must be manually analyzed by officials for oversight, risk management, and prioritization purposes. Automated summarization has the potential to streamline these processes. Analysis focused on the “Findings” section of ~20,000 Single Audits spanning 2016-2018. Following text preprocessing and GloVe embedding, sentence-level k-means clustering was performed to partition sentences by topic and to establish the importance of each sentence. For each audit, key summary sentences were extracted by proximity to cluster centroids. Summaries were judged by non-expert human evaluation and compared to human-generated summaries using the ROUGE metric. Though the goal was to fully automate summarization of A-133 audits, human input was required at various stages due to large variability in audit writing style, content, and context. Examples of human inputs include the number of clusters, the choice to keep or discard certain clusters based on their content relevance, and the definition of a top sentence. Overall, this approach made progress towards automated extractive summaries of A-133 audits, with future work to focus on full automation and improving summary consistency. This work highlights the inherent difficulty and subjective nature of automated summarization in a real-world application.", 
"Twitter has become one of the most important microblogging services of the Web 2.0. Among the possible uses it allows, it can be employed for communicating and broadcasting information in real time. Researchers say relying only on tweets may not be the ideal way to communicate news through Twitter.","Towards automatic tweet generation, A comparative study from the text summarization perspective in the journalism genre","In recent years, Twitter has become one of the most important microblogging services of the Web 2.0. Among the possible uses it allows, it can be employed for communicating and broadcasting information in real time. The goal of this research is to analyze the task of automatic tweet generation from a text summarization perspective in the context of the journalism genre. To achieve this, different state-ofthe-art summarizers are selected and employed for producing multi-lingual tweets in two languages (English and Spanish). A wide experimental framework is proposed, comprising the creation of a new corpus, the generation of the automatic tweets, and their assessment through a quantitative and a qualitative evaluation, where informativeness, indicativeness and interest are key criteria that should be ensured in the proposed context. From the results obtained, it was observed that although the original tweets were considered as model tweets with respect to their informativeness, they were not among the most interesting ones from a human viewpoint. Therefore, relying only on these tweets may not be the ideal way to communicate news through Twitter, especially if a more personalized and catchy way of reporting news wants to be performed. In contrast, we showed that recent text summarization techniques may be more appropriate, reflecting a balance between indicativeness and interest, even if their content was different from the tweets delivered by the news providers.", 
G-FLOW is a novel system for coherent extractive multi-document summarization. It introduces a joint model for selection and ordering that balances coherence and salience. It generates dramatically better summaries than an extractive summarizer based on state-of-the-art sentence selection and reordering.,Towards Coherent Multi-Document Summarization,"This paper presents G-FLOW, a novel system for coherent extractive multi-document summarization (MDS).1 Where previous work on MDS considered sentence selection and ordering separately, G-FLOW introduces a joint model for selection and ordering that balances coherence and salience. G-FLOW’s core representation is a graph that approximates the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference, and more. This graph enables G-FLOW to estimate the coherence of a candidate summary. We evaluate G-FLOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joint model", 
Multidimensional knowledge representation (MKR) is the result of integrative text mining. This paper introduces cross-dimensional text summarization based on dimensional selection and filtering of results retrieved from MKR knowledge base.,Towards Extractive Text Summarization Using Multidimensional Knowledge Representation,"Multidimensional knowledge representation (MKR) is the result of integrative text mining. Analysis results from individual text mining methods such as named entity recognition, sentiment analysis, or topic detection are represented as dimensions in a knowledge base to support knowledge discovery, visualization or complex computer-aided writing tasks. Extractive text summarization is a content-oriented task which uses available information from text to shorten its length in order to summarize it. In this regard, a MKR knowledge base provides a structure which is applicable as an innovative selection instrument for text summarization. This paper introduces cross-dimensional text summarization based on dimensional selection and filtering of results retrieved from MKR knowledge base.", 
"In this paper we present a novel method for automatic text summarization through text extraction. The new idea is to view all the extracted text as a whole and compute a score for the total impact of the summary. The proposed method is largely language independent, though we only evaluate it on English.","Towards Holistic Summarization-Selecting Summaries, Not Sentences","In this paper we present a novel method for automatic text summarization through text extraction, using computational semantics. The new idea is to view all the extracted text as a whole and compute a score for the total impact of the summary, instead of ranking for instance individual sentences. A greedy search strategy is used to search through the space of possible summaries and select the summary with the highest score of those found. The aim has been to construct a summarizer that can be quickly assembled, with the use of only a very few basic language tools, for languages that lack large amounts of structured or annotated data or advanced tools for linguistic processing. The proposed method is largely language independent, though we only evaluate it on English in this paper, using ROUGEscores on texts from among others the DUC 2004 task 2. On this task our method performs better than several of the systems evaluated there, but worse than the best systems.", 
"Static Architecture Conformance Checking (SACC) relies on manual mapping of source code entities. Semi-automatic clustering is a promising approach to improve this. HuGMe is an example of such a technique for use in SACC. However, the variability of the performance is high.",Towards Improved Initial Mapping in Semi Automatic Clustering,"An important step in Static Architecture Conformance Checking (SACC) is the mapping of source code entities to entities in the intended architecture. This step is currently relying on manual work, which is one hindrance for more widespread adoption of SACC in industry. Semi-automatic clustering is a promising approach to improve this, and the HuGMe clustering algorithm is an example of such a technique for use in SACC. But HuGMe relies on an initial set of clustered source code elements and algorithm parameters. We investigate the automatic mapping performance of HuGMe in two experiments to gain insight into what influence the starting set has in a medium-sized open source system, JabRef, which contain a relatively large number of architectural violations. Our results show that the highest automatic mapping performance can be achieved with a low number of elements within the initial set. However, the variability of the performance is high. We find a benefit in favoring source code elements with a high fan-out in the initial set", 
A design pattern is a high-quality reusable solution to a commonly occurring design problem. Paper proposes a text retrieval based approach for automatic selection of the fit design pattern. The approach is based on generating a vector space model of unigrams and topics to the catalogue of patterns.,Towards more accurate automatic recommendation of software design patterns,"Design pattern is a high-quality reusable solution to a commonly occurring design problem in certain context. Using design patterns in software development improves some of the quality attributes of the system including productivity, understandability and maintainability. However, it is hard for novice developers to select a fit design pattern to solve a design problem. The paper proposes a text retrieval based approach for the automatic selection of the fit design pattern. This approach is based on generating a vector space model (VSM) of unigrams and topics to the catalogue of patterns. The topic is a set of words that often appear together. Latent Dirichlet Allocation topic model is adopted to analyze the textual descriptions of the patterns to extract the key topics and discover the hidden semantic. The similarity between the target problem scenario and the collection of patterns is measured using an improved version of the popular Cosine similarity measure. The proposed approach was assessed using Gang of four design patterns catalog and a collection of real design problems. The experimental results showed the effectiveness of the proposed approach which achieved 72 % precision.", 
"Paper proposes new text summarization approaches based on textual unit association networks. Textual units refer to words, phrases, sentences, or paragraphs. Intuitively, units containing much co-occurrence information are semantically more salient in a document.",Towards More Effective Text Summarization Based on Textual Association Networks,"This paper proposes new text summarization approaches based on textual unit association networks. Textual units refer to words, phrases, sentences, or paragraphs. Intuitively, textual units containing much co-occurrence information are semantically more salient in a document. We construct two kinds of textual association networks, namely, word-based association network and sentence-based association network. For the former, we propose a new approach to computing the word weights and sentence weights. For the latter, we develop a new scheme to score each sentence based on its co-occurrence information. Extensive experiments on benchmark data show that our proposed approaches can achieve better summarization performance than the existing methods. Our approaches are unsupervised, independent of languages, and efficient for different text genre.", 
This article explains our submitted approach to the DocEng'19 competition on extractive summarization. We bypass the lack of large annotated news corpora for extractive,Towards Supervised Extractive Text Summarization via RNN-based Sequence Classification,"This article briefly explains our submitted approach to the DocEng’19 competition on extractive summarization [ 3]. We implemented a recurrent neural network based model that learns to classify whether an article’s sentence belongs to the corresponding extractive summary or not. We bypass the lack of large annotated news corpora for extractive summarization by generating extractive summaries from abstractive ones, which are available from the CNN corpus.", 
"Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms stateof-the-art algorithms.","Towards Total Scene Understanding Classification, Annotation and Segmentation in an Automatic Framework","Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. To our knowledge, this is the first model that performs all three tasks in one coherent framework. For instance, a scene of a ‘polo game’ consists of several visual objects such as ‘human’, ‘horse’, ‘grass’, etc. In addition, it can be further annotated with a list of more abstract (e.g. ‘dusk’) or visually less salient (e.g. ‘saddle’) tags. Our generative model jointly explains images through a visual model and a textual model. Visually relevant objects are represented by regions and patches, while visually irrelevant textual annotations are influenced directly by the overall scene class. We propose a fully automatic learning framework that is able to learn robust scene models from noisy web data such as images and user tags from Flickr.com. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms stateof-the-art algorithms.", 
Vision-only systems are currently very popular for use in autonomous driving systems. This article proposes a traffic light recognition system. adaptive thresholding and deep learning are used for region proposal and traffic light localization. LISA open-source dataset is used along with custom augmentation methods.,Traffic Light Detection in Autonomous Driving Systems,"Vision-only systems are currently very popular for use in autonomous driving systems and advanced driver-assistance systems. These systems operate by using input images from a camera and no other sensors. One of the tasks these systems needs to perform is the detection and understanding of traffic lights in a traffic environment, by localizing all relevant traffic lights in an image received from an on board camera mounted on the vehicle. This article proposes a traffic light recognition system where adaptive thresholding and deep learning are used for region proposal and traffic light localization, respectively. The LISA open-source dataset is used along with custom augmentation methods in order to increase the number of available data samples. Performance of the developed system is presented in the form of true and false positive rates obtained on the test data. The classification part of the algorithm gives a total of 89.60% true detection rate, while the regression part of the model produced a correct location of the traffic light in 92.67% of cases.",  
Weka Segmentation is a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. It can be customized to employ user-designed image features or classifiers.,Trainable Weka Segmentation a machine learning tool for microscopy pixel classification,"State-of-the-art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time-consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user-designed image features or classifiers", 
Deep learning in biomedical field is yet to be fully utilized. Method using CNN with magnitude and phase based features can be better than other state-of-the-art approaches. The proposed method shows increase in classification accuracy compared to other MI classification methods.,Transform based feature construction utilizing magnitude and phase for convolutional neural network in EEG signal classification,"Extracting relevant feature and classification are significant in brain-computer interface (BCI) systems. Deep learning have achieved remarkable growth in many fields like speech recognition and computer vision. However, deep learning in biomedical field is yet to be fully utilized. In this paper, We propose a novel methodology for convolutional neural network (CNN) based motor imagery (MI) classification using new form of input. Continuous Wavelet Transform (CWT) is applied to the input Electroencephalography (EEG) signal to extract the features of MI. After transformation, we consider the real part and imaginary part of the transformed signal to exploit magnitude and phase information at the same time. This feature is fed to the CNN having one convolution layer, one max-pooling layer and one fully connected layer. The classification accuracy is tested on two public BCI datasets: BCI competition IV dataset IIb and BCI competition II dataset III. The proposed method shows increase in classification accuracy compared to other MI classification methods. The results show that the method using CNN with magnitude and phase based features can be better than other state-of-the-art approaches.", 
"TREADS is targeted to provide safe, effective, and convenient travel strategies for commuters and tourists. The system consists of an efficient route recommendation service that considers safety and user interest factors. The proposed system can greatly improve the travel experience for tourists in unfamiliar cities.","TREADS, a safe route recommender using social media mining and text summarization","This paper presents TREADS, a novel travel route recommendation system that suggests safe travel itineraries in real time by incorporating social media data resources and points of interest review summarization techniques. The system consists of an efficient route recommendation service that considers safety and user interest factors, a transportation related tweets retriever with high accuracy, and a novel text summarization module that provides summaries of location based Twitter data and Yelp reviews to enhance our route recommendation service. We demonstrate the system by utilizing crime and points of interest data in the Washington DC area. TREADS is targeted to provide safe, effective, and convenient travel strategies for commuters and tourists. Our proposed system, integrated with multiple social media resources, can greatly improve the travel experience for tourists in unfamiliar cities.", 
"Group Key Agreement Protocol (GKAP) is a cryptographic mechanism where members of a group agree on a common key. Sharing a key over a public channel is a security threat and expensive in terms of communication cost. In this paper, we proposed the GKAP based on tree and elliptic curve. The proposed approach is safe against passive attack, collaborative attack, forward secrecy, backward secrecy, and man-in-themiddle attack.",Tree and elliptic curve based effcient and secure group key agreement protocol,"Group Key Agreement Protocol (GKAP) is a cryptographic mechanism where members of a group agree on a common key by sharing their blinded keys over a public channel. Sharing a key over a public channel is a security threat and expensive in terms of communication cost. In this paper, we proposed the GKAP based on tree and elliptic curve. For reducing the communication cost, we have used the divide-and-conquer mechanism with that group is divided into small subgroups and forming a tree-like structure. The modified Elliptic-Curve-Diffie–Hellman used for sharing the blinded key over a public network channel securely. This paper discussing different group key management operations are initialization, join, mass join, leave, mass-leave, merge with their communication cost are the number of rounds, unicast cost, broadcast cost, messages. This paper discussing the establishment of common keys not only for the group but also for the subgroups. Based on communication cost, we have compared the performance of proposed method with the existing approaches like CommunicationComputation Efficient Group Key Algorithm (CCEGK), Tree-based group key agreement (TGDH), Ternary-tree based group key agreement protocol for dynamic group (TTGKAP), Group key generation tree protocol (GKGT), Ternary tree-based group key agreement protocol over elliptic curve for dynamic group (TTEGKAP), Efficient Group key agreement using hierarchical key tree (EGKAKT). From performance analysis, it is cleared that proposed approach performed better in most of the cases than the existing approaches. The proposed approach is safe against passive attack, collaborative attack, forward secrecy, backward secrecy, and man-in-themiddle attack", 
Summarization proved to be an advantage over manually summarizing the large data. In this paper extractive and abstractive methods are framed.,Trends in extractive and abstractive techniques in text summarization,Text Summarization was proved to be an advantage over manually summarizing the large data. It condenses the salient features from the text by preserving the content and serves the meaningful summary. Classification can be done in two ways – extractive and abstractive summarization. Extractive summarization uses statistical and linguistic features to determine the important features and fuse them into a shorter version. Whereas abstractive summarization understands the whole document and then generates the summary. In this paper extractive and abstractive methods are framed., 
"VANET is multidimensional network in which the vehicles continuously change their locations. Sometimes, malicious nodes broadcast bogus information among other nodes. Establishing trust is a challenge while one or more malicious nodes attempt to disrupt route discovery.",Trust based approaches for secure routing in VANET A Survey,"Vehicular Ad-hoc networks (VANETs) require trusted vehicles to vehicles communication. VANET is multidimensional network in which the vehicles continuously change their locations. Secure routing is imperative during the routing process to incorporate mutual trust between these nodes. Sometimes, the malicious node broadcast the bogus information among other nodes. Establishing trust is a challenge while one or more malicious nodes attempt to disrupt route discovery or data transmission in the network. A lot of research has been carried out for secure routing process with trust-based approaches. In this paper, we present survey of various mechanisms to improve different ad-hoc routing protocols for secure routing process by enhancing the trust among different nodes in VANETs.", 
"TSception consists of temporal and spatial convolutional layers. It learns discriminative representations in the time and channel domains simultaneously. System is designed to study the emotional arousal in an immersive virtual reality environment. It achieves a high classification accuracy of 86.03%, which outperforms the prior methods significantly.",TSceptionA Deep Learning Framework for Emotion Detection Using EEG,"In this paper, we propose a deep learning framework, TSception, for emotion detection from electroencephalogram (EEG). TSception consists of temporal and spatial convolutional layers, which learn discriminative representations in the time and channel domains simultaneously. The temporal learner consists of multi-scale 1D convolutional kernels whose lengths are related to the sampling rate of the EEG signal, which learns multiple temporal and frequency representations. The spatial learner takes advantage of the asymmetry property of emotion responses at the frontal brain area to learn the discriminative representations from the left and right hemispheres of the brain. In our study, a system is designed to study the emotional arousal in an immersive virtual reality (VR) environment. EEG data were collected from 18 healthy subjects using this system to evaluate the performance of the proposed deep learning network for the classification of low and high emotional arousal states. The proposed method is compared with SVM, EEGNet, and LSTM. TSception achieves a high classification accuracy of 86.03%, which outperforms the prior methods significantly (p<0.05). Index Terms—Deep learning, convolutional neural network, electroencephalography, emotional arousal, virtual reality", 
The decrease in fertility rates especially among the male population is one of those problems. Machine learning and artificial intelligence algorithms are an emerging methodology as computer aided decision systems in medical diagnosis and health problems.,Two Enhancement Levels for Male Fertility Rate Categorization Using Whale Optimization and Pegasos Algorithms,"Recently, diseases and health problems which were common only in the elderly, became common also among the youth. Some of these medical problems causes are behavioral, environmental and lifestyle factors. The decrease in fertility rates especially among the male population is one of those problems. Now, machine learning and artificial intelligence algorithms are an emerging methodology as computer aided decision systems in medical diagnosis and health problems. In this paper an incorporation of bio-inspired whale optimization algorithm (WOA) and Pegasos algorithm is used to enhance the male fertility rate categorization in two levels. Results: show that implementing WOA as second level of enhancement gives better accuracy than the first level of enhancement in Pegasos algorithm with prediction accuracy value of 90%. using two machine learning algorithms to categorize male fertility rate helped in overall improving for the proposed system performance to give results exceeded all recent researches results for fertility data", 
"Summarization is a way of minimizing a textual document to a meaningful summary. In this research, an extractive-based approach is used to generate a two-level summary from online news articles. News topics covered include politics, sports health, science and movie reviews.",Two-level text summarization from online news sources with sentiment analysis,"People tend to read multiple news articles on a topic since a single article may not contain all important information. A summary of all the articles related to topic will save the time and energy. Text Summarization is a way of minimizing a textual document to a meaningful summary. In this research, an extractive-based approach is used to generate a two-level summary from online news articles. News topics covered include politics, sports health, science and movie reviews from Fox News from USA, NZ Herald from New Zealand, Hindustan Times from India, BBC from UK, etc. The first-level summary generates the summary of each article on all these topics. Sentiment Analysis is performed on the first-level summary to understand the variation in related news articles from different news agencies. The secondlevel summary generates the summary of the combined first-level summaries of two/three related articles on a topic. The ROUGE metric is used to evaluate the performance of summarization.", 
"User authentication has become a significant issue in the online environment. Traditional passwords are still the most common means to verify user identity. This study proposes a novel authentication method based on user interface (UI) preferences. The performance, security, and usability of UIPA have been evaluated. It could also help in addressing the security–usability trade-off problem.",UIPA User authentication method based on user interface preferences for account recovery process,"Because of the global increment of various computer implementation, the usage of the internet has increased significantly. This phenomenon has mainly contributed to the growing usage of the World Wide Web, which is considered as one of the inventions in computer technology that changed the modes of human communication and exchange of information. As new work tools, an increasing number of people are using online systems. Accordingly, user authentication has become a significant issue in the online environment. The traditional password is still the most common means to verify user identity. However, strong passwords are sometimes difficult to remember, and thus a recovery procedure is needed. It can be said that the majority of online systems apply different methods of recovery, such as using challenging questions besides the out-of-band communication (i.e., email or SMS). Nevertheless, these approaches are still suffering from security and usability problems. This study therefore proposes a novel authentication method based on user interface (UI) preferences, called the User Interface Preferences Authentication (UIPA). This method can be applied with online systems that offer UI designs since it provides users with the chance to choose a preferred design of the interface on the basis of their personal characteristics. Hence, the user identity is then verified based on that interface. The performance, security, and usability of UIPA have been evaluated. The results of the experiments show that UIPA has a false positive rate of 0.416% and a false negative rate of 0%. Moreover, a user acceptance questionnaire, which has been designed based on the technology acceptance model, shows that users are pleased and willing to accept the proposed technique. To conclude, UIPA can be applied as an efficient account recovery method compared with the currently used methods. It could also help in addressing the security–usability trade-off problem.", 
"Camera-based VLC, namely OCC, provides many unique features when compared to a single-photodiode-based system. OCC technology represents a promising approach to utilize the benefits of VLC in beyond-5G scenarios. Establishing a long communication channel in OCC as well as non-flickering illumination by using low-frame-rate camera detectors, requires special modulation schemes.",Undersampled-Based Modulation Schemes for Optical Camera Communications,"Widespread use of white light-emitting diodes and ubiquitous smart devices offer the opportunity to establish VLC, which has become a hot research topic based on the growing number of publications over the last decade. Camera-based VLC, namely OCC, provides many unique features when compared to a single-photodiode-based system, such as the ability to separate incident light in the spatial and color domains. OCC technology represents a promising approach to utilize the benefits of VLC in beyond-5G scenarios and is one of the key technologies of the Internet of Things. Establishing a long communication channel in OCC, as well as non-flickering illumination by using low-frame-rate camera detectors, requires special modulation schemes. This article provides an overview of the principles of three categories of modulation schemes for OCC systems using a low-frame-rate camera detector. In addition, a series of undersampled modulation schemes are proposed and discussed to achieve flicker-free OCC with higher spectral efficiency. In addition, framing structures are designed to solve problems occurring in OCC systems using particular modulation schemes. To evaluate the performance of these modulation schemes, measured bit error rate values are shown. Finally, challenges in the implementation of OCC systems are also outlined.", 
"Health care industry has not fully grasped the potential benefits to be gained from big data analytics. A study examines the historical development, architectural design and component functionalities of big data. It also maps the benefits driven by big data in terms of IT infrastructure, operational, organizational and managerial areas.",Understanding its capabilities and potential benefits for healthcare,"To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.", 
"Before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them. The aim of this article is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks. We find that saturated units can move out of saturation by themselves, albeit slowly, and explain the plateaus sometimes seen when training neural Networks.",Understanding the difficulty of training deep feedforward neural networks,"Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.", 
Early word embeddings algorithms like word2vec and GloVe generate static distributional representations for words regardless of the context and the sense in which the word is used in a given sentence. A new wave of algorithms based on training language models like Open AI GPT and BERT,Understanding Word Embeddings and Language Models,"Early word embeddings algorithms like word2vec and GloVe generate static distributional representations for words regardless of the context and the sense in which the word is used in a given sentence, offering poor modeling of ambiguous words and lacking coverage for out-of-vocabulary words. Hence a new wave of algorithms based on training language models such as Open AI GPT and BERT has been proposed to generate contextual word embeddings that use as input word constituents allowing them to generate representations for out-of-vocabulary words by combining the word pieces. Recently, fine-tuning pre-trained language models that have been trained on large corpora have constantly advanced the state of the art for many NLP tasks.", 
Pipeline unifies joint image-text embedding models with multimodal neural language models. Structure-content neural language model disentangles the structure of a sentence to its content. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch.,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,"Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.", 
Generic Visual Categorization (GVC) systems use a vocabulary of visual terms to characterize images. We propose a novel practical approach to GVC based on a universal vocabulary. This framework is applied to two types of local image features: low-level descriptors such as the popular SIFT.,Universal and Adapted Vocabularies for Generic Visual Categorization,"Several state-of-the-art Generic Visual Categorization (GVC) systems use a vocabulary of visual terms - a codebook of local image features - to characterize images with a histogram of visual word counts. We propose a novel practical approach to GVC based on a universal vocabulary, which describes the content of all the considered classes of images, and class vocabularies obtained through the adaptation of the universal vocabulary using class-specific data. The main novelty is that an image is characterized by a set of histograms - one per class - where each histogram describes whether the image content is best modeled by the universal vocabulary or the corresponding class vocabulary. This framework is applied to two types of local image features: low-level descriptors such as the popular SIFT and highlevel histograms of word co-occurrences in a spatial neighborhood. It is shown experimentally on two challenging datasets (an in-house database of 19 categories and the recently released PASCAL VOC 2006) that the proposed approach exhibits state-of-the-art performance at a modest computational cost.", 
"Proposed approach analyzes the mesh of multiple unstructured documents and generates a linked set of multiple weighted nodes by applying multistage Clustering. The outcome of this technique consists of interlinked sub-corpuses through clusters. This interlinked corpus processing follows step by step clustering to search the most relevant parts of the corpus with less cost, time, and improve content detection, authors say.",Unstructured Text Documents Summarization With Multi-Stage Clustering,"In natural language processing, text summarization is an important application used to extract desired information by reducing large text. Existing studies use keyword-based algorithms for grouping text, which do not give the documents’ actual theme. Our proposed dynamic corpus creation mechanism combines metadata with summarized extracted text. The proposed approach analyzes the mesh of multiple unstructured documents and generates a linked set of multiple weighted nodes by applying multistage Clustering. We have generated adjacency graphs to link the clusters of various collections of documents. This approach comprises of ten steps: pre-processing, making multiple corpuses, first stage clustering, creating sub-corpuses, interlinking sub-corpuses, creating page rank keyword dictionary of each sub-corpus, second stage clustering, path creation among clusters of sub-corpuses, text processing by forward and backward propagation for results generation. The outcome of this technique consists of interlinked sub-corpuses through clusters. We have applied our approach to a News dataset, and this interlinked corpus processing follows step by step clustering to search the most relevant parts of the corpus with less cost, time, and improve content detection. We have applied six different metadata processing combinations over multiple text queries to compare results during our experimentation. The comparison results of text satisfaction show that Page-Rank keywords give 38% related text, single-stage Clustering gives 46%, two-stage Clustering gives 54%, and the proposed technique gives 67% associated text. Furthermore, this approach covers/searches the relevant data with a range of most to less relevant content. It provides the systematic query-relevant corpus processing mechanism, which automatically selects the most relevant sub-corpus through dynamic path selection. We used the SHAP model to evaluate the proposed technique, and our evaluation results proved that the proposed mechanism improved text processing. Moreover, combining text summarization features, shown satisfactory results compared to the summaries generated by general models of abstractive & extractive summarization.", 
Diagnosis codes are extracted from medical records for billing and reimbursement and for secondary uses. ICD-9 codes are generally extracted by trained human coders by reading all artifacts available in a patient's medical record following specific coding guidelines. This paper proposes an unsupervised ensemble approach to automatically extract diagnosis codes from EMRs.,Unsupervised Extraction of Diagnosis Codes from EMRs Using Knowledge-Based and Extractive Text Summarization Techniques,"Diagnosis codes are extracted from medical records for billing and reimbursement and for secondary uses such as quality control and cohort identification. In the US, these codes come from the standard terminology ICD-9-CM derived from the international classification of diseases (ICD). ICD-9 codes are generally extracted by trained human coders by reading all artifacts available in a patient’s medical record following specific coding guidelines. To assist coders in this manual process, this paper proposes an unsupervised ensemble approach to automatically extract ICD-9 diagnosis codes from textual narratives included in electronic medical records (EMRs). Earlier attempts on automatic extraction focused on individual documents such as radiology reports and discharge summaries. Here we use a more realistic dataset and extract ICD-9 codes from EMRs of 1000 inpatient visits at the University of Kentucky Medical Center. Using named entity recognition (NER), graph-based concept-mapping of medical concepts, and extractive text summarization techniques, we achieve an example based average recall of 0.42 with average precision 0.47; compared with a baseline of using only NER, we notice a 12% improvement in recall with the graph-based approach and a 7% improvement in precision using the extractive text summarization approach. Although diagnosis codes are complex concepts often expressed in text with significant long range non-local dependencies, our present work shows the potential of unsupervised methods in extracting a portion of codes. As such, our findings are especially relevant for code extraction tasks where obtaining large amounts of training data is difficult.",  
"The recent rapid success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework.",Unsupervised Joint Mining of Deep Features and Image Labels for Large-scale Radiology Image Categorization and Scene Recognition,"The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of “Google Search” and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for joint mining of deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized “convergence” of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75.3%, compared to the state-of-the-art supervised classification accuracy of 81.0% (when both are based on the VGG-VD model) .", 
This work aims to address the flexibility challenge of the Dirichlet distribution by introducing a distribution that adds to it a scale parameter. There is growing need for models that can fully describe the intrinsic nature of datasets. The proposed algorithm makes it possible to discover fault prone modules by harnessing their complexity-based attributes.,Unsupervised Learning of Finite Mixtures using Scaled Dirichlet Distribution and its Application to Software Modules categorization,"We have designed and implemented an unsupervised learning algorithm for finite mixture model using the scaled Dirichlet distribution for multivariate proportional data. In this paper, the task of learning finite mixture model involves estimation of model parameters as well as inferring the hidden class information of our observed data. We made use of the expectation maximization algorithm to find the maximum likelihood estimate of our model parameters. This work, aims to address the flexibility challenge of the Dirichlet distribution by introducing a distribution that adds to it a scale parameter. This is important, because there is growing need for models that can fully describe the intrinsic nature of datasets. In addition, we applied our learning algorithm to synthetic datasets as well as to address the challenge of detecting fault prone software modules. Our proposed algorithm, makes it possible to discover these fault prone modules by harnessing their complexity-based attribute information. Finally, we compare our proposed model classification results with those from the Gaussian and Dirichlet mixture models.", 
We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization. Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality. The benefit of sentence enhancement relies crucially on an event coreference resolution algorithm,Unsupervised Sentence Enhancement for Automatic Summarization,"We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization. Compared to extraction or previous approaches to sentence fusion, sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text. Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality, but better in terms of grammaticality, and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics. We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences.", 
We propose a method to perform unsupervised extractive and abstractive text summarization using sentence embeddings. We compare multiple variants of our systems,Unsupervised Text Summarization Using Sentence Embeddings,"Dense vector representations of words, and more recently, sentences, have been shown to improve performance in a number of NLP tasks. We propose a method to perform unsupervised extractive and abstractive text summarization using sentence embeddings. We compare multiple variants of our systems on two datasets, show substantially improved performance over a simple baseline, and performance approaching a competitive baseline.", 
This paper deals with our recent research in text summarization. We describe the development of our summarizer which is based on latent semantic analysis. The update summarization component determines the redundancy,Update Summarization Based on Latent Semantic Analysis,This paper deals with our recent research in text summarization. We went from single-document summarization through multi-document summarization to update summarization. We describe the development of our summarizer which is based on latent semantic analysis (LSA) and propose the update summarization component which determines the redundancy and novelty of each topic discovered by LSA. The final part of this paper presents the results of our participation in the experiment of Text Analysis Conference 2008., 
The field has moved from multi-document summarization to update summarization. The summarizer assumes prior knowledge of the reader determined by a set of older documents of the same topic. The update summarizer must solve a novelty vs. redundancy problem.,Update Summarization Based on Novel Topic Distribution,"This paper deals with our recent research in text summarization. The field has moved from multi-document summarization to update summarization. When producing an update summary of a set of topic-related documents the summarizer assumes prior knowledge of the reader determined by a set of older documents of the same topic. The update summarizer thus must solve a novelty vs. redundancy problem. We describe the development of our summarizer which is based on Iterative Residual Rescaling (IRR) that creates the latent semantic space of a set of documents under consideration. IRR generalizes Singular Value Decomposition (SVD) and enables to control the influence of major and minor topics in the latent space. Our sentence-extractive summarization method computes the redundancy, novelty and significance of each topic. These values are finally used in the sentence selection process. The sentence selection component prevents inner summary redundancy. The results of our participation in TAC evaluation seem to be promising", 
Epileptic seizure occurs due to neuronal disorder that results in abnormal pattern of brain signal. Electroencephalogram (EEG) signal represents modest measure of electric flow in a human brain. Approximately 1% of the total population in the world is affected by epilepsy.,Usage of Deep Learning in Epileptic Seizure Detection Through EEG Signal,"Epileptic seizure occurs due to neuronal disorder that results in abnormal pattern of brain signal. Electroencephalogram (EEG) signal represents a modest measure of electric flow in a human brain. An EEG is one of the main diagnostic tests for epilepsy. Due to the presence of seizures, normal pattern of brain waves disappears and different other brain waves can be visualized during the recording of EEG. Approximately, 1% of the total population in the world is affected by this disease. This paper is based on a systematic approach for epilepsy detection of human brain by extraction of features and classification of EEG signal. Feature extraction is completed by discrete wavelet transform (DWT) and multilayer perceptron neural network (MLPNN) with deep learning is used for classification. Experimental study of the proposed work is done through Python platform with an encouraging performance", 
"Text Summarization produces a shorter version of large text documents by selecting most relevant information. In extractive text summarization, important sentences are selected based on certain important features. The importance of some extractive features is more than the some other features, so they should have the balance weight in computations.",Use of fuzzy logic and wordnet for improving performance of extractive automatic text summarization,"Text Summarization produces a shorter version of large text documents by selecting most relevant information. Text summarization systems are of two types: extractive and abstractive. This paper focuses on extractive text summarization. In extractive text summarization, important sentences are selected based on certain important features. The importance of some extractive features is more than the some other features, so they should have the balance weight in computations. The purpose of this paper is to use fuzzy logic and wordnet synonyms to handle the issue of ambiguity and imprecise values with the traditional two value or multi-value logic and to consider the semantics of the text. Three different methods: fuzzy logic based method, bushy path method, and wordnet synonyms method are used to generate 3 summaries. Final summary is generated by selecting common sentences from all the 3 summaries and from rest of the sentences in union of all summaries, selection is done based on sentence location. The proposed methodology is compared with three individual methods i.e. fuzzy logic based summarizer, bushy path summarizer, and wordnet synonyms summarizer by evaluating the performance of each on 95 documents from standard DUC 2002 dataset using ROUGE evaluation metrics. The analysis shows that the proposed method gives better average precision, recall, and f-measure", 
Text summarization method is based on the use of the graphs as graphic organizers. Experiment demonstrated that the tool helped students reflect about the main ideas of the text.,Using a Text Mining Tool to Support Text Summarization,"This paper presents a mining tool that is able to extract graphs from texts, and proposes their use in helping students to write summaries. The text summarization method is based on the use of the graphs as graphic organizers, leading students to further reflect about the main ideas of the text before getting to the actual task of writing. An experiment carried out demonstrated that the tool helped students reflect about the main ideas of the text and supported the writing of the summaries.", 
"This paper aims at raising awareness on the issue of using unfixed vulnerabilities for targeted attacks. We demonstrate an attack by using a well-known, yet not fixed whatsapp vulnerability. Once the victim trusts the adversary, social phishing can be used to retrieve further private or even corporate information.",Using a whatsapp vulnerability for profiling individuals,"This paper aims at raising awareness on the issue of using unfixed vulnerabilities for targeted attacks in order to harness private or even corporate information. We demonstrate an attack by using a well-known, yet not fixed whatsapp vulnerability, enabling us to eavesdrop the cell-phone number of a victim. We identified the concrete states, in which whatsapp leaks the cell-phone number of a victim. By using a volunteering individual, we demonstrate the feasibility of profiling the individual and provide further steps on how to disclose private and corporate information by using the leaked cell-phone number and the profiled information to introduce the adversary into a trust relationship with the victim. Once the victim trusts the adversary, social phishing can be used to retrieve further private or even corporate information.", 
In this paper we describe a modified classification method destined for extractive summarization purpose. The classification in this method doesn't need a learning corpus; it uses the input text to do that.,Using clustering and a modified classification algorithm for automatic text summarization,"In this paper we describe a modified classification method destined for extractive summarization purpose. The classification in this method doesn’t need a learning corpus; it uses the input text to do that. First, we cluster the document sentences to exploit the diversity of topics, then we use a learning algorithm (here we used Naive Bayes) on each cluster considering it as a class. After obtaining the classification model, we calculate the score of a sentence in each class, using a scoring model derived from classification algorithm. These scores are used, then, to reorder the sentences and extract the first ones as the output summary.", 
MMR (Maximum Marginal Relevance) is widely used in summarization for its simplicity and efficacy. How to appropriately represent the similarity of two text segments is crucial in MMR. We evaluate different similarity measures in the MMR framework for meeting summarization on the ICSI meeting corpus.,Using corpus and knowledge-based similarity measure in Maximum Marginal Relevance for meeting summarization,"MMR (Maximum Marginal Relevance) is widely used in summarization for its simplicity and efficacy, and has been demonstrated to achieve comparable performance to other approaches for meeting summarization. How to appropriately represent the similarity of two text segments is crucial in MMR. In this paper, we evaluate different similarity measures in the MMR framework for meeting summarization on the ICSI meeting corpus. We introduce a corpusbased measure to capture the similarity at the semantic level, and compare this method with cosine similarity and centroid score that only considers the salient words in the segments. Our experimental results evaluated by the ROUGE summarization metrics show that both the centroid score and the corpus-based similarity measure yield better performance than the commonly used cosine similarity. In addition, adding part-of-speech information in the corpus-based approach helps for the human transcripts condition, but not when using ASR output.", 
Heart disease is the leading cause of death in the world over the past 10 years. Researchers have been using data mining techniques to help health care professionals in the diagnosis of heart disease. This research investigates applying a range of techniques to different types of Decision Trees seeking better performance in heart disease diagnosis.,Using Decision Tree for Diagnosing Heart Disease Patients,"Heart disease is the leading cause of death in the world over the past 10 years. Researchers have been using several data mining techniques to help health care professionals in the diagnosis of heart disease. Decision Tree is one of the successful data mining techniques used. However, most research has applied J4.8 Decision Tree, based on Gain Ratio and binary discretization. Gini Index and Information Gain are two other successful types of Decision Trees that are less used in the diagnosis of heart disease. Also other discretization techniques, voting method, and reduced error pruning are known to produce more accurate Decision Trees. This research investigates applying a range of techniques to different types of Decision Trees seeking better performance in heart disease diagnosis. A widely used benchmark data set is used in this research. To evaluate the performance of the alternative Decision Trees the sensitivity, specificity, and accuracy are calculated. The research proposes a model that outperforms J4.8 Decision Tree and Bagging algorithm in the diagnosis of heart disease patients.", 
Deep learning may be used to solve the intractable problem of categorizing rocks. We train an ensemble of convolutional neural networks to produce multidimensional scaling (MDS) coordinates of images of rocks.,Using Deep-Learning Representations of Complex Natural Stimuli as Input to Psychological Models of Classification,"Tests of formal models of human categorization have traditionally been restricted to artificial categories because deriving psychological representations for large numbers of natural stimuli has been an intractable task. We show that deep learning may be used to solve this problem. We train an ensemble of convolutional neural networks (CNNs) to produce the multidimensional scaling (MDS) coordinates of images of rocks. We then show that not only are the CNNs able to predict the MDS coordinates of a held-out test set of rocks, but that the CNN-derived representations can be used in combination with a formal psychological model to predict human categorization behavior on a completely new set of rocks.", 
Medical Literatures on the web are important sources to help clinicians in patient-care. All medical articles do not come with author written abstracts or summaries. Automatic summarization of medical articles will help clinicians or medical students to find the relevant information.,Using domain knowledge for text summarization in medical domain,"Medical Literatures on the web are the important sources to help clinicians in patient-care. Initially, the clinicians go through the author-written abstracts or summaries available with the medical articles to decide whether articles are suitable for in-depth study. Since all medical articles do not come with author written abstracts or summaries, automatic summarization of medical articles will help clinicians or medical students to find the relevant information on the web rapidly. In this paper we discuss a summarization method, which combines several domain specific features with some other known features such as term frequency, title and position to improve the summarization performance in the medical domain. Our experiments show that the incorporation of domain specific features improves the summarization performance.", 
This work uses a genetic algorithm for automatic recurrentANN development. It has been applied to solve a well-known problem: classification of EEG signals from epileptic patients.,Using genetic algorithms for automatic recurrent ANN development an application to EEG signal classification,"ANNs are one of the most successful learning systems. For this reason, many techniques have been published that allow the obtaining of feed-forward networks. However, few works describe techniques for developing recurrent networks. This work uses a genetic algorithm for automatic recurrent ANN development. This system has been applied to solve a well-known problem: classification of EEG signals from epileptic patients. Results show the high performance of this system, and its ability to develop simple networks, with a low number of neurons and connections", 
"Automatic text summarization attempts to address this problem by taking an input text and extracting the most important content of it. The determination of the salience of information in the text depends on different factors. In this study, we combine these two approaches of summarization.",USING GENETIC ALGORITHMS WITH LEXICAL CHAINS FOR AUTOMATIC TEXT SUMMARIZATION,"With the rapid increase in the amount of online text information, it became more important to have tools that would help users distinguish the important content. Automatic text summarization attempts to address this problem by taking an input text and extracting the most important content of it. However, the determination of the salience of information in the text depends on different factors and remains as a key problem of automatic text summarization. In the literature, there are some studies that use lexical chains as an indicator of lexical cohesion in the text and as an intermediate representation for text summarization. Also, some studies make use of genetic algorithms in order to examine some manually generated summaries and learn the patterns in the text which lead to the summaries by identifying relevant features which are most correlated with human generated summaries. In this study, we combine these two approaches of summarization. Firstly, lexical chains are computed to exploit the lexical cohesion that exists in the text. Then, this deep level of knowledge about the text is combined with other higher level analysis results. Finally, all these results that give different levels of knowledge about the text are combined using genetic algorithms to obtain a general understanding.", 
"Keywords can be considered as condensed versions of documents and short forms of their summaries. A lexical chain holds a set of semantically related words of a text. In this paper, a keyword extraction technique that uses lexical chains is",Using lexical chains for keyword extraction,"Keywords can be considered as condensed versions of documents and short forms of their summaries. In this paper, the problem of automatic extraction of keywords from documents is treated as a supervised learning task. A lexical chain holds a set of semantically related words of a text and it can be said that a lexical chain represents the semantic content of a portion of the text. Although lexical chains have been extensively used in text summarization, their usage for keyword extraction problem has not been fully investigated. In this paper, a keyword extraction technique that uses lexical chains is described, and encouraging results are obtained", 
Automatic Chinese text summarization for dialogue style is a relatively new research area. The approach is highly efficient and improves significantly the coherence of the summary while not compromising informativeness.,Using LSA and text segmentation to improve automatic Chinese dialogue text summarization,"Automatic Chinese text summarization for dialogue style is a relatively new research area. In this paper, Latent Semantic Analysis (LSA) is first used to extract semantic knowledge from a given document, all question paragraphs are identified, an automatic text segmentation approach analogous to TextTiling is exploited to improve the precision of correlating question paragraphs and answer paragraphs, and finally some “important” sentences are extracted from the generic content and the question-answer pairs to generate a complete summary. Experimental results showed that our approach is highly efficient and improves significantly the coherence of the summary while not compromising informativeness.", 
"An automatic medical text summarization system can facilitate rapid medical information access on the web. We approach the problem of automatically generating summary from medical article as a supervised learning task. We treat a document as a set of sentences, which the learning algorithm must learn to classify as positive or negative examples of sentences based on summary worthiness.",Using Machine Learning for Medical Document Summarization,"Summaries or abstracts available with medical articles are useful for the physicians, medical students and patients to know rapidly what is the article about and decide whether articles are suitable for in-depth study. Since all medical text documents do not come with author written abstracts or summaries, an automatic medical text summarization system can facilitate rapid medical information access on the web. We approach the problem of automatically generating summary from medical article as a supervised learning task. We treat a document as a set of sentences, which the learning algorithm must learn to classify as positive or negative examples of sentences based on summary worthiness of the sentences. We apply the machine learning algorithm called bagging to this learning task, where a C4.5 decision tree has been chosen as the base learner. We also compare the proposed approach to some existing summarization approaches.", 
"In this paper, we explore the contribution of various supervised learning algorithms to the sentence ranking task. We introduce a novel sentence ranking methodology based on the similarity score between a candidate sentence and benchmark summaries. The popular linear regression model achieved the best results in all evaluated datasets.",Using Machine Learning Methods and Linguistic Features in Single-Document Extractive Summarization,"Extractive summarization of text documents usually consists of ranking the document sentences and extracting the top-ranked sentences subject to the summary length constraints. In this paper, we explore the contribution of various supervised learning algorithms to the sentence ranking task. For this purpose, we introduce a novel sentence ranking methodology based on the similarity score between a candidate sentence and benchmark summaries. Our experiments are performed on three benchmark summarization corpora: DUC-2002, DUC2007 and MultiLing-2013. The popular linear regression model achieved the best results in all evaluated datasets. Additionally, the linear regression model, which included POS (Part-of-Speech)-based features, outperformed the one with statistical features only.", 
"A major drawback of current event detection methods is that parameters have to be adjusted based on eye movement data quality. We explore the application of random forest machinelearning technique for the detection of fixations, saccades, and post-saccadic oscillations. We conclude that machine-learning techniques lead to superior detection.",Using machine learning to detect events in eye-tracking data,"Event detection is a challenging stage in eye movement data analysis. A major drawback of current event detection methods is that parameters have to be adjusted based on eye movement data quality. Here we show that a fully automated classification of raw gaze samples as belonging to fixations, saccades, or other oculomotor events can be achieved using a machine-learning approach. Any already manually or algorithmically detected events can be used to train a classifier to produce similar classification of other data without the need for a user to set parameters. In this study, we explore the application of random forest machinelearning technique for the detection of fixations, saccades, and post-saccadic oscillations (PSOs). In an effort to show practical utility of the proposed method to the applications that employ eye movement classification algorithms, we provide an example where the method is employed in an eye movement-driven biometric application. We conclude that machine-learning techniques lead to superior detection", 
Proposed sentence relevance estimation is based on normalization of NMF topic space and further weighting of each topic using sentences representation in topic space. The proposed method shows better summarization quality and performance than state of the art methods on DUC 2002 standard dataset.,Using NMF-based Text Summarization to Improve Supervised and Unsupervised Classification,"This paper presents a new generic text summarization method using Non-negative Matrix Factorization (NMF) to estimate sentence relevance. Proposed sentence relevance estimation is based on normalization of NMF topic space and further weighting of each topic using sentences representation in topic space. The proposed method shows better summarization quality and performance than state of the art methods on DUC 2002 standard dataset. In addition, we study how this method can improve the performance of supervised and unsupervised text classification tasks. In our experiments with Reuters-21578 and Classic4 benchmark datasets we apply developed text summarization method as a preprocessing step for further multi-label classification and clustering. As a result, the quality of classification and clustering has been significantly improved.", 
"This paper presents a novel query expansion method, which is combined in the graphbased algorithm for query-focused multi-document summarization. Our approach makes use of both the sentence-to-sentence relations and the sentences to select the query biased informative words from the document set and use them as query expansions.",Using query expansion in graph-based approach for query-focused multi-document summarization,"This paper presents a novel query expansion method, which is combined in the graphbased algorithm for query-focused multi-document summarization, so as to resolve the problem of information limit in the original query. Our approach makes use of both the sentence-to-sentence relations and the sentence-to-word relations to select the query biased informative words from the document set and use them as query expansions to improve the sentence ranking result. Compared to previous query expansion approaches, our approach can capture more relevant information with less noise. We performed experiments on the data of document understanding conference (DUC) 2005 and DUC 2006, and the evaluation results show that the proposed query expansion method can significantly improve the system performance and make our system comparable to the state-of-theart systems.", 
"Deep learning models adapted to leverage temporal relations appear to improve performance of models for detection of incident heart failure with a short observation window of 12–18 months. Data were from a health system's EHR on 3884 incident HF cases and 28 903 controls, identified as primary care patients. When using an 18-month observation window, the AUC for the RNN model increased to 0.883 and was significantly higher than the 0.834 for the best of the baseline methods.",Using recurrent neural network models for early detection of heart failure,"We explored whether use of deep learning to model temporal relations among events in electronic health records (EHRs) would improve model performance in predicting initial diagnosis of heart failure (HF) compared to conventional methods that ignore temporality. Data were from a health system’s EHR on 3884 incident HF cases and 28 903 controls, identified as primary care patients, between May 16, 2000, and May 23, 2013. Recurrent neural network (RNN) models using gated recurrent units (GRUs) were adapted to detect relations among time-stamped events (eg, disease diagnosis, medication orders, procedure orders, etc.) with a 12- to 18-month observation window of cases and controls. Model performance metrics were compared to regularized logistic regression, neural network, support vector machine, and K-nearest neighbor classifier approaches. Using a 12-month observation window, the area under the curve (AUC) for the RNN model was 0.777, compared to AUCs for logistic regression (0.747), multilayer perceptron (MLP) with 1 hidden layer (0.765), support vector machine (SVM) (0.743), and K-nearest neighbor (KNN) (0.730). When using an 18-month observation window, the AUC for the RNN model increased to 0.883 and was significantly higher than the 0.834 AUC for the best of the baseline methods (MLP). Deep learning models adapted to leverage temporal relations appear to improve performance of models for detection of incident heart failure with a short observation window of 12–18 months.", 
"This paper presents a semantic graph-based method for extractive summarization. The method has proven to be an efficient approach to the identification of salient concepts and topics in free text. The system can be easily ported to other domains, as it only requires modifying the knowledge base.",Using semantic graphs and word sense disambiguation techniques to improve text summarization,"This paper presents a semantic graph-based method for extractive summarization. The summarizer uses WordNet concepts and relations to produce a semantic graph that represents the document, and a degree-based clustering algorithm is used to discover different themes or topics within the text. The selection of sentences for the summary is based on the presence in them of the most representative concepts for each topic. The method has proven to be an efficient approach to the identification of salient concepts and topics in free text. In a test on the DUC data for single document summarization, our system achieves significantly better results than previous approaches based on terms and mere syntactic information. Besides, the system can be easily ported to other domains, as it only requires modifying the knowledge base and the method for concept annotation. In addition, we address the problem of word ambiguity in semantic approaches to automatic summarization", 
Arabic text summarization is faced with two main issues: how to extract semantic rela? tionships between textual units and deal with redundancy. Authors propose a hybrid method to generate an extractive summary of Arabic documents. Their approach is based on a two-dimensional undirected and weighted graph with sentences as nodes.,Using Statistical and Semantic Analysis for Arabic Text Summarization,"Automatic text summarization is an essential tool to overcome the problem of information overload. So far this field has not been studied enough for Arabic language and currently only few related works are available. Arabic text summarization is faced with two main issues: how to extract semantic rela? tionships between textual units and deal with redundancy. To overcome these problems, we propose in this paper a hybrid method to generate an extractive summary of Arabic documents. Our approach is based on a two-dimensional undirected and weighted graph with sentences as nodes and each pair of sentences are connected by two edges representing the statistical and semantic similarity measure. The statistical similarity measure builds on the content overlap between two sentences, while the semantic one is based upon semantic information extracted from Arabic WordNet (AWN) ontology. Then, the score of each sentence is computed by performing the ranking algorithm PageRank on the generated graph. Thereafter, the score of each sentence is performed by adding other statistical features of the text such as TF.ISF and sentence position. The final summary is built by selecting the top-ranking sentences. Finally, we deal with redundancy and information diversity issues by using an adapted maximal marginal relevance (MMR) method. Experimental results on EASC dataset show that our proposed approach outperforms some of existing Arabic summarization systems", 
"In this paper, we propose a bigram based supervised method for extractive document summarization. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.",Using Supervised Bigram-based ILP for Extractive Summarization,"In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.", 
"Electroencephalography (EEG) signal classification is an increasingly interesting task in Brain-computer interface systems. In this paper, we combine convolutional neural network and Long Short-Term Memory with center loss function to classify EEG signals. We have achieved a classification accuracy of about 94% on cognitive load recognition and 86.52% on emotion recognition.",Using the Center Loss Function to Improve Deep Learning Performance for EEG Signal Classification,"Electroencephalography (EEG) signal classification is an increasingly interesting task in Brain-computer interface (BCI) systems, but how to learn the pattern from EEG signals and design a general model for EEG classification is still a challenge. In this paper, we combine convolutional neural network and Long Short-Term Memory (CNN+LSTM) with center loss function to classify EEG signals. The 1 × 3 kernels as convolutional filters in 2D CNNs will be used, which is different from other models. Deep metric learning is usually used for image classification such as face recognition. In this paper, we use center loss function to learn more discriminative features to improve classification accuracy. Meanwhile, we experiment our methods on multiple EEG datasets. We have achieved a classification accuracy of about 94% on cognitive load recognition and 86.52% on emotion recognition. On EEG dataset for visual object analysis, our method produces a comparable result compared with who publicly release this dataset. The good performance indicates that our model with the center loss function is widely suitable on EEG signal classification.", 
"Medical applications require high accuracy and ease-of-interpretation in text categorization. The Tsetlin Machine leverages the recently introduced T setlin Machine to address this challenge. Using relatively simple propositional formulae, the accuracy of the Machine either outperforms or performs approximately on par with the best evaluated methods. The main merit of the proposed approach is its capacity for producing human-interpretable rules.",Using the Tsetlin Machine to Learn Human-Interpretable Rules for High-Accuracy Text Categorization With Medical Applications,"Medical applications challenge today’s text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap forward in regard to accuracy, this leap comes at the sacrifice of interpretability. In this paper, we introduce a text categorization approach that leverages the recently introduced Tsetlin Machine to address this accuracy-interpretability challenge. Briefly, we represent the terms of a text as propositional variables. From these variables, we capture categories using simple propositional formulae, such as: IF ‘‘rash’’ AND ‘‘reaction’’ AND ‘‘penicillin’’ THEN Allergy. The Tsetlin Machine learns these formulae from labeled text, utilizing conjunctive clauses to represent the particular facets of each category. Therefore, also the absence of terms (negated features) can be used for categorization purposes. Our empirical comparisons with Naïve Bayes classifiers, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, are quite conclusive. Using relatively simple propositional formulae, the accuracy of the Tsetlin Machine either outperforms or performs approximately on par with the best evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a clinical dataset containing authentic electronic health records (EHRs). On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. The main merit of the proposed approach is thus its capacity for producing human-interpretable rules, while at the same time achieving acceptable accuracy. We believe that our novel approach can have a significant impact on a wide range of text analysis applications, providing a promising starting point for deeper natural language understanding with the Tsetlin Machine.", 
Traditional Arabic text summarization systems are based on bag-of-words representation. dimensionality reduction is greatly needed to increase the power of features discrimination. We present a new method for ATS using a Variational auto-encoder (VAE) model to learn a feature space.,Using Unsupervised Deep Learning for Automatic Summarization of Arabic Documents,"Traditional Arabic text summarization (ATS) systems are based on bag-of-words representation, which involve a sparse and high-dimensional input data. Thus, dimensionality reduction is greatly needed to increase the power of features discrimination. In this paper, we present a new method for ATS using variational auto-encoder (VAE) model to learn a feature space from a high-dimensional input data. We explore several input representations such as term frequency (tf), tf-idf and both local and global vocabularies. All sentences are ranked according to the latent representation produced by the VAE. We investigate the impact of using VAE with two summarization approaches, graph-based and query-based approaches. Experiments on two benchmark datasets specifically designed for ATS show that the VAE using tf-idf representation of global vocabularies clearly provides a more discriminative feature space and improves the recall of other models. Experiment results confirm that the proposed method leads to better performance than most of the state-of-the-art extractive summarization approaches for both graph-based and query-based summarization approaches.", 
28-bit codes can be used to retrieve images that are similar to a query image in a time independent of the size of the database. 256-bit binary codes allow much more accurate matching.,Using Very Deep Autoencoders for Content-Based Image Retrieval,"We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple dierent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.", 
"In this paper we investigate detection of high-level concepts in multimedia content. We use an integrated approach of visual thesaurus analysis and visual context exploitation. A set of algorithms is presented, which modify either the confidence values of detected concepts or the model vectors.",Using Visual Context and Region Semantics for High-Level Concept Detection,"In this paper we investigate detection of high-level concepts in multimedia content through an integrated approach of visual thesaurus analysis and visual context. In the former, detection is based on model vectors that represent image composition in terms of region types, obtained through clustering over a large data set. The latter deals with two aspects, namely high-level concepts and region types of the thesaurus, employing a model of a priori specified semantic relations among concepts and automatically extracted topological relations among region types; thus it combines both conceptual and topological context. A set of algorithms is presented, which modify either the confidence values of detected concepts, or the model vectors based on which detection is performed. Visual context exploitation is evaluated on TRECVID and Corel data sets and compared to a number of related visual thesaurus approaches.", 
"Traditional approaches for extractive summarization score/classify sentences based on features such as position in the text. These features tend to produce satisfactory summaries, but have the inconvenience of being domain dependent",Using Word Sequences for Text Summarization,"Traditional approaches for extractive summarization score/classify sentences based on features such as position in the text, word frequency and cue phrases. These features tend to produce satisfactory summaries, but have the inconvenience of being domain dependent. In this paper, we propose to tackle this problem representing the sentences by word sequences (n-grams), a widely used representation in text categorization. The experiments demonstrated that this simple representation not only diminishes the domain and language dependency but also enhances the summarization performance", 
"V2X technology is intended to enable the communication of individual vehicles with one another and supporting road infrastructure. The topic has drawn interest from a large number of stakeholders, from governmental authorities to automotive manufacturers and mobile network operators. We hope to provide a useful reference for the state of V2X research and development for newcomers and veterans alike.","V2X Access Technologies Regulation, Research, and Remaining Challenges","As we edge closer to the broad implementation of intelligent transportation systems, the need to extend the perceptual bounds of sensor-equipped vehicles beyond the individual vehicle is more pressing than ever. Research and standardization efforts toward V2X, or vehicle to everything, technology is intended to enable the communication of individual vehicles with both one another and supporting road infrastructure. The topic has drawn interest from a large number of stakeholders, from governmental authorities to automotive manufacturers and mobile network operators. With interest sourced from many disparate parties and a wealth of research on a large number of topics, trying to grasp the bigger picture of V2X development can be a daunting task. In this tutorial survey, to the best of our knowledge, we collate research across a number of topics in V2X, from historical developments to standardization activities and a high-level view of research in a number of important fields. In so doing we hope to provide a useful reference for the state of V2X research and development for newcomers and veterans alike.", 
Intelligent transportation systems (ITS) provide a set of standards for vehicular communications. V2V and V2I communications are the main research goals of ITS. This paper reviews some popular architectures of VANETs (Vehicular Ad hoc NETworks),VANET Architectures and Protocol Stacks A Survey,"Intelligent transportation systems (ITS) provide a set of standards for vehicular communications. The main focus of research activities, within ITS, has been on development of safety, traffic efficiency and infotainment related applications. Vehicle to Vehicle (V2V) and Vehicle to Infrastructure (V2I) communications are the main research goals of ITS. This paper reviews some popular architectures of VANETs (Vehicular Ad hoc NETworks) i.e., WAVE by IEEE, CALIM by ISO, C2CNet by C2C consortium / GeoNet. It also includes some recent research regarding these standards, specially focusing on Network and MAC layer issues. This paper also discusses safety related application protocols, i.e. WSMP by WAVE, CALM FAST by ISO and C2CNet by C2C consortium. Various recommendations regarding the above protocol stacks are presented. The recommendations are based on different parameters like flexibility, implementation etc.", 
"Vehicular Ad Hoc Networks (VANETs) are networks that deal with transferring data between moving vehicles. Like all other networks it is subjects to vulnerable attacks, hence, security is a hot topic to consider. Different attacks could take place within the communication scenario; the most harmful","VANET SECURITY DEFENSE AND DETECTION, A REVIEW","Vehicular Ad Hoc Networks (VANETs) are networks that deal with transferring data between moving vehicles in order to avoid accidents and to provide journey comfort and traffic safety. Like all other networks it is subjects to vulnerable attacks, hence, security is a hot topic to consider. This article provides a review on the researches and publications focusing on how to secure the communication between vehicles while transferring the data. Different attacks could take place within the communication scenario; the most harmful of them is Sybil attack. Therefore, in this paper, we shed lights on the researches dealing with the different types of attacks with a focus on Sybil attacks. Sybil detection and defense techniques and methodologies are reviewed in more details.", 
"Smart vehicles and RSUs communicate through unsafe wireless media. By nature, they are vulnerable to threats that can lead to lifethreatening circumstances. Security measures are needed to recognize these VANET assaults.",VANET Towards Security Issues Review,"The Ad-hoc vehicle networks (VANETs) recently stressed communications and networking technologies. VANETs vary from MANETs in tasks, obstacles, system architecture and operation. Smart vehicles and RSUs communicate through unsafe wireless media. By nature, they are vulnerable to threats that can lead to lifethreatening circumstances. Due to potentially bad impacts, security measures are needed to recognize these VANET assaults. In this review paper of VANET security, the new VANET approaches are summarized by addressing security complexities. Second, we're reviewing these possible threats and literature recognition mechanisms. Finally, the attacks and their effects are identified and clarified and the responses addressed together.", 
"In the conventional sequence-to-sequence model for abstractive summarization, the internal transformation structure of recurrent neural networks (RNNs) is completely determined. We propose a Variational neural decoder text summarization model (VND) The model structure can better capture the complex semantics and strong dependence between the adjacent time steps when outputting the summary.",Variational neural decoder for abstractive text summarization,"In the conventional sequence-to-sequence (seq2seq) model for abstractive summarization, the internal transformation structure of recurrent neural networks (RNNs) is completely determined. Therefore, the learned semantic information is far from enough to represent all semantic details and context dependencies, resulting in a redundant summary and poor consistency. In this paper, we propose a variational neural decoder text summarization model (VND). The model introduces a series of implicit variables by combining variational RNN and variational autoencoder, which is used to capture complex semantic representation at each step of decoding. It includes a standard RNN layer and a variational RNN layer [5]. These two network layers respectively generate a deterministic hidden state and a random hidden state. We use these two RNN layers to establish the dependence between implicit variables between adjacent time steps. In this way, the model structure can better capture the complex semantics and the strong dependence between the adjacent time steps when outputting the summary, thereby improving the performance of generating the summary. The experimental results show that, on the text summary LCSTS and English Gigaword dataset, our model has a significant improvement over the baseline model.", 
This article presents new alternatives to the similarity function for the TextRank algorithm. We describe the generalities of the,Variations of the similarity function of textrank for automated summarization,This article presents new alternatives to the similarity function for the TextRank algorithm for automated summarization of texts. We describe the generalities of the algorithm and the different functions we propose. Some of these variants achieve a significative improvement using the same metrics and dataset as the original publication., 
"In CoMoSeF –project we are simplifying the In Vehicle Domain architecture by fusing the Human-machine interface (HMI), as specified by ETSI, and the Application Unit into one Application Unit unit that is running on a smartphone. Both vehicle and roadside ITS stations communicate with other stations using standardized Distributed Environmental Notification Messages, DENMs.",Vehicle ITS station for C2X communication,"This paper suggests a new schema for combining functionalities from the Personal ITS station and Vehicle ITS station. In CoMoSeF –project we are simplifying the In Vehicle Domain architecture by fusing the Human-machine interface (HMI), as specified by ETSI, and the Application Unit into one Application Unit unit that is running on the smartphone. In the Smartphone-Centric Service Model the Application Unit and the HMI reside in a nomadic device and the connection to the Communication and Control Unit (CCU) is done utilizing WiFi. Both vehicle and roadside ITS stations communicate with other stations using standardized Distributed Environmental Notification Messages, DENMs. Those messages are received and sent via the CCU over IEEE802.11p. Messages are used to provide information about a specific driving environment, incident or some other traffic event to other stations. In CoMoSeF-project a basic In-vehicle signage (IVS) application implementation will be released. Also the Probe vehicle or road side unit (RSU) data use case can be easily implemented with this architecture. As an example of the implementation, we introduce a road weather measuring RSU station. The CoMoSeF vehicle ITS station provides a proof-of– concept of a Smartphone-centric service model, which is the first step forward to aftermarket V2X solutions during the transitional phase when VANET systems are not widely available yet.", 
"Vehicle-to-everything (V2X) improves road safety, traffic efficiency, and the availability of infotainment services. Standardization of Long Term Evolution (LTE)-based V2X has been actively conducted by the Third Generation Partnership Project (3GPP) In this article, the overview of requirements and use cases in V2x services in 3GPP is presented.",Vehicle-to-Everything (v2x) Services Supported by LTE-based Systems and 5g,"Vehicle-to-everything (V2X), including vehicle-to-vehicle (V2V), vehicle-to-pedestrian (V2P), vehicle-to-infrastructure (V2I), and vehicle-to-network (V2N) communications, improves road safety, traffic efficiency, and the availability of infotainment services. Standardization of Long Term Evolution (LTE)-based V2X has been actively conducted by the Third Generation Partnership Project (3GPP) to provide solutions for V2X communications, and has benefited from the global deployment and fast commercialization of LTE systems. LTE-based V2X was widely used as LTE-V in the Chinese vehicular communications industry, and LTE-based V2X was redefined as LTE V2X in 3GPP standardization progress. In this article, the overview of requirements and use cases in V2X services in 3GPP is presented. The up-to-date standardization of LTE V2X in 3GPP is surveyed. The challenges and detailed design aspects in LTE V2X are also discussed. Meanwhile, the enhanced V2X (eV2X) services and possible 5G solutions are analyzed. Finally, the implementations of LTE V2X are presented with the latest progress in industrial alliances, research, development of prototypes, and field tests.", 
V2I communication is vital in the successful deployment and operation of intelligent transport systems. There is growing research interest on the effectiveness of V2I in the Fifth Generation (5G) networks. The goal of this survey paper is to present the basic characteristics of V 2I communication.,Vehicle-to-Infrastructure Communication over Multi-Tier Heterogeneous Networks A Survey,Vehicle-to-infrastructure (V2I) communication is vital in the successful deployment and operation of intelligent transport systems (ITS). One can observe a growing research interest on the effectiveness of V2I communication in the Fifth Generation (5G) networks supporting a co-existence of multi-tier heterogeneous wireless networks with diverse radio access technologies (RATs). The goal of this survey paper is to present the basic characteristics of V2I communication in heterogeneous multi-tier network environments. We first provide an overview of notable V2I applications and few of V2I related projects. We then focus on V2I communications over heterogeneous multi-tier networks. We identify several V2I research challenges and discuss possible solutions., 
"Vehicle-to-vehicle communication is an important part of the modern intelligent transportation system. Most of the existing vehicle networks are based on central structures, which are prone to single point of failures. In this model, the base station and roadside units will no longer be necessary facilities.",Vehicle-to-vehicle communication based on a peer-to-peer network with graph theory and consensus algorithm,"Vehicle-to-vehicle communication is an important part of the modern intelligent transportation system. Most of the existing vehicle networks are based on central structures, which are prone to single point of failures, and may consequently result in system paralysis. In order to increase the network fault tolerance and maintain the stability of the network system, the authors propose a vehicle network model based on the peer-to-peer (P2P) network. In this model, the base station and the roadside units will no longer be necessary facilities. The vehicles can be used as relays to directly participate in the exchange and transfer of information. The relay nodes (vehicles) are selected based on the degree distribution and the consensus algorithm. The real-time capability, efficiency and cost effectiveness of the proposed P2P model is verified through experimental results.", 
"Vehicle-to-Vehicle (V2V) communication in Vehicular Ad hoc Networks (VANETs) is of importance in the Intelligent Transportation System (ITS) In this paper, we analyze vehicle to vehicle wireless connectivity by using mathematic models.",Vehicle-to-Vehicle Connectivity Analysis for Vehicular Ad-Hoc Networks,"Vehicle-to-Vehicle (V2V) communication in Vehicular Ad hoc Networks (VANETs) is of importance in the Intelligent Transportation System (ITS) in which vehicles enlisted with wireless devices can communicate with each other. Many applications can save people’s life or time on traffic such as accident alerts or congestion prediction, etc. However, network communication over VANETs is inheritedly unstable because of the high mobility of vehicles. In this paper, we analyze vehicle to vehicle wireless connectivity by using mathematic models. We consider the effect of headway distance, acceleration, association time (i.e. connection setup time), relative speed of vehicles, transmission range and message/data size in short range based V2V communications in the models. The numerical results in simulations validate the analysis.", 
An ad hoc network consisting of vehicles has emerged as an interesting but challenging domain. Large-scale practical implementation still require some time. This paper surveys current challenges and potential applications.,"Vehicular Ad Hoc Network (VANET) A Survey, Challenges, and Applications","An ad hoc network consisting of vehicles has emerged as an interesting but challenging domain where a lot of new application may find their place. Though research in this field is on since last two decades, large-scale practical implementation still require some time. In this paper, a survey of current challenges and potential applications, incorporating medium access control schemes, routing approaches, hardware and spectrum issues, and security and privacy issues for VANETs, is presented.", 
"Automated driving vehicles are capable of sensing the environment and conducting automobile operation. vehicular communication networks (VCNs) connect vehicles, infrastructures, clouds, and all other devices with communication modules. VCNs have attractive potential to enhance onboard sensing-based automated vehicles.",Vehicular Communication Networks in the Automated Driving Era,"Embedded with advanced sensors, cameras, and processors, the emerging automated driving vehicles are capable of sensing the environment and conducting automobile operation, paving the way to modern intelligent transportation systems with high safety and efficiency. On the other hand, vehicular communication networks (VCNs) connect vehicles, infrastructures, clouds, and all other devices with communication modules, whereby vehicles can obtain local and global information to make intelligent operation decisions. Although sensing-based automated driving technologies and VCNs have been investigated independently, their interactions and mutual benefits are still underdeveloped. In this article, we argue that VCNs have attractive potential to enhance onboard sensing-based automated vehicles from different perspectives, such as driving safety, transportation efficiency, as well as customer experience. A case study is conducted to demonstrate that traffic jams can be relieved at intersections with automated driving vehicles coordinated with each other through VCNs. Furthermore, we highlight critical and interesting issues for future research, based on the specific requirements posed by automated driving in VCNs", 
"Vehicular communications have attracted a lot of attention recently due to their potential to support intelligent transportation, various safety applications, and onroad infotainment. This paper provides an overview of recent research on enabling efficient and reliable vehicular communications.",Vehicular Communications A Network Layer Perspective,"Vehicular communications, referring to information exchange among vehicles, infrastructures, etc. have attracted a lot of attentions recently due to its great potential to support intelligent transportation, various safety applications, and onroad infotainment. In this paper, we provide a comprehensive overview of recent research on enabling efficient and reliable vehicular communications from the network layer perspective. First, we introduce general applications and unique characteristics of vehicular communication networks and the corresponding classifications. Based on different driving patterns, we categorize vehicular networks into manual driving vehicular networks and automated driving vehicular networks, and then discuss the available communication techniques, network structures, routing protocols, and handoff strategies applied in these vehicular networks. Finally, we identify the challenges confronted by the current vehicular networks and present the corresponding research opportunities.", 
"Vehicular communications have attracted more and more attention recently from both industry and academia. They have strong potential to enhance road safety, improve traffic efficiency, and provide rich on-board information and entertainment services.",Vehicular Communications A Physical Layer Perspective,"Vehicular communications have attracted more and more attention recently from both industry and academia due to their strong potential to enhance road safety, improve traffic efficiency, and provide rich on-board information and entertainment services. In this paper, we discuss fundamental physical layer issues that enable efficient vehicular communications and present a comprehensive overview of the state-of-the-art research. We first introduce vehicular channel characteristics and modeling, which are the key underlying features differentiating vehicular communications from other types of wireless systems. We then present schemes to estimate the time-varying vehicular channels and various modulation techniques to deal with high-mobility channels. After reviewing resource allocation for vehicular communications, we discuss the potential to enable vehicular communications over the millimeter wave bands. Finally, we identify the challenges and opportunities associated with vehicular communications.", 
"Vehicular cloud computing (VCC) is being expected to be a new paradigm to create a new situation for vehicular data services. In future 5G mobile networks, promoted by the advantages of supporting various connectivity and significantly faster transmission performance, VCC will be enabled to provide more realistic vehicularData cloud services.","Vehicular Data Cloud Platform with 5G Support Architecture, Services, and Challenges","Facilitated by the emerging technologies of cloud computing, vehicular cloud computing (VCC) is being expected to be a new paradigm to create a new situation for vehicular data services. However, with the increasing popularity of vehicular cloud services, accessing vehicular terminals and produced mobile data are experiencing tremendous growth, therefore, how to provide enough capacity for all access requirements and handle all the mobile data traffic to address the challenges posed by richer vehicular applications are critical. In future 5G mobile networks, promoted by the advantages of supporting various connectivity and significantly faster transmission performance of 5G communications, VCC will be enabled to provide more realistic vehicular data cloud services. In this article, an innovative VCC platform with the support of 5G communications for providing effective vehicular data cloud services is first proposed. Then, in this context, a vehicular social-based taxi-sharing cloud service is mainly investigated. Finally, some open issues and challenges with possible solutions for future work are discussed.", 
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. We show that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small ( 3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", 
"In man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points. The algorithm has been tested in a variety of indoors and outdoors scenes and its efficiency and automation makes it amenable for implementation on robotic platforms.",Video Compass,"In this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene. The main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points, which provide strong constraints on camera parameters and relative orientation of the camera withrespect to the scene. By combining efficient image processing techniques in the line detection and initialization stage we demonstrate that simultaneous grouping and estimation of vanishing directions can be achieved in the absence of internal parameters of the camera. Constraints between vanishing points are then used for partial calibration and relative rotation estimation. The algorithm has been tested in a variety of indoors and outdoors scenes and its efficiency and automation makes it amenable for implementation on robotic platforms.", 
Proposed method is based on object recognition in still images combined with methods using temporal information from the video. Segmentation is realized by modeling each background pixel by a single gaussian model. The tracking process isbased on the analysis of connected components position and interest points tracking.,Vision-Based System for Human Detection and Tracking in Indoor Environment,"In this paper, we propose a vision-based system for human detection and tracking in indoor environment using a static camera. The proposed method is based on object recognition in still images combined with methods using temporal information from the video. Doing that, we improve the performance of the overall system and reduce the task complexity. We first use background subtraction to limit the search space of the classifier. The segmentation is realized by modeling each background pixel by a single gaussian model. As each connected component detected by the background subtraction potentially corresponds to one person, each blob is indepentently tracked. The tracking process is based on the analysis of connected components position and interest points tracking. In order to know the nature of various objects that could be present in the scene, we use multiple cascades of boosted classifiers based on Haar-like filters. We also present in this article a wide evaluation of this system based on a large set of videos.", 
"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we introduce a novel visualization technique that gives insight into the function of intermediate feature layers.",Visualizing and Understanding Convolutional Networks,"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.", 
"Message broadcasting in an open access system such as VANET is the main and utmost challenging problem with regard to security and privacy, say authors. They propose a VANet based privacy-preserving communication scheme (VPPCS) that meets the requirements for content and contextual privacy. They say the scheme is secure and impervious to various types of attacks.",VPPCS VANET-Based Privacy-Preserving Communication Scheme,"Over the past years, vehicular ad hoc networks (VANETs) have been commonly used in intelligent traffic systems. VANET’s design encompasses critical features that include autonomy, distributed networking, and rapidly changing topology. The characteristics of VANET and its implementations for road safety have attracted considerable industry and academia interest, particularly in research involving transport systems enhancement that could potentially save lives. Message broadcasting in an open access system, such as VANET, is the main and utmost challenging problem with regard to security and privacy in VANETs. Various studies on VANET security and privacy have been proposed. Nevertheless, none has considered overall privacy requirements such as unobservability. In order to address these shortcomings, we propose a VANET based privacy-preserving communication scheme (VPPCS), which meets the requirements for content and contextual privacy. It leverages elliptic curve cryptography (ECC) and an identity-based encryption scheme. We have carried out a detailed security analysis (burrows–abadi–needham (BAN) logic, random oracle model, security of proof, and security attributes) to validate and verify the proposed scheme. The analysis has shown that our scheme is secure and also shown to be effective in a performance evaluation. The proposed scheme does not only meet the previously mentioned security and privacy requirements, but also impervious to various types of attacks such as replay, impersonation, modification, and man-in-themiddle attacks.", 
"9/11 is said to be a watershed event in the history of international relations. Pakistan was forced to join the Global War on Terrorism in September 2001. War on Terror has greatly affected the economic growth, political and social situation of the country. Pakistan like other countries cannot cure this menace alone by the use of force.",War on Terrorism in Pakistan Challenges and Strategic Steps,"The incident of 9/11 is said to be a watershed event in the history of international relations. After this ferocious incident Pakistan was forced to join the Global War on Terrorism in September 2001 and since then it has faced a lot of challenges. After joining this war, the security situation has become worse within few years, and military forces had to start several operations to fight and eliminate the menace of terrorism. The War on Terror (WOT) has greatly affected the economic growth, political and social situation of the country. This paper discusses the reasons behind the terrorism, the ways it has affected the national security, education and economy, and the steps for its elimination. The general conclusion made by the author is that Pakistan like other countries cannot cure this menace alone by the use of force. The government should adopt a complex and wide strategy focusing in priority on the factors that are responsible for igniting terrorism, and by solving the problems being faced by the poor masses of the country", 
"Emotions are the most fundamental feature for non-verbal communication between human and machine. The proper recognition of human emotion from Electroencephalogram (EEG) has become too much challenging. In this paper, we propose a system of emotion recognition from EEG signal based on Discrete Wavelet Transform.",Wavelet Analysis Based Classification of Emotion from EEG Signal,"Emotions are the most fundamental feature for non-verbal communication between human and machine. To extract the original expectation of mind, emotion recognition and classification is essential. But due to some complexities, the proper recognition of human emotion from Electroencephalogram (EEG) has become too much challenging. In this paper, we propose a system of emotion recognition from EEG signal based on Discrete Wavelet Transform. The most significant features (i) Wavelet Energy and (ii) Wavelet Entropy are calculated for detecting four different emotions namely happy, angry, sad and relaxed. Firstly we rearranged the prepossessed data properly by selecting the proper channel and sub-band. The extracted features are then trained in the KNearest Neighbor (KNN) algorithm to classify emotion separately. Our proposed method showed 78.7±2.6% sensitivity, 82.8±6.3% specificity and 62.3±1.1% accuracy on the internationally authorized ‘DEAP’ database.", 
"Electroencephalogram (EEG) signal contains vital details regarding electrical actions performed by the brain. Analysis of these signals is important for epilepsy detection. However, it can be tricky in nature and requires human expertise. Machine Learning (ML) algorithms were introduced to remove the human factor. Since it uses deep learning it also eliminates the need of feature extraction.",Wavelet based deep learning approach for epilepsy detection,"Electroencephalogram (EEG) signal contains vital details regarding electrical actions performed by the brain. Analysis of these signals is important for epilepsy detection. However, analysis of these signals can be tricky in nature and requires human expertise. The human factor can result in subjective and possible erroneous epilepsy detection. To tackle this problem, Machine Learning (ML) algorithms were introduced, to remove the human factor. However, this approach is counterintuitive in nature as it involves using complex features for epilepsy detection. Hence to tackle this problem we have introduced a wavelet based deep learning approach which eliminates the need of feature extraction and also performs signifcantly better on smaller datasets compared to the present state of the art ML algorithms. To test the robustness of our model we have performed a binary (2-way) and ternary (3-way) classifcation using our model. It is found that the model is much more accurate than the present state of the art models and since it uses deep learning it also eliminates the need of feature extraction.", 
"Feature extraction and classification play an important role in brain–computer interface (BCI) systems. In traditional approaches, methods in pattern recognition field are adopted to solve these problems. In this paper, a method using convolutional neural network can be comparable or better than the other state-of-the-art approaches.",Wavelet Transform Time-Frequency Image and Convolutional Network-Based Motor Imagery EEG Classification,"Feature extraction and classification play an important role in brain–computer interface (BCI) systems. In traditional approaches, methods in pattern recognition field are adopted to solve these problems. Nowadays, the deep learning theory has developed so fast that researchers have employed it in many areas like computer vision and speech recognition, which have achieved remarkable results. However, few people introduce the deep learning method into the study of biomedical signals, especially EEG signals. In this paper, a wavelet transform-based input, which combines the time-frequency images of C3, Cz, and C4 channels, is proposed to extract the feature of motor imagery EEG signal. Then, a 2-Layer convolutional neural network is built as the classifier and convolutional kernels of different sizes are validated. The performance obtained by the proposed approach is evaluated by accuracy and Kappa value. The accuracy on dataset III from BCI competition II reaches 90%, and the best Kappa value on dataset 2a from competition IV is greater than many of other methods. In addition, the proposed method utilizes a resized small input, which reduces calculation complexity, so the training period is relatively faster. The results show that the method using convolutional neural network can be comparable or better than the other state-of-the-art approaches, and the performance will be improved when there is sufficient data.", 
"Segmentation, feature extraction and classification of signal components belong to very common problems in various engineering, economical and biomedical applications. The paper provides a comparison of classification results using different methods of feature extraction most appropriate for EEG signal components detection.",Wavelet Transform Use for Feature Extraction and EEG Signal Segments Classification,"Segmentation, feature extraction and classification of signal components belong to very common problems in various engineering, economical and biomedical applications. The paper is devoted to the use of discrete wavelet transform (DWT) both for signal preprocessing and signal segments feature extraction as an alternative to the commonly used discrete Fourier transform (DFT). Feature vectors belonging to separate signal segments are then classified by a competitive neural network as one of methods of cluster analysis and processing. The paper provides a comparison of classification results using different methods of feature extraction most appropriate for EEG signal components detection. Problems of multichannel segmentation are mentioned in this connection as well.", 
"An increasing number of Web applications are allowing users to play more active roles for enriching the source content. enriched data can be used for various applications such as text summarization, opinion mining and ontology creation. Authors propose a novel Web content summarization method that creates a text summary by exploiting user feedback (comments and tags)","Web content summarization using social bookmarks, a new approach for social summarization","An increasing number of Web applications are allowing users to play more active roles for enriching the source content. The enriched data can be used for various applications such as text summarization, opinion mining and ontology creation. In this paper, we propose a novel Web content summarization method that creates a text summary by exploiting user feedback (comments and tags) in a social bookmarking service. We had manually analyzed user feedback in several representative social services including del.icio.us, Digg, YouTube, and Amazon.com. We found that (1) user comments in each social service have its own characteristics with respect to summarization, and (2) a tag frequency rank does not necessarily represent its usefulness for summarization. Based on these observations, we conjecture that user feedback in social bookmarking services is more suitable for summarization than other type of social services. We implemented prototype system called SSNote that analyzes tags and user comments in del.icio.us, and extracts summaries. Performance evaluations of the system were conducted by comparing its output summary with manual summaries generated by human evaluators. Experimental results show that our approach highlights the potential benefits of user feedback in social bookmarking services.", 
Ads can be matched to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire content of such pages on-the-fly entails prohibitive communication and latency costs. We propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time.,Web Page Summarization for Just-in-Time Contextual Advertising,"Contextual advertising is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, often ads need to be matched to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire content of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low relevance or high latency or high load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time. Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 6% fraction of the page text can sacrifice only 1%–3% in ad relevance. Furthermore, our summaries are fully compatible with the standard JavaScript mechanisms used for ad placement: they can be produced at ad-display time by simple additions to the usual script, and they only add 500–600 bytes to the usual request. We also compared our summarization approach, which is based on structural properties of the HTML content of the page, with a more principled one based on one of the standard text summarization tools (MEAD), and found their performance to be comparable.", 
"Website categorization has recently emerged as a very important task in several contexts. A huge amount of information is freely available through websites, and it could be used for example to accomplish statistical surveys. In this work we propose a practically viable procedure to perform website categorization, based on the automatic generation of data records.",Website Categorization a Formal Approach and Robustness,"Website categorization has recently emerged as a very important task in several contexts. A huge amount of information is freely available through websites, and it could be used for example to accomplish statistical surveys, saving in costs. However, the information of interest for the specific categorization has to be mined among that huge amount. This turns out to be a difficult task in practice. In this work we propose a practically viable procedure to perform website categorization, based on the automatic generation of data records summarizing the content of each entire website. This is obtained by using web scraping and optical character recognition, followed by a number of nontrivial text mining and feature engineering steps. When such records have been produced, we use classification algorithms to categorize the websites according to the aspect of interest. We compare in this task Convolutional Neural Networks, Support Vector Machines, Random Forest and Logistic classifiers. Since in many practical cases the training set labels are physiologically noisy, we analyze the robustness of each technique with respect to the presence of misclassified training records. We present results on real-world data for the problem of the detection of websites providing e-commerce facilities, however our approach is not structurally limited to this case.", 
"Network Traffic Classification carries great importance for both internet service providers (ISPs) and quality of services (QoSs) management. WeChat text and picture messages traffics are classified using two different types of datasets and 4 well-known machine learning algorithms. Using HIT dataset, all ML classifier perform very well, but C4.5 and SVM are the ones that give very effective accuracy results of 99.91% and 99.57% respectively.",WeChat Text and Picture Messages Service Flow Traffic Classification Using Machine Learning Technique,"Network Traffic Classification carries great importance for both internet service providers (ISPs) and quality of services (QoSs) management. During the last two decades, a lot of machine learning models have been proposed and applied on different types of real time applications to classify their real time traffic and obtain very proficient accuracy results. However, no research has been done on WeChat text and picture messages traffic classification. In this paper, WeChat text and picture messages traffics are classified using two different types of datasets and 4 well-known machine learning algorithms. These two datasets, Harbin Institute of Technology (HIT) and Dorm13, are collected from two different network environments. Having captured the traffic 50 features, they are extracted respectively. Thereafter, wellknown four machine learning algorithms C4.5 decision tree, Bayes Net, Naïve Bayes and SVM are used to classify WeChat text and picture messages traffic. Experimental result analysis show that using HIT data set all the applied machine learning classifiers classify WeChat text and picture messages traffic very accurately as compared to Dorm13 dataset. Using HIT dataset, all ML classifier perform very well, but C4.5 and SVM are the ones that give very effective accuracy results of 99.91% and 99.57% respectively as compared to other ML classifiers.", 
"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores. However, gaps still exist between summaries produced by automatic summarizers and human professionals. We consult the Multidimensional Quality Metric1 (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually.",What Have We Achieved on Text Summarization,"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric1 (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results", 
"Most software systems are written in programming languages that are inherently insecure, authors say. Authors: Even the most recent programming languages come with claims that they are designed to be ""secure"" They propose three broad categories that as a minimum any secure language should address. The need for secure programming languages is probably greater now than it has ever been, they say. They write that the shift in design and implementation of software systems is towards microservices.",What is a Secure Programming Language,"Our most sensitive and important software systems are written in programming languages that are inherently insecure, making the security of the systems themselves extremely challenging. It is often said that these systems were written with the best tools available at the time, so over time with newer languages will come more security. But we contend that all of today’s mainstream programming languages are insecure, including even the most recent ones that come with claims that they are designed to be “secure”. Our real criticism is the lack of a common understanding of what “secure” might mean in the context of programming language design. We propose a simple data-driven definition for a secure programming language: that it provides first-class language support to address the causes for the most common, significant vulnerabilities found in real-world software. To discover what these vulnerabilities actually are, we have analysed the National Vulnerability Database and devised a novel categorisation of the software defects reported in the database. This leads us to propose three broad categories, which account for over 50% of all reported software vulnerabilities, that as a minimum any secure language should address. While most mainstream languages address at least one of these categories, interestingly, we find that none address all three. Looking at today’s real-world software systems, we observe a paradigm shift in design and implementation towards service-oriented architectures, such as microservices. Such systems consist of many fine-grained processes, typically implemented in multiple languages, that communicate over the network using simple web-based protocols, often relying on multiple software environments such as databases. In traditional software systems, these features are the most common locations for security vulnerabilities, and so are often kept internal to the system. In microservice systems, these features are no longer internal but external, and now represent the attack surface of the software system as a whole. The need for secure programming languages is probably greater now than it has ever been.", 
"Most systems use one stage of feature extraction in which the filters are hard-wired. Using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages ofFeature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101.",What is the Best Multi-Stage Architecture for Object Recognition,"In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (> 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).", 
"Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeling technique is Latent Dirichlet allocation. When running on different datasets, LDA suffers from ""order effects"" different topics are generated if the order of training data is shuffled. This can lead to misleading results and inaccurate topic descriptions.",What is Wrong with Topic Modeling (and How to Fix it Using Search-based Software Engineering),"Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeling technique is Latent Dirichlet allocation. When running on different datasets, LDA suffers from “order effects”, i.e., different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results; specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results. To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. We use LDADE, a search-based software engineering tool which uses Differential Evolution (DE) to tune the LDA’s parameters. LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of Software Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark) across Linux platform and for different kinds of LDAs (VEM, Gibbs sampling). Results were scored via topic stability and text mining classification accuracy.", 
FOSS project well-being depends on turning project newcomers into 'good contributors' That is to say into individuals that substantially contribute to the project - but also that perform citizenship behaviors that protect and nurture its community. This study investigates the socialization factors that influence contributor performance in large FOSS projects.,What makes a good contributor Understanding contributor behavior within large FreeOpen Source Software projects â€“ A socialization perspective,"Attracting new contributors is a necessary but not a sufficient condition, to ensure the survival and long-term success of Free/Open Source Software (FOSS) projects. The well-being of a FOSS project depends on the turning of project newcomers into ‘good contributors’ that is to say into individuals that substantially contribute to the project - but also that perform citizenship behaviors that protect and nurture its community. This study is a mixedmethods investigation of the socialization factors that influence contributor performance in large FOSS projects. A qualitative research component resulted into the development of a FOSS socialization framework as well as into the identification of key FOSS project citizenship behaviors. A conceptual model was then developed and empirically examined with 367 contributors from 12 large FOSS projects. The model hypothesizes the mediating effect of two proximal socialization variables, social identification and social integration, between FOSS newcomer socialization factors and contributor performance (conceptualized as task performance and community citizenship behaviors). The results demonstrate the influence of social identification and social integration in predicting contributor performance, as well as the importance of key socialization factors that are: task segregation, task purposefulness, interaction intensity, and supportiveness. Theoretical and practical implications are discussed.", 
"An event in a static image is defined as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. We show that our system is capable of classifying these event classes at 73.4% accuracy.","What, where and who-Classifying events by scene and object recognition","We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4% accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance.", 
"WhatsApp is a widely adopted mobile messaging application with over 800 million users. Recently, a calling feature was added to the application. In this work, we describe how we were able to decrypt the network traffic and obtain forensic artifacts.",WhatsApp Network Forensics WhatsApp network forensics Decrypting and understanding the WhatsApp call signaling messages,"WhatsApp is a widely adopted mobile messaging application with over 800 million users. Recently, a calling feature was added to the application and no comprehensive digital forensic analysis has been performed with regards to this feature at the time of writing this paper. In this work, we describe how we were able to decrypt the network traffic and obtain forensic artifacts that relate to this new calling feature which included the: a) WhatsApp phone numbers, b) WhatsApp server IPs, c) WhatsApp audio codec (Opus), d) WhatsApp call duration, and e) WhatsApp's call termination. We explain the methods and tools used to decrypt the traffic as well as thoroughly elaborate on our findings with respect to the WhatsApp signaling messages. Furthermore, we also provide the community with a tool that helps in the visualization of the WhatsApp protocol messages", 
"Forensically sound is a term used extensively in the digital forensics community to qualify and, in some cases, justify the use of a particular forensic technology or methodology. This paper examines the various definitions of forensic computing. Admissibility and evidentiary weight play a common role, and four criteria are provided for determining whether a digital forensic",WHEN IS DIGITAL EVIDENCE FORENSICALLY SOUND,"“Forensically sound” is a term used extensively in the digital forensics community to qualify and, in some cases, to justify the use of a particular forensic technology or methodology. Indeed, many practitioners use the term when describing the capabilities of a particular piece of software or when describing a particular forensic analysis approach. Such a wide application of the term can only lead to confusion. This paper examines the various definitions of forensic computing (also called digital forensics) and identifies the common role that admissibility and evidentiary weight play. Using this common theme, the paper explores how the term “forensically sound” has been used and examines the drivers for using such a term. Finally, a definition of “forensically sound” is proposed and four criteria are provided for determining whether or not a digital forensic process may be considered to be “forensically sound.”", 
integration of smart wearables and intelligent vehicles is difficult to realize due to their intrinsic characteristics. WeVe can easily be integrated with the existing protocol stack. A feasible hub-centric communication architecture is proposed for WeVe.,When Smart Wearables meet Intelligent VehIcles challenges and Future dIrections,"IoT has become the largest network worldwide, of which smart wearables and intelligent vehicles constitute essential parts. The integration of smart wearables and intelligent vehicles, although enabling a wide spectrum of promising applications, is difficult to realize due to their intrinsic characteristics, which consequently brings up unprecedented challenges in terms of communication technologies, security, and privacy. Therefore, it is of great necessity to develop such an integrated system. In light of this, in this article, we overview the applications, characteristics, and challenges of developing the integrated system of smart WeVe. Moreover, a feasible hub-centric communication architecture is proposed for WeVe, which can easily be integrated with the existing protocol stack.", 
"Authorship methods which focus on linguistic characteristics currently have accuracy rates ranging from 72% to 89%. This article presents a computational, stylometric method which has obtained 95% accuracy.",Who's At The Keyboard Authorship Attribution in Digital Evidence Investigations,"In some investigations of digital crime, the question of who was at the keyboard when incriminating documents were produced can be legitimately raised. Authorship attribution can then contribute to the investigation. Authorship methods which focus on linguistic characteristics currently have accuracy rates ranging from 72% to 89%, within the computational paradigm. This article presents a computational, stylometric method which has obtained 95% accuracy and has been successfully used in investigating and adjudicating several crimes involving digital evidence. The article concludes with a brief review of the current admissibility status of authorship identification techniques.", 
"We present WikiHow, a dataset of more than 230,000 article and summary pairs. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing methods on WikiHow to present its challenges and set some baselines to further improve it.",WikiHow A Large Scale Text Summarization Dataset,"Sequence-to-sequence models have recently gained the state of the art performance in summarization. However, not too many large-scale high-quality datasets are available and almost all the available ones are mainly news articles with specific writing style. Moreover, abstractive human-style systems involving description of the content at a deeper level require data with higher levels of abstraction. In this paper, we present WikiHow, a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing methods on WikiHow to present its challenges and set some baselines to further improve it", 
"We present WikiHow, a dataset of more than 230,000 article and summary pairs. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing methods on WikiHow to present its challenges and set some baselines to further improve it.","WikiHow, A Large Scale Text Summarization Dataset","Sequence-to-sequence models have recently gained the state of the art performance in summarization. However, not too many large-scale high-quality datasets are available and almost all the available ones are mainly news articles with specific writing style. Moreover, abstractive human-style systems involving description of the content at a deeper level require data with higher levels of abstraction. In this paper, we present WikiHow, a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing methods on WikiHow to present its challenges and set some baselines to further improve it.", 
"Wireless sensors and actuators connected by the Internet-of-Things (IoT) are central to the design of advanced cyberphysical systems (CPSs) In such complex, heterogeneous systems, communication links must meet stringent requirements on throughput, latency, and range.",Wireless Communication and Security Issues for Cyberâ€“ Physical Systems and the Internet-of-Things,"Wireless sensors and actuators connected by the Internet-of-Things (IoT) are central to the design of advanced cyber-physical systems (CPSs). In such complex, heterogeneous systems, communication links must meet stringent requirements on throughput, latency, and range, while adhering to tight energy budget and providing high levels of security. In this paper, we first summarize wireless communication principles from the perspective of the connectivity needs of IoT and CPS. Based on these principles, we then review the most relevant wireless communication standards before focusing on the key security issues and features of such systems. In particular, the gap between the security features in the communication standards used in CPSs and IoT and their actual vulnerabilities are pointed out with practical examples and recent attacks. We emphasize the need for a more in-depth study of the security issues across all the protocol layers, including both logical layer security and physical layer security.", 
"In this paper, we propose a new method, using higher-order singular value decomposition (HOSVD) for extracting the concept of the words from word-document-time three-dimensional tensor. We measure WordNet-based semantic similarity between sentences and remove redundancy sentences with less importance.",Word concept extraction using HOSVD for automatic text summarization,"Computers understand little about the meaning of human language. Vector space models of semantics are beginning to overcome these limits. In this regard, one of the modern issues is using high dimensional data, which is formulated as tensors. Also, due to the increased information and texts, automatic text summarization has become one of the most important issues in information retrieval and natural language processing. In this paper, we propose a new method, using higher-order singular value decomposition (HOSVD) for extracting the concept of the words from word-document-time three-dimensional tensor and then select important sentences with more cosine similarity to this concept. In the following, we measure WordNet-based semantic similarity between sentences and remove redundancy sentences with less importance. The evaluation of the proposed method is done using the ROUGE evaluation on the DUC 2007 standard data set that the obtained results indicate the predominance of our method over many dominant systems.", 
The main problem for generating an extractive text summary is to detect the most relevant information in the source document. We employ the so-called n-grams and maximal frequent word sequences,Word Sequence Models for Single Text Summarization,"The main problem for generating an extractive automatic text summary is to detect the most relevant information in the source document. For such purpose, recently some approaches have successfully employed the word sequence information from the self-text for detecting the candidate text fragments for composing the summary. In this paper, we employ the so-called n-grams and maximal frequent word sequences as features in a vector space model in order to determine the advantages and disadvantages for extractive text summarization.", 
"Extractive summarization aims to automatically produce a short summary of a document by concatenating several sentences. In this paper, we propose a novel wordsentence co-ranking model named CoRank. CoRank combines the word-sentence relationship with the graph-based unsupervised ranking model.",Word-sentence co-ranking for automatic extractive text summarization,"Extractive summarization aims to automatically produce a short summary of a document by concatenating several sentences taken exactly from the original material. Due to its simplicity and easy-to-use, the extractive summarization methods have become the dominant paradigm in the realm of text summarization. In this paper, we address the sentence scoring technique, a key step of the extractive summarization. Specifically, we propose a novel wordsentence co-ranking model named CoRank, which combines the word-sentence relationship with the graph-based unsupervised ranking model. CoRank is quite concise in the view of matrix operations, and its convergence can be theoretically guaranteed. Moreover, an redundancy elimination technique is presented as an supplement to CoRank, so that the quality of automatic summarization can be further enhanced. As a result, CoRank can serve as an important building-block of the intelligent summarization systems.", 
This technique is proposed based upon Key Sentences using statistical method and WordNet. Experimental results show that our approach compares favourably to,WordNet-based Document Summarization,"This paper presents an improved and practical approach to automatically summarizing document by extracting the most relevant sentences from original document. This technique is proposed based upon Key Sentences using statistical method and WordNet. Experimental results show that our approach compares favourably to a commercial text summarizer, and some refinement techniques improves the summarization quality significantly.", 
Energy efficiency is of the utmost importance in the design of intelligent connected objects. The wireless communication between the distributed sensor devices and the host stations can consume significant energy. The platform allows the building of an heterogeneous long-short range network architecture to reduce the latency and reduce the power consumption.,WULoRa An Energy Efficient IoT End-Node for Energy Harvesting and Heterogeneous Communication,"Intelligent connected objects, which build the IoT, are electronic devices usually supplied by batteries that significantly limit their life-time. These devices are expected to be deployed in very large numbers, and manual replacement of their batteries will severely restrict their large-scale or widearea deployments. Therefore energy efficiency is of the utmost importance in the design of these devices. The wireless communication between the distributed sensor devices and the host stations can consume significant energy, even more when data needs to reach several kilometers of distance. In this paper, we present an energy-efficient multi-sensing platform that exploits energy harvesting, long-range communication and ultra-low-power shortrange wake-up radio to achieve self sustainability in a kilometer range network. The proposed platform is designed with power efficiency in mind and exploits the always-on wake-up radio as both receiver and a power management unit to significantly reduce the quiescent current even continuously listening the wireless channel. Moreover the platform allows the building of an heterogeneous long-short range network architecture to reduce the latency and reduce the power consumption in listening phase at only 4.6?W. Experimental results and simulations demonstrate the benefits of the proposed platform and heterogeneous network.", 
"YOLO9000 is a state-of-the-art, real-time object detection system that can detect over 9000 different object categories. The improved model, YOLOv2, is state- of- the-art on standard detection tasks like PASCAL VOC and COCO. At 40 FPS, the same model outperforms state- Of theart methods like Faster RCNN with ResNet and SSD.","YOLO9000 Better, Faster, Stronger","We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time", 
YOLO is a new way to detect objects in images. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. YOLO processes images in real-time at 45 frames per second.,"You Only Look Once Unified, Real-Time Object Detection","Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork", 
