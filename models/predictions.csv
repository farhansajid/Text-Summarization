,Generated Summary,Actual Text
0,Different data traffic has different Quality of Service (QoS) requirements. 5G mobile network aims to address the limitations of previous cellular standards. It is a prospective key enabler for future Internet of Things (IoT),"5G network is projected to support large amount of data traffic and massive number of wireless connections. 5G networks support a wide range of applications such as smart home, autonomous driving, drone operations, health and mission critical applications."
1,"The goal of Intelligent Transport Systems (ITS) is to enhance network performance of Vehicular Ad hoc NETworks (VANETs) Blockchain, which has decentralization, transparency and immutability as some of its properties, is designed to ensure trust in networking platforms. Simulation results substantially guarantee an efficient network performance, while also ensuring there is trust among the entities.",The goal of Intelligent Transport Systems (ITS) is to enhance network performance of vehicular Ad hoc NETworks (VANETs) There are some security concerns including the need to establish trust among the connected peers. The 5G communication system is seen as the technology to cater for the challenges in VANets.
2,This article proposes an exploratory procedure to detect and classify clusters and outliers in a multivariate spatial data set. The procedure is fully implemented using free and open source geospatial software and libraries.,The detection of spatial clusters and outliers is critical to a number of spatial data analysis techniques. This article proposes an exploratory procedure to detect and classify clusters and outlier. The procedure is fully implemented using free and open source geospatial software and libraries.
3,"In this paper, a new method is introduced for automatic text summarization problem. We use cellular learning automata for calculating similarity of sentences. The results show that second method performs better than the first method and the benchmark methods.",A high quality summary is a main goal and challenge for any automatic text summarization. We use cellular learning automata for calculating similarity of sentences and particle swarm optimization method for weighting to the features according to their importance. The results show that second method performs better than the first method and benchmark methods.
4,Automatic summarization of texts is now crucial for several information retrieval tasks. We employ concepts and metrics of complex networks to select sentences for an extractive summary. The use of complex networks to represent texts appears therefore as suitable for automatic summarization.,"The huge amount of information available in digital media has increased the demand for simple, language-independent extractive summarization strategies. In this paper, we employ concepts and metrics of complex networks to select sentences for an extractive summary. When applied to a corpus of Brazilian Portuguese texts, some versions performed better than summarizers that do not employ deep linguistic knowledge."
5,The importance of automatic text summarization stands as a helping source in the growing data. This paper discusses the basic blocks of the automatic text summarization and its feature in identifying the intricate properties of the meaningful text through various approaches.,Automatic text summarization condenses the text documents into meaningful phrases and textual messages. This paper discusses the basic blocks of the automatic text summarizing and its feature in identifying the intricate properties.
6,"Techniques for creating synopses of massively generated data have been in the forefront of the research in the recent times. Text Summarization was one such aspect of the research which focused on representing the idea of the context in a short representation. The paper compares all the prevailing systems, their shortcomings, and a combination of technologies used to achieve improved results.","Text Summarization was one of the key areas of research in the recent times. Efforts were put to create a system which was able to generate effective summaries. The paper compares all the prevailing systems, their shortcomings, and a combination of technologies."
7,"Current EEG-based BCI research usually requires a subjectspecific adaptation step before a BCI can be employed by a new user. The subject-independent scenario, where a well-trained model can be directly applied to new users without pre-calibration, is particularly desired. We present a Convolutional Recurrent Attention Model (CRAM) that utilizes a convolutional neural network to encode the highlevel representation of EEG signals.",The EEG signal is a medium to realize a BCI system due to its zero clinical risk and portable acquisition devices. We present a Convolutional Recurrent Attention Model (CRAM) that utilizes a convolutional neural network to encode the highlevel representation of EEG signals. The proposed model is capable of exploiting the underlying invariant EEG patterns across different subjects.
8,"Multi-document extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. We propose a new approach under the Hub-Authority framework: in this paper. Our approach combines the text content with some cues such as ""cue phrase"", ""sentence length"" and ""first sentence"".",Multi-document extractive summarization relies on the concept of sentence centrality. We propose a new approach under the Hub-Authority framework. We provide an evaluation of our method on DUC 2004 data.
9,sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. It is traditionally performed by a sleep expert who assigns to each 30 s of signal a sleep stage. We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance.,Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance.
10,Every day 2.5 quintillion bytes of data are created worldwide. It is a very complicated task to manage and extract useful information from social media platforms. We have considered Twitter as our social media platform to filter the medical data from the general category data.,"Every day 2.5 quintillion bytes of data are created worldwide. It is a complicated task to manage and extract useful information from social media platforms. We have considered Twitter as our social media platform to filter the medical data. In this article, we have used the Open Biomedical Annotator (OBA) to annotate raw text."
11,fasting blood sugar is the most important attribute which gives better classification against the other attributes but its gives not better accuracy.,"In this paper, we select 14 important clinical features, i.e., age, sex, chest pain type, cholesterol, fasting blood sugar, resting ecg, max heart rate, exercise induced angina, old peak, slope, number of vessels colored, thal and diagnosis of heart disease. We develop a prediction model using J48 decision tree for classifying heart disease."
12,"the most significant fragments of the original document are determined in the process of rhetorical analysis with the help of discursive markers. The experiments showed that this method is effective, needs a comparatively small amount of training data and can be adapted to processing texts of different subject fields in other languages.","In this paper, we present an approach that leverages neural language models and deep learning techniques in combination with standard classification approaches for product matching and categorization. We use structured product data as supervision for training feature extraction models. To minimize the need for lots of data for supervision, we use neural word embeddings from large quantities of publicly available product data."
13,"this paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment. We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the benefits of summarization. We found positive correlations between ROUGE scores and two different summary types.","This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment. It uses a new method for measuring agreement, Relevance-Prediction, which compares subjects' judgments on summaries with their own judgments on full text documents."
14,"proposed approach relies on only two resources for any language: a word segmentation system and a dictionary of words along with their document frequencies. The summarizer initially takes a collection of related documents, and transforms them into a matrix. After using a binary hierarchical clustering algorithm, the most important sentences of the most important clusters form the summary.",New multidocument multi-lingual text summarization technique based on singular value decomposition and hierarchical clustering is proposed. The system has been successfully tested on summarizing several Persian document collections.
15,Timely prediction of renal dysfunction can help medical staffs intervene early to avoid catastrophic consequences. We proposed a multi-task deep and wide neural network (MT-DWNN) for predicting fatal complications during hospitalization. The algorithm was tested on a dataset collected from Chinese PLA General Hospital.,Renal dysfunction is one of the most common complications of heart failure. Timely prediction of renal dysfunction can help medical staffs intervene early to avoid catastrophic consequences. The proposed MT-DWNN model achieves better prediction performance on renal dysfunction in HF patients than conventional models.
16,"Several features including autoregressive model parameters, band power and fractal dimension are used for the purpose of classification. Most of the selected channels are located in the prefrontal and temporal lobes confirming neuropsychological and neuroanatomical findings.",This paper is concerned with a two stage procedure for analysis and classification of electroencephalogram (EEG) signals for schizophrenic patients and twenty age-matched control participants. Most of the selected channels are located in the prefrontal and temporal lobes confirming neuropsychological and neuroanatomical findings.
17,weight assigned to the edges of the graph is the crucial parameter for the sentence ranking. Most graph-based techniques use the common words based similarity measure to assign the weight. We propose a new graph-based summarization technique that considers the similarity between the sentences and the overall (input) document.,"In graph-based extractive text summarization techniques, the weight assigned to the edges of the graph is the crucial parameter for the sentence ranking. In this paper, we propose a new graph- based summarization technique that takes into account the similarity among the individual sentences as well as the similarity between the sentences and the overall document. The evaluation results of the proposed method demonstrate a signifcant improvement of the summary quality."
18,Information overload becomes a serious problem in the digital age. It negatively impacts understanding of useful information. This study proposes a novel approach based on recent hierarchical Bayesian topic models. The proposed model incorporates the concepts of n-grams into hierarchically latent topics.,How to alleviate information overload is the main concern of research on natural language processing. This study proposes a novel approach based on recent Bayesian topic models. The proposed model incorporates the concepts of n-grams into hierarchically latent topics.
19,"Emotion recognition based on electroencephalogram (EEG) signal is attracting more and more attention. Many feature engineering based models have been investigated. To reduce the manual effort on features used in EEG-based recognition and improve the performance, we propose an end-to-end model which is based on Convolutional Neural Networks (CNNs).","Emotion recognition based on electroencephalogram (EEG) signal is attracting more and more attention. We propose an end-to-end model which is based on Convolutional Neural Networks (CNNs) In order to represent the EEG signals better, the original channels of EEG are firstly rearranged by Pearson Correlation Coefficient."
20,originality of technique lies on exploiting local and global properties of words and identifying significant words. The local property of word can be considered as the sum of normalized term frequency multiplied by its weight. Global property can be thought of as maximum semantic similarity between a word and title words.,Text summarization is a process that reduces the size of the text document and extracts significant sentences from a text document. We present a novel technique for text summarization. The originality of technique lies on exploiting local and global properties of words and identifying significant words.
21,"Left and right hand motor imagery electroencephalogram (MI-EEG) signals are widely used in brain-computer interface (BCI) systems to identify a participant intent in controlling external devices. The recognition of left and right hand MI-EEG signals is vital for the application of BCI systems. In this paper, we propose an algorithm that combines continuous wavelet transform (CWT) and a simplified convolutional neural network (SCNN) to improve the recognition rate of MI-EEG signals.","Motor imagery electroencephalogram (MI-EEG) signals are widely used in brain-computer interface (BCI) systems. There are few effective deep learning algorithms applied to BCI systems, particularly for MI based BCI. We propose an algorithm that combines continuous wavelet transform (CWT) and a simplified convolutional neural network."
22,Coronavirus or COVID-19 has incurred huge losses to the lives of people throughout the world. It may take a year or two to make a safe and effective vaccine available for the world. A positive coronavirus patient can be traced through CDRA and contact tracing.,A positive coronavirus patient can be traced through CDRA and contact tracing. The technique can track the path traversed by the patient and collect the cell numbers of all those people who have met with the patient. A COVID-19 patient is geo tagged and alerts are sent if any violation of isolation is done by the patients.
23,"COVID-19 (2019 Novel Coronavirus) has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. Due to the nature of such sites, there are always a limited number of relevant questions and answers to search from. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses.","COVID-19 ( 2019 Novel Coronavirus) has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The situation has made it difficult to access accurate, on-demand information regarding the disease. With the advancements in the field of natural language processing, it has become possible to design chatbots that can automatically answer consumer questions. The authors say such models are rarely applied and evaluated in the healthcare domain."
24,"In this paper, we propose a deep learning approach to tackle the automatic summarization tasks. We incorporate topic information into the convolutional sequence-to-sequence (ConvS2S) model and using self-critical sequence training (SCST) for optimization. The empirical results demonstrate the superiority of our proposed method in the abstractive summarization.","In this paper, we propose a deep learning approach to tackle the automatic summarization tasks. We incorporate topic information into the convolutional sequence-to-sequence (ConvS2S) model and use self-critical sequence training (SCST) for optimization."
25,"summarization corpora are numerous but fragmented, making it challenging for researchers to efficiently pinpoint corpora most suited to a given summarization task. We introduce a repository containing corpora available to train and evaluate automatic summarization systems. Agreeing on a data standard for summarization corpora would be beneficial to the field.","Summarization corpora are numerous but fragmented, making it challenging for researchers to pinpoint the corpora most suited to a given summarization task. Each corpus is organized differently, which makes it time-consuming to experiment a new summarization algorithm on many corpora. More large-scale corpora for summarization are needed. Agreeing on a data standard would be beneficial to the field."
26,Vehicular ad hoc network VANET is a promising approach for future intelligent transportation system its. These networks have no fixed infrastructure and instead rely on the vehicles themselves to provide network functionality.,"Summarization corpora are numerous but fragmented, making it challenging for researchers to pinpoint the corpora most suited to a given summarization task. Each corpus is organized differently, which makes it time-consuming to experiment a new summarization algorithm on many corpora. More large-scale corpora for summarization are needed. Agreeing on a data standard would be beneficial to the field."
27,"We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or ""concept"" level, to a sentence-level model previously solved with an ILP. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference.","We present a model for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or ""concept""-level, to a sentence-level model. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables."
28,communicating in an open-access environment makes security and privacy issues a real challenge. Researchers have proposed many solutions to these issues. We present the general secure process and point out authentication methods involved in these processes.,"Security and privacy issues may affect the large-scale deployment of VANETs. Researchers have proposed many solutions to these issues. Privacy preserving methods are reviewed, and the tradeoff between security and privacy is discussed."
29,"text summarization of document or multi-documents has been acknowledged as one of the most challenging tasks in information system community. The approach makes use of phrasal decomposition of the text where each sentence is ascribed a scoring function, which will then be used to identify the most relevant sentences in the sequel. The system architecture as well as its linguistics processing parts are described.",New query-based extractive summary methodology is put forward. The approach makes use of phrasal decomposition of the text where each sentence is ascribed a scoring function. The scoring function is expressed as a convex combination of a set of features that are extracted beforehand.
30,Automatic text summarization is technique of compressing the original text into shorter form which will provide same meaning and information as provided by original text. The overall motive of text summarization is to convey the meaning of text by using less number of words and sentences. This paper concentrates on survey and performance analysis of automatic text summarizers for Marathi language.,"This paper concentrates on survey and performance analysis of automatic text summarizers for Marathi language. Summaries are of two types: Abstractive summaries and Extractive summary. Various linguistic features for selecting important sentences in summary are: Marathi headlines identification, identification of lines just next to headlines. Weights of features are determined using mathematical regression."
31,Automatic text summarization is a technique which compresses large text to a shorter text which includes the important information. There are two types of summaries: Extractive summaries and Abstractive summaries. Several text summarization techniques have been proposed in past years for English and various European languages.,"In the era of Big Data, textual data is rapidly growing and is available in many different languages. In the fast-moving world, it's difficult to read all the text-content. Automatic text summarization is a technique which compresses large text to a shorter text which includes the important information."
32,Brain Computer Interfacing (BCI) is a communication system which uses these commands to interact with outside world. Selection and optimization of the efficient processing technique to extract discriminative features from the EEG signals is a key to develop high performance and robust system.,"Brain Computer Interfacing (BCI) is a communication system which uses commands to interact with outside world. BCI's have extensive applications such as to classify motor imagery tasks, and to detect abnormalities and loss of functions of brain."
33,The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. Fifth Generation (5G) mobile network aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT.,"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world. Current demand for Machine-Type Communications (MTC) has resulted in a variety of communication technologies. 5G mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT."
34,"It is very difficult for humans to understand and interpret the content of the text. This paper collectively summarizes and deciphers the various methodologies, challenges and issues of abstractive summarization. State of art benchmark datasets and their properties are being explored.","Summarization is the task of extracting salient information from the original text document. It is very difficult for humans to understand and interpret the content of the text. This paper collectively summarizes and deciphers the various methodologies, challenges and issues of abstractive summarization."
35,the ability to decipher through such a massive amount of data to extract useful information is a significant undertaking. The ability to decipher through such a massive amount of data to extract useful information is a significant undertaking. This paper will offer an in-depth introduction to automatic text summarization.,"Textual content on the web, in particular, is growing at an exponential rate. The ability to decipher through such a massive amount of data to extract useful information is a significant undertaking. Text summarization systems intent to assist with content reduction keeping the relevant information and filtering the non-relevant parts of the text. This paper offers an in-depth introduction to automatic text summarization."
36,"manual assessment is expensive, subjective and not applicable in real time or to large collections. We introduce an automatic framework for the evaluation of metrics that does not require any human annotation. We evaluate the existing assessment metrics on a Wikipedia data set and a collection of scientifc articles using this framework.","The increasing volume of textual information on any topic requires its compression to allow humans to digest it. These challenges have led to new developments in the area of Natural Language Processing (NLP) and Information Retrieval (IR) Despite some progress over recent years with several solutions for information extraction and text summarization, the problems of generating consistent narrative summaries are still unresolved."
37,Heart disease is the one of the most common disease. This survey paper explores a different models based on such algorithms and techniques and analyse their performance. Models based on supervised learning algorithms are discovered very mainstream among the researchers.,Heart disease is the one of the most common disease. We used different attributes which can relate to this heart diseases well to locate the better method to predict. We additionally used algorithms for prediction.
38,"driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The main thrust of MEC research is to seamlessly merge the two disciplines of wireless communications and mobile computing.","Main feature of MEC is to push mobile computing, network control and storage to the network edges. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. Research aims to seamlessly merge the two disciplines of wireless communications and mobile computing. Advancements in these directions will facilitate the transformation of M EC from theory to practice."
39,"unique characteristics of VANETs make security, privacy, and trust management challenging issues in VANETs' design. Because of the predictable dynamics of vehicles, anonymity is necessary but not sufficient to thwart tracking an attack that aims at the drivers' location profiles. We give a much-needed update on the latest mobility and network simulators as well as the integrated simulation platforms.","Vehicular ad hoc networks (VANETs) are becoming the most promising research topic in intelligent transportation systems. They provide information to deliver comfort and safety to both drivers and passengers. But unique characteristics of VANets make security, privacy, and trust management challenging issues."
40,this paper presents an automatic process for text assessment that relies on fuzzy rules on a variety of extracted features to find the most important information in the assessed texts. The automatically produced summaries of these texts are compared with reference summaries created by domain experts. The proposed summarization method has been trained and tested in experiments using a dataset of Brazilian Portuguese texts.,The text summarization task has gained much importance because of the large amount of online data. This paper presents an automatic process for text assessment that relies on fuzzy rules on a variety of extracted features to find the most important information. The proposed approach can benefit development and use of future expert systems able to automatically assess writing.
41,"With the continuous development of deep learning, pre-trained models have achieved sound effects in the field of natural language processing. In this paper, we propose a topic information fusion and semantic relevance for text summarization based on Fine-tuning BERT(TIF-SR) We extract topic keywords and fusion them with source documents as part of the input.","Text summarization research is far from what people want, especially in abstractive summarization. A high-quality summarization system needs to focus on the topic content of the document and the similarity between the summary and the source document. We propose a topic information fusion and semantic relevance for text summarization based on Fine-tuning BERT(TIF-SR)."
42,"alarms reported by the tools need to be inspected manually by developers, which is inevitable and costly. We propose a defect identification model based on machine learning. We design a set of novel features at variable level, called variable characteristics, for building the classification model.","Static analysis tools can detect source code defects at an early phase during the software development process. They need to be inspected manually by developers, which is inevitable and costly. A large proportion of alarms are found to be false positives. We propose a defect identification model based on machine learning to classify the reported alarms."
43,"Sequence to sequence (Seq2Seq) learning has recently been used for abstractive and extractive summarization. We propose a novel Document-Context based Seq2Seq models using RNNs for abstractive and extractive summarizations. The output summaries are more document centric, than being generic.","Seq2Seq learning has recently been used for abstractive and extractive summarization. In this study, we propose that Seq2seq models should be started with contextual information at the first time-step of the input to obtain better summaries. The output summaries are more document centric, than being generic, overcoming one of the major hurdles of using generative models."
44,"due to the difficulty of abstractive summarization, the great majority of past work on document summarization has been extractive. The recent success of sequence-to-sequence framework has made abstractive summarization viable. We propose to add an attention mechanism on output sequence to avoid repetitive contents.",This work proposes to add an attention mechanism on output sequence to avoid repetitive contents. We applied our model to the public dataset provided by NLPCC 2017 shared task3. The evaluation results show that our system achieved the best ROUGE performance among all the participating teams.
45,"In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks. We show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture.","In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks. We propose several novel models that address critical problems in summarization. We also propose a new dataset consisting of multi-sentence summaries."
46,"shallow machine learning models including Bagging tree (BT), Support vector machine (SVM), linear discriminant analysis (LDA) and Bayesian linear discriminant analysis (BLDA) models were used to make emotional binary classification experiments on DEAP datasets in valence and arousal dimensions. Deep CNN models which require no feature engineering achieved the best recognition performance on temporal and frequency combined features in both valence and arousal dimensions.","An EEG emotional feature learning and classification method using deep Convolution Neural Network (CNN) was proposed based on temporal features, frequential features and their combinations of EEG signals. The experimental results showed that the deep CNN models which require no feature engineering achieved the best recognition performance on temporal and frequency combined features."
47,"This paper proposes a novel Adversarial Reinforcement Learning architecture for Chinese text summarization. We use a generator to generate summaries, a discriminator to distinguish between generated summaries and real ones. Experimental Results showed that our model significantly outperforms previous deep learning models on rouge score.","This paper proposes a novel Adversarial Reinforcement Learning architecture for Chinese text summarization. In our model, we use a generator to generate summaries, a discriminator to distinguish between generated summaries and real ones, and reinforcement learning to evolve the generator. Experiments were run on two Chinese corpora, respectively consisting of long documents and short texts."
48,"the next generation of mobile systems, 5G, will be the communication standard that accommodates the proliferation of the Internet of Things (IoT) Unmanned aerial vehicles (UAVs) are envisioned to support many applications in providing 5G connectivity to the IoT. The management of UAVs induces high exchange of control messages with the ground control station, resulting in the crowded spectrum used by cellular networks.","The next generation of mobile systems, 5G, will be the communication standard that accommodates the proliferation of the Internet of Things (IoT) Unmanned aerial vehicles (UAVs) are envisioned to support many applications in providing 5G connectivity to the IoT. The authors raise the problem of degrading the network spectrum with UAVs' management messages."
49,"Ambient backscatter communication technology has been introduced recently, and is quickly becoming a promising choice for self-sustainable communication systems. By leveraging existing RF signal resources, ambient backscatter technology can support sustainable and independent communications. We study an integration of ambient backscatter with wireless powered communication networks (WPCNs)",Ambient backscatter communication technology can support sustainable and independent communications. It can open up a whole new set of applications that facilitate the Internet of things (IoT) The authors propose a novel hybrid transmitter design by combining the advantages of both ambient backscattering and wireless powered communications.
50,Android forensics is one of the most studied topics in the mobile forensics literature. There does not appear to have a formal model that captures activities undertaken during a forensic investigation. We adapt a widely used adversary model from the cryptographic literature to formally capture a forensic investigator's capabilities.,Android forensics is one of the most studied topics in the mobile forensics literature. There does not appear to have a formal model that captures the activities undertaken during a forensic investigation. We adapt a widely used adversary model from the cryptographic literature to capture a forensic investigator's capabilities. We demonstrate the utility of the model using five popular Android social apps.
51,"Different methodologies are developed till now depending upon several parameters to find the summary. The relevance of the sentences within the text is derived by Simplified Lesk algorithm and WordNet, an online dictionary. The proposed approach gives around 80% accurate results on 50% summarization of the original text.","Automatic summarization has been a hot topic in the field of natural language processing. In this paper, we introduce LexRank to Chinese texts to make up for the deficiency of LexRank. We propose an approach of extractive summarization for Chinese text based on the combination of spectral clustering and Lexrank."
52,This paper describes an approach to generic Bengali text summarization using latent semantic analysis (LSA) Our proposed LSA based single document summarization method uses the latent semantic analysis technique to identify semantically important sentences. The results demonstrate that our proposed Bengali text summarization approach is effective.,This paper describes an approach to generic Bengali text summarization using latent semantic analysis. We have compared our proposed approach with some state-of-the-art.
53,early risk identification of an unexpected sudden cardiac death (SCD) in a person who is suffering malignant ventricular arrhythmias is highly significant for timely intervention and increasing the survival rate. We have presented an automated strategy for prediction of SCD with a high-level accuracy by using measurable arrhythmic markers in this paper. The effectiveness and usefulness of the proposed method is evaluated using a database of measured ECG data acquired from 28 SCD and 18 normal patients.,Early risk identification of an unexpected sudden cardiac death (SCD) is highly significant for timely intervention and increasing survival rate. We present an automated strategy for prediction of SCD with a high-level accuracy by using measurable arrhythmic markers. The automated strategy can predict SCD in less than one second with an average accuracy of 98.91%. The method could be more practical and efficient if applied in portable smart devices with real-time requirements in hospital settings or at home.
54,"Security defects are common in large software systems because of their size and complexity. We propose an automatic vulnerability classification framework based on conditions that activate vulnerabilities. Different machine learning techniques (Random Forest, C4.5 Decision Tree, Logistic Regression, and Naive Bayes) are employed to construct a classifier with the highest F-measure in labelling an unseen vulnerability by the framework.",Developers need to know more about characteristics and types of residual vulnerabilities in systems to adopt suitable countermeasures in current and next versions. We propose an automatic vulnerability classification framework based on conditions that activate vulnerabilities. We evaluate the effectiveness of the classification by analysing 580 software security defects of the Firefox project.
55,"EEG signals, recording abnormal discharge of neurons in the brain, are widely used in epilepsy detection. In practical application, sparse auto-encoder can get all the significant information at lower sample rate than sampled by Nyquist sampling principle. Experimental result demonstrate that the classification rates in this work outperform the current state-of-the-art EEG seizure detection methods.",An EEG signal classification method based on sparse auto-encoders and support vector machine (SVM) is proposed to greatly reduce the sample rate and enhance the efficiency of the vision detection. The classification rates in this work outperform the current state-of-the-art EEG seizure detection methods.
56,paper proposes an effective method to extract salient sentences using contextual information and statistical approaches for text summarization. The proposed method combines two consecutive sentences into a bi-gram pseudo sentence so that contextual information is applied to statistical sentence-extraction techniques. Salient bi-gram pseudo sentences are first selected by the statistical sentence-extraction techniques.,Proposed method combines two consecutive sentences into a bi-gram pseudo sentence. contextual information is applied to statistical sentence-extraction techniques. The proposed method showed better performance than other sentence- Extraction methods in both single- and multi-document summarization.
57,There are a lot of techniques and methods that are proposed for text summarization. The aim of this study is to present an idea to combine Sequential Pattern Mining (SPM) and Deep Learning (DL) for better text summarization process and result.,The aim of this study is to present an idea to combine Sequential Pattern Mining (SPM) and Deep Learning (DL) for better text summarization process and result. The findings of the study are presented as a logical design and mapping of current text representation that can be implemented.
58,"Electroencephalogram (EEG) signals can reveal many important features of our thought which make it as a better tool for deception detection. In this paper, we proposed a deep learning based classification of EEG signals for the given visual stimuli. By training the model properly we got a mean accuracy of 82.21% which is far better than the models using conventional machine learning methods.","Brain signals were started to use in deception detection process from last few years. Electroencephalogram (EEG) signals can reveal many important features of our thought which make it as a better tool for deception detection. In this paper, we proposed a deep learning based classification of EEG signals for the given visual stimuli."
59,"Based on electroencephalography (EEG) signals, a classification model was developed to identify whether a subject has a malicious intention under scenarios of being forced to become an insider threat. The model also distinguishes insider threat scenarios from everyday conflict scenarios with 93.47% accuracy. These findings could be utilized to support the development of insider threat mitigation systems along with existing trustworthiness assessments in the nuclear industry.","This study proposes a scheme to identify insider threats in nuclear facilities through the detection of malicious intentions of potential insiders. Based on electroencephalography (EEG) signals, a classification model was developed to identify whether a subject has a malicious intention. By using EEG signals obtained while contemplating becoming an insider threat, the subject-wise model identified malicious intentions with 78.57% accuracy."
60,"an optimal nonlinear feature extractor for extracting energy features under two different kinds of patterns is proposed. It carries out the simultaneous diagonalization of two signal covariance matrices in a high-dimensional kernel transformed space. Two operations, whitening transform and projection transform, are involved in kernel spaces.",An optimal nonlinear feature extractor for extracting energy features under two different kinds of patterns is proposed. It carries out the simultaneous diagonalization of two signal covariance matrices in a high-dimensional kernel transformed space.
61,"Cloud computing has become one of the most game changing technologies in the recent history of computing. Due to its infancy, it encounters challenges in strategy, capabilities, as well as technical, organizational, and legal dimensions. Cloud service providers and customers do not yet have any proper strategy or process that paves the way for a set procedure on how to investigate or go about issues within the cloud.","Cloud service providers and customers do not yet have any proper strategy or process that paves the way for a set procedure on how to investigate or go about the issues within the cloud. This paper provides an overview of the emerging field of cloud forensics and highlights its capabilities, strategy, investigation, challenges, and opportunities."
62,It is very difficult for human beings to summarize manually large documents of text. Nowadays text summarization is one of the most favorite research areas in natural language processing. There are also much more close relationships between text mining and text summarization.,"Text summarization is the process of automatically creating and condensing form of a given document. It is difficult for human beings to summarize manually large documents of text. Text mining and text summarization are close relationships, according to this study."
63,"This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector. Summaries are then generated by assembling the highest ranked sentences.","Artex is another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence. Summaries are then generated by assembling the highest ranked sentences."
64,"Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. Major disease areas that use AI tools include cancer, neurology and cardiology.","Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data. Popular AI techniques include machine learning methods for structured data and natural language processing for unstructured data."
65,"Brain Computer Interface (BCI) controlled assistive technology is the new paradigm, providing assistance and rehabilitation for the paralyzed. Most of these devices are error prone and also hard to get continuous control because of the dynamic nature of the brain signals. Artificial Muscle Intelligence with Deep Learning (AMIDL) system is proposed in this paper. AMIDL integrates user intentions with artificial muscle movements in an efficient way to improve the performance.","Artificial Muscle Intelligence with Deep Learning (AMIDL) system is proposed in this paper. Human thoughts captured using Electroencephalogram (EEG) sensors are transformed into body movements. The system also provides a feature for communicating human intentions as alert message to caregivers. The proposed system will be a great communication aid for paralyzed to express their thoughts and feelings with dear and near ones, thereby enhancing the quality of life."
66,this work investigates user attitudes towards personalized summaries generated from a coarse-grained user model based on document aspects. We explore user preferences for summaries at differing degrees of fit with their stated interests.,This work investigates user attitudes towards personalized summaries. It uses a coarse-grained user model based on document aspects.
67,"extractive summarization approaches have limitations, possibly not fully capturing the informativeness of a text. A potential strategy to address this problem is the adoption of sentence simplification methods. Fifteen sentence scoring methods for summarization are applied in combination with the simplification methods to a corpus of 1,038 news articles in English.","Automatic text summarization can be useful to sieve relevant content from the Internet and digital libraries. However, it may not fully capture the informativeness of a text. A potential strategy to address this problem is the adoption of sentence simplification methods."
68,"Natural Language Processing (NLP) is a field of computer science and linguistics, concerned with the unique conversation between computers and human languages. It processes data through Lexical analysis, Syntax analysis, Semantic analysis, Discourse processing and Pragmatic analysis. This paper presents an automatic text summarizer for text documents using soft computing approach.","An intelligent text summarization is one of the most challenging tasks in Natural language processing. This paper presents an automatic text summarizer for text documents using soft computing approach. It processes data through POS Tagger, NLP Parser, ambiguity removal, Semantic Representation, Sentence Reduction and Sentence Combination. The summarizer was tested on the standard DUC 2007 dataset as well as a corpus of hundred text documents."
69,"With the explosion of the abundant data present on social media, it has become important to analyze this text for seeking information. Opinions play a pivotal role in decision making in the society. Other's opinions and suggestions are the base for an individual or a company while making decisions.","Text Summarization is condensing of text such that, redundant data are removed and important information is extracted. Opinions play a pivotal role in decision making in the society. In this paper, we propose a graph based technique that generates summaries of redundant opinions."
70,Electroencephalography (EEG) is one of the most promising methods in the field of Brain-Computer Interfaces (BCIs) One of the major challenges for EEG signal analysis is the small size of its datasets. We propose a novel generative adversarial network (GAN) model that can learn the statistical characteristics of the EEG signal and augment its datasets size to enhance the performance of classification models. Results show that the proposed model significantly outperforms other generative models on the utilized EEG dataset.,Electroencephalography (EEG) is one of the most promising methods in the field of Brain-Computer Interfaces (BCIs) One of the major challenges for EEG signal analysis is the small size of its datasets. This challenge can limit the performance of EEG signal classification models.
71,"Modern organizations deals with terabytes of text, such as email, that often plays a significant role in their day-to-day operations. Even small and medium-sized organizations are dealing with growing volumes of text that require rapid access and meaningful analysis. One possible means is to use text categorization and summarization.","Modern organizations deals with terabytes of text, such as email, that often plays a significant role in their day-to-day operations. Identifying useful information from these data is quite difficult and requires some mechanism. One possible means is to use text categorization and summarization."
72,limited research has been completed to yield suitable solutions for Arabic news. We developed software for browsing a collection of about 237K Arabic news articles. We developed tailored stemming and automatic classification methods to work with the taxonomy.,"Arabic news articles in electronic collections are difficult to work with. Browsing by category is rarely supported. We developed tailored stemming (i.e., a new Arabic light stemmer) and automatic classification methods. We showed that our approach to stemming and classification is superior to state-of-the-art techniques."
73,This paper deals with the development of an extraction based summarization technique which works on Bangla text documents. The system summarizes a single document at a time. 83.57% of summary sentences selected by the system agreed with those made by human professionals.,A new technique has been proposed for Bangla text summarization. The system summarizes a single document at a time. Attributes like cue words and skeleton of the document are included in the process to make the summary more relevant to the content. The proposed technique was compared with summary of documents generated by humans.
74,our main aim is to build a classifier that is capable of classifying newly incoming bug reports into two predefined classes: corrective (defect fixing) report and perfective (major maintenance) report. The results of the proposed feature set achieved high accuracy in classification with SVM classification algorithm reporting an average accuracy of (93.1%) on three different open source projects.,"We target the problem of software bug reports classification. Our main aim is to build a classifier that is capable of classifying newly incoming bug reports into two predefined classes. This helps maintainers to quickly understand these bug reports and hence, allocate resources for each category."
75,Electroencephalograms (EEG) are widely used to detect epilepsy accurately. Visual inspection of the EEG signal by observing a change in frequency or amplitude in long-duration signals is an arduous task for the clinicians. The proposed methodology focuses on automated detection of epilepsy using a novel stop-band energy (SBE) minimized orthogonal wavelet filter bank.,Epilepsy is a neural disorder that is associated with the central nervous system. Electroencephalograms (EEG) are widely used to detect epilepsy accurately. The interpretation of a particular type of abnormality using the EEG signal is a subjective affair and may vary from clinician-to-clinician. The proposed methodology focuses on automated detection of epilepsy using a novel stop-band energy filter bank.
76,Writing an abstract requires a conscientious analysis since the contents would affect both the readers' interestness and disinterestedness on a particular or overall research topic. The aim of this study is constructing automation for summarizing Indonesian articles as an alternative approach to an abstract.,"In a scientific work, an abstract always contains main information of an article including at least a researched problem, aim(s), methodology, and result of the study. People generally write manually by summarizing the article. This study is constructing automation for summarizing Indonesian articles as an alternative approach to an abstract."
77,"not all violent events are reported in the news, and while monitoring only news agencies is sufficient for projects such as ICEWS which have a global focus, more news sources are required when assessing a local situation. We used WhatsApp as a news source to identify the occurrence of violent incidents in South Africa. Using machine learning, we have shown how violent incidents can be coded and recorded.","Machine learning can be used to extract events from news sources for quantitative analysis. We used WhatsApp as a news source to identify the occurrence of violent incidents in South Africa. Using machine learning, we have shown how violent incidents can be coded and recorded. This allows for a local level recording of these events over time."
78,"stress has been identified as one of the contributing factors to vehicle crashes which create a significant cost. It is essential to build a practical system that can detect drivers' stress levels in real time with high accuracy. Most of the current works use traditional machine learning techniques to fuse multimodal data at different levels (e.g., feature level) to classify drivers' stress levels. The results show that the proposed model outperforms model built using the traditional machine learning techniques based on handcrafted features.","Stress has been identified as one of the contributing factors to vehicle crashes. Motivated by the need to address the significant costs of driver stress, it is essential to build a practical system that can detect drivers' stress levels. A driver stress detection model often requires data from different modalities, including ECG signals, vehicle data and contextual data. We propose a multimodal fusion model based on convolutional neural networks and long short-term memory."
79,"Program comprehension is an imperative prerequisite for several software tasks, including testing, maintenance, and evolution. The implementation of a software system can be captured using a call graph. We present an innovative approach that can automatically construct and visualize the static call graph for a system written in Python.","Understanding the software system requires investigating the high-level system functionality and mapping it to its low-level implementation. This manual mapping process is expensive, time-consuming and creates a cognitive gap between the system's overall functionality and its implementation. In this paper, we present an innovative approach that can automatically construct and visualize the static call graph for a system written in Python."
80,"Summarization is the process of reducing a text document to create a summary that retains the most important points of the original document. Most extractive summarization techniques revolve around the concept of finding keywords and extracting sentences that have more keywords than the rest. In this paper, we proposed an algorithm to extract keyword automatically for text summarization in e-newspaper datasets.","Summarization is the process of reducing a text document to create a summary that retains the most important points. In this paper, we propose an algorithm to extract keywords automatically for text summarization in e-newspaper datasets."
81,"we conducted a case study of opinions in Stack Overflow using a benchmark dataset of 4522 sentences. We observed that opinions about diverse API aspects (e.g., usability) are prevalent and offer insights that can shape developers' perception and decisions related to software development. We built a suite of techniques to automatically mine and categorize opinions about APIs from forum posts.","We observed that opinions about diverse API aspects are prevalent in Stack Overflow forums. We built a suite of techniques to automatically mine and categorize opinions from forum posts. Opiner is available online as a search engine, where developers can search for APIs by their names."
82,academic research is crucial to the development of science and technology and is an important factor that affects national strength. The highly complex deep learning operations can correctly generate sequences and find correlations between sequences. The method proposed in this study improves the accuracy compared with traditional text extraction methods.,"Academic research is crucial to the development of science and technology. When writing an academic research paper, a rhetorical structure is typically used to present the paper's ideas. Some studies have adopted text mining to assist with the writing, but the existing methods still require human intervention."
83,"Using design patterns in software development improves the product's quality, understandability and productivity. The paper proposes a methodology for automatic selection of the fit design pattern from a list of patterns. The recommended design pattern is the closest to the problem scenario.",Design patterns are a reusable solution to a commonly occurring design problem in certain context. Paper proposes a methodology for automatic selection of the fit design pattern from a list of patterns. The proposed methodology is based on Text retrieval approach.
84,"goal is to develop a deep learning-based method by using single channel electroencephalogram (EEG) that automatically exploits the timefrequency spectrum of EEG signal. The timefrequency RGB color images for EEG signal are extracted using continuous wavelet transform (CWT) The transfer learning of a pre-trained convolution neural network, squeezenet is employed to classify these CWT images into its sleep stages.","Deep learning-based method that exploits the timefrequency spectrum of EEG signal. The method can achieve near state of the art accuracy even using single channel EEG signal, they say."
85,"Smartphones and tablets provide access to the Web anywhere and anytime. Automatic Text Summarization techniques aim to extract the fundamental information in documents. Making automatic summarization work in portable devices is a challenge, in several aspects.",Summarization techniques aim to extract the fundamental information in documents. This paper presents an automatic summarization application for Android devices.
86,"system implements sentence-based extractive summarization technique, which consists in determining most important sentences in document due to their computed salience. The presented attempt is intended to serve as the baseline for future solutions.",This paper describes the automatic summarization system developed for the Polish language. The system implements sentence-based extractive summarization technique. The presented attempt is intended to serve as the baseline for future solutions.
87,"Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents.",Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate.
88,Lexical chains is a technique for identifying semantically-related terms in text. Lexical chains is a technique for identifying semantically-related terms in text. The resulting concept chains are then used to identify candidate sentences useful for extraction.,Concept chains are a technique for identifying semantically-related terms in text. The resulting concept chains are then used to identify candidate sentences useful for extraction. The goal is to create an efficient tool that is able to summarize large documents automatically.
89,research about text summarization has been quite an interesting topic over the years. This paper's goal is to define a measurement for text summarization using Semantic Analysis Approach for Documents in Indonesian language. The applied measurement requires Indonesian version of WordNet which had been implemented roughly.,This paper's goal is to define a measurement for text summarization using Semantic Analysis Approach for Documents in Indonesian language. The applied measurement requires Indonesian version of WordNet which had been implemented roughly.
90,"Reducing text without losing the meaning not only can save time to read, but also maintain the reader's understanding. One of many algorithms to summarize text is TextTeaser. This algorithm calculates four elements, such as title feature, sentence length, sentence position and keyword frequency.","Reducing text without losing the meaning not only can save time to read, but also maintain the reader's understanding. One of many algorithms to summarize text is TextTeaser. Originally, this algorithm is intended to be used for text in English."
91,Wikipedia articles are given as input to system and extractive text summarization is presented by identifying text features and scoring the sentences accordingly. Two novel approaches implemented are using the citations present in the text and identifying synonyms. The scores are used to classify the sentence to be in the summary text or not with the help of a neural network.,"Text summarization systems are used to identify the most important information from the given text. In this paper, Wikipedia articles are given as input to system. Text is pre-processed to tokenize the sentences and perform stemming operations. We then score the sentences using the different text features."
92,It is first time that this system has been developed for Punjabi language and is available online at: http://pts.learnpunjabi.org/. Punjab is one of Indian states and Punjabi is its official language. Various linguistic resources for Punjabi were also developed first time as part of this project. High scored sentences in proper order are selected for final summary.,This paper concentrates on single document multi news Punjabi extractive summarizer. Selection of sentences is on the basis of statistical and linguistic features of sentences. Various linguistic resources for Punjabi were also developed first time as part of this project. It is first time that this system has been developed for Punjabi language and is available online at: http://pts.learnpunjab.org/. Punjab is one of Indian states and Punjabi is its official language.
93,This paper focuses on the automatic text summarization by sentence extraction with important features based on fuzzy logic. We propose a method using fuzzy logic for sentence extraction and compare our result with the baseline summarizer and Microsoft Word 2007 summarizers.,"This paper focuses on the automatic text summarization by sentence extraction with important features based on fuzzy logic. The results show that the highest average precision, recall, and F-mean for the summaries are conducted from fuzzy method."
94,"Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches.","Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet. Researchers have been trying to improve ATS techniques since the 1950s. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical."
95,Automatic Text Summarization is a Natural Language Processing task which has experienced great development in recent years. We need methods and tools that help users to manage large amounts of information. It is the creation of a shortened version of a text by a computer program.,Text Summarization is the creation of a shortened version of a text by a computer program. The product of this procedure still contains the most important points of the original text.
96,This paper proposes a trust model called Implicit Web of Trust in VANET (IWOT-V) to reason out the trustworthiness of vehicles. The idea of IWOT-V is mainly inspired by web page ranking algorithms such as PageRank. The performance of Cooperative Intelligent Transport System (C-ITS) applications improves.,A trust model called Implicit Web of Trust in VANET (IWOT-V) is used to reason out the trustworthiness of vehicles. The idea of IWot-V is mainly inspired by web page ranking algorithms such as PageRank. The simulation results show that IWOT -V can accurately identify trusted and untrusted nodes.
97,"Currently, the world is witnessing a mounting avalanche of data due to the increasing number of mobile network subscribers, Internet websites, and online services. Big data analytics can process large amounts of raw data and extract useful, smaller-sized information. In this paper, we conduct a survey on the role that big data analytics can play in the design of data communication networks.","Big data analytics can process large amounts of raw data and extract useful, smaller-sized information. Integrating big data analytics with the networks' control/traffic layers might be the best way to build robust data communication networks. This is the first survey that addresses the use of big data Analytics techniques for the design of a broad range of networks."
98,"health care industry has not fully grasped the potential benefits to be gained from big data analytics. This study examines the historical development, architectural design and component functionalities of big data. We also mapped the benefits driven by big data in terms of IT infrastructure, operational, organizational, managerial and strategic areas.","Health care industry has not fully grasped the potential benefits to be gained from big data analytics. A study examines the historical development, architectural design and component functionalities of big data. It also maps the benefits driven by big data in terms of IT infrastructure, operational, organizational and managerial areas."
99,"With the exponential growth of information in World Wide Web, extracting relevant information from huge amount of data has become a critical task. Text summarization has been appeared as one of the solution to such problem. In this paper, a comparative analysis of few meta-heuristic approaches such as Cuckoo Search (CS), Cat Swarm Optimization (CSO), Harmony Search (HS), and Differential Evolution (DE) algorithm is presented for single document summarization problem.","Text summarization has been appeared as one of the solution to text search problem. The main objective is to retrieve a condensed document that pertain the original information. In this paper, a comparative analysis of few meta-heuristic approaches such as Cuckoo Search is presented."
100,Lexical chaining is a technique for identifying semantically-related terms in text. We propose concept chaining to link semantically-related concepts within biomedical text together. Precision is measured at 0.90 and recall at 0.92.,Concept chaining is a technique for identifying semantically-related terms in text. It is proposed that concept chaining could be used in biomedical text to link concepts together. The resulting concept chains are then used to identify candidate sentences useful for extraction. The extracted sentences are used to produce a summary of the biomedical text.
101,"Given the increasing number of documents, sites, online sources, and the users' desire to quickly access information, automatic textual summarization has caught the attention of many researchers in this field. This study selects extractive method out of different summarizing methods (e.g. abstract method). A summarization issue would be unsolvable by exact methods in a reasonable time with considering documents with high amount of information.","Extractive method involves summarizing text through objective extraction of some parts of a text like word, sentence, and paragraph. A summarization issue would be unsolvable by exact methods in a reasonable time with considering documents with high amount of information. A biogeography - based optimization algorithm (BBO) is used in this article."
102,"Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.","High dimensionality is critical to high performance, say the authors. A 100K-dim feature can achieve significant improvements over both its low-dimensional version and the state-of-the-art. With our proposed sparse projection method, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality."
103,we describe a general method for building cascade classifiers from part-based deformable models such as pictorial structures. We focus primarily on the case of star-structured models and show how a simple algorithm based on partial hypothesis pruning can speed up object detection by more than one order of magnitude.,"A simple algorithm based on partial hypothesis pruning can speed up object detection. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. We outline a cascade detection algorithm for a general class of models defined by a grammar formalism."
104,"Life cycle and chain of digital evidence are very important parts of digital investigation process. Investigators and expert witness must know all details on how the evidence was handled every step of the way. This paper presents a basic concept of ""chain of custody of digital evidence"" and ""life cycle of digital evidence"".",Life cycle and chain of digital evidence are very important parts of digital investigation process. Investigators and expert witness must know all details on how the evidence was handled every step of the way.
105,Experimental results show that the combination of summarization technology and classification technology can not only reduce the time of feature selection and classification but also improve the performance of text classification.,"Two approaches to text classification based on summarization technique are proposed. In the first approach, the heuristic rules of auto-summarization are used to select and weight features for every category."
106,high accuracy for the classification of electroencephalogram (EEG) signal is an important basis for a brain-computer interface (BCI) system. We proposed a novel approach to enhance the classification performance in identifying EEG signals. The experimental results on EEG signals of motor imagery indicate that the proposed method is able to achieve a classification accuracy of 91.13%.,High accuracy for the classification of electroencephalogram (EEG) signal is an important basis for a brain-computer interface (BCI) system. The proposed method is able to achieve a classification accuracy of 91.13%. Using this method might enhance the performance of a BCI system.
107,ELM is used to classify five mental tasks from different subjects using electroencephalogram (EEG) signals available from a well-known database. Performance of ELM is compared in terms of training time and classification accuracy with a Backpropagation Neural Network (BPNN) classifier and also Support Vector Machines (SVMs) The classification accuracy of ELM is similar to that of SVMs and BPNN.,"A machine learning algorithm called Extreme Learning Machine (ELM) is used to classify five mental tasks from different subjects. ELM needs an order of magnitude less training time compared with SVMs, the study says. smoothing of the classifiers' outputs can significantly improve their classification accuracy."
108,"use of electroencephalography (EEG) signals for motor imagery based brain-computer interface (MI-BCI) has gained widespread attention. Deep learning has also gained widespread attention and used in various application such as natural language processing, computer vision and speech processing. The effectiveness of the proposed framework has been evaluated using dataset IVa of the BCI Competition III.",Deep learning has been rarely used for MI EEG signal classification. The proposed framework outperforms all other competing methods in terms of reducing the maximum error. The framework can be used for developing BCI systems using wearable devices as it is computationally less expensive and more reliable.
109,"Graph based methods such as TextRank have been used for sentence extraction from news articles. These methods model text as a graph with sentences as nodes and edges based on word overlap. The spontaneous speech in meetings leads to incomplete, ill-formed sentences with high redundancy.","Spontaneous speech in meetings leads to incomplete, ill-formed sentences with high redundancy. We propose an extension of the TextRank algorithm that clusters the meeting utterances and uses these clusters to construct the graph."
110,"there is a lack of software tools that can automatically analyze the Python source-code and construct its static call graph. We introduce a prototype Python tool, named code2graph, which automates the tasks of (1) analyzing the Python source-code and extracting its structure.","A static call graph is an imperative prerequisite used in most interprocedural analyses and software comprehension tools. Currently, there is a lack of software tools that can automatically analyze the Python source-code. Code2graph is a prototype tool that automates the tasks of analyzing Python source code and constructing static call graphs."
111,"fluency of the produced summaries has been mostly ignored. Diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered.","In citation-based summarization, text is leveraged to identify important aspects of a target paper. Previous work has focused on extraction aspect of summaries, but not on fluency or readability. In this work, we present an approach for producing readable and cohesive summaries."
112,"intuition behind the approach is to conceive the justification as a summary of the most relevant and distinguishing aspects of the item, automatically obtained by analyzing its reviews. We designed a pipeline of natural language processing techniques including aspect extraction, sentiment analysis and text summarization.","In this paper we present a methodology to justify recommendations that relies on the information extracted from users' reviews discussing the available items. The approach is able to make the recommendation process more transparent, engaging and trustful for the users."
113,"Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease from the heart disease data set. The classification function algorithms used and tested in this work are Logistics, Multi Layer Perception and Sequential Minimal Optimization algorithms.","Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. Various data mining techniques are available for predicting diseases. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease."
114,"In this paper, a comparative study of two population-based stochastic optimization techniques has been proposed for document summarization. It specifies the relationship among sentences based on similarity and minimizes the weight of each sentence to extract summary sentences at different compression level.",Intelligent-based techniques have developed in literature for document summarization. Study of two population-based stochastic optimization techniques has been proposed. It specifies the relationship among sentences based on similarity.
115,"Summarization techniques in Tamil, Kannada, Odia, Bengali, Punjabi and Gujarathi are taken for the purpose of comparison.",Summarization has been an area of interest since many years. This paper presents a comparison of various text summarization methods seen in Indian languages. Sample text consisting of three sentences is taken as an example.
116,Choosing a proper color space is a very important issue for color image segmentation process. In this paper a comparative analysis is performed between these two color spaces with respect to color image segmentation. HSV color space is performing better than L*A*B*.,Choosing a proper color space is a very important issue for color image segmentation process. L*A*B* and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces.
117,paper compares algorithms for extractive summarization of microblog posts. We present two algorithms that produce summaries by selecting several posts from a given set. We evaluate the generated summaries by comparing them to both manually produced summaries and summaries produced by several leading traditional summarization systems.,This paper compares algorithms for extractive summarization of microblog posts. We evaluate the generated summaries by comparing them to both manually produced summaries and summaries produced by several leading traditional summarization systems.
118,locating captured cells in the device by manual counting bottlenecks data processing by being tedious. We have trained four ML algorithms on three different datasets. The trained ML algorithms locate and classify thousands of possible cells in a few minutes rather than a few hours.,Locating captured cells in the device by manual counting bottlenecks data processing. Some recent work has been done to automate the cell location and classification process. The trained ML algorithms locate and classify thousands of possible cells in a few minutes rather than a few hours. Optimal algorithm selection depends on the peculiarities of the individual dataset.
119,the reliability of electrocardiography (ECG)-independent algorithm for pressure-only data is essential. The main purpose of developing a new ECG-independent algorithm was to raise the detection rates over the entire heart cycle despite irregular heartbeats. Both algorithms provided nearly identical values of iFR or dPR without systemic bias.,"Recently, instantaneous wave-free ratio (iFR) or diastolic pressure-ratio (dPR) have been used in practice. Study sought to compare the current to a new ECG-independent algorithm for calculating resting physiologic indices. Both algorithms provided nearly identical values of iFR or dPR without systemic bias."
120,The popularization of social networks and digital documents has quickly increased the multilingual information available on the Internet. This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. We describe three compressive CLTS approaches that analyze the text in the source and target languages to compute the relevance of sentences.,This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. Clusters of similar sentences are compressed using a multi-sentence compression (MSC) method and single sentences are compression using a Neural Network model.
121,focus of this paper is to develop a new algorithm to fuse a color visual image and a corresponding IR image for such a concealed weapon detection application. The fused image obtained by the proposed algorithm will maintain the high resolution of the visual image.,Image fusion is studied for detecting weapons or other objects hidden underneath a person's clothing. The focus of this paper is to develop a new algorithm to fuse a color visual image and a corresponding IR image.
122,"traditional NLP models may be inappropriate for programs. We propose a novel tree-based convolutional neural network (TBCNN) for programming language processing. TBCNN outperforms baseline methods, including several neural models for NLP.","Programming language processing (similar to natural language processing) is a hot research topic. Traditional NLP models may be inappropriate for programs, say authors. They propose a novel tree-based convolutional neural network (TBCNN) for programming language processing."
123,"cardiovascular disease (CVD) is the top cause of death worldwide. In the United States alone, 25% of deaths were attributed to heart disease, killing over 630,000 Americans annually. coronary heart disease is the most common, causing over 360,000 American deaths due to heart attacks in 2015. The developed DNN learning model is based on a deeper multilayer perceptron architecture with regularization and dropout using deep learning.","Heart disease is the top cause of death worldwide, killing over 17 million people a year. In the U.S., 25% of deaths are attributed to heart disease. An enhanced deep neural network (DNN) learning was developed to aid patients and healthcare professionals. The models can be used to aid healthcare professionals and patients throughout the world to advance both public health and global health, especially in developing countries and resource-limited areas."
124,"The propaganda efforts of new extremist groups include creating new propaganda videos from fragments of old terrorist attack videos. This article presents a web-scraping method for retrieving relevant videos. Automatic novelty verification is now possible, which can potentially reduce and improve journalist research work.",Extremists create new propaganda videos from fragments of old terrorist attack videos. pHash-based algorithm identifies the original content of a video. Automatic novelty verification is now possible.
125,"the use of intelligent technologies in clinical decision making in the telehealth environment has begun to play a vital role in improving the quality of patients' lives. In this paper, an effective medical recommendation system that uses a fast Fourier transformation-coupled machine learning ensemble model is proposed for short-term disease risk prediction. The experimental results show that the proposed system yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations.",Intelligent technologies have begun to play a vital role in improving the quality of patients' lives and helping reduce the costs and workload involved in their daily healthcare. A new system that uses a fast Fourier transformation-coupled machine learning ensemble model is proposed for short-term disease risk prediction. The proposed system yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations.
126,Evaluation of operator Mental Workload (MW) levels via ongoing electroencephalogram (EEG) is quite promising in Human-Machine (HM) collaborative task environment to alarm the temporal operator performance degradation. An adaptive Stacked Denoising AutoEncoder (SDAE) is developed to tackle such cross-session MW classification task. The weights of the shallow hidden neurons could be adaptively updated during the testing procedure.,Evaluation of Mental Workload (MW) levels via ongoing electroencephalogram (EEG) is quite promising in Human-Machine (HM) collaborative task environment. Recognition ofMW states via a static pattern classifier with training and testing EEG signals recoded on separate days is particularly challenging. An adaptive Stacked Denoising AutoEncoder (SDAE) is developed to tackling such cross-session MW classification task.
127,The Brain-Computer Interface (BCI) is a system able to serve as a mean of communication between machine and human. One of the most used brainwaves is the sensorimotor rhythm (SMR) which appears for real or imagined motor movement. Deep learning approaches permit the processing of the raw data without any transformation.,Brain-Computer Interface (BCI) is a system able to serve as a mean of communication between machine and human. Brainwaves are the control signals acquired by electroencephalography (EEG) One of the most used brainwaves is the sensorimotor rhythm (SMR) which appears for real or imagined motor movement.
128,"Using an MRC model trained on the SQuAD1.1 dataset as a core system component, we first build an extractive query-based summarizer. We further leverage pre-trained machine translation systems to abstract our extracted summaries. Our models achieve state-of-the-art results on the publicly available CNN/Daily Mail and Debatepedia datasets.",Machine reading comprehension (MRC) and query-based text summarization are two related tasks. We build a system that transfers knowledge between the two tasks. Our models achieve state-of-the-art results on the publicly available CNN/Daily Mail and Debatepedia datasets.
129,"Based on RSU and V2V schemes, a vehicle movement database is established, and sequential pattern data mining is carried out. The support and confidence of the movement rules generated by vehicle routing patterns are calculated to extract the probability of frequent driving trajectories.","According to the characteristics of vehicle sequential pattern, the topology of vehicular ad hoc network (VANET) is linked to the real road vehicle movement trajectory. Some new definitions related to sequential patterns in VANET environment are proposed."
130,Deep learning has the advantage of approximating the complicated function and alleviating the optimization difficulty associated with deep models. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network which takes advantages of deep learning and extreme learning machine.,"Deep learning has aroused wide interest in machine learning fields. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network. We apply MLELM to EEG classification in this paper. By simulating and analyzing the results of the experiments, the effectiveness of DELM in EEG classification is confirmed."
131,Data mining is the process of data analyzing from various perspectives and combining it into useful information. This technique is used for finding heart disease. Based on risk factor the heart diseases can be defined very easily.,"This technique is used for finding heart disease. Based on risk factor the heart diseases can be defined very easily. Compared to KNN, Convolution Neural Network provides better performance."
132,"DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets) When learned as classifiers to recognize about 10; 000 face identities in the training set, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. 97:45% verification accuracy on LFW is achieved with only weakly aligned faces.",This paper proposes to learn a set of high-level feature representations through deep learning. The proposed features are extracted from various face regions to form complementary and over-complete representations. 97:45% verification accuracy on LFW is achieved with only weakly aligned faces.
133,"Motor Imagery (MI) EEG has attracted us due to its significant applications in daily life. The process began by intensely surfing the well-known specialized digital libraries and, as a result, 40 related papers were gathered. Deep neural networks build robust and automated systems for the classification of MI EEG recordings.",Motor Imagery (MI) EEG has attracted us due to its significant applications in daily life. Deep neural networks build robust and automated systems for the classification of MI EEG recordings by exploiting the whole input data. Convolutional neural networks (CNN) and hybrid-CNN (h-CNN) are the dominant architectures with high performance.
134,"physiological data in the form of 1D signals have yet to be beneficially exploited from this novel approach to fulfil the desired medical tasks. We survey the latest scientific research on deep learning in physiological signal data such as electromyogram (EMG), electrocardiogram (ECG), electroencephalogram (EEG), and electrooculogram (EOG) The objective of this paper is to conduct a detailed study to comprehend, categorize, and compare the key parameters of the deep?learning approaches that have been used in physiological signal analysis for various medical applications.",Deep Learning (DL) has proved its high potential in 2D medical imaging analysis. physiological data in the form of 1D signals have yet to be exploited from this novel approach to fulfil the desired medical tasks. In this paper we survey the latest scientific research on deep learning in physiological signal data such as electromyogram (EMG) and electrocardiogram (ECG).
135,"deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data. Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. Since the medical field of radiology mostly relies on extracting useful information from images, it is a very natural application area for deep learning.","Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks. They have often matched or exceeded human performance. Since radiology relies on extracting useful information from images, it is a natural application area for deep learning. In this article, we review the clinical reality of radiology and discuss the opportunities for application of deep learning algorithms."
136,This paper addresses and tries to solve the problem of extractive text summarization which works by selecting a subset of phrases or sentences from the original document(s) to form a summary. Multilayer ELM (Extreme Learning Machine) which is based on the underlying deep network architecture is trained over this feature set to classify the sentences as important or unimportant.,This paper addresses and tries to solve the problem of extractive text summarization. It works by selecting a subset of phrases or sentences from the original document(s) to form a summary. Selections are done based on certain criteria which formulates a feature set. Multilayer ELM (Extreme Learning Machine) is trained over this feature set.
137,"Alternative splicing (AS) is a fundamental step in mRNA maturation and gene expression. With the recent advances in machine learning, there is an interest in developing accurate deep learning based computational models for AS prediction. We propose a convolutional neural network and multilayer perceptron models to tackle the AS prediction task as classification and regression.",Alternative splicing (AS) is a fundamental step in mRNA maturation and gene expression. Advances in RNA sequencing technologies have shed light on the role of AS in increasing protein isoform diversity. AS is recognized to be involved in the regulation of both physiological and pathological functions.
138,DeePM is a latent graphical model based on the state-of-the-art R-CNN framework. It learns an explicit representation of the object-part configuration with flexible type sharing. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012.,DeePM is a latent graphical model based on the state-of-the-art R-CNN framework. It learns an explicit representation of the object-part configuration with flexible type sharing. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012.
139,"we propose a method for recognizing attributes, such as the gender, hair style and types of clothes of people under large variation in viewpoint, pose, articulation and occlusion typical of personal photo album images. We use a part-based approach based on poselets. Our parts implicitly decompose the aspect (the pose and viewpoint) We train attribute classifiers for each such aspect and combine them together in a discriminative model.","We propose a method for recognizing attributes, such as the gender, hair style and types of clothes of people. We use a part-based approach based on poselets. We train attribute classifiers for each such aspect and combine them together in a discriminative model."
140,"this paper introduces an application research on taxi calling and dispatching system (TCnS in short) The paper gives the basic design, modules division, GIS display algorithm design, test results and feature analysis of TCnS prototype.",This paper introduces an application research on taxi calling and dispatching system (TCnS) It is a prototype of LBS application based on GPS mobile phone.
141,"Weapon detection is a difficult problem with numerous applications, particularly in the world of airport security. We present an attempt to identify pistols in x-ray images using Chamfer Matching. Our approach builds upon the basic Chamfer method to address issues with occlusions.",Chamfer Matching builds upon the basic Chamfer method to address issues with occlusions. It combines results from sub-polygon templates using voting and machine learning.
142,"most evaluations of computer object detection methods focus only on robustness to natural form deformations such as people's pose changes. To determine whether algorithms truly mirror the flexibility of human vision, they must be compared against human vision at its limits. In Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization.","In Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization, sometimes to the point that human vision often fails to recognize them. In this paper, we evaluate existing object detection methods on these abstract renditions of objects, comparing human annotators to four state-of-the-art object detectors on a corpus of Picasso paintings."
143,DoS attack in VANET involves a malicious node flooding a huge amount of traffic using spoofed identities. The detection scheme uses a cuckoo filter and IP detection technique to detect the attack in the network. Once the attack is detected it generates a broadcast message to all the other vehicles that are present in the network.,"VANET is a subset of Mobile Ad-Hoc Networks (MANET) It enables communication between the vehicles (V2V) and vehicles to infrastructure. VANET can be used to coordinate the traffic, improve safety measures, support the drivers for hassle-free driving."
144,"digital forensics starts to show its role and contribution in the society as a solution in disclosure of cybercrime. The essential in digital forensics is chain of custody, which is an attempt to preserve the integrity of digital evidence.",Chain of custody is an attempt to preserve the integrity of digital evidence as well as a procedure for performing documentation chronologically toward evidence. Handling chain of custody has become more complicated and complex. A number of researchers have contributed to provide solutions for the digital chain custody.
145,"law enforcement and digital forensic units must establish and maintain an effective quality assurance system. An acceptable and thorough Digital Forensics (DF) process depends on the sequential DF phases, and each phase depends on sequential DF procedures. The proposed analytical procedure model for digital investigations at a crime scene is developed and defined for crime scene practitioners.","Law enforcement and digital forensic units must establish and maintain an effective quality assurance system. An acceptable and thorough Digital Forensics process depends on the sequential DF phases. Each case is unique and needs special examination, it is not possible to cover every aspect of crime scene digital forensics, but the proposed procedure model is supposed to be a general guideline for practitioners. The proposed analytical procedure model for digital investigations at a crime scene is developed and defined for crime scene practitioners."
146,"Digital forensics has a number of branches and different parts, and image forensics is one of them. The budget for the images branch goes up every day in response to the need.","Digital forensics has a number of branches and different parts, and image forensics is one of them. The budget for the images branch goes up every day in response to the need. We offer general information about digital forensics, focusing on images."
147,Summarization is one of the key features of human intelligence. It plays an important role in understanding and representation. Traditional methods process texts empirically and neglect the fundamental characteristics and principles of language use and understanding. This paper summarizes previous text summarization approaches in a multi-dimensional classification space.,"Summarization is one of the key features of human intelligence. With rapid expansion of texts, pictures and videos in cyberspace, automatic summarization becomes more and more desirable. Traditional methods process texts empirically and neglect the fundamental characteristics and principles of language use and understanding."
148,"This paper presents FEAT, an approach that automatically extracts topoi, which are summaries of the main capabilities of a program. By mining the available source code, possibly augmented with code-level comments, hierarchical agglomerative clustering groups similar code functions.","Topoi are summaries of the main capabilities of a program, given under the form of collections of code functions along with an index. FEAT acts in two steps: Clustering. By mining the available source code, possibly augmented with code-level comments, hierarchical agglomerative clustering groups similar code functions."
149,"Optimization-based design is an effective and promising approach to realizing collective behaviours for robot swarms. Unfortunately, the domain literature often remains vague about the exact role played by the human designer, if any. It is our contention that two cases should be disentangled: semi-automatic design, in which a human designer operates and steers an optimization process.","Optimization-based design is an effective and promising approach to realizing collective behaviours for robot swarms. However, the literature often remains vague about the exact role played by the human designer, if any. In this Perspective, we briefly review the relevant literature and illustrate the hypotheses, characteristics and core challenges."
150,Numerous car accidents have been reported that were caused by distracted drivers. Our aim was to improve the performance of detecting drivers' distracted actions. The developed system involves a dashboard camera capable of detecting distracted drivers through 2D camera images.,"Distracted driver behaviors include texting, talking on the phone, operating the radio, drinking, reaching behind and talking to the passenger. The developed system involves a dashboard camera capable of detecting distracted drivers through 2D camera images."
151,"code comments convey information about the programmers' intention in a more explicit but less rigorous manner than source code. We analyzed more than 450 000 comments across 136 popular open-source software systems coming different domains. We propose an automatic approach to determine whether a method needs a header comment, known as commenting necessity identification.","Code comments convey information about the programmers' intention in a more explicit but less rigorous manner than source code. This information can assist programmers in various tasks, such as code comprehension, reuse, and maintenance. Researchers analyzed more than 450 000 comments across 136 popular open-source software systems."
152,early diagnosis of Alzheimer's disease (AD) is a proceeding hot issue along with a sharp upward trend in the incidence rate. Early diagnosis of AD employing Electroencephalogram (EEG) as a specific hallmark has been an increasingly significant hot topic area. We demonstrate that it can be settled well with multi-task learning strategy based on discriminative convolutional high-order Boltzmann Machine with hybrid feature maps.,"Early diagnosis of Alzheimer's disease (AD) is a proceeding hot issue along with a sharp upward trend in the incidence rate. Recently, early diagnosis of AD employing Electroencephalogram (EEG) as a specific hallmark has been an increasingly significant hot topic area. How to extract more abstract features for better generalization still remains tremendously troubling."
153,"This paper proposes to use deep convolutional neural networks, and deep residual learning, to predict the mental states of drivers from electroencephalography (EEG) signals. We have developed two mental state classification models called EEG-Conv and EEG-Conv-R. Tested on intra- and inter-subject, our results show that both models outperform the traditional LSTM- and SVM-based classifiers.","Driver fatigue is the main cause of traffic accidents, which bring great harm to society and families. This paper proposes to use deep convolutional neural networks to predict the mental states of drivers from electroencephalography (EEG) signals."
154,"how to design an appropriate DL model to accurately and efficiently classify electroencephalogram (EEG) signals is still a challenge. SincNet is an efficient classifier for speaker recognition, but it has some drawbacks in dealing with EEG signals classification. In this paper, we improve and propose a SincNet-based classifier, SincNet-R.","SincNet is an efficient classifier for speaker recognition, but it has some drawbacks in dealing with EEG signals classification. In this paper, we improve and propose a SincNet-based classifier, Sinc net-R, which consists of three convolutional layers and three deep neural network (DNN) layers. We then make use of Sincnet-R to test the classification accuracy and robustness by emotional EEG signals."
155,wavelet-IT2FLS method considerably dominates the comparable classifiers on both datasets. It outperforms the best performance on the Ia and Ib datasets reported in the brain-computer interface (BCI) competition II by 1.40% and 2.27% respectively.,"Nonlinear, noisy and outlier characteristics of electroencephalography (EEG) signals inspire the employment of fuzzy logic. This paper introduces an approach to classify motor imagery EEG signals using an interval type-2 fuzzy logic system (IT2FLS) and wavelet transformation. The proposed approach yields great accuracy and requires low computational cost."
156,Brain Computer Interface (BCI) is the method of communicating the human brain with an external device. People who are incapable to communicate conventionally due to spinal cord injury are in need of Brain Computer Interface. Artificial Neural Network (ANN) is a functional pattern classification technique which is trained all the way through the error Back-Propagation algorithm.,"Brain Computer Interface (BCI) is the method of communicating the human brain with an external device. It uses the brain signals to take actions, control, actuate and communicate with the world directly using brain integration with peripheral devices and systems. Brain waves are in necessitating to eradicate noises and to extract the valuable features."
157,Mixture of experts (ME) is modular neural network architecture for supervised learning. A double-loop Expectation-Maximization (EM) algorithm has been introduced to the ME network structure for detection of epileptic seizure. The detection of epileptiform discharges in the EEG is an important component in the diagnosis of epilepsy.,Mixture of experts (ME) is modular neural network architecture for supervised learning. A double-loop Expectation-Maximization (EM) algorithm has been introduced to the ME network structure for detection of epileptic seizure.
158,"In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data. We introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input.","In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods."
159,"a large number of extractive summarization techniques have been developed in the past decade. Very few enquiries have been made as to how these differ from each other or what are the factors that actually affect these systems. We examine the roles of three principle components of an extractive summarization technique: sentence ranking algorithm, sentence similarity metric and text representation scheme.","A large number of extractive summarization techniques have been developed in the past decade. We show that using a combination of several different sentence similarity measures, rather than only one, significantly improves performance of the resultant meta-system. Even simple ensemble techniques prove to be very effective in improving overall performance."
160,"Wearable health monitoring systems (WHMSs) will play an increasingly important role in future e-healthcare. Given its sensitivity, the health data should be protected against unauthorized access. It is critical to design an end-to-end mutual authentication protocol that enables secure communication between the wearable sensor and medical professionals.","Wearable health monitoring systems (WHMSs) will play an increasingly important role in future e-healthcare. Given its sensitivity, the health data should be protected against unauthorized access. It is critical to design an end-to-end mutual authentication protocol that enables secure communication."
161,"system uses 5 structural features, 1 of which is newly proposed and 3 are semantic features whose values are extracted from Turkish Wikipedia links. The features are combined using the weights calculated by 2 novel approaches.","A new Turkish text summarization system that combines structural and semantic features. The system uses 5 structural features, 1 of which is newly proposed and 3 are semantic features extracted from Turkish Wikipedia links. The features are combined using the weights calculated by 2 novel approaches."
162,"Owing to the phenomenal growth in communication technology, most of us hardly have time to read books. For visually challenged people, the situation is even worse. We develop a better and more accurate methodology than the existing ones. The proposed algorithm would highly be useful for blind people.","In this work, we modify the Weighted TF_IDF (Term Frequency Inverse Document Frequency) algorithm. We compare the modified algorithm with that of the existing algorithms of TextRank Algorithm, Luhn's Algorithm and others. We find that the proposed algorithm would highly be useful for blind people."
163,Many people locate it tough to read the whole passage of records so that it will gather the essential key factors. We use textual content summarization to reduce the burden of analyzing big passages.,Textual content summarization is the system of extracting essential facts from the given facts. Text summarization has been used in lots of application like business evaluation and marketplace overview.
164,Automatic text summarization (ATS) is the process of generating a summary by condensing text document by a computer machine. Voting-based methods are sensitive to initial ranking process. We proposed reciprocal ranking-based sentence scoring approach that alleviates the feature weighting and initial ranking problem.,Automatic text summarization is the process of generating a summary by condensing text document by a computer machine. Main issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize the quality of summary.
165,"SemEval-2019 Task 6 was OffensEval: Identifying and Categorizing Offensive Language in Social Media. We present the approaches used by the Embeddia team, who qualified as fourth, eighteenth and fifth on the three sub-tasks.","SemEval- 2019 Task 6 was OffensEval: Identifying and Categorizing Offensive Language in Social Media. The task was divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification."
166,most crucial issues are feature extractions and classifier selection. This work proposes an innovative method that hybridizes the principal component analysis (PCA) and t-statistics for feature extraction. The proposed method has been applied on the SEED dataset (SJTU Emotion EEG Dataset) that yielded significant channels and features for getting higher classification accuracy.,This work proposes an innovative method that hybridizes the principal component analysis (PCA) and t-statistics for feature extraction. The proposed method has been applied on the SEED dataset (SJTU Emotion EEG Dataset) that yielded significant channels and features for getting higher classification accuracy.
167,Machine Learning (ML) techniques implicates artificial intelligence which are commonly used. The major utilization of ML is to predict the conclusion established on the extant data. Different ML and Deep Learning (DL) networks established on ANN have been extensively recommended for the disclosure of heart disease in antecedent researches.,"Machine Learning (ML) techniques are commonly used to solve many problems in data science. In this paper, we used UCI Heart Disease dataset to test ML techniques along with conventional methods. A superlative increase of 2.1% accuracy for anemic classifiers was attained with the help of an ensemble voting based model."
168,Electronic medical records contain a variety of valuable medical information for patients. This paper proposes an Enhanced Character-level Deep Convolutional Neural Networks (EnDCNN) model for cardiovascular disease prediction. The character-level model based on text region embedding can well map risk factors and their labels as a unit into a vector. Downsampling plays a crucial role in improving the training efficiency of deep CNN.,"Risk factor identification extraction model achieved 0.9073 of F-score, and prediction model achieved. 0.9516 of F -score. The prediction result is better than the most previous methods. The character-level model based on text region embedding can well map risk factors and their labels as a unit into a vector."
169,"this article focuses very specifically upon the way in which data and data analytics are envisioned within the marketing rhetoric of the data analytics industry. It is argued that to understand the spread of data analytics and the adoption of certain analytic strategies, we first need to look at the projection of promises upon that data. The way that data and analytics are imagined shapes their incorporation and appropriation into practices and organisational structures.","The power of data is located in what they are used to reveal, argues author. We have little understanding of the role played by the emerging industry of data analytics in the interpretation and use of big data. This article draws upon a sample of 34 data analytics companies."
170,"Haiduc et al. proposed text summarization based approaches to the automatic generation of class and method summaries. Our study partially replicates the original study by Haiduc et al. in that it uses the objects, the instruments, and a subset of the summaries from the original study.",In software evolution a developer must investigate source code to locate entities that must be modified to complete a change task. Haiduc et al. proposed text summarization based approaches to the automatic generation of class and method summaries. In this paper we propose a new topic modeling based approach to source code summarization.
171,"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Transferring this model to summaries generated by several state-of-the-art models reveals that this highly scalable approach substantially outperforms previous models.","Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency. This highly scalable approach substantially outperforms previous models, including those trained with strong supervision."
172,we present results regarding the experimental validation of connected automated vehicle design. We propose a class of connected cruise control algorithms with feedback structure originated from human driving behavior. We test the connected cruise controllers using real vehicles under several driving scenarios.,A class of connected cruise control algorithms with feedback structure originated from human driving behavior would be needed. The algorithms would use beyond-line-of-sight motion information from neighboring human-driven vehicles via vehicle-to-everything (V2X) communication.
173,"In this work, we evaluate deep content selection methods for multi-document summarization based on the CST model. Our methods consider summarization preferences and focus on the overall main problems of multi-document treatment.",Automatic summaries have become very important resources. This work evaluate deep content selection methods for multi-document summarization based on the CST model. Results show that the use of CST model helps to improve informativeness and quality.
174,making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED14 dataset with two popular CNN architectures pretrained on ImageNet. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED14 from 34:95% to 38:74%.,"We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset."
175,most of the text summarization research carried out to date has been concerned with the summarization of short documents. Very little work if any has been done on the summarization of very long documents. We introduce a new data set specifically designed for the evaluation of systems for book summarization.,Most of the text summarization research to date has been concerned with the summarization of short documents. Little work has been done on summarizing very long documents. We introduce a new data set specifically designed.
176,HIERSUM utilizes a hierarchical LDA-style model to represent content specificity as a hierarchy of topic vocabulary distributions. It yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system.,Generative probabilistic models for multi-document summarization. We construct a sequence of models each injecting more structure into the representation of document set content. We also explore HIERSUM's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.
177,"the recent artificial intelligence studies have witnessed great interest in abstractive text summarization. We propose a novel Hybrid learning model for Abstractive Text Summarization (HATS) The model consists of three major components, a knowledge-based attention network, a multitask encoder-decoder network, and a generative adversarial network.","The recent artificial intelligence studies have witnessed great interest in abstractive text summarization. Motivated by the humanlike reading strategy that follows a hierarchical routine, we propose a novel Hybrid learning model for Abstractive Text Summarization. The model consists of three major components, a knowledge-based attention network, a multitask encoder-decoder network, and a generative adversarial network."
178,the automatic text summarization (ATS) task consists in automatically synthesizing a document to provide a condensed version of it. This study proposes a new method for the ATS task that takes advantage of semantic information to improve keyword detection. The experimental results indicate that the proposed method outperformed previous methods with a standard collection.,ATS task consists in automatically synthesizing a document to provide a condensed version of it. This study proposes a new method for the ATS task that takes advantage of semantic information to improve keyword detection. Results indicate that the proposed method outperformed previous methods with a standard collection.
179,ExDoS is the first approach to combine both supervised and unsupervised algorithms in a single framework and an interpretable manner for document summarization purpose. We evaluate our model both automatically (in terms of ROUGE factor) and empirically (human analysis) on the benchmark datasets: the DUC2002 and CNN/DailyMail. Results show that our model obtains higher ROUGE scores compared to most state-of-the-art models.,The exponential growth of the Web documents has constituted the need for automatic document summarization. ExDoS is the first approach to combine both supervised and unsupervised algorithms in a single framework. The model obtains higher ROUGE scores comparing to most state-of-the-art models. The human evaluation also demonstrates that our model is capable of generating informative and readable summaries.
180,this paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. We formulate the extractive summarization task as a semantic text matching problem. We believe the power of this matching-based summarization framework has not been fully exploited.,"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually, we model the relationship between sentences as a semantic text matching problem. We believe the power of this matching-based summarization framework has not been fully exploited."
181,"extracting data from publication reports is a standard process in systematic review (SR) development. The data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications.","Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications."
182,"we propose to evaluate extractive summarization algorithms from a completely new perspective. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with the textual units.","An extractive summarization algorithm selects a subset of the textual units in the input data for inclusion in the summary. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with them."
183,"Electroencephalogram (EEG) is a widely used non-invasive brain signal acquisition technique that measures voltage fluctuations from neuron activities of the brain. EEGs are typically used to diagnose and monitor disorders such as epilepsy, sleep disorders, and brain death. This paper proposes a deep learning based model to detect the presence of the artifacts and to classify the kind of the artifact.","Electroencephalogram (EEG) is a widely used non-invasive brain signal acquisition technique that measures voltage fluctuations from neuron activities of the brain. EEGs are typically used to diagnose and monitor disorders such as epilepsy, sleep disorders, and brain death. This paper proposes a deep learning based model to detect the presence of the artifacts."
184,"many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. We apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target","Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. It trains the very deep VGG16 network 9 faster than R- CNN, is 213 faster at test-time, and achieves a higher mAP on PASCAL VOC 2012."
185,"Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. We introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.","State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network."
186,Feature extraction is a process to extract information from the electroencephalogram (EEG) signal to represent the large dataset before performing classification. This paper is intended to study the use of discrete wavelet transform (DWT) in extracting feature from EEG signal obtained by sensory response from autism children.,"The neurological changes in alertness and drowsiness states can be asses by electroencephalogram (EEG) signals. In this paper, the nonstationary characteristic of the EEG signal is explored by tunable Q-factor wavelet transform (TQWT) The results of the KW-test show that the proposed features are effectively discriminative of the alertness."
187,"This paper presents a review on signal analysis method for feature extraction of electroencephalogram (EEG) signal. It is an important aspect in signal processing as the result obtained will be used for signal classification. The techniques are Hilbert-Huang transform, Principal Component Analysis, Independent Component Analysis and Local Discriminant Bases.",A good technique for feature extraction is necessary in order to achieve robust classification of signal. Several techniques have been implemented for extracting features in EEG signal. Local Discriminant Bases algorithm is introduced in the present paper as another powerful adaptive feature extraction technique for EEG signal which is not reported elsewhere.
188,"brain-computer interface technology interprets the EEG signals displayed by the human brain's neurological thinking activities through computers and instruments. The emergence of brain-computer interface technology has brought practical value to many fields. Based on the mechanism and characteristics of motion imaging EEG signals, this paper designs the acquisition experiment of EEG signals.","Brain-computer interface technology interprets the EEG signals displayed by the human brain. It uses the interpreted information to manipulate the outside world, thereby abandoning the human peripheral nerves and muscle systems. The method has a high accuracy rate for the recognition of motor imagery EEG."
189,"Feature selection, as a preprocessing stage, is a challenging problem in various sciences such as biology, engineering, computer science, and other fields. In addition to filter methods, FeatureSelect consists of optimisation algorithms and three types of learners. It provides a user-friendly and straightforward method of feature selection for use in any kind of research.","Feature selection, as a preprocessing stage, is a challenging problem in various sciences such as biology, engineering, computer science, and other fields. FeatureSelect is a feature or gene selection software application which is based on wrapper methods. It provides a user-friendly and straightforward method of feature selection for use in any kind of research."
190,WhatsApp is a giant mobile instant message IM application with over 1billion users. The huge usage of IM like WhatsApp through giant smart phone 'Android' makes the digital forensic researchers to study deeply. Many hardware and software tools for mobile and forensics are used to collect as much digital evidence as possible from persistent storage on android device.,"WhatsApp is a giant mobile instant message IM application with over 1billion users. The huge usage of IM like WhatsApp through giant smart phone ""Android"" makes the digital forensic researchers to study deeply. The artefacts left behind in the smartphone play very important role in any electronic crime, or any terror attack."
191,We proposed a novel text summarization model based on 01 non-linear programming problem. This proposed model covers the main content of the given document through sentence assignment. We implemented our model on multi-document summarization task.,We propose a novel text summarization model based on 01 non-linear programming problem. This proposed model covers the main content of the given document(s) through sentence assignment. When comparing our method to several methods.
192,5G promises a robust solution by o?ering ultra-low latency and high bandwidth for data transmission. It can accelerate network performance without making a large investment in new hardware. Dynamic reconfgurability and in-field programming features of FPGAs compared to fixed function ASICs help in developing better wireless systems.,"Next generation communication relies on standardized protocols, heterogeneous architecture and advanced technologies. FPGA has the potential to be resource/power efficient, it can be used for building up constituents of 5G infrastructure. Dynamic reconfgurability and in-field programming features of FPGAs compared to fixed function ASICs help in developing better wireless systems."
193,"this study investigated how assessors with di?erent level of expertise categorized Beaujolais wines from general to more specific levels of categorization. For each wine categorization level, three sets of 12 wines were tasted by three panels of 60 assessors. At each level, with a few exceptions, a clearer separation was observed between the two categories in the conceptual condition than in the perceptual condition.","Study investigated how assessors with expertise categorized Beaujolais wines from general to more specific levels of categorization. In both perceptual and conceptual conditions, assessors were asked to perform a binary sorting task, followed by a verbalisation task. At each level, with a few exceptions, a clearer separation was observed between the two categories in the conceptual condition."
194,"convolutional networks exceed the state-of-the-art in semantic segmentation. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32] into fully convolutional networks.","Convolutional networks are powerful visual models that yield hierarchies of features. We show that they can exceed the state-of-the-art in semantic segmentation. We build ""fully convolutional"" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning."
195,"This paper proposed an automatic text summarization approach based on sentence extraction using fuzzy logic, genetic algorithm, semantic role labeling and their combinations to generate high quality summaries. Experimental results showed that the summaries produced by the proposed approaches are better than other approaches produced by Microsoft Word 2007.","This paper proposes an automatic text summarization approach based on sentence extraction. It uses fuzzy logic, genetic algorithm, semantic role labeling and their combinations to generate high quality summaries. The proposed approaches are better than other approaches produced by Microsoft Word 2007, Copernic Summarizer, and MANYASPECTS summarizers."
196,This paper focuses on extraction approach. The goal of text summarization based on extraction approach is sentence selection. We proposed text summarization based on fuzzy logic to improve the quality of the summary created by the general statistic method.,Text summarization can be classified into two approaches: extraction and abstraction. This paper focuses on extraction approach. We propose text summarization based on fuzzy logic to improve the quality of the summary. We compared our results with the baseline summarizer and Microsoft Word 2007 summarizers.
197,the aim of automatic text summarization systems is to select the most relevant information from an abundance of text sources. The weights obtained from the swarm experiment were used to adjust the text features scores and then the features scores were used as inputs for the fuzzy inference system to produce the final sentence score. The proposed method got a good performance outperforming the swarm model and benchmark methods.,The aim of automatic text summarization systems is to select the most relevant information from an abundance of text sources. The sentences were ranked in descending order based on their scores and then the top n sentences were selected as final summary. experiments showed that the incorporation of fuzzy logic with swarm intelligence could play an important role in the selection process.
198,"Word Embeddings encompass a set of language modeling and feature learning techniques from Natural Language Processing (NLP) In this research, we apply the ELM-based Word Embeddings to the NLP task of Text Categorization. The results show that ELM-based Word Embeddings slightly outperforms the aforementioned two methods in the Sentiment Analysis and Sequence Labeling tasks. Only one hyperparameter is needed using ELM whereas several are utilized for the other methods.",Word Embeddings are low-dimensional distributed representations that encompass a set of language modeling and feature learning techniques from Natural Language Processing. Extreme Learning Machine (ELM) proposed using an ELM for generating word embeddings. ELM-based Word Embedding slightly outperforms state-of-the-art methods: Word2Vec and GloVe models.
199,"Automatic text summarization has become a relevant topic due to the information overload. In this paper a novel approach for automatic extractive text summarization called SENCLUS is presented. Using a genetic clustering algorithm, SENCLUS clusters the sentences as close representation of the text topics using a fitness function based on redundancy and coverage.",Automatic text summarization has become a relevant topic due to the information overload. This automatization aims to help humans and machines to deal with the vast amount of text data offered on the web.
200,"Global navigation satellite system (GNSS) is a proven technology to provide precise timing information in many distributed systems. It is well recognized to be the primary means for vehicle positioning and velocity determination in VANETs. This paper examines the requirements, potential benefits, and feasibility of GNSS time synchronization in VANets.",VANETs are becoming increasingly important for emerging cooperative intelligent transport systems. Global navigation satellite system (GNSS) is a proven technology to provide precise timing information in many distributed systems. The availability of GNSS time synchronization is characterized by almost 100% in experiments in high-rise urban streets.
201,one of the most pressing problems for forensic investigators is the huge amount of data to analyze per case. We propose to use file deduplication across devices as well as file whitelisting rigorously in investigations. These improvements happen in an automatic fashion and completely transparent to the forensic investigator.,One of the most pressing problems for forensic investigators is the huge amount of data to analyze per case. In this paper we propose to use file deduplication across devices as well as file whitelisting rigorously in investigations.
202,"Nearly all of the existing methods rely on the hand-crafted descriptors (e.g., LBP, CENTRIST and SIFT) Their limited discriminative power indeed leads to the unsatisfactory performance. We propose DeepCloud as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features.","Ground-based cloud image categorization is a critical but challenging task that has not been well addressed. In the recent years, the deep Convolutional Neural Network has achieved the promising results in lots of computer vision and image understanding fields. We propose ""DeepCloud"" as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features."
203,this article presents a novel proposal to identify people with an apparent heart attack in colour images by detecting characteristic postures of heart attack. The method of identifying infarcts makes use of convolutional neural networks. These have been trained with a specially prepared set of images that contain people simulating a heart attack.,The method of identifying infarcts makes use of convolutional neural networks. These have been trained with a specially prepared set of images that contain people simulating a heart attack. The promising results in the classification of infarcts show 91.75% accuracy and 92.85% sensitivity.
204,"Optimization algorithms have the advantage of dealing with complex non-linear problems with a good flexibility and adaptability. In this paper, we exploited the Fast Correlation-Based Feature Selection (FCBF) method to filter redundant features. The results demonstrate the efficacy and robustness of the proposed hybrid method in processing various types of data for heart disease classification.","Heart disease is one of the complex diseases and globally many people suffered from this disease. On time and efficient identification of heart disease plays a key role in healthcare, particularly in the field of cardiology. In this article, we propose an efficient and accurate system to diagnosis heart disease. The system is based on machine learning techniques."
205,"each image is expressed as a bag of orderless pairs, each of which includes a local feature vector encoded over a visual dictionary. The side information is used for hierarchical clustering of the encoded local features. The extensive experiments over the Caltech-UCSD Birds 200, Oxford Flowers 17 and 102 show the state-of-the-art performances from these two exemplar algorithms.","Each image is expressed as a bag of orderless pairs. Each image includes a local feature vector encoded over a visual dictionary, and its corresponding side information from priors or contexts. The side information is used for hierarchical clustering of the encoded local features. A hierarchical matching kernel is derived as the weighted sum of the similarities over the encoded features pooled within clusters at different levels."
206,"In this paper, we propose a universal solution to web search and web browsing on handheld devices for visually impaired people. We propose to automatically cluster web page results and summarize all the information in web pages.","In this paper, we propose a universal solution to web search and web browsing on handheld devices for visually impaired people."
207,Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.,"In this paper, we develop a neural summarization model which can effectively process multiple input documents. Our model augments a previously proposed Transformer architecture. We represent cross-document relationships via an attention mechanism."
208,"Information extraction and query-oriented summarization method are applied here to reply people's query. Due to the characteristics of social network messages, we pay more attention to reducing the noise and eliminating the redundancy of the messages to ensure the quality of the final reply. Experimental results show that the research is effective in filtering the redundancy and noise of social network messages.","Information extraction and query-oriented summarization method are applied here to reply people's query. There are few effective and commonly used methods on filtering the redundancy and noise of the raw data, which results in the poor quality of the reply. Experimental results show that the research is effective in filtering the noise and redundancy."
209,high-dimensional signatures are important to obtain state-of-the-art results on large datasets. We tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies. We report results on two large databases  ImageNet and a dataset of 1M Flickr images.,"The larger the training set, the higher the impact of the dimensionality on the accuracy. High-dimensional signatures are important to obtain state-of-the-art results on large datasets. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement."
210,"Clustering is considered an important issue in Vehicular Ad-hoc Networks (VANETs) to ensure network robustness and throughput. In this letter, we present a model to characterize the throughput of VANET clusters, by taking into account the relative mobility effect.",Clustering is considered an important issue in Vehicular Ad-hoc Networks (VANETs) There is still a lack of understanding of how relative mobility between cluster members and the cluster head affects the throughput of VANET clusters.
211,Instant Messaging has gained popularity by users for both private and business communication. Most mobile messaging apps did not protect confidentiality or integrity of the messages. Press releases about mass surveillance performed by intelligence services such as NSA and GCHQ motivated many people to use alternative messaging solutions to preserve the security and privacy of their communication on the Internet.,"Until recently, most mobile messaging apps did not protect confidentiality or integrity of the messages. Press releases about mass surveillance performed by intelligence services such as NSA and GCHQ motivated many people to use alternative messaging solutions. A messaging app that claims to provide secure instant messaging and has attracted a lot of attention is TEXTSECURE."
212,"Heart disease is one of the significant reason of death and disability. This paper proposes new heart disease prediction system that combine all techniques into one single algorithm, it called hybridization. The result confirm that accurate diagnose can be taken by using a combined model from all techniques.","Heart disease is one of the significant reason of death and disability. The shortage of Doctors, experts and ignoring patient symptoms lead to big challenge. This paper proposes new heart disease prediction system that combine all techniques into one single algorithm."
213,"Database audit records are important for investigating suspicious actions against transactional databases. Their admissibility as digital evidence depends on satisfying Chain of Custody (CoC) properties during their generation, collection and preservation. In this paper, we propose a forensically-aware distributed database architecture that implements CoC properties as functional requirements to produce admissible audit records.",Audit records are important for investigating suspicious actions against transactional databases. Admissibility as digital evidence depends on satisfying Chain of Custody (CoC) properties. We propose a forensically-aware distributed database architecture that implements these properties as functional requirements.
214,"EEG signals reflect electrical activity in the brain, which also informs the condition of post-stroke patient recovery. Wavelet is used for EEG signal information extraction as a feature of machine learning. The accuracy of the testing data was 90% with amplitude and Beta features compared to 70% without amplitude or Beta.",Deep learning can be used to identify post-stroke patients using convolutional neural networks. The accuracy of the testing data was 90% with amplitude and Beta features compared to 70% without. The experimental results also showed that adaptive moment estimation model was more stable compared to Stochastic gradient descent.
215,"Continuous assessment of source codes produced by students on time is a challenging task for teachers. This research presents the A-Learn EvId method, having as the main differential the evaluation of high-level skills instead of technical aspects.","This research is contextualized in the teaching of computer programming. This research presents the A-Learn EvId method, having as the main differential the evaluation of high-level skills instead of technical aspects."
216,inability to reliably assess seizure risk is a major burden for epilepsy patients and prevents developing better treatments. Algorithms using scalp electroencephalography (EEG) and electrocardiography (EKG) have also achieved better than chance performance. We apply deep learning - a powerful method to extract information from complex data.,"Recent advances have paved the way for increasingly accurate seizure preictal state detection algorithms. To develop seizure forecasting for broad clinical and ambulatory use, less complex and invasive modalities are needed. We apply deep learning to a large epilepsy data set containing multi-day, simultaneous recordings of EKG, ECoG, and EEG, using a variety of feature sets."
217,developing radio access technologies (RATs) that enable reliable and low-latency vehicular communications has become of paramount importance. Dedicated short-range communications (DSRCs) and cellular V2X (C-V2X) are two present-day technologies that are capable of supporting day-1 vehicular applications. Both the DSRC and C-V2X are undergoing extensive enhancements in order to support advanced vehicular applications.,Dedicated short-range communications (DSRCs) and cellular V2X (C-V2X) are two present-day technologies. These RATs fall short of supporting communication requirements of many advanced vehicular applications. Both the DSRC and C-V 2X are undergoing extensive enhancements to support advanced applications. These can supplement today's vehicular sensors in enabling autonomous driving.
218,This work is a broad investigation of computer vision and machine learning methods that acts as a step towards applying these established methods to more sophisticated materials recognition or characterization tasks. The approach presented here could offer improvements over established stereological measurements by removing the requirement of expert knowledge (bias) for interpretation of image data prior to characterization.,"Computer vision and machine learning methods were applied to the challenge of automatic microstructure recognition. Two classification tasks were completed, and involved distinguishing between micrographs that depict dendritic morphologies from those that do not contain this particular microstructural feature. Results demonstrate that deep learning algorithms can successfully be applied to micrograph recognition tasks. This work is a step towards applying these established methods to more sophisticated materials recognition or characterization tasks."
219,This study aims to compare the performances of the three text summarization systems developed by the authors with some of the existing Summarization systems available. All the three represent approaches to achieve connectionism. Semantic net approach performs better than the MS Word summarizer as far as the semantics of the original text was concerned.,"Three approaches to text summarization are based on semantic nets, fuzzy logic and evolutionary programming respectively. Semantic net approach performs better than the MS Word summarizer as far as the semantics of the original text was concerned. The second approach based on fuzzy logic results in an efficient system since fuzzy logic mimics decision making of humans. Third system showed promising results in precision and F-measure than all the other approaches."
220,this paper presents two different approaches to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image's location. The graph-based method uses text cohesion techniques to identify information relevant to a location.,This paper presents two different approaches to automatic captioning of geo-tagged images. The graph-based method uses text cohesion techniques to identify information relevant to a location.
221,"system employs autoregressive (AR) modeling as the features extraction algorithm, and sparse-deep belief networks (sparse-DBN) as the classification algorithm. The sparsity in sparse-DBN is achieved with a regularization term that penalizes a deviation of the expected activation of hidden units from a fixed low-level prevents the network from overfitting.","This paper presents an improvement of classification performance for electroencephalography-based driver fatigue classification. The system employs autoregressive (AR) modeling as the features extraction algorithm and sparse-deep belief networks as the classification algorithm. Compared to other classifiers, sparse-DBN is a semi supervised learning method which combines unsupervised learning for modeling features in the pre-training layer and supervised learning for classification in the following layer."
222,Multi document summarization is a process to produce a single summary from a set of related documents collected from heterogeneous sources. The performance of a multi document summarization system heavily depends on the sentence similarity measure used for removing redundant sentences from the summary. This paper presents an enhanced method for computing sentence similarity aiming for improving multi-document summarization performance.,Multi document summarization is a process to produce a single summary from a set of related documents. The performance of a summarization system heavily depends on the sentence similarity measure used. This paper presents an enhanced method for computing sentence similarity aiming for improving multi-document summarization performance.
223,"Pointer Generators have been the de facto standard for modern summarization systems. This architecture faces two major drawbacks: Firstly, the pointer is limited to copying the exact words while ignoring possible inflections or abstractions. The copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text.","Pointer Generators have been the de facto standard for modern summarization systems. This architecture faces two major drawbacks: the pointer is limited to copying the exact words and ignoring possible inflections or abstractions. In this paper, we address these problems by allowing the model to ""edit"" pointed tokens instead of always hard copying them."
224,"Deploying a generalized, multifunctional mechanism that produces good results seems to be a panacea for most of the text-based, information retrieval needs. We present the keyword extraction techniques, exploring the effects that part of speech tagging has on the summarization procedure of an existing system.",Text Summarization and categorization have always been two of the most demanding information retrieval tasks. We present the keyword extraction techniques to explore the effects that part of speech tagging.
225,"In this paper we present a system for generating summary by sentence extraction. To determine the weight of sentence, we use text features, such as sentence position, sentence relative length, average term frequency. We also investigate the effect of semantic feature, using latent semantic analysis, on the summarization task.","In this paper we present a system for generating summary by sentence extraction. We use text features to determine the weight of sentence. We also investigate the effect of semantic feature, using latent semantic analysis, on the task."
226,"In the process of brain-computer interface (BCI), variations across sessions/subjects result in differences in the properties of potential of the brain. This issue may lead to variations in feature distribution of electroencephalogram (EEG) across subjects, which greatly reduces the generalization ability of a classifier. We propose an instance transfer subject-independent (ITSD) framework combined with a convolutional neural network to improve the classification accuracy of the model during motor imagery (MI) task.","In brain-computer interface (BCI), variations across sessions/subjects result in differences in the properties of potential of the brain. This may lead to variations in feature distribution of electroencephalogram (EEG) across subjects, which greatly reduces the generalization ability of a classifier. We propose an instance transfer subject-independent (ITSD) framework combined with a convolutional neural network (CNN) to improve the classification accuracy of the model during motor imagery (MI) task."
227,Identifying intravenous immunoglobulin-resistant patients is essential for the prompt and optimal treatment of Kawasaki disease. Data-driven approaches have the potential to identify the high-risk individuals by capturing complex patterns of real-world data. This study highlights the integration of co-clustering and supervised learning methods for incomplete clinical data mining.,Identifying intravenous immunoglobulin-resistant patients is essential for the prompt and optimal treatment of Kawasaki disease. Data-driven approaches have the potential to identify the high-risk individuals by capturing the complex patterns of real-world data. This study highlights the integration of co-clustering and supervised learning methods for incomplete clinical data mining.
228,"With the growing size of textual information on the world-wide web, automatic data summarization becomes essential for both web users as well as data base and search engine developers. In this paper, we propose a novel approach for extracting the most relevant sentences from an original document to form a summary. The approach utilizes fuzzy measures and inference to find the most significant sentences.","In this paper, we propose a novel approach for extracting the most relevant sentences from an original document. The approach utilizes fuzzy measures and inference to find the most significant sentences."
229,"This paper combines contributions by Term Frequency weight, position weight and commendation weight to form an effective extraction based summary. We have analyzed the significance of each feature and we found that a combination of all three weights results in a better summary as close as human generated summary.","This paper combines contributions by Term Frequency weight, position weight and commendation weight to form an effective extraction based summary. We also show that our summarizer outperforms commercially available summarizers like Copernic summarizer and Microsoft Summarizer."
230,"Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. We evaluate our approaches on the Labeled Faces in the Wild data set, a large and very challenging data set of faces from Yahoo! News. Confidence scores obtained for face identification can be used for many applications e.g. clustering or recognition from a single training example.","Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures. We show our methods benefit from richer training data, much more so than the current state-of-the-art method."
231,With the rapid growth of information on the Internet there is a demand on its efficient and cost-effective summarization. The creation of automatic summarization methods is considered as a very important task of natural language processing.,In this paper we present an extractive summarization method for the Kazakh language based on fuzzy logic. We aimed to extract and concatenate important sentences from the primary text to obtain its shorter form. We also applied our method on CNN/Daily Mail dataset.
232,Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. We explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract.,Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text.
233,"Automatic text summarization (ATS) has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale corpora. This paper presents an unsupervised approach that combines rhetorical structure theory, deep neural model, and domain knowledge concern for ATS. Experimental results show that, on a large-scale Chinese dataset, our proposed approach can obtain comparable performances compared with existing methods.","Automatic text summarization has recently achieved impressive performance thanks to advances in deep learning. There is still no guarantee that the generated summaries are grammatical, concise, and convey all salient information as the original documents have. This paper presents an unsupervised approach that combines rhetorical structure theory, deep neural model, and domain knowledge concern for ATS."
234,"most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system.","Labeled Faces in the Wild is an aid in studying the unconstrained face recognition problem. The database contains labeled face photographs spanning the range of conditions. It exhibits ""natural"" variability in factors such as pose, lighting, race, accessories, occlusions, and background."
235,"automatic software categorization will be in great demand. In this paper, we propose an enhancement called LACTA based on LACT to tackle this problem. LACTA extensively employs Android domain knowledge in the process and uses LDA to extract meaningful software topics for classification.",Many software repositories collect and archive Android applications to facilitate the dissemination of Android applications. LACTA extensively employs Android domain knowledge in the process. LDA uses LDA to extract meaningful software topics for classification.
236,recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. We consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.,Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial speech recognition service.
237,learning Mahanalobis distance metrics in a high-dimensional feature space is very difficult especially when structural sparsity and low rank are enforced to improve computational efficiency. This paper addresses both aspects by an ensemble metric learning approach that consists of sparse block diagonal metric ensembling and joint metric learning as two consecutive steps. Our algorithm considers all pairwise or triplet constraints generated from training samples with explicit class labels.,An ensemble metric learning approach that consists of sparse block diagonal metric ensembling and joint metric learning. Its applications to face verification and retrieval outperform existing state-of-the-art methods in accuracy.
238,"Automatic summarization is developed to extract the representative contents or sentences from a large corpus of documents. This paper presents a new hierarchical representation of words, sentences and documents in a corpus. The sentence-based latent Dirichlet allocation (SLDA) is accordingly established for document summarization.","This paper presents a new hierarchical representation of words, sentences and documents in a corpus. It infers the Dirichlet distributions for latent topics and latent themes in word level and sentence level. The proposed SLDA outperforms other methods for document summarization in terms of precision, recall and F-measure."
239,Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the largescale visual recognition challenge. Learning CNNs amounts to estimating millions of parameters and requires a very large number of annotated image samples. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset.,Convolutional neural networks (CNN) have shown outstanding image classification performance in the largescale visual recognition challenge (ILSVRC2012) Success of CNNs is attributed to their ability to learn rich midlevel image representations. Learning CNNs amounts to estimating millions of parameters and requires a large number of annotated image samples. This property currently prevents application of CNN's to problems with limited training data.
240,"many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. This paper reports on the project called 'lecture summarization service', a python-based RESTful service that utilizes the BERT model for text embeddings and K-Means clustering to identify sentences closest to the centroid for summary selection.","This paper reports on the project called ""lecture summarization service"", a python-based RESTful service that utilizes the BERT model for text embeddings and K-Means clustering to identify sentences closest to the centroid for summary selection. The service also includes lecture and summary management, storing content on the cloud which can be used for collaboration."
241,"Various techniques are compared here that have done for multi-document summarization. In this paper, automatic multiple documents text summarization task is addressed and different procedure of various researchers are discussed.",Automatic text summarization is a system of summarizing text by computer where a text is given to the computer as input and the output is a shorter and less redundant form of the original text. Research was first started naively on single document abridgement but recently information is found from various sources about a single topic. Various techniques are compared here that have done for multi-document summarization.
242,"Search engines like Google, Yahoo were developed to retrieve information from databases. Actual results were not obtained as electronic information is increasing day by day. Automatic summarization gathers several documents as input and provides the shorter summarized version as output.",Research in the field of text summarization began in the 1950s and until now there is no system that can produce summaries such as professionals or humans. The extractive approach is still in demand in the past three years because the extractive is easier than abstractive.
243,"Decision Tree algorithms have been successfully applied in various fields especially in medical science. The liver disease dataset which is select for this study is consisting of attributes like total bilirubin, direct bilirubin, age, gender, total proteins, albumin and globulin ratio.",This research work explores the early prediction of liver disease using various decision tree techniques. Analysis proves that Decision Stump provides the highest accuracy than other techniques. The main purpose of this work is to calculate the performance of various decision Tree techniques.
244,"ow-Power Wide Area Networks (LPWANs) are prominent technologies in the field of the Internet of Things (loT) With their unique characteristics, LPWANs can be used in many applications and in different environments such as urban, rural and even indoor. Three distinct scenarios are proposed and tested, one rural and two urban scenarios in Portugal.","Low-Power Wide Area Networks (LPWANs) are the ideal technologies to send small data occasionally. With their long range capabilities and low energy consumption, they are ideal technologies for the Internet of Things."
245,"Existing Android malware analysis techniques can be broadly categorized into static and dynamic analysis. We present two machine learning aided approaches for static analysis of Android malware. Our permission-based model is computationally inexpensive, and is implemented as the feature of OWASP Seraphimdroid Android app.","The widespread adoption of Android devices and their capability to access significant private and confidential information have resulted in these devices being targeted by malware developers. In this paper, we present two machine learning aided approaches for static analysis of Android malware."
246,"security against various malicious attacks are achieved by using malicious vehicles identification and trust management (MAT) algorithm. The proposed MAT algorithm performs in two dimensions, they are (i) Node trust and (ii) information trust accompanied with a digital signature and hash chain concept. The public group key is common for each participant but everyone maintain their own private key to produce the secret key.","The security against various malicious attacks are achieved by using malicious vehicles identification and trust management (MAT) algorithm. This key exchanging algorithm is useful to prevent the various attacks, like impersonate attack, man in middle attack, etc. The proposed MAT algorithm accurately evaluates the trustworthiness of each node as well as information to control different attacks."
247,"Recognizing materials in real-world images is a challenging task. We introduce a new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC) We combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild. MINC is an order of magnitude larger than previous material databases, while being more diverse and well-sampled across its 23 categories.","Material recognition in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter. We introduce a new, large-scale, open dataset of materials in the wild. We combine this dataset with deep learning to achieve material recognition and segmentation."
248,Emotion analysis using EEG signals is one such problem that has been studied and worked upon extensively in recent times. We have proposed a novel methodology to classify emotions using signal processing techniques such as wavelet transform and statistical measures for feature extraction and dimensionality reduction.,Emotion analysis using EEG signals is one such problem that has been studied extensively in recent times. A merged LSTM model has been proposed for binary classification of emotions.
249,microexpressions are instantaneous and involuntary reflections of human emotion. Existing recognition methods are often ineffective at handling subtle face displacements. Experimental results on four benchmark datasets demonstrate higher recognition performance and improved interpretability.,"Microexpressions are fleeting, lasting only a few frames within a video sequence. They are difficult to perceive and interpret correctly, and they are challenging to identify and categorize automatically. Existing recognition methods are often ineffective at handling subtle face displacements. Facial Dynamics Map is proposed to characterize the movements of a microexpression."
250,"Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset.",MLUM is the first large-scale MultiLingual SUMmarization dataset. It contains 1.5M+ article/ summary pairs in five different languages. It was obtained from online newspapers.
251,"over the past few years, mobile devices have become the most popular form of communication around the world. Mobile technology is among the fastest developing technologies that have changed the way we live our daily lives. This paper aims at testing the harmonised digital forensic investigation process through a case study of a mobile forensic investigation.",Mobile devices can also be used to perform activities that may be of malicious intent or criminal in nature. This makes mobile devices a valuable source of digital evidence. This paper aims at testing the harmonised digital forensic investigation process through a case study of a mobile forensic investigation.
252,"Motor imagery (MI) based electroencephalogram (EEG) signals are a widely used form of input in brain computer interface systems. In this paper, an attempt is made to find the best classification algorithm and feature extraction technique by comparing some of the prominently used algorithms on a same base dataset. Experimental results show that a combination of DWT and LSSVM classifier outperforms the other procedures.","Motor imagery (MI) based electroencephalogram (EEG) signals are a widely used form of input in brain computer interface systems (BCIs). There are a number of ways to classify data, a question still persists as to which technique should be employed in the domain of MI based EEG signals."
253,Sentiment Analysis has been a keen research area for past few years. This paper proposes a method using which one can analyze different languages to find sentiments in them and perform sentiment analysis. The method leverages different techniques of machine learning to analyze the text.,This paper proposes a method using which one can analyze different languages to find sentiments in them and perform sentiment analysis. The method leverages different techniques of machine learning to analyze the text.
254,"Skin diseases remain a major cause of disability worldwide and contribute approximately 1.79% of the global burden of disease measured in disability-adjusted life years. In the United Kingdom alone, 60% of the population suffer from skin diseases during their lifetime. We propose an intelligent digital diagnosis scheme to improve the classification accuracy of multiple diseases.",Skin diseases remain a major cause of disability worldwide and contribute approximately 1.79% of the global burden of disease measured in disability-adjusted life years. We propose an intelligent digital diagnosis scheme to improve the classification accuracy of multiple diseases.
255,Cellular Vehicle to Anything (C-V2X) has become more applicable with the release of the first sets of 5G (5th Generation) system specifications. The highly capable 5G systems will therefore support even a larger number of moving objects. This study aims to present a sophisticated clustering mechanism that enables cellular systems to accommodate a massive number of moving Machine Type Communication (MTC) objects.,Cellular systems are facing the ever-increasing demand for vehicular communication aimed at applications such as advanced driving assistance and ultimately fully autonomous driving. This study aims to present a sophisticated clustering mechanism that enables cellular systems to accommodate a massive number of moving Machine Type Communication (MTC) objects. The study achieved a sufficient level of prediction accuracy with fewer training data through a learned prediction function.
256,"To systematically analyze large numbers of textual documents, it is often desirable to manage documents (and their metadata) in a multi-dimensional text database. Such structure provides flexibility of understanding local information with different granularities. We propose a new phrase ranking measure to leverage the relation between document subsets induced by multi-dimensional context.","To analyze large numbers of textual documents, it is desirable to manage documents in a multi-dimensional text database. We propose a new phrase ranking measure to leverage the relation between document subsets induced by multi- dimensional context. We develop a cube-based analytical platform that implements an efficient solution."
257,we formulate text summarization task as a multi-objective optimization problem. We define information coverage and diversity as two conflicting objective functions.,Text summarization aims to generate condensed summary from a large set of documents on the same topic. The proposed method generates high ROUGE score summaries and is comparable to the state-of-the-art summarization methods.
258,this paper presents a method for generating multi-document text summary building on single document text summaries. Single document text summaries are combined after calculating cosine similarity between the different single document text summaries generated. The average F-measure of 0.30493 on DUC 2002 dataset has been observed.,"This paper presents a method for generating multi-document text summary building on single document text summaries. The average F-measure of 0.30493 on DUC 2002 dataset has been observed, which is comparable to two of five top performing multi- document text summarization systems."
259,"Document summarization is an emerging technique for understanding the main purpose of any kind of documents. To visualize a large text document within a short duration and small visible area like PDA screen, summarization provides a greater flexibility and convenience. In this paper we study various text summarization techniques e.g. RANDOM, LEAD and MEAD.","Document summarization is an emerging technique for understanding the main purpose of any kind of documents. To visualize a large text document within a short duration and small visible area like PDA screen, summarization provides a greater flexibility and convenience. In this paper we study various text summarization techniques e.g. RANDOM, LEAD and MEAD."
260,The medical information represents an invaluable source of knowledge concerning the medical history of the patient. The manner of their presentation make it badly exploited. This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to ICD-10 (International Classification of Diseases) codes.,"This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to the ICD-10 (International Classification of Diseases) codes. This approach is based on Machine learning techniques, which is evaluated over CépiDC corpus."
261,"In this paper, we introduce a multi-modal sentence summarization task that produces a short summary from a pair of sentence and image. This task is more challenging than sentence summarization. We propose a modality-based attention mechanism to pay different attention to image patches and text units.",A multi-modal sentence summarization task that produces a short summary from a pair of sentence and image is more challenging than sentence summarizing. We propose a modality-based attention mechanism to pay different attention to image patches and text units. We design image filters to selectively use visual information to enhance the semantics.
262,"rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video. We propose an extractive multi-modal summarization method that can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The experimental results obtained on this dataset demonstrate that our method outperforms other competitive baseline methods.","The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video. In this work, we propose an extractive multi- modal summarizing method that can automatically generate a textual summary. We further introduce an MMS corpus in English and Chinese, which is released to the public."
263,"deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. Global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. This paper presents a simple but effective scheme called multiscale orderless pooling.","Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness. This paper presents a simple but effective scheme called multiscale orderless pooling (MOP-CNN)."
264,"we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. We build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation. We extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer.","A system based on the deep autoencoder architecture is designed to extract discriminant features in the multimodal data representation. Autoencoder can be seen as a compression approach, we extend it to handle multimodals at the encoder layer, reconstructed and retrieved at the decoder layer."
265,"the proposed emotion recognition model is based on the hierarchical long-short term memory neural network (LSTM) for video-electroencephalogram (Video-EEG) signal interaction. The inputs are facial-video and EEG signals from the subjects when they are watching the emotion-stimulated video. The experimental results prove that the classification rate (CR) and F1-score of the proposed emotion recognition model are significantly increased by at least 2% and 0.015, respectively.",The proposed emotion recognition model is based on the hierarchical long-short term memory neural network (LSTM) for video-electroencephalogram (Video-EEG) signal interaction. The inputs are facial-video and EEG signals from the subjects when they are watching the emotion-stimulated video.
266,we perform multi-document summarization by generating compressed versions of source sentences as summary candidates. We combine a parse-and-trim approach with a novel technique for producing multiple alternative compressions for source sentences. We also describe experiments using a new paraphrase-based feature for redundancy checking.,We perform multi-document summarization by generating compressed versions of source sentences as summary candidates. We combine a parse-and-trim approach with a novel technique for producing multiple alternative compressions.
267,"main purpose of this paper is to apply several approaches to classify motor imageries originating from the brain in a more robust manner. For this study, dataset II from BCI competition III was used. To extract features from the brain signal, discrete wavelet transform decomposition was used.","The main purpose of this paper is to apply several approaches to classify motor imageries originating from the brain in a more robust manner. For this study, dataset II from BCI competition III was used. Several classic classifiers were implemented to be utilized in the multiple classifier system."
268,Summarization can be classified into two main categories i.e. extractive summarization and abstractive summarization. This paper presents a novel approach to generate abstractive summary from extractive summary using WordNet ontology.,"Text Summarization plays an important role in the area of text mining and natural language processing. This paper presents a novel approach to generate abstractive summary from extractive summary using WordNet ontology. An experimental result shows the generated summary in well-compressed, grammatically correct and human readable format."
269,"Named data networking and software defined networking share mutual courage in changing legacy networking architectures. Nave VNs are based on the IP-based legacy, which is prone to several issues due to the dynamic network topology among other factors. We present an architecture that combines SDN functionalities within VNs to retrieve the required content using NDN.","Named data networking and software defined networking share mutual courage in changing legacy networking architecture. In this article, we first see both SDN and NDN enabled VNs from a bird's eye view. We present an architecture that combines SDN functionalities within VNs to retrieve the required content using NDN."
270,"There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization).","Sentence-level extractive text summarization is substantially a node classification task of network mining. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly. In this paper, we propose HAHSum, which well models different levels of information, including words and sentences."
271,Traditional approaches to extractive summarization rely heavily on human-engineered features. We propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor.,We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. We train our models on large scale corpora containing hundreds of thousands of document- summary pairs.
272,Natural Language Processing (NLP) based technique is used to extract context of seed keyword from initially filtered news headlines. Each news headline in a filtered news group is labelled and assigned a seed keyword term as a category label.,"News categorization scheme proposed to filter out and categorize news headlines related to Pakistan Stock Exchange (PSX) using negligible manual effort. By using domain knowledge, category names are selected manually. Natural Language Processing (NLP) based technique is used to extract context of seed keyword."
273,"Traditionally, the most general mechanism of personal authentication was using alphanumeric passwords. Graphical passwords can be an alternative, but it is vulnerable to shoulder-surfing attacks. This paper looks through a number of recently developed graphical password systems.","Traditionally, the most general mechanism of personal authentication was using alphanumeric passwords. Graphical passwords can be an alternative, but it is vulnerable to shoulder-surfing attacks. This paper introduces a personal authentication system using a machine learning technique with electroencephalography (EEG) signals."
274,"long documents often hamper trivial work. An automatic summarizer is vital towards reducing human effort. In this project, we present a novel technique for generating the summarization of domain specific text.",Long documents often hamper trivial work. Text summarization is a major research topic in Natural Language Processing. We present a novel technique for generating the summarization of domain specific text.
275,"Due to a large variety of noisy information embedded in Web pages, Web-page classification is much more difficult than pure-text classification. We propose to improve the Web-page classification performance by removing the noise through summarization techniques. Experimental results show that the classification algorithms (NB or SVM) augmented by any summarization approach can achieve an improvement by more than 5.0%.",Web-page classification is much more difficult than pure-text classification. We propose to improve the performance by removing the noise through summarization techniques. We give empirical evidence that ideal Web-page summaries generated by human editors can indeed improve performance.
276,This paper applies NMF to feature extraction for Electroencephalogram (EEG) signal classification. The basic idea is to decompose the magnitude spectra of EEG signals from six channels via NMF.,Nonnegative matrix factorization ( NMF) is a powerful feature extraction method for nonnegative data. The basic idea is to decompose the magnitude spectra of EEG signals from six channels via NMF.
277,"highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research with a significant improvement of 25% in the performance of the running time of the algorithm.","Machine learning (ML) and data mining (DM) techniques have a vital role in healthcare systems. It is proven that a chance of 12% error remains in the diagnosis of the diseases by the medical practitioners. The highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research."
278,Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We attempted to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units.,Aims to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units. Findings suggest the possibility for future efforts towards more efficient systems.
279,"ProMine ontology enrichment solution was applied in IT audit domain of an e-learning system. After seven cycles of the application ProMine, the number of automatically identified new concepts are significantly increased.",ProMine has two main contributions; one is the semantic-based text mining approach for automatically identifying domain-specific knowledge elements. The other is the automatic categorization of these extracted knowledge elements by using Wiktionary.
280,"To enter scientific evidence into a United States court, a tool must be reliable and relevant. The reliability of evidence is tested by applying ""Daubert"" guidelines. Open source tools may more clearly and comprehensively meet the guidelines than closed source tools.","To enter scientific evidence into a U.S. court, a tool must be reliable and relevant. To date, there have been few legal challenges to digital evidence. This paper examines the Dau"
281,"We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We introduce a novel deep learning approach to localization by learning to predict object boundaries.",This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We introduce a novel deep learning approach to localization by learning to predict object boundaries.
282,"PageRank vector needs to be calculated, that implies calculations for a stationary distribution, stochastic matrix. The matrices hold the link structure and the guidance of the web surfer. As links are added every day, and the number of websites goes beyond billions, the modification of the web link's structure in the web affects the PageRank.","PageRank is based on a relationship between search results and the structure of web links. The PageRank vector needs to be calculated, that implies calculations for a stationary distribution, stochastic matrix. Even though it is a simple formula, PageRank runs a successful business."
283,recent advancements in the Internet of Things (IoT) have enabled the development of smart parking systems that use services of third-party parking recommender system to provide recommendations of personalized parking spot to users based on their past experience. The indiscriminate sharing of users' data with an untrusted (or semitrusted) parking recommender system may breach the privacy because users' behavior and mobility patterns could be inferred by analyzing their past history.,Smart parking systems use services of third-party parking recommender system to provide recommendations of personalized parking spots. Indiscriminate sharing of users' data with an untrusted system may breach privacy. Users' behavior and mobility patterns could be inferred by analyzing their past history. We present two solutions that preserve privacy of users by using k-anonymity and differential privacy techniques.
284,"Clustering of sentences is performed where the importance of each sentence in a document is attributed with three features. Two clustering techniques, namely, k-means and fuzzy C-means are considered.","This article presents an extractive text summarization technique for single document using partition based clustering algorithms. The importance of each sentence in a document is attributed with three features namely, term score, keywords and average cosine similarity."
285,Electroencephalogram (EEG) recording is relatively safe for the patients who are in deep coma or quasi brain death. The objective of this paper is to apply deep learning method to EEG signal analysis in order to confirm clinical brain death diagnosis. A deep CNN was trained to obtain the similarity degree of the patients' EEG signals with the clinical diagnosed symptoms.,EEG recording is often used to verify the diagnosis of brain death in clinical practice. A deep CNN was trained to obtain the similarity degree of the patients' EEG signals with the clinical diagnosed symptoms. This method can evaluate the condition of the brain damage patients.
286,"In VANETs, vehicle nodes communicate with each other using wireless links. However, the nodes are highly mobile and the topology of the network changes rapidly. The design of routing protocols in VANETs is still a crucial issue.","Vehicular Ad hoc Networks (VANET) is one of the emerging mobile ad hoc networking paradigms (MANET) In VANETs, vehicle nodes communicate with each other using wireless links. The topology of the network changes rapidly and routing protocols are still a crucial issue."
287,"More news articles which are unstoppable increasing, causing problems with grouping news according to appropriate kind of label. This study provides an understanding of the SVM method for news categorization on Indonesian news dataset. Use of Information Gain as feature selection improve accuracy than without any feature selection.","This study provides an understanding of the SVM method for news categorization on Indonesian news dataset. Use of Information Gain as feature selection improve accuracy than without any feature selection. Our model give satisfying result with 98,057% accuracy of Indonesia news classification."
288,"The importance of text summarization grows rapidly as the amount of information increases exponentially. In this paper, we present new method for Persian Text Summarization based on fractal theory.","In this paper, we present new method for Persian Text Summarization based on fractal theory. The main goal of this method is"
289,This paper focuses on the person identification based on feature extracted from the EEG which can show a direct connection between EEG and the genetic information of subjects. Correct classification scores at the range of 80% to 100% reveal the potential of our approach for person classification/identification. The novelty of this work is in the combination of AR parameters and the network type (competitive network) that we have used.,Direct connection between ElectroEncephaloGram (EEG) and the genetic information of individuals has been investigated by neurophysiologists and psychiatrists since 1960's. In this work the full EO EEG signal of healthy individuals are estimated by an autoregressive (AR) model and AR parameters are extracted as features.
290,"Until February 23rd 2020, more than 77 000 cases of 2019-nCoV infection have been confirmed in China. This paper suggests the necessary medical protective measures for oral and maxillofacial surgery outpatients and wards.",More than 77 000 cases of 2019-nCoV infection have been confirmed in China. It has interferred with ordinary medical practice of oral and maxillofacial surgery seriously. This paper suggests the necessary medical protective measures.
291,"Traditionally, medical discoveries are made by observing associations and then designing experiments to test these hypotheses. In this paper, we use deep learning, a machine learning technique that learns its own features, to discover new knowledge from retinal fundus images. We predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images.",Deep learning is a machine learning technique that learns its own features. It can be used to discover new knowledge from retinal fundus images. We predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images.
292,Machine learning is a technique converts the raw clinical data into an informational data that helps for decision making and prediction. Cardiovascular disease is one of the major causes of mortality around the world. Pre-processing will be done first considering the clinical data. It will be spited into train and test data with which accuracy can be achieved.,Cardiovascular disease is one of the major causes of mortality around the world. Prediction of cardiovascular disease is more important in the clinical survey analysis. Predictions can be achieved by selecting a correct combination of prediction models and features. Pre-processing will be done first considering the clinical data.
293,heart diseases are currently a major cause of death in the world. This problem is severe in developing countries in Africa and Asia. We propose a method named CardioHelp which predicts the probability of the presence of cardiovascular disease in a patient by incorporating a deep learning algorithm called convolutional neural networks (CNN) Experimental results show that the proposed method outperforms the existing methods in terms of performance evaluation metrics.,"Heart diseases are currently a major cause of death in the world. This problem is severe in developing countries in Africa and Asia. A heart disease predicted at earlier stages not only helps the patients prevent it, but can also help the medical practitioners learn the major causes of a heart attack."
294,"Healthcare is being discovered among highly visible fields like marketing, e-business, and retail. There is an opulence of data available within the healthcare systems. There is a scarcity of useful analysis tool to find hidden relationships in data.","There is an opulence of data available within the healthcare systems. However, there is a scarcity of useful analysis tool to find hidden relationships in data. Some experiment has been conducted to compare the execution of predictive data mining technique on the same dataset."
295,Identification of different risk factors and early prediction of mortality for patients with heart failure are crucial for guiding clinical decision-making in Intensive care unit cohorts. We developed a comprehensive risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF) The experimental results showed that the developed risk model was superior to those used in previous studies and the conventional random survival forest-based model with an out-of-bag C-statistic value of 0.821.,"Early prediction of mortality for patients with heart failure is crucial for guiding clinical decision-making. In this paper, we developed a risk model for predicting heart failure mortality with a high level of accuracy using an improved random survival forest (iRSF) The developed iRSF-based risk model could serve as a valuable tool for clinicians."
296,Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI) The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions. We present two EEG-based preference classification studies: using (1) kNN for a 10-subject EEG classification problem; (2) deep learning for an expanded 16-subject EEG classification problem.,Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI) The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions.
297,Punjabi Text Summarization is the process of condensing the source Punjabi text into a shorter version. It comprises two phases: 1) Pre Processing 2) Processing. Pre Processing is structured representation of the Punjabi text.,Punjabi Text Summarization is the process of condensing the source Punjabi text into a shorter version. It comprises two phases: 1) Pre Processing and 2) Processing.
298,"problem in existing methods is that the apps are uploaded from untrusted sources and static features extracted for categorization can be easily masked by obfuscation or encryption. To solve this problem and improve the categorization accuracy, we propose to extract features from usage data generated by apps running on mobile devices. We propose a new privacy-preserving categorization method of mobile apps based on learning patterns from a large scale of usage data.",Categorization of mobile apps is essential for app stores in maintaining a huge quantity of apps efficiently and securely. The problem in existing methods is that the apps are uploaded from untrusted sources and the static features extracted for categorization can be easily masked by obfuscation or encryption. We propose a new privacy-preserving categorization method based on learning patterns from a large scale of usage data.
299,"Current solutions are able to provide protection to patients' data during data transmission to some extent. We propose a practical framework called PrivacyProtector, patient privacy protected data collection. The framework is secure and privacy-protected against various attacks.",Medical devices are more vulnerable to numerous security threats and attacks than other network devices. PrivacyProtector includes the ideas of secret sharing and share repairing (in case of data loss or compromise) for patients' data privacy. The framework uses a distributed database consisting of multiple cloud servers.
300,"Face to face communication is a real-time process operating at a time scale in the order of 40 milliseconds. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time. The system has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator.","Face to face communication is a real-time process operating at a time scale in the order of 40 milliseconds. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time. It has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator."
301,"automatic text summarization system generates a summary, i.e. short length text that includes all the important information of the document. This paper presents a comprehensive survey of recent text summarization extractive approaches developed in the last decade. It presents a comprehensive survey of recent text summarization extractive approaches developed in the last decade.","There is growing interest among the research community for developing new approaches to automatically summarize the text. Since the advent of text summarization in 1950s, researchers have been trying to improve techniques for generating summaries. Summary can be generated through extractive as well as abstractive methods. During a decade, several extractive approaches have been developed for automatic summary generation."
302,Recognition of motor imagery intention is one of the hot current research focuses of brain-computer interface (BCI) studies. It can help patients with physical dyskinesia to convey their movement intentions. This paper proposes a new deep multi-view feature learning method for the classification task of motor imagery electroencephalogram (EEG) signals.,Motor imagery intention is one of the hot current research focuses of brain-computer interface (BCI) studies. It can help patients with physical dyskinesia to convey their movement intentions. This paper proposes a new deep multi-view feature learning method for the classification task of motor imagery electroencephalogram (EEG) signals.
303,Deep learning (DL) has been demonstrated to be effective in helping to determine and forecast a huge amount of data produced by the health industry. This paper aims to find the key features of the prediction of cardiovascular diseases through the use of machine learning techniques. The prediction model is adding various combinations of features and various established methods of classification.,"Heart disease is one of the crucial impacts of mortality in the country. In clinical data analysis, predicting cardiovascular disease is a primary challenge. This paper aims to find the key features of the prediction of cardiovascular diseases through the use of machine learning techniques. The prediction model is adding various combinations of features and various established methods of classification."
304,Many existing works for text summarization are generally evaluated by using recall-oriented understudy for gisting evaluation (ROUGE) scores. Korean is an agglutinative language that combines various morphemes into a word that express several meanings. We propose evaluation metrics that reflect semantic meanings of a reference summary and the original document.,"Text summarization is the process of generating a shorter form of text from a source document to preserve salient information. Many existing works for text summarization are evaluated by using recall-oriented understudy scores. Because Korean is an agglutinative language that combines various morphemes into a word, ROUGE is not suitable for Korean summarization."
305,RMP classifies an image by extracting feature vectors at multiple subwindows at multiple locations and scales. RMP outperforms the state-of-the-art performance by a wide margin on the challenging PASCAL VOC2012 dataset for human action recognition on still images.,"We propose Regularized Max Pooling (RMP) for image classification. RMP classifies an image (or an image region) by extracting feature vectors at multiple subwindows at multiple locations and scales. Unlike Spatial Pyramid Matching, RMP"
306,"code summarization (aka comment generation) provides a high-level natural language description of the function performed by code. This paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. The experimental results demonstrate that our approach outperforms the baselines by around 22% to 45% in BLEU-1.","Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code. Authors say state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. They say their new code summarization approach using a hierarchical attention network incorporates code features, including type-augmented abstract syntax trees and program control flows. Their approach outperforms the baselines by around 22% to 45% in BLEU-1 and outperforms other approaches."
307,"Platooning is a promising intelligent transportation framework that can improve road capacity, on-road safety, and fuel efficiency. In this paper, subchannel allocation scheme and power control mechanism are proposed for LTE-based inter-vehicle communications in a multiplatooning scenario. Simulation results are given to demonstrate that the proposed approaches can reduce the communication delay compared to D2D-unicast based RA scheme.","Platooning is a promising intelligent transportation framework that can improve road capacity, on-road safety, and fuel efficiency. An efficient resource allocation (RA) approach is required for the timely and successful delivery of inter-vehicle information within multiplatoons. Subchannel allocation scheme and power control mechanism are proposed for LTE-based inter- vehicle communications."
308,"RTS implements the TextRank algorithm, so it can be exploited to generate very short summaries. We validate the feasibility of RTS on blog sites and show that runtime performance incurred by our current implementation is negligible.","Responsive text summarization is an approach to web design aimed at allowing desktop web pages to be read in response to the size of the device a user is browsing with. RTS implements the TextRank algorithm, so it can be exploited to generate very short summaries or longer summaries."
309,The purpose of this study is to judge whether this combination method of multispectral image and convolutional neural network (CNN) method can be used to distinguish amnestic mild cognitive impairment.,This study is to judge whether this combination method of multispectral image and convolutional neural network (CNN) method can
310,Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. Scientific article summarization is one such case that is different from general domain summarization.,"Summarization of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. We show that, contrary to the common belief, RouGE is not much reliable in evaluating scientific articles."
311,"recent years have seen remarkable success in the use of deep neural networks on text summarization. There is no clear understanding of why they perform so well, or how they might be improved. We seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas.","Deep neural networks have been used for remarkable success in text summarization. There is no clear understanding of why they perform so well, or how they might be improved. This paper seeks to better understand how neural extractive summarization systems could benefit."
312,"Information safety is considered as most critical issue in any network system and it also the case in VANET. In VANETs wireless conversation between automobiles thus attackers breach confidentiality, privacy, and authenticity properties. Different classes of VANET attacks are also discussed in this paper.",Vehicular Ad-hoc Network(VANET) attained vast interest and research initiatives is also increased due to the range of solutions it can provide. This paper presents the safety challenges and existing threads in the VANET system.
313,"security, privacy and trust are still the major concerns for such networks. Insufficient enforcement of these requirements introduces non-negligible threats to M-IoT devices and platforms. This paper covers such requisites and presents comparisons of statethe-art solutions for IoT which are applicable to security, privacy, and trust in smart and connected M-IoT networks.","IoT facilitates operations through ubiquitous connectivity by providing Internet access to all the devices with computing capabilities. Security, privacy and trust are still the major concerns for such networks. Lack of enforcement of these requirements introduces non-negligible threats to devices and platforms. This paper presents comparisons of state-of-the-art solutions for IoT which are applicable to security, privacy, and trust."
314,"selective search strategy is needed to enable the use of more expensive features and classifiers. We propose to generate many approximate locations over few and precise object delineations. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image.","The current state-of-the-art is based on exhaustive search. We propose to generate many approximate locations over few and precise object delineations. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image."
315,"One hundred twenty eight channel EEG signal was used in the experiments. The signal was recorded for 40 people, during the process of imagining right and left hand movements.","One hundred twenty eight channel EEG signal was used in the experiments. Signal was recorded for 40 people, during the process of imagining right and left hand movements. Feature extraction was performed using frequency analysis with a resolution of 1Hz."
316,"We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The experimental results show that the proposed selective encoding model outperforms the state-ofthe-art baseline models.","Model consists of a sentence encoder, a selective gate network, and an attention equipped decoder. It is built with recurrent neural networks. Second level representation is tailored for sentence summarization task, which leads to better performance."
317,Automatic summarization of Indian text has received a very little attention so far. We propose an approach for summarizing Hindi text based on semantic graph of the document using Particle Swarm Optimization (PSO) algorithm. PSO is one of the most powerful bio-inspired algorithms used to obtain optimal solution.,Automatic text summarization can be defined as a process of extracting and describing important information from given document using computer algorithms. Researchers propose an approach for summarizing Hindi text based on semantic graph of the document using Particle Swarm Optimization (PSO) algorithm.
318,"Topic modeling techniques have been recently applied to analyze and model source code. Such techniques exploit the textual content of source code to provide automated support for several basic software engineering activities. The textual content of source code, embedded in its identifiers, comments, and string literals, tends to be sparse in nature.","State-of-the-art topic modeling techniques tend to be data intensive, so this paper proposes a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. The results show that our approach produces stable, more interpretable, and more expressive topics."
319,"This paper presents sentence extraction-based automatic speech summarization techniques for making abstracts from spontaneous presentations. We propose a summarization technique using dimension reduction based on singular value decomposition which effectively focuses on the most salient topics of each presentation. The correlation analysis between subjective and objective evaluation scores confirms that summarization accuracy, sentence F-measure, and 2 and 3-gram recall are the most effective among the objective evaluation metrics investigated in this paper.",This paper presents sentence extraction-based automatic speech summarization techniques for making abstracts from spontaneous presentations. We propose a summarization technique using dimension reduction based on singular value decomposition which focuses on the most salient topics of each presentation. We also investigate the combination of confidence measure and linguistic likelihood to effectively extract sentences with less recognition error.
320,"Data coming from a real-world complex system are usually contaminated by certain levels of noise or some irrelevant components, which do not contribute to improve signal classification accuracy. We propose the multi-scale principal component analysis (PCA) method, which combines discrete wavelet transform and PCA for de-noising and decomposing complex biomedical signals. We also develop a new classification method, called Empirical Classification (EC), based on the characteristics of data we analyzed.","Noise or irrelevant components can affect signal classification accuracy. In signal de-noising, the performance of any statistical method may be impacted by the noise. Authors propose the multi-scale principal component analysis (PCA) method for de- noising complex biomedical signals."
321,Results are compared with a couple of other existing text summarization methods keeping the DUC2002 data as benchmark. The initial results obtained seem promising and encouraging for future work in this area.,This paper presents an extraction based single document text summarization technique using Genetic Algorithms. A given document is represented as a weighted Directed Acyclic Graph. A fitness function is defined to mathematically express the quality of a summary.
322,the transition of transportation sector from Internal Combustion Engines (ICE) to Electric Vehicles (EVs) has raised many concerns about their users. The development of Smart Station Search Assistance (S3A) system is an innovative step in this direction.,The transition of transportation sector from Internal Combustion Engines (ICE) to Electric Vehicles (EVs) has raised many concerns about their users. The development of Smart Station Search Assistance (S3A) system is an innovative step in this direction.
323,"the proliferation of smartphones introduces new opportunities in digital forensics. In this paper we examine the feasibility of ad-hoc data acquisition from smartphone sensors by implementing a device agent for their collection in Android. We discuss our experience regarding the data collection of smartphone sensors, as well as legal and ethical issues that arise from their collection.","The proliferation of smartphones introduces new opportunities in digital forensics. Smartphones are equipped with sensors which can be used to infer the user's context. This context can aid in the rejection or acceptance of an alibi, or even reveal a suspect's actions or activities."
324,"Traditional summarization research has focused on extracting informative sentences from standard documents. This paper explores such an approach by modeling Web documents and social contexts into a unified framework. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries.","Traditional summarization research has focused on extracting informative sentences from standard documents. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient algorithm is designed to learn the proposed factor graph model."
325,there has been a growing interest in applying deep learning techniques for automatic generation of software. We present here an approach to this problem using a set of low-level features derived from lexical analysis of software code. We compare different feature sets for categorizing software and also apply different supervised machine learning algorithms to perform the classification task.,"In recent years, there has been a growing interest in applying deep learning techniques to software categorization. We present an approach using a set of low-level features derived from lexical analysis of software code. We evaluate our approach by applying it to categorize popular Python projects from Github."
326,Numerous software design patterns have been introduced and cataloged either as a canonical or a variant solution to solve a design problem. We exploit a text categorization based approach via Fuzzy c-means (unsupervised learning technique) that targets to present a systematic way to group the similar design patterns and suggest the appropriate design pattern(s) to developers related to the specification of a given design problem. We also propose an evaluation model to assess the effectiveness of the proposed approach in the context of several real design problems and design pattern collections.,"Existing automatic techniques for design pattern selection are limited to semi-formal specification, multi-class problem, adequate sample size and individual classifier training. We exploit a text categorization based approach via Fuzzy c-means (unsupervised learning technique) that targets to present a systematic way to group the similar design patterns. We also propose a new feature selection method Ensemble-IG to overcome the multi- class problem and improve the classification performance."
327,"The efficient features extraction is the key challenge for classifying real authors of specific source codes. The Program Dependence Graph with Deep Learning (PDGDL) methodology is proposed to identify authors from different programming source codes. The results are appreciable in outperforming the existing techniques from the perspective of classification accuracy, precision, recall, and f-measure metrics.","SCAA is to find the real author of source code in a corpus. It is a privacy threat to open-source programmers, but, it may be significantly helpful to develop forensic based applications. Such as, ghostwriting detection, copyright dispute settlements, and other code analysis applications. The proposed work is analyzed on 1000 programmers' data, collected from Google Code Jam (GCJ)"
328,"Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224224) input image. This requirement is 'artificial' and may hurt the recognition accuracy for images or sub-images of an arbitrary size/scale. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale.","Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224224) input image. In this work, we equip the networks with a more principled pooling strategy, ""spatial pyramid pooling"", to eliminate the above requirement. Our SPP-net achieves state-ofthe-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
329,the absence of sentence boundaries in the recognized text complicates the summarization process. We use a syntactic analyzer to identify continuous segments in the recognized text. The results of the proposed system were compared with the results of sentence summarization in the reference articles.,The system generates text summaries from input audio using three independent components. The absence of sentence boundaries in the recognized text complicates the summarization process. The readers marked two aspects of the summaries: readability and information relevance. The results of the proposed system were compared with the results of sentence summarization.
330,Automatic text summarization attempts to provide an effective solution to today's unprecedented growth of textual data. This paper proposes an innovative graph-based text summarization framework for generic single and multi document summarization. Experimental results indicate that the proposed summarizer outperforms all state-of-the-art related comparators in the single document summarization based on the ROUGE-1 and ROUGE-2 measures.,This paper proposes an innovative graph-based text summarization framework for generic single and multi document summarization. The summarizer benefits from two well-established text semantic representation techniques as well as the constantly evolving collective human knowledge in Wikipedia. Experimental results indicate that the proposed summarizer outperforms all state-of-the-art related comparators.
331,"We investigate feature stability in the context of clinical prognosis derived from high-dimensional electronic medical records. To reduce variance in the selected features that are predictive, we introduce Laplacian-based regularization into a regression model.",We investigate feature stability in the context of clinical prognosis derived from electronic medical records. We introduce Laplacian-based regularization into a regression model. We demonstrate better feature stability and goodness
332,Graph-based approaches for multi-document summarization have been widely used to extract top sentences for a summary. We propose StarSum a star bipartite graph which models sentences and their topic signature phrases.,StarSum is a star bipartite graph which models sentences and their topic signature phrases. The approach ensures sentence similarity and content importance from the graph structure. A DUC experiment shows the effectiveness of StarSum compared to different baselines.
333,"In this paper, a social media based traffic status monitoring system is established. The system is initiated by a transportation related keyword generation process. An association rules based iterative query expansion algorithm is applied to extract real time transportation related tweets for incident management purpose. Comparison results show that our query expansion method for tweets extraction outperforms the previous ones.",This paper describes a social media based traffic status monitoring system. It uses an association rules based iterative query expansion algorithm to extract real time transportation related tweets. The algorithm is used to extract tweets for incident management purpose.
334,"RST is an analytic framework designed to account for text structure at the clause level. CIT is an integrated concept of syntactic, semantic and pragmatic information. System extracts rhetorical structure of text and compound of rhetorical relations between sentences.",In this paper we present an automatic text summarization method based on natural language understanding. The system extracts the rhetorical structure of text and the compound of the rhetorical relations between sentences. It then cuts out less important parts from the extracted structure.
335,"Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks.","Deep learning (DL) techniques are gaining more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks."
336,"Emotion recognition from Electroencephalography (EEG) is proved to be a good choice as it cannot be mimicked like speech signals or facial expressions. EEG signals of emotions are not unique and it varies from person to person as each one has different emotional responses to the same stimuli. In this paper, a subject independent emotion recognition technique is proposed from EEG signals using Variational Mode Decomposition (VMD) as a feature extraction technique and Deep Neural Network as the classifier.","Emotion recognition from Electroencephalography (EEG) is proved to be a good choice as it cannot be mimicked like speech signals or facial expressions. EEG signals of emotions are not unique and it varies from person to person as each one has different emotional responses to the same stimuli. In this paper, a subject-independent emotion recognition technique is proposed from EEG signals using Variational Mode Decomposition (VMD) and Deep"
337,this study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints.,This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. Our experiments with the NTCIR ACLIA test collections show
338,The World Wide Web is a prime example of a collection where information changes both frequently and significantly over time. This paper proposes different approaches to generate summaries using extractive summarization techniques. It is observed that the approach using the LDA model outperforms all the other approaches.,"In real world settings, collections and individual documents rarely stay unchanged over time. The World Wide Web is a prime example of a collection where information changes both frequently and significantly. The goal is to obtain a summary that describes the most significant changes made to a document during a given period. This paper proposes different approaches to generate summaries using extractive summarization techniques. It is observed that the approach using the LDA model outperforms all the other approaches."
339,"Finding key sentences or paragraphs from a document is an important and challenging problem. In recent years, the amount of text data has grown astronomically and this growth has produced a great demand for text summarization. We propose a new text summarization process by text mining and social network methods.",Text data has grown astronomically and this growth has produced a great demand for text summarization. We propose a new summarization process by text mining and social network
340,Summarization techniques need to be used to reduce the users time in reading the whole information available on web. We propose a Malayalam text summarization system which is based on MMR technique with successive threshold. Here the sentences are selected based on the concept of maximal marginal relevance.,Text summarization is nothing but summarizing the content of given text document. In this paper propose a Malayalam text summarization system which is based on MMR technique with successive threshold. The key idea is to use a unit step function at each step to decide the maximum marginal relevance.
341,"scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. We propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.","Scene categorization is a fundamental problem in computer vision. Scene understanding research has been constrained by the limited scope of currently-used databases. The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images."
342,"Treating all features equally causes poor summary generation. In this paper, we investigate the effect of the feature structure on the features selection using particle swarm optimization. The particle swarm optimization is trained using DUC 2002 data to learn the weight of each feature.",The features are the main entries in text summarization. Treating all features equally causes poor summary generation. The particle swarm optimization is trained using DUC 2002 data to learn the weight of each feature. The experimental results shown that simple features are less effective than the combined features.
343,Automatic text summarization systems aim to make their created summaries closer to human summaries. The summary creation under the condition of the redundancy and the summary length limitation is a challenge problem. The automatic text summarization system which is built based on exploiting of the advantages of different techniques in form of an integrated model could produce a good summary for the original document.,Automatic text summarization systems aim to make their created summaries closer to human summaries. The system is built based on exploiting of the advantages of different techniques. The experimental results showed that our model got the best performance over all methods.
344,"Quite often, the summarized text lacks cohesion and becomes difficult to comprehend. The objective of this paper is to investigate the proposed Swarm LSA-PSO model performs better than alternative methods.",Swarm LSA-PSO model shows promising results in context based text summarization using BOW clustering approach. The input text documents were downloaded from Document Understanding Conference (DUC) 2002 dataset. The terms matrix was constructed from co-occurrence of terms using Bag-of-Words.
345,"Automatic text summarization is the process of condensing an original document into shorter form to create smaller, compact version from the abundant information that is available. Fuzzy logic has appeared as a powerful theoretical framework for studying human reasoning.","Automatic text summarization is the process of condensing an original document into shorter form. Fuzzy logic has appeared as a powerful theoretical framework for studying human reasoning. This paper is a systematic literature review to gather, analyze, and report the trends, gaps and prospects of using fuzzy logic."
346,"Relevance Prediction is a more intuitive measure of an individual's performance on a real-world task than interannotator agreement. This measure is shown to be a more reliable measure of task performance than LDC Agreement, a current gold-standard based measure used in the summarization evaluation community. The significance level for detected differences is higher for the former than for the latter.",Relevance Prediction is a more intuitive measure of an individual's performance on a real-world task than interannotator agreement. This measure is shown to be a more reliable measure of task performance than LDC Agreement. The significance level for detected differences is higher for the former than for the latter.
347,Traditional taxi systems in metropolitan areas often suffer from inefficiencies due to uncoordinated actions as system capacity and customer demand change. Large amounts of information regarding customer demand and system status can be collected in real time. We present a receding horizon control (RHC) framework to dispatch taxis.,Traditional taxi systems often suffer from inefficiencies due to uncoordinated actions as system capacity and customer demand change. Large amounts of information regarding customer demand and system status can be collected in real time. This information provides opportunities to perform various types of control and coordination for large-scale intelligent transportation systems.
348,"research investigates the causes and impact of war against terrorism on Pakistan's economy, governance and social life. Empirical results show that terrorism has substantially affected Pakistan's economy and governance.","This research investigates the causes and impact of war against terrorism on Pakistan's economy, governance and social life. The data was collected through sources available on official websites, available literature and by using a questionnaire from 320 respondents living in FATA area."
349,"There are large number of secure patterns, and it is quite difficult to choose an appropriate pattern. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization. F-measure for 17 different design problems shows around 81% accuracy with recall up to 0.69%.","Secure patterns provide a solution for the security requirement of the software. There are large number of secure patterns, and it is difficult to choose an appropriate pattern. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization."
350,"This paper aims to perform text feature weighting for summarization of documents in bahasa Indonesia using genetic algorithm. Only using f2, f4, f5, and f11 can deliver a similar performance using all eleven features.",This paper aims to perform text feature weighting for summarization of documents in bahasa Indonesia using genetic algorithm. We investigate the effect of the first ten sentence features on the summarization task. We use latent semantic feature to increase the accuracy. All feature score functions are used to train a genetic algorithm model to obtain a suitable combination of feature weights.
351,study is focused on how the algorithm can be applied on number of documents. The working principle and steps which should be followed for implementation of TF-IDF are elaborated.,Study is focused on how the algorithm can be applied on number of documents. strengths and weaknesses of TD-IDF algorithm are compared. Future research directions are discussed.
352,Summarization is a very interesting and useful task that gives support to many other tasks. It takes advantage of the techniques developed for related Natural Language Processing tasks.,Text Summarization is a challenging problem these days. It is a useful task that gives support to many other tasks. It takes advantage of the techniques developed for related Natural Language Processing tasks.
353,"Summaries formed by our model can appease readers with vision difficulties while keeping them updated. We apply a handful of available semantic information (definition, sentimental polarity) of words to enhance edgeweights.","In this paper, we introduce an unsupervised graph based ranking model for text summarization. Our model builds a graph by collecting words, and their lexical relationships from the document. We apply a handful of available semantic information to enhance edgeweights (interconnectivity) between nodes."
354,"automatic text summarization is required to automate the process of summarizing text by extracting only the salient information from the documents. In this paper, we propose a text summarization model based on classification using Adaptive Neuro-Fuzzy Inference System (ANFIS) The model can learn to filter high quality summary sentences.","The information overload faced by today's society has created a big challenge for people who want to look for relevant information from the internet. In this paper, we propose a text summarization model based on classification using Adaptive Neuro-Fuzzy Inference System (ANFIS) The model can learn to filter high quality summary sentences."
355,"Oil and gas industry could not escape from this predicament. We develop an Automated Text Summarization System known as AutoTextSumm to extract the salient points of oil and gas drilling articles. The system performance of AutoTextSumm is evaluated using the formulae of precision, recall and F-score.",WWW has caused rapid growth of information explosion. Readers are overloaded with too many lengthy text documents. Oil and gas industry could not escape from this predicament. We develop an Automated Text Summarization System known as AutoTextSumm.
356,"Enormous amount of online information, available in legal domain, has made legal text processing an important area of research. We attempt to survey different text summarization techniques that have taken place in the recent past.","Legal text processing has become an important area of research. In this paper, we look at different text summarization techniques in the recent past. We briefly cover a few software tools used in legal text summarizing."
357,"this paper presents a text summarization in Android mobile devices. The research has been inspired by the new paradigm shift in accessing information ubiquitously at anytime, anywhere and anyway on mobile devices. It is a challenge to browse large documents in a mobile device because of its small screen size and information overload problems.",This paper presents a text summarization in Android mobile devices. It is a challenge to browse large documents in a mobile device because of its small screen size and information overload problems. The objectives of the paper are to integrate WordNet 3.1 into the proposed system called TextSumIt.
358,this paper proposes two approaches to address text summarization: modified corpus-based approach (MCBA) and LSA-based T.R.M. approach (LSA + T.R.M.) We evaluate LSA + T.R.M. both with single documents and at the corpus level to investigate the competence of LSA in text summarization. The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles.,"This paper proposes two approaches to address text summarization. The first is a trainable summarizer, which takes into account several features to generate summaries. The second uses latent semantic analysis to derive the semantic matrix of a document or a corpus. The two novel approaches were measured at several compression rates."
359,"constructing a software program to summarize web pages or electronic documents would be a useful technique. Such technique would speed up of reading, information accessing and decision making process. Arabic NEWSWIRE-a corpus is used as a data set in the algorithm evaluation.","The amount of textual information available on the web is estimated by terra bytes. Such technique would speed up of reading, information accessing and decision making process. This paper investigates a graph based centrality algorithm on Arabic text summarization problem (ATS)"
360,"In this paper, by using BabelNet knowledge base and its concept graph, a system for summarizing text is offered. The proposed method by compared to other methods produces summaries with more quality and fewer redundancies.","In this paper, by using BabelNet knowledge base and its concept graph, a system for summarizing text is offered. The proposed method produces summaries with more quality and fewer redundancies. To compare and evaluate the performance of the proposed method, DUC2004 is used."
361,"major issues for text summarization are eliminating redundant information, identifying important difference among documents and covering the informative content. In this paper, we propose a Sentence-Level Semantic Graph Model (SLSGM) which exploits the semantic information of sentence.",Sentence-Level Semantic Graph Model (SLSGM) exploits the semantic information of sentence. SLSGM considers sentences as vertices and semantic relationship between sentences as edges. Model can be used for text summarization.
362,"Various techniques have been presented in literature and many are used in commercially available systems. Sentiment analysis is already being used in various domains for analysis of large scale text data interpretation and opinion mining. The proposed scheme is found to be efficient, in particular for 50% summarization.","Sentiment analysis is already being used in various domains for analysis of large scale text data interpretation and opinion mining. This work shows that sentiment analysis can also be used efficiently for the purpose of text summarization. The proposed scheme is found to be efficient, in particular for 50% summarization."
363,it is not easy to extract information because these reports are written in natural language. This paper presents a system that converts a medical text into a table structure. Experimental results demonstrate empirically that syntactic information can contribute to the method's accuracy.,This paper presents a system that converts a medical text into a table structure. The system's core technologies are medical event recognition modules and a negative event identification module. It also proposes an SVM-based classifier using syntactic information.
364,Graph based methods rank sentences using syntactic and semantic information. The proposed method's summary results outperform other topic model based summary results using ROUGE metrics evaluated on DUC 2005 dataset.,This work combined two-tiered topic model with graph based TextRank method to extract better summary sentences. The proposed method's summary results outperform other topic model based summary results using ROUGE metrics.
365,In our experiments we use two types of texts: news paper texts and government texts. Our results show that text type as well as other aspects of texts of the same type influence the performance. Adapting a text summarizer for a particular genre can improve text summarization.,Combining PageRank and Random Indexing provides the best results on government texts. Adapting a text summarizer for a particular genre can improve text summarization.
366,Electroencephalogram (EEG)-based sleep stage analysis is helpful for diagnosis of sleep disorder. The accuracy of previous EEG-based method is still unsatisfactory. Deep learning method provides better classification performance compared to other methods.,Electroencephalogram (EEG)-based sleep stage analysis is helpful for diagnosis of sleep disorder. The accuracy of previous EEG-based method is still unsatisfactory. We propose an EEG- based automatic sleep stage classification method. The proposed method achieves the best accuracy of 88.83%.
367,"deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance, etc. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients. With the burst of multi-omics data, including genomics data, transcriptomics data and clinical information in cancer studies, we believe that deep learning would potentially improve cancer prognosis.","Deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance. Cancer prognosis is to estimate the fate of cancer, probabilities of cancer recurrence and progression, and to provide survival estimation to the patients. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients."
368,"recent studies show that uncompilable commits exist even in high-profile open-source software. Identifying broken code, a potential symptom of careless development, and analyzing how software changes when it becomes uncompilable can shed light on how software quality evolves when developers do not follow best practices.","Recent studies show uncompilable commits exist even in high-profile open-source software. Identifying broken code can shed light on how software quality evolves when developers do not follow best practices. This paper explores the relations between commit type, size and compilability."
369,this study aims at investigating the effect of teaching the summarization of text as a cognitive strategy on achievement of male and female students' reading comprehension. 60 English undergraduates studying at the University of Tonekabon participated in this study. The findings suggest that teaching summarization strategy empowers students' reading comprehension ability.,This study aims at investigating the effect of teaching the summarization of text as a cognitive strategy on achievement of male and female students' reading comprehension. 60 English undergraduates studying at the University of Tonekabon participated in the study.
370,580 residents of Hong Kong aged 60 or above were interviewed regarding their decision to whether to make a trip by an ordinary/accessible taxi to attend a non-compulsory social activity in hypothetical scenarios. The elderly using crutches or a wheelchair had a stronger preference for accessible taxis rather than the non-walking aid users.,"Accessible taxis are infrequently used by elderly passengers in Hong Kong, due in part to high fares and long walking and wait times. The elderly using crutches or a wheelchair had a stronger preference for accessible taxis. A step-wise taxi fare subsidy scheme is recommended to integrate with current transport policy measures to look after the needs of the elderly. A supply-side subsidy is also recommended to support local taxi operators to purchase accessible taxis and increase the fleet size."
371,"This paper focuses on comparing the impact of the local attention in Long Short-Term Memory (LSTM) model to generate an abstractive text summarization. The global attention-based model produces better ROUGE-1, where it generates more words contained in the actual summary.",This paper focuses on comparing the impact of the local attention in Long Short-Term Memory (LSTM) model to generate an abstractive text summarization. Developing a model using a dataset of Amazon Fine Food Reviews and evaluating it using dataset of GloVe.
372,"paper builds a stacked autoencoder deep learning classification network consist of an input layer, two autoencoder hidden layers and a softmax classifier output layer based on SJTU Emotion EEG Dataset (SEED) The well-trained network is used to classify three emotion states including happy, neural and grief. Fourteen experiments are performed with 5-fold cross validation.","Emotion recognition is of great significance in artificial intelligence, health care, distance education, military field and so on. The paper builds a stacked autoencoder deep learning classification network. The well-trained network is used to classify three emotion states including happy, neural and grief. Fourteen experiments are performed with 5-fold cross validation."
373,"millimeter-wave imaging techniques and systems have been developed at the Pacific Northwest National Laboratory (PNNL), Richland, WA. These techniques were derived from microwave holography techniques that utilize phase and amplitude information recorded over a two-dimensional aperture to reconstruct a focused image of the target.","Millimeter-wave imaging is well suited for the detection of concealed weapons or other contraband. Millimeter-waves are nonionizing, readily penetrate common clothing material, and are reflected from the human body. Practical weapon detection systems for airport or other high-throughput applications require high-speed scanning on the order of 3 to 10 s."
374,"Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets.","Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. In this study, by following preprocessing steps, a Feature Dependent Naives (FDNB) classification method is proposed."
375,"TAAS outperforms BART, a well-recognized SOTA model, by 2%, 8%, and 12% regarding the F measure of ROUGE-1, ROUGE-2, and ROUGE-L. Training PEGASUS and ProphetNet requires enormous computing capacity beyond what we used in this study.","Automatic text summarization aims at condensing a document to a shorter version while preserving the key information. Most current state-ofthe-art (SOTA) abstractive summarization methods are based on the Transformer-based encoder-decoder architecture. This study proposes a topic-aware abstractive. summarization (TAAS) framework by leveraging the underlying semantic structure. of documents. TAAS outperforms BART by 2%, 8% and 12% regarding the F measure of ROUGE-1, RouGE-2, and ROUAGE-L, respectively."
376,"topological data analysis is a relatively new branch of machine learning that excels in studying high-dimensional data. Data objects with mixed numeric and categorical attributes are ubiquitous in real-world applications. In this paper, we propose a novel topological machine learning method for mixed data classification.",Topological data analysis is a branch of machine learning that excels in studying high-dimensional data. Data objects with mixed numeric and categorical attributes are ubiquitous in real-world applications. The proposed method outperforms several state-of-the-art algorithms in the prediction of heart disease.
377,"Internet of Things (IoT) is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. Vehicles can easily exchange safety, efficiency, infotainment, and comfort-related information with other vehicles and infrastructures using vehicular ad hoc networks (VANETs) SIoV is a vehicular instance of the Social IoT (SIoT) where vehicles are the key social entities in machine-to-machine vehicular social networks.","Main vision of Internet of Things is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. Internet of Vehicles is a vehicular instance of the Social IoT (SIoT), where vehicles are the key social entities in the machine-to-machine vehicular social networks."
378,"processing large text corpora using these methods reveals expensive from both the organizational and the financial perspective. We propose micro-task crowdsourcing to evaluate both the intrinsic and extrinsic quality of query-based extractive text summaries. Correlating results of crowd and laboratory ratings reveals high applicability of crowdsourcing for the factors overall quality, grammaticality, non-redundancy, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness.","The intrinsic and extrinsic quality evaluation is an essential part of the summary evaluation methodology. Processing large text corpora using these methods is expensive from both the organizational and the financial perspective. For the first time, we propose crowdsourcing to evaluate the quality of query-based extractive text summaries."
379,this paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document. The approach can naturally make full use of the reinforcement between sentences and keywords by fusing three kinds of relationships between sentences and words. Experimental results show the effectiveness of the proposed approach for both tasks.,This paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single documents. The approach can naturally make full use of the reinforcement between sentences and keywords. The corpus-based approach is validated to work almost as well as the knowledge-
380,"In recent years, Twitter has become one of the most important microblogging services of the Web 2.0. Different state-ofthe-art summarizers are selected and employed for producing multi-lingual tweets in two languages. The goal of this research is to analyze the task of automatic tweet generation from a text summarization perspective.","Twitter has become one of the most important microblogging services of the Web 2.0. Among the possible uses it allows, it can be employed for communicating and broadcasting information in real time. Researchers say relying only on tweets may not be the ideal way to communicate news through Twitter."
381,Multidimensional knowledge representation (MKR) is the result of integrative text mining. This paper introduces cross-dimensional text summarization based on dimensional selection and filtering of results retrieved from MKR knowledge base.,Multidimensional knowledge representation (MKR) is the result of integrative text mining. This paper introduces cross-dimensional text summarization based on dimensional selection and filtering of results retrieved from MKR knowledge base.
382,"the aim has been to construct a summarizer that can be quickly assembled, with the use of only a very few basic language tools. The proposed method is largely language independent, though we only evaluate it on English in this paper.","In this paper we present a novel method for automatic text summarization through text extraction. The new idea is to view all the extracted text as a whole and compute a score for the total impact of the summary. The proposed method is largely language independent, though we only evaluate it on English."
383,"This paper proposes new text summarization approaches based on textual unit association networks. We construct two kinds of textual association networks, namely word-based association network and sentence-based association network. Extensive experiments on benchmark data show that our proposed approaches can achieve better summarization performance than the existing methods.","Paper proposes new text summarization approaches based on textual unit association networks. Textual units refer to words, phrases, sentences, or paragraphs. Intuitively, units containing much co-occurrence information are semantically more salient in a document."
384,This article briefly explains our submitted approach to the DocEng19 competition on extractive summarization. We implemented a recurrent neural network based model that learns to classify whether an article's sentence belongs to the corresponding extractive summary or not.,This article explains our submitted approach to the DocEng'19 competition on extractive summarization. We bypass the lack of large annotated news corpora for extractive
385,Deep learning have achieved remarkable growth in many fields like speech recognition and computer vision. Deep learning in biomedical field is yet to be fully utilized. We propose a novel methodology for convolutional neural network (CNN) based motor imagery (MI) classification using new form of input. The proposed method shows increase in classification accuracy compared to other MI classification methods.,Deep learning in biomedical field is yet to be fully utilized. Method using CNN with magnitude and phase based features can be better than other state-of-the-art approaches. The proposed method shows increase in classification accuracy compared to other MI classification methods.
386,"this paper presents TREADS, a novel travel route recommendation system that suggests safe travel itineraries in real time by incorporating social media data resources and points of interest review summarization techniques. The system consists of an efficient route recommendation service that considers safety and user interest factors.","TREADS is targeted to provide safe, effective, and convenient travel strategies for commuters and tourists. The system consists of an efficient route recommendation service that considers safety and user interest factors. The proposed system can greatly improve the travel experience for tourists in unfamiliar cities."
387,Group Key Agreement Protocol (GKAP) is a cryptographic mechanism where members of a group agree on a common key by sharing their blinded keys over a public channel. Sharing a key over a public channel is a security threat and expensive in terms of communication cost. We propose the GKAP based on tree and elliptic curve.,"Group Key Agreement Protocol (GKAP) is a cryptographic mechanism where members of a group agree on a common key. Sharing a key over a public channel is a security threat and expensive in terms of communication cost. In this paper, we proposed the GKAP based on tree and elliptic curve. The proposed approach is safe against passive attack, collaborative attack, forward secrecy, backward secrecy, and man-in-themiddle attack."
388,People tend to read multiple news articles on a topic since a single article may not contain all important information. Text Summarization is a way of minimizing a textual document to a meaningful summary. The ROUGE metric is used to evaluate the performance of summarization.,"Summarization is a way of minimizing a textual document to a meaningful summary. In this research, an extractive-based approach is used to generate a two-level summary from online news articles. News topics covered include politics, sports health, science and movie reviews."
389,ICD-9 codes are generally extracted by trained human coders by reading all artifacts available in a patient's medical record following specific coding guidelines. This paper proposes an unsupervised ensemble approach to automatically extract ICD-9 diagnosis codes from textual narratives included in electronic medical records. We extract ICD-9 codes from EMRs of 1000 inpatient visits at the University of Kentucky Medical Center.,Diagnosis codes are extracted from medical records for billing and reimbursement and for secondary uses. ICD-9 codes are generally extracted by trained human coders by reading all artifacts available in a patient's medical record following specific coding guidelines. This paper proposes an unsupervised ensemble approach to automatically extract diagnosis codes from EMRs.
390,"this paper aims at raising awareness on the issue of using unfixed vulnerabilities for targeted attacks. We demonstrate an attack by using a well-known, yet not fixed whatsapp vulnerability, enabling us to eavesdrop the cell-phone number of a victim. Social phishing can be used to retrieve further private or even corporate information.","This paper aims at raising awareness on the issue of using unfixed vulnerabilities for targeted attacks. We demonstrate an attack by using a well-known, yet not fixed whatsapp vulnerability. Once the victim trusts the adversary, social phishing can be used to retrieve further private or even corporate information."
391,"Heart disease is the leading cause of death in the world over the past 10 years. Most research has applied J4.8 Decision Tree, based on Gain Ratio and binary discretization. Gini Index and Information Gain are two other successful types of Decision Trees that are less used in the diagnosis of heart disease.",Heart disease is the leading cause of death in the world over the past 10 years. Researchers have been using data mining techniques to help health care professionals in the diagnosis of heart disease. This research investigates applying a range of techniques to different types of Decision Trees seeking better performance in heart disease diagnosis.
392,Automatic text summarization is an essential tool to overcome the problem of information overload. We propose in this paper a hybrid method to generate an extractive summary of Arabic documents. Experimental results on EASC dataset show that our proposed approach outperforms some of existing Arabic summarization systems.,Arabic text summarization is faced with two main issues: how to extract semantic rela? tionships between textual units and deal with redundancy. Authors propose a hybrid method to generate an extractive summary of Arabic documents. Their approach is based on a two-dimensional undirected and weighted graph with sentences as nodes.
393,"detection is based on model vectors that represent image composition in terms of region types. The latter deals with two aspects, namely high-level concepts and region types of the thesaurus.","In this paper we investigate detection of high-level concepts in multimedia content. We use an integrated approach of visual thesaurus analysis and visual context exploitation. A set of algorithms is presented, which modify either the confidence values of detected concepts or the model vectors."
394,Intelligent transportation systems (ITS) provide a set of standards for vehicular communications. Vehicle to Vehicle (V2V) and Vehicle to Infrastructure (V2I) communications are the main research goals of ITS. This paper reviews some popular architectures of VANETs.,Intelligent transportation systems (ITS) provide a set of standards for vehicular communications. V2V and V2I communications are the main research goals of ITS. This paper reviews some popular architectures of VANETs (Vehicular Ad hoc NETworks)
395,Vehicular Ad Hoc Networks (VANETs) deal with transferring data between moving vehicles. Different attacks could take place within the communication scenario; the most harmful of them is Sybil attack.,"Vehicular Ad Hoc Networks (VANETs) are networks that deal with transferring data between moving vehicles. Like all other networks it is subjects to vulnerable attacks, hence, security is a hot topic to consider. Different attacks could take place within the communication scenario; the most harmful"
396,"main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points.","In man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points. The algorithm has been tested in a variety of indoors and outdoors scenes and its efficiency and automation makes it amenable for implementation on robotic platforms."
397,"Message broadcasting in an open access system, such as VANET, is the main and utmost challenging problem with regard to security and privacy. None has considered overall privacy requirements such as unobservability. We propose a VANET based privacy-preserving communication scheme (VPPCS) which meets the requirements for content and contextual privacy.","Message broadcasting in an open access system such as VANET is the main and utmost challenging problem with regard to security and privacy, say authors. They propose a VANet based privacy-preserving communication scheme (VPPCS) that meets the requirements for content and contextual privacy. They say the scheme is secure and impervious to various types of attacks."
398,most systems use only one stage of feature extraction in which the filters are hard-wired. We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one.,"Most systems use one stage of feature extraction in which the filters are hard-wired. Using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages ofFeature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101."
399,This paper presents an improved and practical approach to automatically summarizing document by extracting the most relevant sentences from original document. Experimental results show that our approach compares favourably to a commercial text summarizer.,This technique is proposed based upon Key Sentences using statistical method and WordNet. Experimental results show that our approach compares favourably to
